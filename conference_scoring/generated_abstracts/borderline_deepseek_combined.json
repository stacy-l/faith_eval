[
  {
    "title": "AdaCorr: Adaptive Correlation-Based Learning Rate Scheduling for Stochastic Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose AdaCorr, an optimization algorithm that adaptively schedules learning rates by tracking gradient correlations across mini-batches. Unlike conventional schedulers that depend on validation loss, AdaCorr uses an inexpensive online estimate of gradient alignment to modulate step sizes, aiming to accelerate convergence when gradients remain consistent and decelerate when they diverge. Our method requires minimal extra computation\u2014only a running average of inner products between successive gradients\u2014yet yields modest but consistent improvements across CIFAR-10, ImageNet, and Penn Treebank training pipelines. In extensive experiments with ResNet-18 and Transformer-small models, AdaCorr reduces training epochs by 8\u201312% relative to cosine annealing, without tuning additional hyper-parameters beyond the base learning rate. Ablation studies show that the correlation window length and momentum coefficient are robust across tasks within a small range. However, gains diminish on very large batch training (\u22654k) and on noisy-label datasets, where gradient correlation becomes less reliable. While AdaCorr does not outperform task-specific finely tuned schedules, it offers a general, parameter-efficient alternative that is competitive out-of-the-box. The simplicity and public PyTorch implementation encourage wide adoption, although deeper theoretical justification and broader benchmarks remain future work.",
    "id": 4
  },
  {
    "title": "Revisiting Momentum in Adam-Style Optimizers: A Spectral Perspective",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Rodriguez, M."
    ],
    "abstract": "Momentum-based adaptive optimizers like Adam dominate deep-learning training, yet the community lacks consensus on why heavy-ball momentum ($\\beta_1$) accelerates convergence. We hypothesize that the benefit is tied to conditioning of the *pre-conditioned* Hessian rather than the raw Hessian. By analyzing random-feature linear models and shallow CNNs, we show that momentum suppresses outlier eigenvalues of $\\mathrm{diag}({\\bf G})^{-1/2} H \\,\\mathrm{diag}({\\bf G})^{-1/2}$, yielding up to 1.6\u00d7 speed-up on CIFAR-10. However, on language and speech tasks the effect vanishes, suggesting the spectral explanation is partial at best. Empirically, we find that turning off $\\beta_1$ for the last 20\\% of epochs matches full-momentum baseline accuracy while halving the sharpness of minima, hinting at a simple trick for practitioners. While our theoretical results are limited to quadratic objectives and small models, we argue the spectral lens offers a complementary view to the ubiquitous \"noise-cancellation\" narrative. Code and Jupyter notebooks are released to encourage scrutiny.",
    "id": 14
  },
  {
    "title": "Improved Adam with Layer-wise Learning Rates for Fine-Tuning Large Language Models",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Rodriguez, S."
    ],
    "abstract": "We propose Layer-wise Adaptive Moments optimizer for Fine-tuning (LAMF), a simple modification to Adam that assigns independent learning rates to each transformer layer during supervised fine-tuning of pre-trained language models. Motivated by the observation that gradient statistics vary across layers, LAMF maintains separate exponential moving averages of for each layer, allowing more aggressive updates in upper layers while stabilizing early layers that exhibit larger gradient variance. On the GLUE benchmark, LAMF achieves 0.8% average improvement over Adam with default hyperparameters when fine-tuning RoBERTa-base, and 0.5% gains on RoBERTa-large. The method introduces negligible memory overhead and requires no additional hyperparameter tuning beyond standard Adam. While improvements are consistent across most tasks, gains are marginal on larger models and ablations reveal that benefits diminish with careful learning rate scheduling. Theoretically, we provide convergence guarantees for a simplified version of LAMF in the convex setting. Though incremental, our results suggest that layer-wise adaptive methods may offer a practical alternative to extensive hyperparameter search during fine-tuning, particularly for practitioners with limited computational resources.",
    "id": 22
  },
  {
    "title": "Revisiting Dropout for Uncertainty Estimation: A Kernel Perspective",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Adams, P."
    ],
    "abstract": "Dropout is widely used both as a regularizer and as an inexpensive way to obtain model uncertainty, yet the relationship between these two roles remains poorly understood. We reinterpret dropout as an implicit kernel approximation that trades representational capacity for regularization. By deriving the corresponding kernel in closed form for single-hidden-layer networks, we show that standard Monte-Carlo dropout corresponds to a biased estimator of model variance. We propose a simple correction term that reduces this bias without additional forward passes. Empirically, on small-scale regression benchmarks (UCI and toy datasets), our adjusted dropout yields better-calibrated confidence intervals while maintaining the same computational cost. Theoretical results are confined to shallow networks, but ablations suggest the correction continues to help deeper architectures. While our contribution is incremental, it clarifies an implicit assumption in a common approximation and offers a drop-in refinement for uncertainty estimation in resource-constrained settings.",
    "id": 36
  },
  {
    "title": "Improved Gradient Noise Scaling for Large-Batch Training with a Modified Learning-Rate Warmup Schedule",
    "authors": [
      "Jiang, K.",
      "Okafor, C.",
      "M\u00fcller, H."
    ],
    "abstract": "Large-batch stochastic optimization can speed up neural-network training but often leads to accuracy loss. Existing noise-scaling rules adjust learning rates by assuming gradient noise is isotropic; however, empirical evidence shows that this assumption degrades as batch size grows. We propose Coordinated Noise Scaling (CNS), a simple modification that incorporates layer-wise gradient covariances estimated from the first handful of training steps. Coupled with a piecewise-linear warmup schedule that slows the initial learning-rate ramp, CNS preserves the total number of optimizer updates while yielding consistent loss trajectories across batch sizes. On ImageNet with ResNet-50, scaling the batch size from 256 to 4,096, CNS recovers 0.62% top-1 accuracy compared to standard linear scaling and matches small-batch performance after 90 epochs. Experiments on IWSLT14 German-English with the Transformer small model show similar, albeit smaller, gains (0.3 BLEU). We also derive a heuristic bound relating the number of covariance samples to the batch-size multiplier, offering limited theoretical support. Despite gains on canonical benchmarks, we observe negligible improvements when training longer or with stronger regularization. Our PyTorch implementation, requiring fewer than 30 lines of code, is included in the supplementary material. Overall, CNS is a lightweight extension to existing large-batch recipes that can modestly improve accuracy but appears less beneficial in regimes where stronger data augmentation or extended training schedules are employed.",
    "id": 42
  },
  {
    "title": "Revisiting Reset Schedules for Policy Optimization: When Warm Starts Meet Periodic Restarts",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Policy restart strategies are commonly used in deep reinforcement learning to escape local optima, yet the theoretical underpinnings of when and how to restart remain poorly understood. We revisit the classic optimization idea of warm-started periodic restarts and adapt it to policy gradient methods. Our method, WR-PO, periodically resets the policy to a geometric mixture of the current iterate and an earlier checkpoint, with restart frequency and mixing coefficient chosen via a simple grid-search heuristic. On a suite of nine continuous-control tasks, WR-PO improves average return over the vanilla PPO baseline by 4.7% with similar sample complexity, and matches SAC on four of nine tasks. Ablation studies indicate that the benefit is largest in environments with sparse rewards, suggesting that controlled resets help exploration. While our approach is easy to implement and yields consistent gains, the improvements are incremental and the heuristic nature of the schedule limits generality. We provide partial convergence guarantees under strong convexity assumptions, but the general case remains open. Code and 50 random seeds are provided to ensure reproducibility.",
    "id": 48
  },
  {
    "title": "Improved Gradient Alignment for Meta-Learning with Noisy Labels",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Meta-learning algorithms often assume clean training data, but real-world tasks frequently contain label noise that degrades few-shot performance. We study the interaction between gradient-based meta-learning and noisy labels, revealing that corrupted support examples disproportionately bias task-specific adaptation. To address this, we propose Gradient Alignment with Noise Suppression (GAINS), a simple plug-in that re-weights support gradients according to their agreement with a small trusted clean set. On two benchmarks, GAINS improves 5-way 1-shot accuracy by 2.3% under 20% symmetric noise and by 1.8% under more realistic asymmetric noise, while adding negligible overhead. Ablations show the clean set can be as small as 5% of the support size. Although gains are incremental and experiments are limited to sinusoidal regression and mini-ImageNet classification, our work highlights an under-explored robustness issue in meta-learning and provides a lightweight mitigation strategy. Code will be made available.",
    "id": 50
  },
  {
    "title": "Momentum-Scheduled Sharpness-Aware Minimization for Better Generalization",
    "authors": [
      "Kumar, S.",
      "Nguyen, T.",
      "Garcia, L."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising optimization technique for improving generalization by seeking flat minima. However, we observe that the standard SAM update rule can be overly conservative early in training and insufficiently adaptive later on. We propose Momentum-Scheduled SAM (MS-SAM), which dynamically adjusts the perturbation radius based on the momentum history of gradients. Our key idea is to schedule the perturbation magnitude according to the stability of the optimization trajectory, allowing for more aggressive exploration initially and finer adjustments as training progresses. We provide theoretical analysis showing that MS-SAM converges at the same rate as SAM under standard assumptions. Empirically, we evaluate MS-SAM on CIFAR-10, CIFAR-100, and ImageNet with ResNet and Vision Transformer architectures, achieving 0.3-0.7% improvements over SAM. While these gains are consistent, they remain relatively modest. Additionally, our method introduces two new hyperparameters that require careful tuning. Our code is available at [anonymous link].",
    "id": 53
  },
  {
    "title": "Gradient Amplification for Stabilizing GAN Training with Uneven Learning Rates",
    "authors": [
      "Garcia, M.",
      "Kumar, S.",
      "Thompson, L."
    ],
    "abstract": "We propose gradient amplification, a simple plug-in technique that re-weights generator and discriminator updates in GANs when the two networks are trained with different learning rates\u2014a common practical heuristic that often leads to unstable oscillations. Starting from a local bilinear game approximation, we derive a closed-form coefficient that amplifies generator gradients when the discriminator learns faster and shrinks them in the opposite regime. On CIFAR-10, gradient amplification improves Inception score from 6.18 \u00b1 0.12 to 6.43 \u00b1 0.07 without architectural changes, while on the more challenging 128\u00d7128 ImageNet subset it reduces training FID by 8.2%. Ab but only 56% of runs still diverge, suggesting that higher-order dynamics remain unaccounted for. The method introduces one extra hyper-parameter \u03b2; while \u03b2 = 1/2 works well in all experiments, we lack a principled automatic tuning rule. Theoretical analysis is limited to a two-parameter quadratic game and does not extend to general non-convex settings. Despite these limitations, gradient amplification can be implemented in 5 lines of code and integrates seamlessly with existing optimizers, offering practitioners a lightweight stabilizer for large-scale GAN training.",
    "id": 56
  },
  {
    "title": "Rethinking Batch Normalization: A Gradient-Norm Perspective for Improved Optimization",
    "authors": [
      "Kumar, A.",
      "Jiang, S.",
      "Bennett, K."
    ],
    "abstract": "Batch Normalization (BN) has become a staple in deep learning, yet its precise mechanism for accelerating optimization remains debated. We propose a new perspective that explains BN's benefits through gradient norm equalization across layers. By analyzing the spectral properties of the Jacobian, we show that BN implicitly balances gradient magnitudes without requiring careful initialization. We introduce Layer-Adaptive Batch Normalization (LABN), a lightweight modification that adapts the normalization strength based on gradient statistics. On CIFAR-100 and ImageNet, LABN achieves modest improvements of 0.3-0.7% over standard BN while reducing training time by 5-10%. However, the gains diminish on very deep architectures like ResNet-152, suggesting our method works best for moderately deep networks. We also explore LABN's interaction with different optimizers, finding it particularly effective with SGD but showing limited benefits with Adam. While our theoretical analysis provides new insights into BN's role in optimization, it relies on simplifying assumptions that may not hold in practice. Code and pretrained models are available.",
    "id": 63
  },
  {
    "title": "Revisiting Momentum in Adam-Style Optimizers Through a Learning-Rate Curvature Lens",
    "authors": [
      "Dubois, M.",
      "Chatterjee, P.",
      "Garcia, R."
    ],
    "abstract": "Adaptive optimizers such as Adam and RMSProp dominate deep-learning practice, yet their second-order moment estimation combined with heavy-ball momentum remains poorly understood. We observe that the effective learning-rate schedule of Adam depends on the local curvature of the loss landscape and propose Curvature-Aware Momentum (CAM), a lightweight plug-in that reweights momentum updates by the empirical inverse curvature trace. On CIFAR-10/100 and ImageNet-1k, CAM yields marginal top-1 gains of 0.3\u20130.5 pp over vanilla AdamW while reducing training time by  \u22487%. Further ablations show that CAM's benefit is strongest in low-batch or high-LR regimes, where curvature noise is elevated. A regret bound for smooth non-convex objectives suggests CAM converges in O(\u03b5\u22122) iterations, matching Adam, but our proof requires a bounded curvature ratio assumption that is hard to verify in practice. Although preliminary, CAM highlights an under-explored interplay between curvature, momentum, and adaptive learning-rate schedules, offering a simple drop-in that can complement existing optimizers. Code and learning-rate curvature visualizations are provided at anonymous-url.",
    "id": 64
  },
  {
    "title": "Revisiting Weight Averaging with Cyclic Learning Rates for Better Generalization",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Goldman, J."
    ],
    "abstract": "Weight averaging techniques such as stochastic weight averaging (SWA) and exponential moving average (EMA) have become standard tools for improving generalization in deep learning. While these methods typically operate on weights collected near the end of training, we investigate whether averaging weights from earlier stages\u2014when learning rates are still high\u2014can yield comparable or better performance. We propose Cyclic Weight Averaging (CWA), a simple extension that collects snapshots during cyclic learning-rate schedules and averages them with a trainable convex combination. On CIFAR-10, CIFAR-100, and ImageNette, CWA improves top-1 accuracy by 0.4\u20130.9% over SWA without extra hyper-parameters, and by 0.2\u20130.5% over EMA while using 30% fewer parameters. Theoretical analysis in a two-layer linear network suggests that early-cycle averages can lie in flatter minima, supporting our empirical observations. Although gains are consistent, they are incremental and diminish on larger models; on ImageNet we observe only 0.1% improvement. Code is available at anonymized link.",
    "id": 70
  },
  {
    "title": "Revisiting Dropout Schedules: A Frequency-Domain Perspective on Regularization Trade-offs",
    "authors": [
      "Kumar, S.",
      "Nguyen, T.",
      "Anderson, J."
    ],
    "abstract": "Dropout schedules\u2014deterministic annealing paths that interpolate between dropout rates of 0 and 1\u2014were introduced as a theoretically principled alternative to standard dropout, yet reported gains on small vision benchmarks have proven hard to reproduce at scale. We re-examine these schedules through the lens of Fourier analysis, arguing that the implicit regularization strength can be quantified by the energy retained in high-frequency components of mini-batch gradients. Under this view, existing monotone schedules correspond to low-pass filters that may over-regularize when label noise is limited. We propose SpectralDrop, a simple cosine-modulated schedule whose cutoff adapts to the observed signal-to-noise ratio. On CIFAR-10/100 we obtain 0.6\u20130.9% accuracy improvements over both standard dropout and the original dropout schedule with no additional hyper-parameters, while on ImageNet the same procedure yields only 0.2% top-1 gain and increases training time by 12%. Theoretical analysis shows the method minimizes a tighter PAC-Bayesian bound under data-dependent priors, but the bound relies on a variance term that is unidentifiable without distributional assumptions. Code and checkpoints are provided.",
    "id": 93
  },
  {
    "title": "Improving Generalization in Meta-Learning with Task-Agnostic Noise Injection",
    "authors": [
      "Nguyen, T.",
      "Kowalski, J.",
      "Singh, A."
    ],
    "abstract": "Recent gradient-based meta-learning algorithms have shown strong performance in few-shot learning, yet they remain vulnerable to overfitting on task-specific noise. We propose Task-Agnostic Noise Injection (TANI), a simple regularizer that adds calibrated isotropic Gaussian noise to inner-loop gradients before the meta-parameter update. Unlike prior noise-based regularizers, TANI requires no knowledge of task semantics and adds negligible computational overhead. Across five few-shot image classification benchmarks, TANI yields 1.3%\u20132.8% absolute gains in 5-shot accuracy over MAML with identical architectures and training budgets, while also lowering meta-gradient variance. Ablation studies show that TANI\u2019s benefit persists when the outer-loop optimizer is Adam or proximal regularization is added, but vanishes when the inner-loop learning rate is above 0.08. Theoretical analysis reveals that TANI implicitly constrains the Lipschitz constant of the meta-loss surface, providing a loose bound on generalization error that grows as O(\u221aK/\u221aN) where K is the number of inner steps and N is the number of meta-training tasks. Our results suggest that lightweight, task-agnostic perturbations can improve meta-generalization; however, gains are incremental and diminish on larger backbones or when the base algorithm already employs second-order MAML variants.",
    "id": 98
  },
  {
    "title": "Revisiting Dropout Through the Lens of Adaptive Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Dropout remains one of the most widely used regularization techniques in deep learning, yet its theoretical understanding is still incomplete. We propose Adaptive Dropout (AD), a simple modification that adjusts dropout rates based on the magnitude of activations during training. Our key insight is that neurons with larger activations contribute more to the network's output variance and thus should be dropped with higher probability. We derive the regularization effect of AD and show it approximates a data-adaptive form of L2 regularization, with stronger regularization on features with higher variance. Experiments on CIFAR-10, CIFAR-100, and ImageNet show AD achieves 0.2-0.8% accuracy improvements over standard dropout on ResNet-18 and VGG-16 architectures. However, we find these gains diminish on larger models and are task-dependent. While our theoretical analysis provides some insight into when AD might help, the assumptions required limit its generality. We provide an efficient implementation with minimal computational overhead. Though the improvements are modest, our work suggests that re-examining classical techniques through modern theoretical tools can yield practical benefits and highlights the importance of adaptive regularization in deep learning.",
    "id": 101
  },
  {
    "title": "Revisiting Entropy-Regularized Policy Optimization with Mirror Extragradient Updates",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Nguyen, T.",
      "White, S."
    ],
    "abstract": "Entropy-regularized reinforcement learning (RL) algorithms have shown strong empirical success, yet their convergence behavior remains poorly understood when combined with modern policy parameterizations. We revisit the classical entropy-regularized policy optimization framework and introduce a simple modification using mirror extragradient updates that improves stability without sacrificing convergence guarantees. Our method, called MEPPO, applies a predictor-corrector scheme to the natural policy gradient direction, requiring only minor computational overhead. We prove that MEPPO converges to the optimal regularized policy at a O(1/T) rate for tabular MDPs, matching the best-known bounds while operating under weaker assumptions about the behavior policy. In deep RL experiments on continuous control tasks, MEPPO demonstrates modest improvements over PPO and SAC on 6 out of 12 MuJoCo environments, though gains diminish with larger network architectures. While our theoretical results are limited to tabular settings and our empirical evaluation shows variance across seeds, this work suggests extragradient methods may offer a practical path toward more stable policy optimization. Code is available at anonymous-url.github.io.",
    "id": 102
  },
  {
    "title": "LoRa-C: A Lipschitz-Regularized Curriculum for Improving Generalization in Reinforcement Learning",
    "authors": [
      "Kumar, V.",
      "Chen, J.",
      "Nguyen, T."
    ],
    "abstract": "Deep reinforcement learning agents often struggle with unstable training and poor generalization to unseen environments. We propose LoRa-C, a lightweight curriculum-based method that applies a learned Lipschitz regularizer to policy updates, encouraging smoother value functions without heavy computational overhead. Our approach alternates between phases of aggressive policy improvement and conservative regularization, guided by a simple schedule derived from the agent\u2019s average TD-error. Across five continuous-control tasks in MuJoCo, LoRa-C achieves a 7\u201312% improvement in average return over standard PPO when transferring to environments with perturbed dynamics, while introducing only a 3% increase in wall-clock training time. Ablation studies indicate that the regularizer alone contributes roughly half the gain, with the curriculum schedule providing the remainder. Although our method is limited to low-dimensional state spaces and does not consistently outperform strong baselines such as SAC or DrQ on the original training domains, it offers a computationally cheap way to boost robustness for practitioners who face domain-shift at deployment. Our code and hyper-parameter configurations are publicly available.",
    "id": 106
  },
  {
    "title": "Revisiting Entropy Regularization with Amortized Tempering for Improved Off-Policy Reinforcement Learning",
    "authors": [
      "Kumar, A.",
      "Nguyen, T.",
      "Robinson, S."
    ],
    "abstract": "Entropy regularization is widely used to encourage exploration in reinforcement learning, but choosing the right temperature parameter remains challenging. We propose Amortized Tempering for Entropy Regularization (ATER), a simple method that adaptively adjusts the temperature parameter during training through a learned amortization network. Our approach maintains a distribution over temperatures and updates it based on policy improvement signals, eliminating the need for manual tuning. We evaluate ATER on continuous control tasks from the DeepMind Control Suite and find that it achieves competitive performance with the best fixed temperature in hindsight on 6 out of 10 environments, while requiring 30% fewer hyperparameter sweeps. Additionally, we provide theoretical analysis showing that ATER converges to a near-optimal temperature under certain regularity conditions. While our method shows promise for reducing hyperparameter sensitivity, we observe that the performance gains are modest compared to recent adaptive regularization techniques, and the computational overhead may not justify deployment in all scenarios. Our code is available at [anonymous link].",
    "id": 111
  },
  {
    "title": "Revisiting Dropout for Uncertainty Estimation in Small-Scale Neural Networks",
    "authors": [
      "Chen, L.",
      "Vasquez, J.",
      "Kumar, A."
    ],
    "abstract": "Bayesian neural networks promise calibrated uncertainty but remain prohibitively expensive for modest-sized applications. We reconsider standard dropout training as a lightweight alternative and ask whether simple post-hoc corrections can turn arbitrary networks into reliable uncertainty estimators. Starting with the observation that usual Monte-Carlo dropout severely under-estimates epistemic variance, we derive a data-dependent scaling term that re-calibrates predictive variance without model changes. On UCI regression benchmarks our procedure yields 5-15% lower NLL than vanilla dropout and is competitive with Deep Ensembles while using a single model. However, calibration gains vanish on high-dimensional inputs (CIFAR-10), and the method still lags behind gold-standard Hamiltonian Monte-Carlo. Theoretical analysis shows the proposed adjustment is equivalent to a misspecified prior, explaining limited robustness under covariate shift. Code and trained weights are provided, but the calibration procedure requires held-out data, raising fairness concerns in production regimes. Our results suggest semi-Bayesian tricks can trade a small accuracy drop for moderate uncertainty gains, yet fall short of full Bayesian validity.",
    "id": 115
  },
  {
    "title": "Improving Transformer Generalization with Frequency-Sensitive Positional Encodings",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "While transformers have shown impressive performance across sequence modeling tasks, they often struggle to generalize to sequences longer than those seen during training. We hypothesize that standard positional encodings fail to capture frequency-based patterns that are crucial for length extrapolation. To address this, we introduce Frequency-Sensitive Positional Encodings (FSPE), which incorporate learnable sinusoidal components that adapt to the frequency content of the training data. Our method augments existing relative positional encodings by learning optimal frequency parameters through a lightweight auxiliary objective that predicts local sequence periodicities. We evaluate FSPE on synthetic algorithmic tasks, language modeling, and image sequence prediction. Results show modest improvements: 8-12% better length extrapolation on algorithmic tasks and 1.5-2.1% lower perplexity when generalizing beyond training lengths. Ablation studies reveal that the frequency learning component contributes most benefits for sequences with periodic structure, while providing minimal gains on natural language data. While our approach offers a simple plug-in enhancement for existing transformer architectures, the improvements are task-specific and require additional hyperparameter tuning. These mixed results suggest that frequency-aware positional representations may help particular extrapolation scenarios but are not a universal solution to transformer length generalization.",
    "id": 118
  },
  {
    "title": "Improving Transformer Generalization with Iterative Self-Distillation",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Chen, Y."
    ],
    "abstract": "While transformers have achieved strong performance across many domains, they often struggle with out-of-distribution generalization. We propose Iterative Self-Distillation (ISD), a simple technique that improves generalization by having the model teach itself through successive distillation steps. Our method works by periodically updating a teacher model as the exponential moving average of the student, then distilling knowledge from this teacher to refine the student's predictions. We evaluate ISD on image classification and natural language understanding tasks, showing 2-4% improvements in accuracy on corrupted test sets compared to standard training. Theoretical analysis suggests ISD acts as a form of regularization that encourages smoother decision boundaries. While our empirical results are encouraging, we acknowledge that the improvements are modest and the theoretical understanding remains incomplete. We hope this work sparks further investigation into self-distillation as a practical regularization technique for improving robustness in neural networks.",
    "id": 129
  },
  {
    "title": "Improved Generalization via Iterative Sample Reweighting",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Empirical risk minimization can over-fit training distributions that differ from deployment data. We propose Iterative Sample Reweighting (ISR), a simple algorithm that re-ights each mini-batch according to estimated generalization gaps accumulated from prior epochs. ISR alternates between learning a predictor and updating per-sample weights via a held-out validation loss, without requiring domain knowledge or extra models. On CIFAR-10-C and three text-classification benchmarks, ISR improves test accuracy by 0.8\u20131.4 pp over ERM and 0.2\u20130.5 pp over recent reweighting schemes, while adding <5% training time. Theoretically, we bound the expected risk under covariate shift by the weighted empirical risk plus a term that decays with effective sample size, offering limited but novel justification. Ablation studies reveal that gains diminish with heavy data augmentation and that hyper-parameter sensitivity is non-trivial. Code will be released upon acceptance.",
    "id": 136
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Small-Batch Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose AdaNoise, a simple modification to standard stochastic gradient descent that adaptively injects Gaussian noise during optimization. While previous work has shown noise injection can improve generalization, existing methods rely on fixed schedules or require additional hyper-parameters. AdaNoise estimates the gradient noise scale online and adjusts the injection variance proportionally. Our theoretical analysis shows this approach approximately preserves the stationary distribution of SGD while encouraging exploration in sharp minima. On CIFAR-10 and ImageNet, AdaNoise improves test accuracy by 0.5-1.2% compared to strong baselines when training ResNet-18 with batch sizes 32-128. However, benefits diminish with larger batches and deeper networks. Ablations reveal the method is sensitive to the noise estimation window size, and performance varies significantly across architectures. While AdaNoise provides consistent improvements for small-batch scenarios common in resource-constrained settings, its impact appears limited for standard large-batch training regimes. Code is available at anonymized-url.",
    "id": 165
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum Scheduling for Language Model Fine-Tuning",
    "authors": [
      "Kumar, S.",
      "Obinwa, C.",
      "Liu, H."
    ],
    "abstract": "Fine-tuning large language models (LLMs) often suffers from unstable gradients that hinder convergence and degrade downstream performance. We propose Adaptive Gradient Clipping with Momentum Scheduling (AGCMS), a simple extension to AdamW that rescales gradients based on a rolling percentile estimate and couples the clipping threshold to a cyclical momentum schedule. On the GLUE benchmark, AGCMS improves average scores by 0.7 % over AdamW while reducing gradient norm variance by 23 %. Ablation studies indicate that the momentum schedule accounts for most gains, whereas the clipping component primarily stabilizes early training. Although the method is straightforward to implement and requires negligible extra memory, we observe only marginal improvements on larger models (>7 B parameters) and limited transfer across domains. Theoretically, we provide a convex convergence guarantee that matches standard bounds up to a constant factor. Code and hyper-parameters are publicly available.",
    "id": 167
  },
  {
    "title": "LoRa-Flow: Lightweight Low-Rank Normalizing Flows for Fast Density Estimation",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Liu, Y."
    ],
    "abstract": "Normalizing flows provide flexible probabilistic models by learning invertible transformations, but their computational cost limits deployment on edge devices. We propose LoRa-Flow, which injects low-rank constraints into the weight matrices of coupling layers to reduce parameters without redesigning the network. On five UCI benchmarks, LoRa-Flow achieves 1.5\u00d7 speed-up and 60 % fewer parameters at the cost of <2 % bits-per-dimension increase versus unconstrained RealNVP. Theoretical analysis shows the low-rank bottleneck still preserves universal approximation in the affine coupling class. However, ablations reveal degradation on high-dimensional images, suggesting the low-rank prior is too restrictive for modalities with complex spatial correlations. Code is available.",
    "id": 173
  },
  {
    "title": "Improved Generalization via Parameter-Averaging with Cyclical Step Sizes",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Roberts, J."
    ],
    "abstract": "We propose Cyclical Weight Averaging (CWA), a simple variant of stochastic weight averaging that cycles learning-rate schedules and keeps an exponentially-decayed running mean of parameter checkpoints. By synchronizing the averaging window to learning-rate valleys, CWA yields flatter minima and marginally lower test error on CIFAR-10/100 and ImageNet without additional hyper-parameters. Theoretically, we bound the deviation between CWA iterate and (S)GD trajectory by O(\u03b7^{1/2}T^{3/4}), suggesting the average remains close to the optimization path. On ResNet-18 and WRN-28-10, CWA improves baseline generalization by 0.3\u20130.7% while adding <1% compute overhead, and is complementary to strong augmentations. However, gains diminish on larger models (EfficientNet-B3) and are insignificant when training already uses aggressive regularization. Compared to SWA, CWA converges 15% faster but yields similar final accuracy; hence the primary benefit is training speed rather than ultimate performance. We release code and checkpoints to facilitate future work.",
    "id": 179
  },
  {
    "title": "Gradient Alignment Improves Transfer Learning in Limited-Data Regimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Fine-tuning large pre-trained models has become the dominant approach for transfer learning, yet its performance degrades sharply when target data are scarce. We propose Gradient Alignment Transfer (GAT), a simple regularizer that encourages the fine-tuning gradients to align with the pre-trained feature directions. By penalizing gradient components orthogonal to the principal subspace of pre-training updates, GAT constrains optimization to parameters that matter most for the source domain. On 8 few-shot vision and NLP benchmarks, GAT yields 2\u20134% absolute gains over standard fine-tuning while adding <1% overhead. Theoretical analysis shows GAT is equivalent to a soft constraint on the distance between source and target feature covariances, clarifying when the method helps. Although the improvements are consistent, they are modest and diminish as data increase; above 5k examples per class, GAT matches vanilla fine-tuning. Our code, tuned for stability rather than speed, requires 30% more memory because of second-order moment tracking. Overall, GAT offers a lightweight, interpretable way to nudge transfer learning, but its narrow scope and incremental gains may limit broader impact.",
    "id": 182
  },
  {
    "title": "Layerwise Learning Rate Warmup for Transformers: A Simple but Effective Trick?",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Anderson, K."
    ],
    "abstract": "Training large transformer models often requires extensive hyperparameter tuning, particularly for learning rate schedules. We propose Layerwise Learning Rate Warmup (LLRW), a simple modification to standard warmup that applies increasing learning rates to different layers sequentially during early training epochs. Our method is motivated by the observation that lower layers in transformers tend to converge faster than higher layers when training on NLP tasks. We evaluate LLRW on BERT-base and GPT-2 small models across GLUE tasks and WikiText-103, showing 1.2-2.3% average improvements in downstream metrics compared to standard linear warmup, with minimal computational overhead. Theoretically, we provide convergence guarantees for two-layer linear networks under simplifying assumptions. However, our experiments are limited to relatively small models due to compute constraints, and the improvements, while consistent, are modest in magnitude. We also find that LLRW's benefits diminish with larger models and may interact negatively with advanced optimizers like AdamW. Our code is available at [anonymous link]. While LLRW offers a practical training trick for certain settings, its theoretical justification and broader applicability require further investigation.",
    "id": 184
  },
  {
    "title": "Improved Convergence of Gradient Descent via Learning-Rate Scheduling with Bounded Memory",
    "authors": [
      "Kovacs, L.",
      "Choudhury, A.",
      "Steiner, B."
    ],
    "abstract": "We revisit plain gradient descent for smooth, convex optimization and propose SimpleStep, a lightweight scheduling rule that uses only the last two iterates to adapt the learning rate online. Unlike line-search or adaptive methods, SimpleStep maintains O(1) memory and avoids additional gradient evaluations or function calls. Our key idea is to shrink the step size when successive iterates are poorly aligned with the gradient direction, a condition we show is sufficient to guarantee non-increasing potential in a Lyapunov sense. For L-smooth objectives we prove that SimpleStep achieves an O(1/\u221aT) convergence rate\u2014matching the worst-case lower bound\u2014while empirical traces on common logistic-regression benchmarks exhibit up to 30% faster reduction in gradient norm compared to tuned constant steps. Extending the schedule to stochastic training yields modest gains on CIFAR-10 with ResNet-20 when the baseline learning rate is slightly misspecified. Although our theoretical contribution is incremental and the method does not outperform carefully tuned Adam or cosine-annealing baselines, SimpleStep offers practitioners a drop-in replacement that requires no hyper-parameter grid search, making it attractive for resource-constrained environments. Code is provided for full reproducibility.",
    "id": 186
  },
  {
    "title": "LoRAMA: Low-Rank Adaptation with Memory Attention for Efficient LLM Fine-Tuning",
    "authors": [
      "Kumar, S.",
      "Jones, L.",
      "Nguyen, P."
    ],
    "abstract": "We propose LoRAMA, a that augments Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning of large language models by incorporating a lightweight memory attention mechanism. While LoRA has shown strong empirical results, its fixed low-rank decomposition may limit adaptation capacity for more complex downstream tasks. LoRAMA introduces a small memory bank of trainable vectors that are dynamically attended to during forward passes, allowing the low-rank matrices to vary in a task-dependent manner. Our method adds only 0.3M parameters beyond standard LoRA while enabling more expressive adaptation. We evaluate LoRAMA on instruction tuning and domain adaptation benchmarks using Llama-2 7B and 13B models. Results show modest improvements of 1-2% accuracy over LoRA baselines across most tasks, with some degradation on simpler datasets. Abprisingly, LoRAMA reduces training time by 12% compared to standard LoRA due to our memory attention implementation. Theoretical analysis suggests our method interpolates between LoRA and full fine-tuning, though convergence guarantees remain limited. ablation studies reveal performance is sensitive to memory bank size and initialization strategy. LoRAMA provides a simple yet effective extension to LoRA that may be particularly useful for practitioners seeking slightly improved adaptation capacity without substantial computational overhead.",
    "id": 193
  },
  {
    "title": "Improving Gradient Flow in Residual Networks with Learnable Skip-Scaling",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rodriguez, A."
    ],
    "abstract": "Deep residual networks have shown remarkable success in training very deep architectures, yet the choice of how skip connections are weighted remains largely heuristic. We propose a simple method for learning per-layer, data-independent scalar coefficients that dynamically rescale the residual branch during training. Our approach introduces only one additional parameter per residual block and is optimized jointly with the network weights via standard back-propagation. We demonstrate that learnable rescaling can accelerate convergence on CIFAR-10 and ImageNet by up to 18% in wall-clock time and yield modest accuracy gains of 0.3\u20130.5% over the original ResNet baseline. Although the technique generalizes across common vision benchmarks, gains diminish when strong regularization or modern architectural refinements are present. Extensive ablations indicate that improvements are most pronounced in mid-depth networks (20\u201350 layers), while deeper models benefit less. The method requires no architectural redesign and is easily integrated into existing frameworks. Our code and trained models are publicly available.",
    "id": 199
  },
  {
    "title": "Re-eval: Improving Model Reuse via Task-Agnostic Weight Re-initialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "Transfer learning typically begins from weights trained on ImageNet or large language corpora, but little guidance exists on which layers should be kept, fine-tuned, or randomly re-initialized for a new task. We propose Re-eval, a simple heuristic that re-initializes the final half of network parameters before fine-tuning. Across 12 vision and 8 NLP benchmarks, Re-eval yields an average 1.3% improvement over standard fine-tuning while reducing trainable parameter count. Theoretically, we show that layerwise re-initialization can be viewed as an implicit form of curriculum learning. Although the gains are consistent, they rarely exceed 2%, and performance drops when the source and target tasks are dissimilar. Code and scripts are publicly available.",
    "id": 206
  },
  {
    "title": "Self-Adjusting Step-Size Schedules for Stochastic Gradient Descent: A Kernel Regression Approach",
    "authors": [
      "Lee, J.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "While carefully tuned step-size schedules significantly improve SGD performance, existing adaptive methods rely on heuristics with limited theoretical backing. We propose KISS, a kernel regression approach that predicts near-optimal step sizes from the recent gradient history. On convex problems, KISS achieves a 1.9\u00d7 speed-up over tuned polynomial decay schedules and matches Adam's final accuracy without momentum or second-order statistics. The method requires only one extra hyper-parameter (window size) and adds O(w) memory and compute per iteration. Experiments on CIFAR-10/100 and WikiText-2 show consistent gains, but improvements diminish on very large-batch or Transformer training. Theoretically, we bound expected progress for quadratics; however, general convex and non-convex analyses remain incomplete. Code and tuned schedules are provided to ensure reproducibility.",
    "id": 214
  },
  {
    "title": "Adaptive Gradient Rescaling for Improved Training Stability in Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Training stability remains a critical challenge in deep learning, particularly for architectures with varying layer scales and activation magnitudes. We propose Adaptive Gradient Rescaling (AGR), a simple yet effective method that dynamically adjusts gradient magnitudes during training based on layer-wise gradient statistics. Unlike existing adaptive optimizers that modify learning rates through momentum-based estimates, AGR directly rescales gradients using a running estimate of gradient norms, normalized by the exponential moving average of historical gradient magnitudes across layers. Our method requires minimal hyperparameter tuning and can be seamlessly integrated with standard optimizers like SGD and Adam. We evaluate AGR on CIFAR-10/100 and ImageNet classification tasks, as well as on a variety of architectures including ResNet, Vision Transformer, and deep MLPs. Results show that AGR reduces training loss oscillations by 15-20% compared to vanilla optimizers and leads to marginally improved final accuracy (0.3-0.5% on average). While the performance gains are modest, AGR demonstrates consistent improvements across settings without introducing significant computational overhead. Theoretical analysis suggests AGR implicitly bounds the gradient Lipschitz constant, potentially explaining its stabilizing effect. Code and experiments will be made publicly available.",
    "id": 228
  },
  {
    "title": "Lookahead Learning: A Simple Momentum-Based Correction to SGD with Polyak Step-Size",
    "authors": [
      "Kwon, S.",
      "Rodriguez, A.",
      "Nguyen, T."
    ],
    "abstract": "We propose Lookahead-Learning, a lightweight wrapper that augments stochastic gradient descent with Polyak step-size (SGD-PS) using a momentum-guided correction. While SGD-PS adaptively chooses step-size via function sub-optimality, its practical performance degrades when the estimated optimal value is misspecified. Our method maintains two sets of weights: a fast \u2018inner\u2019 SGD-PS iterate and a slow \u2018outer\u2019 iterate updated by an exponential moving average. At every k steps the outer weights are replaced by the inner ones, effectively injecting a Polyak-calibrated momentum. On convex problems Lookahead-Learning inherits the O(1/t) rate of SGD-PS without requiring knowledge of the optimal value; on over-parameterized neural networks it yields 5\u201312 % faster convergence in wall-clock time compared to vanilla SGD-PS on CIFAR-10/100 and ImageNette. The wrapper adds two lines of code and introduces a single extra hyper-parameter, the synchronization period k, which we show is stable across tasks when set between 5 and 10. Theoretically, we prove that the method preserves convergence under standard smoothness assumptions, but the analysis does not demonstrate strict improvement over SGD-PS. Although the empirical gains are consistent, they are modest and appear limited to small- and medium-scale vision benchmarks. We release PyTorch code to encourage reproducibility.",
    "id": 232
  },
  {
    "title": "Improved Generalization via Gradient Norm Clipping with Momentum-Based Adaptive Thresholds",
    "authors": [
      "Liu, M.",
      "Kumar, S.",
      "Anderson, B."
    ],
    "abstract": "Deep neural networks often achieve low training error but struggle with generalization, particularly in small-data regimes. Motivated by empirical observations that the distribution of gradient norms correlates with generalization performance, we propose Momentum-GNC, an enhancement to standard gradient norm clipping that adaptively sets clipping thresholds based on a running estimate of gradient norm statistics. While gradient clipping is traditionally used for training stability, our method exploits it as a regularizer by modulating the effective learning rate per iteration. We derive a PAC-Bayesian generalization bound that incorporates the proposed adaptive mechanism, showing theoretical improvement when gradient norms follow a sub-gaussian tail. On CIFAR-10/100 and ImageNet-1K, Momentum-GNC yields 0.8\u20131.2% top-1 accuracy gains for ResNet-50 and 0.5% for Vision Transformers relative to vanilla SGD with momentum, while converging 5\u201310% faster in wall-clock time. Nonetheless, gains diminish on larger datasets and heavily tuned baselines, limiting broad applicability. The method requires one additional hyper-parameter that interacts non-trivially with learning-rate schedules, complicating adoption. Code and checkpoints are provided for reproducibility. Overall, Momentum-GNC offers a lightweight modification that can slightly boost generalization and training speed, but its benefits may be incremental for practitioners already employing strong regularization protocols.",
    "id": 243
  },
  {
    "title": "Gradient Flow Regularization: A Light-Weight Technique for Improving Generalization in Neural Networks",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Tanaka, H."
    ],
    "abstract": "We propose Gradient Flow Regularization (GFR), a simple training augmentation that encourages parameters to follow flatter trajectories in loss landscapes. Motivated by the empirical observation that mini-batch gradients oscillate more in regions of high curvature, GFR adds a quadratic penalty on the difference between successive gradient estimates. Unlike prior curvature-smoothing schemes, GFR introduces no additional forward or backward passes and incurs <1% overhead. On CIFAR-10/100 and ImageNet, GFR improves the average test error of ResNet-18/50 and Vision-Transformer-Small by 0.6\u20131.2% compared to vanilla training and 0.2\u20130.4% compared to strong baselines such as SAM and ShakeDrop. Ablation studies confirm the effect is complementary to data augmentation and label smoothing. Theoretical analysis shows GFR upper-bounds the trace of the Hessian under standard assumptions, suggesting a connection to flat-minima regularization. While the gains are consistent, they are modest and appear largest on medium-scale architectures. Code and hyper-parameters will be provided for reproducibility.",
    "id": 253
  },
  {
    "title": "Variance-Reduced Q-Learning with Periodic Policy Updates",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Johnson, T."
    ],
    "abstract": "We propose VRQ-Update, a variance-reduced variant of Q-learning that periodically freezes target policies to stabilize off-policy updates. Our method maintains two Q-networks: a fast online network updated every step and a slowly evolving target network updated only when a variance threshold is exceeded. We prove convergence to a neighborhood of the optimal Q-function under standard assumptions, with convergence rate depending on the threshold parameter. On a suite of 6 Gym environments, VRQ-Update shows 12-18% faster convergence compared to standard Q-learning, though gains diminish with longer training. The method introduces two additional hyper-parameters that require environment-specific tuning. While our theoretical analysis assumes finite state spaces, we demonstrate empirical performance in continuous control tasks by combining with neural function approximation. Our approach provides a middle ground between fully offline and fully online target updates, potentially useful for applications with limited hyper-parameter tuning budgets.",
    "id": 275
  },
  {
    "title": "Revisiting Momentum Schedules for Practical SGD: Smooth Warmup Improves Both Stability and Final Accuracy",
    "authors": [
      "Morales, L.",
      "Chaudhary, A.",
      "Kim, S."
    ],
    "abstract": "Stochastic gradient descent with momentum remains the de-facto optimizer for large-scale deep learning, yet the community has not converged on a principled way to schedule the momentum coefficient during training. We propose SWARM, a Simple WArmup-inspired momentum Rescheduling Method that linearly increases momentum from 0.5 to 0.9 over the first five epochs while simultaneously decaying the learning rate. Our experiments on CIFAR-10/100, ImageNet-1k and a subset of GLUE tasks show that SWARM yields a 0.2\u20130.4 % absolute improvement over the standard constant-momentum baseline with no extra hyper-parameters and negligible overhead. Ablations reveal that smoothing the momentum transition reduces early-training gradient noise, which correlates with better generalization. While the gains are consistent across architectures, they diminish on problems where the baseline already trains for hundreds of epochs, suggesting limited scalability. Theoretical justification is limited: we bound the extra regret compared to constant momentum in convex quadratic settings, but the bound does not clearly predict the empirical improvements. Code is provided for reproducibility.",
    "id": 304
  },
  {
    "title": "Amortized Learning Rate Scheduling via Meta-Gradient Descent",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Chen, L."
    ],
    "abstract": "Learning rate scheduling is critical for training neural networks, yet most approaches rely on hand-tuned schedules or domain-specific heuristics. We propose Meta-LR, a gradient-based method to learn adaptive learning rate schedules for SGD without manual intervention. Our approach treats the learning rate as a function of training statistics, represented by a small neural network whose parameters are optimized through meta-gradients computed on a validation set. Unlike full-blown meta-learning methods, Meta-LR requires only a single meta-update per batch, making it computationally efficient. We evaluate on CIFAR-10/100 and ImageNet training, demonstrating 0.5-1.2% accuracy improvements over cosine decay baselines with similar compute budget. However, gains are inconsistent across architectures and datasets, with negative results on language modeling tasks. We provide theoretical analysis showing Meta-LR converges to stationary points under restricted assumptions. While not a universal solution, our method offers a practical compromise between manual tuning and expensive meta-learning approaches.",
    "id": 307
  },
  {
    "title": "Gradient Descent with Periodic Momentum Reset: A Simple Trick for Better Generalization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Thompson, K."
    ],
    "abstract": "We propose a simple modification to standard momentum-based gradient descent that periodically resets the momentum buffer to zero during training. While momentum acceleration is widely used to speed up optimization, we observe that it can accumulate overly aggressive updates that potentially harm final generalization. Our method, Periodic Momentum Reset (PMR), requires only a single hyperparameter - the reset frequency - and can be implemented in a few lines of code. Through experiments on CIFAR-10/100 and ImageNet, we show that PMR achieves comparable or slightly better test accuracy than standard SGD with momentum across ResNet and EfficientNet architectures, with particularly notable improvements (up to 1.2%) when training with small batch sizes or noisy labels. Theoretical analysis in a simplified quadratic setting suggests that momentum reset prevents overshooting in poorly conditioned directions, though we acknowledge this analysis does not fully capture the deep learning regime. While our approach shows consistent improvements in specific settings, the gains are modest and highly dependent on the reset schedule. We anticipate this work might be most useful as a practical trick for practitioners facing generalization challenges, rather than as a fundamental advance in optimization theory.",
    "id": 320
  },
  {
    "title": "Gradient Dropout: Improving Stochastic Optimization via Randomized Gradient Subsampling",
    "authors": [
      "Chen, Y.",
      "Kumar, V.",
      "Zhang, L.",
      "M\u00fcller, K."
    ],
    "abstract": "We propose Gradient Dropout, a simple modification to standard stochastic gradient descent that randomly drops out a subset of gradients during each optimization step. Inspired by dropout's success in preventing overfitting and signal propagation bottlenecks, we show that randomly discarding gradient components can lead to improved training dynamics and final generalization. Our method introduces a single hyperparameter controlling the fraction of gradients retained, which we show can be automatically tuned using a validation set. Through extensive experiments on CIFAR-10/100 and ImageNet with ResNet architectures, we demonstrate that Gradient Dropout achieves modest but consistent improvements over standard SGD (0.5-1.2% accuracy gains) with minimal computational overhead. While our theoretical analysis reveals some limitations\u2014particularly in convex settings where the method can degrade convergence\u2014we empirically validate its effectiveness in deep learning contexts. However, we acknowledge that the gains saturate with larger models and sophisticated optimizers like AdamW, suggesting the technique may be most valuable in resource-constrained scenarios. Code is available at [URL omitted for anonymity].",
    "id": 321
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient compression is essential for distributed training at scale, but existing methods rely on hand-crafted schedules that may not adapt to varying training dynamics. We propose Adaptive Quantized SGD (AQ-SGD), which learns compression parameters jointly with model parameters using a meta-learning approach. By framing quantization levels as continuous variables that are periodically projected onto discrete values, our method discovers instance-specific compression schedules without communicating additional metadata. Experiments on CIFAR-10 and ImageNet with ResNet-50 show AQ-SGD achieves 40\u00d7 gradient compression while maintaining within 1.2% of baseline accuracy, matching state-of-the-art static methods on most tasks. However, we observe instabilities on Transformer architectures and convergence issues when batch sizes are small, suggesting our current formulation may not capture all necessary dynamics. Theoretical analysis proves convergence to stationary points under bounded gradient assumptions, but requires stronger conditions than uncompressed SGD. Our method shows promise for adaptive distributed training, though practical deployment may require additional stabilization techniques for certain architectures.",
    "id": 322
  },
  {
    "title": "Gradient Amplification Through Layer-wise Learning Rate Modulation in Deep Networks",
    "authors": [
      "Smith, J.",
      "Chen, L.",
      "Park, K.",
      "Garcia, M."
    ],
    "abstract": "We propose Layer-wise Learning Rate Amplification (LLRA), a simple modification to standard SGD that applies different learning rate scaling factors to network layers based on their relative gradient magnitudes. Motivated by the observation that gradient norms vary significantly across layers in deep networks, LLRA amplifies updates for layers with smaller gradients while dampening those with larger gradients, using a theoretically-motivated ratio derived from the Lipschitz constants of each layer. Our method requires minimal hyperparameter tuning and can be implemented with a few lines of PyTorch code. Experiments on ResNet-50 and Vision Transformer architectures show modest improvements in convergence speed (8-12% reduction in training epochs) and final accuracy gains of 0.3-0.7% on ImageNet, CIFAR-10, and CIFAR-100. While these improvements are consistent across different architectures and datasets, we acknowledge they are relatively small compared to recent advances in optimizer design. Our theoretical analysis reveals that LLRA can be viewed as a diagonal preconditioner with convergence guarantees comparable to Adam, though with tighter bounds for certain network configurations. The method is most beneficial for very deep networks (>50 layers) and less effective for smaller models. Code will be made available upon publication.",
    "id": 330
  },
  {
    "title": "Improved Convergence Rates for SGD with Time-Varying Step Sizes via Loss-Dependent Bounds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We provide tighter convergence guarantees for stochastic gradient descent (SGD) when the loss function exhibits certain regularity properties. Motivated by the empirical success of cosine annealing schedules in deep learning, we derive non-asymptotic bounds for general time-varying step sizes that depend on the accumulated gradient noise rather than worst-case quantities. Our analysis combines traditional martingale techniques with a novel loss-dependent decomposition of the update rule, yielding rates that can be significantly better than the standard O(1/\u221aT) when the training loss decreases quickly. For overparameterized linear regression, we obtain an explicit O(1/T\u00b2) rate under mild assumptions on the data. While our theoretical results are restricted to convex losses, experiments on ResNet-18 training with CIFAR-10 demonstrate 5-10% faster convergence compared to standard schedules when using our theoretically-motivated decay parameters. However, our bounds become vacuous for highly non-convex settings typical in modern deep learning, and our theoretical contributions are incremental rather than transformative.",
    "id": 338
  },
  {
    "title": "Improved Convergence Rates for Stochastic Gradient Descent with Adaptive Polyak Step-Size",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We revisit the classical Polyak step-size for stochastic gradient descent (SGD) and propose a simple adaptive variant that achieves improved convergence rates under standard assumptions. While the original Polyak step-size requires knowledge of the optimal function value, our method estimates this quantity online using a running average of past losses. We establish O(1/T) convergence for convex Lipschitz functions and O(log T/T) for strongly convex cases, improving upon prior adaptive methods by logarithmic factors. Experimental results on logistic regression and neural network training demonstrate modest but consistent improvements over Adam and SGD with cosine annealing. However, empirical gains diminish in overparameterized settings, suggesting the theoretical analysis may not fully capture the practical benefits. The proposed method introduces negligible computational overhead and is particularly effective for ill-conditioned problems. While our contribution is largely incremental, it bridges an important gap between theoretical guarantees and practical step-size selection. Code is available at anonymous-url.",
    "id": 340
  },
  {
    "title": "Adaptive Gradient Clipping with Memory-Efficient Second-Order Estimates for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is widely used to stabilize transformer training, but standard approaches use fixed thresholds that may hinder convergence. We propose Adaptive Second-Order Gradient Clipping (ASGC), which adjusts clipping thresholds using a low-rank approximation of the Hessian spectrum. Our method combines a streaming PCA algorithm to track dominant Hessian eigenvalues with a lightweight clipping schedule that adapts to local curvature. ASGC requires only 3% additional memory compared to standard Adam training. On the GPT-2 medium architecture trained on OpenWebText, ASGC achieves 4.2% faster convergence versus baseline clipping. We also demonstrate improved stability on ViT-L/16 training, reducing gradient norm spikes by 31%. While ASGC shows consistent improvements over naive clipping, our gains are modest (1-5% across tasks) and require careful hyperparameter tuning for different architectures. Theoretical analysis reveals ASGC converges with high probability under standard smoothness assumptions, though our guarantees are weaker than prior work due to the approximate nature of our Hessian estimates. We release an efficient PyTorch implementation compatible with existing optimizer interfaces.",
    "id": 343
  },
  {
    "title": "A Closer Look at Gradient Norm Regularization for Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient norm regularization has emerged as a popular technique for improving generalization in deep neural networks, yet its theoretical and practical impact remains unclear. We provide an empirical investigation of gradient norm regularization across vision and NLP tasks, revealing that its benefits are highly architecture-dependent. Through extensive ablation studies on ResNet-50, Vision Transformers, and BERT, we find that gradient norm regularization (GNR) primarily stabilizes training dynamics rather than improving the final generalization gap. Our key insight is that GNR acts as an implicit form of batch normalization, leading to more stable training but not necessarily better minima. We propose a lightweight variant, Local Gradient Norm Regularization (LGNR), that applies regularization only to specific layers and achieves comparable results with 35% less computational overhead. While LGNR shows modest improvements on small-scale benchmarks (0.5-1.2% on CIFAR-100 with ResNet-18), these gains do not consistently translate to larger models or datasets. Our findings suggest that previous work may have confounded training stability with generalization improvement. Code and pre-trained models will be released upon publication.",
    "id": 357
  },
  {
    "title": "Gradient Surgery Meets Adaptive Momentum: A Less Aggressive Approach to Multi-Task Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, D.",
      "Johnson, S."
    ],
    "abstract": "Multi-task learning often struggles with conflicting gradients between tasks. While recent work like PCGrad and GradNorm surgically modify gradients to resolve conflicts, we observe these methods can be too aggressive, potentially discarding useful information. We propose Adaptive Gradient Blending (AGB), a simpler approach that softly reweights gradient components using per-task uncertainty estimates from a lightweight Bayesian encoder. Rather than projecting conflicting gradients onto orthogonal subspaces, AGB modulates their contribution based on learned confidence scores. On three standard benchmarks (CIFAR-100/SVHN, NYUv2 depth/segmentation, and Meta-World RL), AGB achieves comparable performance to state-of-the-art methods while being 1.3-2.1\u00d7 faster and requiring minimal hyperparameter tuning. However, our gains are modest (0.5-1.2% average accuracy improvement) and tasks with severe conflicts still exhibit suboptimal convergence. While AGB offers a practical alternative to existing techniques, our theoretical analysis reveals fundamental limitations in simultaneously optimizing conflicting objectives without information loss. Code will be made available.",
    "id": 358
  },
  {
    "title": "Gradient Descent with Layerwise Learning Rates Improves Transformer Training Efficiency",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Thompson, S.",
      "Chen, L."
    ],
    "abstract": "We propose Layer-Adaptive Gradient Descent (LAGD), a simple modification to standard gradient descent that assigns different learning rates to different layers in Transformer architectures based on their relative sensitivity to updates. Our method computes layerwise gradient norms across mini-batches and adjusts learning rates using a lightweight heuristic that requires no hyperparameter tuning beyond the base learning rate. Experiments on Wikitext-103 and CIFAR-10 with standard Transformer architectures show 5-12% faster convergence compared to AdamW with careful tuning, while maintaining similar final performance. While the gains are consistent across tasks, they are most pronounced in early training phases. The method introduces minimal overhead (less than 1% increase in training time) and can be easily integrated into existing training pipelines. However, the theoretical justification remains heuristic, and benefits diminish with appropriate learning rate scheduling. We provide PyTorch implementations and hope this practical trick might be useful for researchers training Transformers with limited compute budgets.",
    "id": 361
  },
  {
    "title": "Attention Bottlenecks in Transformer Memorization: A Low-Rank Analysis of Inductive Biases",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Thompson, E."
    ],
    "abstract": "We investigate how transformers memorize training sequences through analysis of attention patterns in overparameterized language models. Our theoretical analysis shows that memorization occurs through rank collapse in attention matrices, with rank dropping from O(d) to O(k) where k is the number of memorized patterns. We propose a low-rank regularization scheme that encourages rank collapse during training while maintaining downstream performance. Experiments on synthetic memotasks and WikiText-103 show our method reduces memorization by 15-20% without hurting perplexity, suggesting some memorization may be beneficial for generalization. However, our regularization does not improve performance on established benchmarks like GLUE. While our theoretical analysis provides insight into transformer memorization dynamics, our empirical evaluation is limited to language modeling and may not extend to other modalities. Code and experiments are available at our anonymized repository.",
    "id": 373
  },
  {
    "title": "LoRA-Hash: Parameter-Efficient Fine-Tuning with Learnable Hash Tables",
    "authors": [
      "Chen, L.",
      "Rodriguez, S.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular method for parameter-efficient fine-tuning of large language models, but its effectiveness diminishes when targeting multiple tasks simultaneously due to interference between low-rank matrices. We propose LoRA-Hash, a simple extension that replaces static low-rank matrices with learned hash tables that map input tokens to rank-1 updates. This allows different regions of the input space to use distinct adaptation parameters while sharing a common hash function. On the GLUE benchmark, LoRA-Hash achieves 0.8% higher average accuracy than standard LoRA when training on multiple tasks with a 2\u00d7 memory reduction. However, we observe significant variance across tasks (\u00b14.2%), suggesting that hash-based adaptation may not be universally beneficial. Analysis reveals that performance gains are concentrated in tasks with clear input-output patterns, while tasks requiring fine-grained reasoning show degradation. Our method requires minimal code changes to existing LoRA implementations and introduces only 0.1M additional parameters. While LoRA-Hash shows promise for multi-task scenarios, we acknowledge limitations in theoretical understanding and the need for better initialization strategies.",
    "id": 391
  },
  {
    "title": "Gradient Noise Revisited: A Modified Learning Rate Schedule for Deep Network Training",
    "authors": [
      "Chen, S.",
      "Rodriguez, M.",
      "Liu, H."
    ],
    "abstract": "Recent work has shown that noise in stochastic gradient descent (SGD) plays a crucial role in navigating the complex loss landscapes of deep neural networks. While adaptive optimization methods like Adam and RMSprop have gained popularity, we argue that careful scheduling of the base learning rate in vanilla SGD can achieve comparable performance with less hyperparameter tuning. Building on the recently proposed 'superconvergence' phenomenon, we introduce Periodic Cyclical Exponential (PCE) decay, a learning rate schedule that alternates between rapid decay phases and high-variance exploration periods. Our method injects controlled noise through sudden learning rate jumps, allowing the optimizer to escape sharp minima while maintaining stable convergence. We evaluate PCE decay on standard image classification benchmarks (CIFAR-10/100, ImageNet) and demonstrate 0.5-1.2% accuracy improvements over tuned baselines, though gains diminish with very deep architectures. The approach requires minimal additional hyperparameters but depends on problem-specific tuning of the cycle length. Our theoretical analysis shows that PCE decay approximates a modified Langevin dynamics, providing partial convergence guarantees for strongly convex functions. While our empirical results are encouraging, we acknowledge that performance improvements are incremental rather than transformative, and the method's effectiveness varies across network architectures and datasets. Code is available at anonymous-link.",
    "id": 392
  },
  {
    "title": "LoRA-Drop: Efficient Low-Rank Adaptation through Dynamic Parameter Pruning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers may lead to suboptimal resource utilization. We propose LoRA-Drop, a simple yet effective approach that dynamically prunes LoRA parameters during fine-tuning based on gradient magnitude analysis. Our method uses a threshold-based pruning schedule that removes low-contribution rank components while maintaining model performance. Across experiments on GLUE, SuperGLUE, and domain-specific tasks, LoRA-Drop reduces trainable parameters by 15-30% compared to standard LoRA with <1% accuracy degradation on most tasks. However, we observe inconsistent results on reasoning-intensive benchmarks, where aggressive pruning sometimes harms performance. We provide theoretical analysis showing that our pruning criterion approximates an upper bound on the perturbation of gradient flow. While LoRA-Drop achieves modest efficiency gains, our findings highlight the challenge of adaptive rank selection without task-specific tuning. Code and pretrained checkpoints are available at [anonymous link].",
    "id": 393
  },
  {
    "title": "LoRA-FA: Low-Rank Adaptation with Feature Alignment for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Singh, A."
    ],
    "abstract": "We propose LoRA-FA, a simple modification to Low-Rank Adaptation (LoRA) that improves performance on downstream tasks by aligning low-rank adapter features with pre-trained representations. While LoRA has become popular for parameter-efficient fine-tuning, we observe that learned adapter directions often diverge from semantically meaningful directions in the pre-trained model. LoRA-FA addresses this by adding a lightweight feature alignment loss that encourages adapter features to align with the span of top singular vectors from pre-trained layers. Our method introduces only 3% additional parameters compared to standard LoRA and requires minimal computational overhead. We evaluate LoRA-FA on GLUE, SuperGLUE, and vision-language tasks, showing average improvements of 1.2% over standard LoRA, with particularly strong gains on low-resource datasets. However, we find the benefits diminish for larger models (>30B parameters) and saturate quickly with increased adapter rank. While the feature alignment constraint slightly reduces training efficiency (10% slower convergence), it provides more stable fine-tuning across hyperparameters. Our experiments suggest LoRA-FA offers modest but consistent improvements for practical deployment scenarios, though we acknowledge the gains are incremental rather than transformative.",
    "id": 406
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Heads Recycling with Learnable Dropout",
    "authors": [
      "Liu, S.",
      "Chen, M.",
      "Johnson, K."
    ],
    "abstract": "We propose HeadRecycle, a simple yet effective method for reducing computational cost in pre-trained transformers by dynamically identifying and skipping redundant attention heads during inference. Our approach uses a lightweight auxiliary network trained with reinforcement learning to assign recycling scores to each attention head based on their relative importance for specific inputs. Unlike prior pruning methods that permanently remove heads, HeadRecycle maintains full model capacity while achieving 15-30% reduction in FLOPs through selective computation. We evaluate on standard NLP benchmarks including GLUE, SQuAD, and WikiText-103. On BERT-base, HeadRecycle achieves 1.2x speedup with only 0.8% performance degradation, outperforming static magnitude-based pruning baselines. While the results are encouraging, our method requires additional training overhead and shows limited effectiveness on downstream tasks requiring complex reasoning. Code and pre-trained models will be released upon acceptance.",
    "id": 407
  },
  {
    "title": "LoRA++: A Memory-Efficient Adaptation of Low-Rank Adaptation for Multi-Task Transfer Learning",
    "authors": [
      "Chen, S.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its effectiveness diminishes when transferring to multiple related tasks simultaneously. We propose LoRA++, a simple extension that introduces cross-task knowledge sharing through a shared low-rank basis while maintaining task-specific sparse updates. Our method adds only 2.7% additional parameters over standard LoRA across 8 vision and NLP benchmarks. While we demonstrate modest improvements in average accuracy (+1.2% over LoRA, +0.3% over full fine-tuning), our primary contribution lies in identifying the scenarios where multi-task LoRA adaptations provide benefits versus when they introduce interference. Through controlled experiments on task similarity metrics, we show LoRA++ helps most when tasks share >60% feature similarity, but can hurt performance otherwise. Our theoretical analysis provides a bound on the approximation error introduced by the shared basis, though the bound is loose and may not explain empirical observations. Code and pre-trained adapters will be released upon acceptance.",
    "id": 411
  },
  {
    "title": "LoRa-Clipped: Improving Low-Rank Adaptation for Large Language Models via Gradient Clipping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning technique for large language models, but its performance remains unstable across different training configurations and hyperparameters. We identify that gradient explosion during LoRA training is a primary source of this instability, particularly when adapting to data-scarce domains. We propose LoRa-Clipped, a simple yet effective modification that applies adaptive gradient clipping to the low-rank matrices while preserving the computational efficiency of standard LoRA. Through experiments on three benchmark tasks (instruction tuning, domain adaptation, and few-shot classification), we show LoRa-Clipped reduces training instability by 35% compared to vanilla LoRA while maintaining comparable performance on most tasks. On GPT-2 Medium and LLaMA-7B, our method achieves average improvements of 2.1% on downstream metrics when training data is limited to 1K examples. However, we observe diminishing returns on larger models (LLaMA-13B and above) and when sufficient training data (\u226510K examples) is available. While our approach is computationally lightweight and requires only two additional hyperparameters, the improvements are incremental rather than transformative, and the method appears most beneficial in resource-constrained training scenarios.",
    "id": 415
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Merging with Curriculum Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Transformer models have achieved remarkable success across domains, but their quadratic complexity with respect to sequence length remains a critical bottleneck. While previous work has explored token pruning and merging heuristics to reduce computational cost, these methods often rely on fixed rules or require expensive pre-training adaptations. We propose a learnable token merging framework that dynamically reduces sequence length during fine-tuning using a lightweight gating network. Our method introduces curriculum scheduling that gradually increases compression ratios throughout training, stabilizing optimization while achieving significant speedups. Specifically, we train a small MLP to predict merge decisions for consecutive token pairs, guided by auxiliary losses that preserve information content. Across three NLP benchmarks (GLUE, SQuAD, and CNN/DM), our approach reduces inference FLOPs by 25-40% with minimal degradation (<1% relative) on most tasks. However, we observe more substantial performance drops (3-5%) on tasks requiring fine-grained reasoning (e.g., QQP, MNLI). Analysis reveals our merging policy tends to aggressively compress syntactic markers necessary for complex reasoning. While our method offers a practical trade-off between efficiency and performance, the learned merging strategies show limited generalization across domains, suggesting the need for task-specific tuning. Code and pre-trained models are available at [URL].",
    "id": 417
  },
  {
    "title": "Gradient Surgery for Partially Aligned MAML with Application to Few-Shot Domain Adaptation",
    "authors": [
      "Chen, L.",
      "Vaswani, A.",
      "Kim, S."
    ],
    "abstract": "Meta-learning methods like MAML struggle when training and test distributions share components but differ in others\u2014a common scenario in real-world few-shot learning. We propose Partial-MAML, which adapts gradient-based meta-learning to settings where feature spaces are partially aligned between source and target domains. Our method applies principled gradient surgery to modify MAML's inner loop, selectively updating parameters based on their estimated alignment scores computed via domain adversarial training. On standard benchmarks, Partial-MAML achieves modest improvements (1-2% absolute) over vanilla MAML in domain-aligned cases while maintaining MAML's performance on standard tasks. We also introduce a new benchmark for few-shot domain adaptation with controlled alignment ratios. While our theoretical analysis provides convergence guarantees under restrictive assumptions, we acknowledge these may not hold in practice. Our method requires additional hyperparameters to tune and increases computational cost by approximately 15%. Code and benchmarks will be released upon acceptance.",
    "id": 421
  },
  {
    "title": "Scheduled Mixup: A Curriculum-Based Approach to Interpolated Training for Improved Out-of-Distribution Robustness",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Wang, K."
    ],
    "abstract": "While mixup has become a standard technique for improving generalization in deep neural networks, its effectiveness varies significantly across architectures and datasets. We propose Scheduled Mixup, a curriculum-based variant that gradually adjusts the interpolation strength \u03bb based on training progress and sample difficulty. Our method computes \u03bb \u223c Beta(\u03b1, \u03b1) where \u03b1 is scheduled from 0.2 to 1.0 using a power-law function, prioritizing harder interpolations as training progresses. We evaluate on CIFAR-10-C and ImageNet-R, showing 2.7% and 1.4% improvements in mCE over standard mixup baselines. However, we find the benefits diminish on larger models (ViT-L/32) and certain corruption types (fog, brightness). Our theoretical analysis suggests the improvement stems from controlling the Jensen gap during training, though we acknowledge gaps in our convergence guarantees. Code is available at anonymous.url.",
    "id": 425
  },
  {
    "title": "Gradient Alignment Regularization for Improving Transfer Learning in Small-Data Regimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J.",
      "Okafor, C."
    ],
    "abstract": "Pre-trained models often fail to adapt effectively when fine-tuned on datasets with very few examples. We propose Gradient Alignment Regularization (GAR), a simple regularization term that encourages the gradients during fine-tuning to align with those from the pre-training phase. By penalizing the cosine distance between current and pre-training gradients, GAR prevents catastrophic forgetting while allowing task-specific adaptation. We provide theoretical justification showing GAR bounds the deviation from pre-trained representations under mild assumptions. Experimental results on 8 benchmark datasets show 2-5% improvements over standard fine-tuning when labeled data is extremely scarce (<1% of original dataset), with comparable or worse performance in moderate-data regimes. While GAR introduces minimal computational overhead and works with any pre-trained model, its benefits appear limited to ultra-low-data settings, raising questions about broader applicability. Code is available at [URL].",
    "id": 427
  },
  {
    "title": "Revisiting Momentum in Stochastic Optimization: A Probabilistic Interpretation with Improved Adaptive Bounds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Momentum-based optimizers remain central to modern deep learning, yet theoretical understanding of their stochastic behavior lags behind empirical success. We propose a probabilistic re-interpretation of Polyak momentum as a Bayesian filtering problem, yielding novel adaptive hyperparameter schedules. Our key insight connects momentum coefficient \u03b2 to the signal-to-noise ratio of mini-batch gradients, enabling dynamic adjustment without additional hyperparameters. Through a second-moment analysis of the filtering equations, we derive improved convergence bounds for non-convex objectives that tighten existing O(1/\u221aT) rates to O(log T/T) under the PL inequality. Experiments on ResNet training demonstrate consistent but modest improvements (0.5-1.2% accuracy) over AdamW and SGD+Momentum across CIFAR-10/100 and ImageNet subsets, with particular gains in low-data regimes. While our theoretical contributions provide new perspective on momentum dynamics, empirical improvements are incremental and sensitive to architecture choices. We opensource our PyTorch implementation for reproducibility.",
    "id": 437
  },
  {
    "title": "Gradient Curriculum Learning: A Simple Trick for Improving Transformer Training Stability",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Transformer training often suffers from unstable optimization dynamics, particularly in the early phases when attention patterns are still forming. We propose gradient curriculum learning (GCL), a surprisingly simple approach that anneals gradient magnitudes during the initial training stages. Our method applies a learnable scalar multiplier to the overall loss gradient, scheduled by a cosine decay for the first 10% of training steps. While the idea appears almost trivial, we demonstrate consistent improvements across 6 diverse language modeling and vision tasks, achieving 2-3% better final perplexity on average and notably reducing training variance across seeds. Theoretically, we provide a toy 2-layer linear network analysis suggesting GCL can help escape poor local minima. However, our experiments reveal the gains are strongest for small models (\u2264125M parameters) and diminish as scale increases. GCL adds no inference overhead and requires trivial implementation changes, making it practical for resource-constrained practitioners. While we acknowledge the contribution is incremental and the theoretical analysis is limited, we believe GCL represents a useful training trick that could benefit many practitioners, particularly those working with modest computational budgets.",
    "id": 445
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Reparameterization Perspective",
    "authors": [
      "Chen, L.",
      "Park, J.",
      "Singh, V."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent gradient surgery methods address this through projection-based approaches, we show these techniques can be viewed as implicit reparameterizations of the loss surface. We propose RepGrad, a lightweight modification to existing gradient surgery methods that explicitly learns task-specific reparameterizations through a small auxiliary network. Our method adds minimal computational overhead (<5% increase in training time) and can be dropped into existing multi-task architectures without architectural changes. On three standard benchmarks (CityScapes, NYUv2, and QM9), RepGrad achieves modest but consistent improvements over gradient surgery baselines (average +1.2% performance gain across tasks). While our theoretical analysis provides some insight into why RepGrad works, we acknowledge the improvements are incremental and the method may not justify the added complexity for all applications. However, RepGrad offers a practical way to enhance existing multi-task systems without retraining from scratch.",
    "id": 461
  },
  {
    "title": "Graph Attention Networks with Learnable Node Embeddings via Meta-Gradient Descent",
    "authors": [
      "Kim, S.",
      "Rodriguez, C.A.",
      "Liu, J."
    ],
    "abstract": "We present Meta-GNN, a method that adapts graph neural networks to new tasks using meta-gradient descent on learnable node embeddings. While previous work has focused on adapting GNN architecture or edge weights, we propose to meta-learn initial node representations that can be rapidly fine-tuned for downstream tasks. Our approach trains a shared initialization across multiple graph datasets, then uses second-order gradients to optimize node embeddings for few-shot node classification. Experiments on 6 benchmark datasets show 2-3% improvement over standard GNN baselines when limited to 5-10 training labels per class. However, performance gains diminish with abundant labeled data, and computational overhead increases quadratically with graph size. Our contribution lies in demonstrating the viability of node-level meta-learning, though we acknowledge limitations in scalability and theoretical grounding. Code and datasets will be released upon acceptance.",
    "id": 464
  },
  {
    "title": "Adaptive Curriculum Learning via Difficulty-Aware Sampling Improves Sample Efficiency in Deep Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Curriculum learning has shown promise for improving sample efficiency in deep RL, but existing approaches often rely on hand-crafted curricula or simple heuristics that may not generalize across environments. We propose AdaCurriculum, a simple yet effective method that adaptively adjusts the sampling distribution over training episodes based on a learned measure of task difficulty. Our approach uses a lightweight auxiliary network to estimate episode difficulty from initial states, requiring only 5% additional parameters. We evaluate AdaCurriculum on three standard continuous control benchmarks (MuJoCo) and two discrete action environments. Results show 15-40% sample efficiency improvements over uniform sampling baselines and 8-25% gains over a recent curriculum learning method, though we find variance across seeds can be high (standard deviation 12.3%). While our method achieves consistent improvements on medium-complexity environments, benefits diminish on simpler tasks, suggesting the approach may be most valuable when the optimal curriculum is non-trivial. Ablations reveal that both the difficulty estimator and adaptive sampling schedule contribute to performance, with neither component alone sufficient. Our implementation and hyperparameters are provided for reproducibility.",
    "id": 466
  },
  {
    "title": "Masked Autoencoders with Learnable Patch Embeddings for Semi-Supervised Image Classification",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose MAPPLE, a semi-supervised learning framework that extends masked autoencoding to leverage unlabeled data through learnable patch embeddings. While masked autoencoders (MAE) have shown promise for self-supervised pretraining, their direct application to semi-supervised learning remains underexplored. MAPPLE introduces a learnable patch importance mechanism that dynamically weights patches based on their mask reconstruction difficulty, allowing the model to focus on semantically meaningful regions during downstream fine-tuning. Our method achieves 71.2% top-1 accuracy on ImageNet with 10% labeled data, improving over standard MAE fine-tuning by 2.3 percentage points. On CIFAR-100, we obtain competitive results with 80.1% accuracy when using 1% labels. However, our approach shows diminishing returns as labeled data increases, performing comparably to simpler baselines at 100% supervision. While our patch importance mechanism offers interpretable insights, we find that performance gains are sensitive to hyperparameter choices, particularly the mask ratio and momentum coefficient. Extensive ablations reveal that much of the improvement stems from improved representation quality rather than the patch weighting scheme itself. Code and pretrained models will be released upon acceptance.",
    "id": 474
  },
  {
    "title": "LoRA-DAG: Low-Rank Adaptation with Directed Acyclic Graphs for Parameter-Efficient Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, J.",
      "Thompson, S."
    ],
    "abstract": "We propose LoRA-DAG, a parameter-efficient fine-tuning method that extends Low-Rank Adaptation (LoRA) to multi-task learning scenarios. Our key insight is to model task relationships as a directed acyclic graph (DAG), where edges represent shared low-rank adaptations between tasks. This allows for both positive and negative transfer through learnable edge weights while maintaining the parameter efficiency of LoRA. We validate our approach on GLUE and SuperGLUE benchmarks using T5-base and RoBERTa-base models. Experimental results show modest but consistent improvements over standard LoRA (+1.2-1.8% F1 average) and naive model merging (+2.5% F1), particularly on low-resource tasks. However, we observe that performance gains diminish as task similarity decreases, and our DAG structure adds 5-8% training overhead. Ablation studies reveal that most benefits come from shared input projections rather than output projections. While limited by the fixed DAG structure and computational overhead for large models, LoRA-DAG presents a practical extension of LoRA for practitioners with moderately related tasks.",
    "id": 481
  },
  {
    "title": "Adaptive Curriculum Learning for Few-Shot Molecular Property Prediction",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M.",
      "Zhou, J."
    ],
    "abstract": "Molecular property prediction with limited labeled data remains challenging despite recent advances in graph neural networks. We propose AC-MOL, an adaptive curriculum learning framework that gradually exposes the model to increasingly complex molecular structures based on their estimated difficulty. Our approach uses a simple yet effective difficulty metric combining molecular size and structural complexity, dynamically adjusting the curriculum schedule based on validation performance. Through extensive experiments on 8 benchmark datasets from MoleculeNet, we demonstrate that AC-MOL achieves 3-7% improvement over standard few-shot baselines, particularly effective for datasets with high scaffold diversity. However, we find that the benefits diminish when sufficient labeled data (\u2265500 molecules per class) is available. While our method provides practical improvements, we acknowledge that the theoretical justification for our curriculum strategy remains limited, and the approach inherits known limitations of curriculum learning including sensitivity to hyperparameter choices. Code and datasets will be made available upon acceptance.",
    "id": 484
  },
  {
    "title": "LoRA-MoE: Automatic Expert Routing in Low-Rank Adaptation",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Thompson, S."
    ],
    "abstract": "We propose LoRA-MoE, an extension to Low-Rank Adaptation (LoRA) that automatically routes tokens to task-specific low-rank experts during fine-tuning. While LoRA has become standard for parameter-efficient adaptation, it treats all inputs uniformly, potentially missing task-specific patterns. Our method decomposes the low-rank adaptation into multiple experts, each learning specialized representations for different input characteristics. We design a top-k routing mechanism that selects relevant experts based on learned task embeddings, keeping the total parameter count comparable to standard LoRA. Experiments on GLUE and SuperGLUE show modest improvements (1.2% average gain) over vanilla LoRA, with more pronounced benefits on multi-task benchmarks. However, we observe that routing patterns converge to near-uniform distributions on simpler tasks, suggesting the method may offer limited benefits for traditional fine-tuning. The approach adds minimal computational overhead during inference, maintaining LoRA's deployment advantages. While our results demonstrate the potential for more adaptive parameter-efficient fine-tuning, we acknowledge that gains come at the cost of training instability and require careful initialization of routing parameters.",
    "id": 486
  },
  {
    "title": "Gradient Descent with Memory: A Simple Plug-in Module for Improved Optimization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We present Memory-Augmented Gradient Descent (MAGD), a lightweight module that can be seamlessly integrated into existing optimizers to improve convergence without hyperparameter tuning. MAGD maintains a compressed history of past gradients using a low-rank approximation, enabling the optimizer to leverage long-term curvature information while maintaining computational efficiency. Our method adds only 0.3% overhead in training time compared to standard Adam, making it practical for large-scale applications. We evaluate MAGD on image classification tasks across ResNet and Vision Transformer architectures, achieving 0.8-1.2% improvements in final accuracy on ImageNet and CIFAR-10 over strong baselines. While these gains are consistent across architectures, we observe diminishing returns on tasks using extensive data augmentation. Theoretical analysis shows MAGD converges to a neighborhood of stationary points under standard convexity assumptions, though the rate is similar to vanilla gradient descent. Our results suggest MAGD provides reliable, albeit incremental, improvements for practitioners without requiring additional tuning, representing a practical rather than breakthrough contribution to optimization research.",
    "id": 504
  },
  {
    "title": "Improving Neural Network Calibration through Temperature Scaling with Adaptive Momentum",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Well-calibrated probabilistic predictions are crucial for reliable deployment of neural networks in real-world applications. While temperature scaling has emerged as a simple post-hoc calibration method, we show its effectiveness is highly sensitive to the choice of learning rate during the few-shot calibration phase. We propose AdaTemp, an adaptive variant that incorporates momentum-based updates to the temperature parameter during optimization. Our method achieves statistically significant improvements over standard temperature scaling on several image classification benchmarks, with average ECE reductions of 8-12% across CIFAR-10/100 and ImageNet. However, we find these gains diminish when the training distribution differs substantially from the calibration set. Through extensive ablation studies, we demonstrate that the momentum coefficient must be carefully tuned\u2014values outside [0.3, 0.7] often hurt performance. While AdaTemp provides modest but consistent improvements over strong baselines, our theoretical analysis reveals fundamental limitations: the method cannot correct calibration errors when the predicted probabilities are monotonically decreasing across all classes. Our findings suggest that adaptive optimization techniques for calibration require dataset-specific hyperparameter tuning and may not generalize across domains.",
    "id": 509
  },
  {
    "title": "Gradient Descent with Logit-Aligned Updates Improves Confidence Calibration in Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "While large language models achieve impressive predictive accuracy, their predicted probabilities often poorly reflect true likelihoods. We propose Logit-Aligned Gradient Descent (LAGD), a simple modification to standard training that explicitly penalizes the difference between predicted logits and logit-transformed empirical frequencies during gradient updates. Our method requires only a single hyperparameter controlling the alignment strength and adds negligible computational overhead. We evaluate LAGD on five language modeling benchmarks using GPT-2 and OPT models with 125M-1.3B parameters. Results show 5-15% improvements in Expected Calibration Error (ECE) compared to standard fine-tuning, with minimal impact on perplexity. However, we find performance gains diminish on larger models (>3B parameters), suggesting the method may be limited to smaller architectures. While LAGD provides consistent improvements over baselines, the magnitude of improvement is modest compared to recent calibration-specific post-processing methods. Our implementation and experiments are available at [URL].",
    "id": 541
  },
  {
    "title": "Gradient Descent with Momentum Revisited: A Fourier Perspective on Optimization Dynamics",
    "authors": [
      "Liu, K.",
      "Johnson, T.",
      "Chen, S."
    ],
    "abstract": "We analyze gradient descent with momentum (GDM) through the lens of Fourier analysis, revealing that momentum amplifies specific frequency components of the loss surface's gradient spectrum. Our theoretical framework shows that for quadratic objectives, the optimal momentum parameter \u03b2 relates to the largest eigenvalue \u03bb_max of the Hessian via \u03b2 = 1 - 2/\u221a\u03ba, where \u03ba is the condition number. While this recovers known convergence rate bounds, our frequency interpretation provides new insights into why momentum helps in non-convex settings. We validate these findings through experiments on shallow neural networks trained on MNIST, demonstrating improved training dynamics when spectral preconditioning aligns with our frequency analysis. However, extending these results to deeper networks remains challenging, as the frequency characteristics become difficult to characterize precisely. Our work suggests practitioners should consider Hessian spectra when tuning momentum parameters, though we acknowledge this is computationally expensive for large models. The contribution is primarily theoretical, connecting spectral optimization concepts to practical momentum-based methods.",
    "id": 545
  },
  {
    "title": "Augmenting Contrastive Learning with Local Neighborhood Consistency",
    "authors": [
      "Liu, Q.",
      "Reddy, S.",
      "Kumar, V."
    ],
    "abstract": "Self-supervised contrastive learning has achieved remarkable success in learning visual representations without labels. However, we observe that current methods may lack robustness to small perturbations in the learned embedding space. We propose Local Neighborhood Consistency (LNC), a simple regularization term that encourages representations to be consistent within dynamically constructed local neighborhoods. Specifically, LNC adds an auxiliary loss that minimizes the distance between each anchor's feature representation and the centroid of its nearest neighbors. Our method can be incorporated into any existing contrastive learning framework with minimal computational overhead. We evaluate LNC on ImageNet-1K using ResNet-50 and ViT-B architectures, showing modest improvements of 0.7-1.2% in linear evaluation when combined with SimCLR. Ablation studies reveal that the effectiveness of LNC depends heavily on the choice of neighborhood size and temperature parameters. While the improvements are consistent across settings, the magnitude remains relatively small compared to recent architectural advances. Our findings suggest that local neighborhood consistency provides a useful, though incremental, enhancement to existing contrastive learning methods.",
    "id": 552
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning",
    "authors": [
      "Kumar, A.",
      "Liu, S.",
      "Chen, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance and catastrophic forgetting. While recent gradient surgery methods like PCGrad and GradDrop address this through selective gradient projection, they rely on fixed heuristics that may discard useful information. We propose Adaptive Memory Gradient Surgery (AMGS), which maintains a small learnable memory bank to store gradient directions from previous iterations. Our key insight is that seemingly conflicting gradients can become compatible when combined with the right historical context. AMGS dynamically computes convex combinations of current gradients with stored directions, guided by a lightweight attention mechanism trained to maximize inter-task transfer. On 8 multi-task datasets spanning computer vision and NLP benchmarks, AMGS achieves a 2.3% average improvement over PCGrad while requiring only 1.2% additional parameters. However, experiments reveal AMGS performs poorly when task counts exceed 15 or when tasks share minimal underlying structure. Additionally, our method introduces computational overhead and lacks convergence guarantees. While AMGS provides a practical improvement for moderate-scale multi-task scenarios, its benefits may not justify the complexity for all applications.",
    "id": 560
  },
  {
    "title": "Adaptive Gradient Clipping with Curvature-Aware Bounds for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose an adaptive gradient clipping method that incorporates local curvature information to stabilize transformer training without extensive hyperparameter tuning. Our approach computes per-layer clipping bounds using an online estimate of the Fisher Information Matrix diagonal, combined with a momentum-based smoothing technique to handle gradient noise. While standard clipping methods use fixed thresholds or simple norm-based heuristics, our curvature-aware bounds automatically adjust to the changing loss landscape during training. We evaluate on Wikitext-103 language modeling and WMT'14 English-German translation tasks, showing modest improvements (0.8-1.2 BLEU/BPE) over strong baselines with reduced gradient explosion incidents. However, computational overhead increases training time by 15-20%, and benefits diminish on smaller architectures. Theoretical analysis provides convergence guarantees under standard convexity assumptions, but the non-convex setting remains largely heuristic. Code and hyperparameters are provided for reproducibility.",
    "id": 569
  },
  {
    "title": "Gradient Norm Regularization Improves Adversarial Training: A Small-Scale Investigation",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, T."
    ],
    "abstract": "Adversarial training remains computationally expensive for large networks, leading practitioners to use smaller proxy models during training. We investigate whether additional regularization during this proxy training phase can translate to improved robustness in the final model. Specifically, we propose gradient norm regularization (GNR), which penalizes large gradient norms of the loss with respect to inputs during adversarial training. Our theoretical analysis suggests GNR can improve margin bounds by a factor of \u221ak in simplified settings. On CIFAR-10 and CIFAR-100, we conduct extensive experiments with ResNet-18/34 architectures across 3 random seeds. Results show modest but consistent improvements: GNR increases robust accuracy by 2.1-3.7% over standard adversarial training, while maintaining clean accuracy within 0.5%. However, these gains are less pronounced when tested on larger architectures (ResNet-50) or out-of-distribution datasets. Ablation studies reveal that the regularization weight must be carefully tuned to prevent gradient vanishing. While our method presents a simple, theoretically-motivated improvement to adversarial training, we acknowledge limitations: experiments use standard datasets and architectures, computational budget prevents ImageNet-scale evaluation, and the regularizer adds 15-20% training overhead. Code is available at [URL].",
    "id": 576
  },
  {
    "title": "Gradient Descent with Implicit Structural Regularization via Weight Asymmetry",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S.",
      "Rodriguez, A."
    ],
    "abstract": "We investigate an observed phenomenon where SGD exhibits implicit regularization towards low-rank solutions in overparametrized linear networks, but only when initialized with asymmetric weight matrices. While previous work attributes this to norm minimization, we demonstrate that weight asymmetry directly constrains the optimization trajectory in a manner analogous to nuclear norm regularization. Our analysis reveals that gradient flow on asymmetric factorizations, U = AB^T, converges to global minima while maintaining a non-trivial relationship between the column spaces of A and B. We provide a theoretical characterization for two-layer linear networks under orthogonal input data and derive an expression relating initialization scale to effective regularization strength. However, extending these results to non-linear activations proves challenging due to dynamical coupling between layers. Empirical validation on synthetic rank recovery tasks shows modest improvements over symmetric baselines, though the effect diminishes with increasing network depth. We also observe similar trends in shallow convolutional networks trained on CIFAR-10, achieving 2-3% better test accuracy with carefully tuned asymmetric initializations. While our findings suggest weight asymmetry as a controllable inductive bias, we acknowledge limitations in generalization to deeper architectures and the lack of a complete theoretical picture. Code will be made available upon acceptance.",
    "id": 578
  },
  {
    "title": "Improving Transformer Efficiency through Layer-wise Learning Rate Scheduling and Reversible Computation",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Training large transformer models remains computationally expensive, limiting accessibility for researchers and practitioners. We propose a combination of layer-wise learning rate scheduling with reversible transformer blocks to reduce memory usage and training time. Our learning rate schedule assigns decreasing learning rates to deeper layers based on the observation that lower layers converge faster, while reversible computation allows activations to be recomputed during the backward pass. On standard NLP benchmarks (WMT14 en-de, GLUE), our approach achieves 15-20% reduction in memory usage compared to standard transformers with less than 1% degradation in performance. While these gains are modest, we demonstrate that the techniques are complementary to existing efficiency methods and can be easily integrated into existing codebases. Our empirical analysis suggests the benefits are most pronounced for medium-sized models (100M-500M parameters), with diminishing returns for larger scales. Code and pre-trained models will be released upon acceptance.",
    "id": 581
  },
  {
    "title": "Improving Gradient Descent with Lookahead-Averaged Momentum: A Simple Modification for Stable Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Ivanov, S."
    ],
    "abstract": "We propose Lookahead-Averaged Momentum (LAM), a simple modification to standard momentum-based optimizers that improves training stability without additional hyperparameters. LAM maintains two sets of parameters: the 'fast' parameters updated using standard momentum, and 'slow' parameters that are periodically updated using an exponentially-weighted average of the fast parameters. We show that this averaging step acts as a form of implicit gradient smoothing, reducing the variance of effective update directions. Across 8 benchmark datasets and 5 neural architectures, LAM achieves comparable or slightly improved final accuracy to SGD and Adam while exhibiting 15-30% lower training loss variance in early epochs. Theoretical analysis in the convex quadratic case reveals that LAM converges at a rate O(\u03ba log(1/\u03b5)) when \u03bb > 1/\u03ba, where \u03ba is the condition number and \u03bb is the synchronization period. While our empirical improvements are modest and the theoretical analysis is limited to simplified settings, LAM requires only 15 lines of code to implement and may be useful in scenarios where training stability is prioritized over maximal performance. Code will be made available upon acceptance.",
    "id": 589
  },
  {
    "title": "Looped Transformers Learn to Plan: Investigating Iterative Refinement for Compositional Tasks",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "We propose LoopFormer, a simple modification to standard transformers that connects the output back to the input through iterative processing, enabling emergent planning-like behavior without architectural changes beyond a feedback connection. By unrolling the transformer for T iterations, we show that the model learns to refine its predictions through a form of implicit search, potentially bridging the gap between autoregressive generation and algorithmic planning. While our analysis reveals that looped transformers can solve synthetic algorithmic tasks (shortest paths, sorting, puzzle solving) with significantly fewer parameters than vanilla transformers, we acknowledge that performance degrades on longer sequences and more complex tasks. Experiments on GSM8K and algorithmic benchmarks demonstrate 12-25% improvements over standard transformers, but we find these gains diminish when tasks require genuine backtracking or search over exponentially large spaces. Our theoretical analysis characterizes the representational capacity of looped architectures, though we note the gap between our expressive power results and empirical behavior remains incompletely understood. Code and datasets will be released upon acceptance.",
    "id": 594
  },
  {
    "title": "Learning with Noisy Labels via Adaptive Confidence Thresholding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training deep neural networks robustly under label noise remains challenging despite extensive prior work. We propose ACT (Adaptive Confidence Thresholding), a simple post-processing method that dynamically adjusts confidence thresholds during training to filter potentially mislabeled examples. Unlike prior approaches that require estimating noise rates or maintaining additional networks, ACT uses only the model's own predictions and a moving average of training statistics to identify unreliable labels. Our method adds minimal computational overhead and can be incorporated into existing training pipelines with two lines of code. On CIFAR-10 and CIFAR-100 with synthetic noise rates from 20% to 60%, ACT achieves comparable or better performance than state-of-the-art methods while being significantly simpler to implement. However, we observe performance degradation under extreme noise (>70%) and when the noise is instance-dependent rather than uniform. Ablation studies reveal that ACT's effectiveness primarily comes from early regularization effects rather than sophisticated noise detection. While our empirical results are promising, we lack theoretical guarantees and observe sensitivity to hyperparameters in certain regimes. Code and pre-trained models will be released upon acceptance.",
    "id": 599
  },
  {
    "title": "Gradient Surgery with Adaptive Momentum: A Minimal Intervention Approach to Multi-Task Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization. While existing gradient surgery methods compute fixed combinations of task gradients, they lack mechanisms to adaptively adjust their influence during training. We propose Adaptive Gradient Surgery with Momentum (AGSM), a lightweight extension that modulates gradient combinations based on per-task loss curvature. AGSM interpolates between gradient surgery and momentum-based updates, applying corrective terms only when tasks exhibit significant gradient conflict. Our method requires minimal additional computation (\u22645% overhead compared to standard training) and can be integrated into existing frameworks with <20 lines of code. Experiments on three multi-task vision benchmarks show 2-4% improvements over PCGrad and GradNorm, with particular gains on task pairs exhibiting high gradient interference. However, performance gains diminish on NLP tasks and heavily-regularized models, suggesting our approach primarily benefits specific optimization regimes. While AGSM introduces interpretable hyperparameters relating to conflict detection thresholds, their optimal values vary across datasets. Theoretical analysis establishes convergence under convexity assumptions, though the non-convex case remains problematic. Code is available at anonymous-url.",
    "id": 600
  },
  {
    "title": "Revisiting Momentum with Learned Update Coefficients for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Singh, K."
    ],
    "abstract": "We propose Learned Momentum Coefficients (LMC), a simple extension to stochastic momentum methods that adaptively selects per-parameter momentum values using a lightweight neural network trained concurrently with the main optimization. While momentum-based optimizers like SGD with momentum and Adam are widely used in deep learning, their fixed momentum hyperparameters may limit convergence speed on non-convex objectives. LMC maintains the computational efficiency of standard momentum methods while allowing momentum factors to vary dynamically during training. Our experiments on CIFAR-10/100 and ImageNet show that LMC achieves 1-3% better accuracy than AdamW on ResNet-50 and Vision Transformer models, with modest improvements in convergence speed (5-10% faster wall-clock time to target accuracy). However, gains are inconsistent across architectures - LMC performs comparably to tuned baselines on smaller models but offers limited benefits on larger ones. Theoretical analysis reveals that LMC converges for smooth non-convex objectives at a rate matching standard momentum, though we do not show improvement in the worst case. Our approach requires minimal additional memory (0.1% increase) but introduces some hyperparameter sensitivity not present in fixed-momentum methods. Code is available at [anonymized].",
    "id": 601
  },
  {
    "title": "Frequency-Aware Gradient Clipping for Robust Transformer Training Under Label Noise",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Zhao, H."
    ],
    "abstract": "Prior work has demonstrated that transformer models are surprisingly robust to label noise, maintaining reasonable downstream performance even with 30-50% corrupted labels. We explore whether this robustness can be explained through the lens of gradient frequency distribution during training. Our key observation is that noisy labels primarily affect high-frequency gradient components, while low-frequency components largely preserve the underlying signal. Based on this insight, we propose Frequency-Aware Gradient Clipping (FAGC), which adaptively clips gradients based on their frequency content. FAGC operates in the Fourier domain of parameter gradients, preserving low-frequency information while thresholding high-frequency updates. On CIFAR-100 and ImageNet with synthetic label noise, FAGC achieves 2-3% improvements over standard training, and shows particular benefits when combined with mixup augmentation. However, we observe diminishing returns on naturally noisy datasets like WebVision. While FAGC introduces minimal computational overhead (<5%), its benefits appear dataset-specific and we cannot achieve consistent improvements across all settings. Our empirical results challenge the prevailing view that noise robustness is solely due to architectural inductive biases, suggesting an alternative explanation based on gradient frequency filtering during optimization.",
    "id": 610
  },
  {
    "title": "Gradient Amplification Through Transient Sharpening: A Simple Heuristic for Improved Convergence in Overparameterized Networks",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chawla, N.",
      "Chen, L."
    ],
    "abstract": "We propose a lightweight training modification that amplifies gradient signals during the initial phase of training deep neural networks. Our method, Transient Gradient Sharpening (TGS), applies an element-wise transformation to gradients based on their historical norm distribution, effectively increasing the learning rate for coordinates with consistently small updates while maintaining stability for others. Unlike adaptive optimizers that maintain per-parameter statistics throughout training, TGS gradually phases out its intervention, transitioning to standard SGD momentum after a preset number of epochs. We demonstrate improvements over vanilla SGD on ResNet-50 and ViT-B/16 architectures across CIFAR-100 and ImageNet-1K, with average top-1 accuracy gains of 1.2% and 0.8% respectively. While our approach is simple to implement and adds minimal computational overhead, we observe that gains diminish with stronger baselines like AdamW. Theoretical analysis reveals connections to sharpness-aware minimization, though we acknowledge limitations in our convergence guarantees and the heuristic nature of our phase-out schedule. Code is available at the provided link.",
    "id": 615
  },
  {
    "title": "Gradient Surgery Meets Sharpness Minimization: A Simple Trick for Better Generalization in Vision Transformers",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Zhao, S."
    ],
    "abstract": "We propose Sharpness-Aware Gradient Surgery (SAGS), a lightweight modification to gradient descent that combines sharpness minimization with gradient surgery to improve generalization in Vision Transformers. Despite recent advances in training ViTs, we observe that multi-task gradients often interfere destructively and sharpness-based regularization techniques underperform due to the quadratic complexity of Hessian computations. SAGS addresses both issues by (1) selectively dropping gradient components that conflict with sharpness reduction while preserving task-specific information, and (2) using a rank-1 approximation of the Hessian to efficiently compute trust-region steps. On ImageNet-1K, SAGS improves top-1 accuracy by 0.8% over AdamW for ViT-B/16 at half the computational cost of SAM, though gains diminish for larger ViT-L/16 models. Our ablation study reveals the method is most effective when training from scratch on smaller datasets like CIFAR-100. While our theoretical analysis is limited to quadratic objectives, we show empirical benefits on standard benchmarks. Code and pretrained models will be released.",
    "id": 626
  },
  {
    "title": "LoRA-Soup: Iterative Merging of Low-Rank Adaptations for Task-Agnostic Transfer Learning",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Hassan, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient method for adapting large language models to downstream tasks. While effective for single-task adaptation, the sequential application of LoRA modules leads to catastrophic forgetting in multi-task scenarios. We propose LoRA-Soup, an iterative weight-averaging approach that merges LoRA modules without requiring task-specific identifiers or joint training. Our method builds on the observation that LoRA updates lie in low-dimensional subspaces with favorable geometric properties. By carefully aligning these subspaces through a novel similarity metric based on principal angles, we achieve stable merging across diverse tasks. Experiments on GLUE and SuperGLUE benchmarks show modest improvements over sequential fine-tuning (average gain of 1.3%), with particular gains in few-shot settings. However, our approach slightly underperforms compared to task-parallel methods. Theoretical analysis reveals that merging success depends heavily on the alignment between LoRA subspaces, which may not hold for dissimilar tasks. While LoRA-Soup offers a practical compromise between efficiency and performance, its benefits are task-dependent and may be limited when task distributions are highly divergent.",
    "id": 629
  },
  {
    "title": "Gradient Surgery for Language Models: Reducing Catastrophic Forgetting with Parameter-Specific Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Continual learning in large language models remains challenging due to catastrophic forgetting when fine-tuning on new tasks. While recent methods focus on architectural modifications or explicit memory systems, we propose a simpler approach that dynamically adjusts learning rates for different parameter groups based on their sensitivity to task-specific gradients. Specifically, we compute gradient norms across mini-batches to identify parameters critical for previous tasks, then apply selective learning rate decay to maintain performance. Our method requires only minimal hyperparameter tuning and no additional memory beyond standard training. Experiments on GLUE tasks show a 12% improvement in average retention compared to standard fine-tuning when learning 5 sequential tasks, though performance lags behind more sophisticated continual learning baselines. Importantly, our approach maintains inference-time efficiency and can be integrated with existing pre-trained architectures without modification. While our gains are modest, the simplicity and practical benefits suggest value for resource-constrained deployments. Code and trained models will be released upon publication.",
    "id": 630
  },
  {
    "title": "Gradient Surgery Revisited: Why Existing Methods Overfit in Adaptive Federated Optimization",
    "authors": [
      "Chen, Z.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Federated Learning (FL) faces the challenge of client drift when local objectives diverge from the global objective. Recent gradient surgery methods attempt to address this by projecting conflicting gradients, but we show these approaches systematically overfit to local client distributions. Through theoretical analysis of the gradient projection geometry, we prove that existing methods implicitly amplify gradient directions aligned with local data manifolds, leading to worse generalization. We propose Federated Gradient Decorrelation (FGD), a simple modification that adds controlled noise to break this alignment. While FGD achieves competitive accuracy on standard benchmarks (82.3% on CIFAR-10 with 100 clients, vs 81.1% for FedAvg), we find the improvement diminishes as communication rounds increase. Our theoretical bounds suggest the method's benefit is fundamentally limited by client heterogeneity levels. Experiments on additional datasets confirm this limitation, with mixed results across tasks. Our analysis reveals fundamental tensions between gradient alignment and generalization in federated settings that current methods fail to resolve.",
    "id": 644
  },
  {
    "title": "LoRA-Drop: Adapting Low-Rank Adaptation via Dynamic Parameter Pruning for Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers may be suboptimal for downstream tasks. We propose LoRA-Drop, a simple yet effective approach that dynamically prunes LoRA parameters during fine-tuning based on gradient-based importance scores. Our method maintains the training-time efficiency of standard LoRA while achieving additional parameter reduction by up to 40% across GLUE tasks without significant performance degradation. We validate LoRA-Drop on RoBERTa-base and T5-base architectures, demonstrating comparable or marginally improved performance to full LoRA while using fewer parameters. However, our experiments reveal that the benefits of dynamic pruning diminish for larger models (\u22653B parameters), suggesting fundamental limitations in rank allocation heuristics. While LoRA-Drop provides practical improvements for resource-constrained deployments, our theoretical analysis indicates that the pruning strategy may be overly conservative for more complex adaptation scenarios. Code is available at anonymous-github.com/lora-drop.",
    "id": 653
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning Through Selective Gradient Dropping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that degrade performance across tasks. While recent gradient surgery methods like PCGrad and GradNorm address this by projecting conflicting gradients, they require careful tuning of projection thresholds and fail to leverage historical gradient information. We propose Gradient Surgery with Adaptive Memory (GSAM), a simple yet effective approach that maintains an adaptive memory bank of recent gradients to dynamically adjust surgery thresholds. The key insight is that gradient conflicts exhibit temporal patterns that can be exploited to reduce surgical interventions. Our method introduces a time-decayed memory mechanism that computes task-specific surgery probabilities based on historical gradient alignment. On standard multi-task benchmarks including NYUv2 and Cityscapes, GSAM achieves modest but consistent improvements over PCGrad (0.8-1.2% mIoU) with 15% fewer projection operations. While our method is theoretically justified and empirically validated, we acknowledge significant computational overhead (1.3x training time) and limited generalization to extreme task imbalance scenarios. The simplicity of our approach makes it a practical plug-and-play improvement over existing gradient surgery methods, though we recognize the gains may be insufficient for applications requiring aggressive gradient surgery.",
    "id": 659
  },
  {
    "title": "Improving Transformer Training with Cyclical Learning Rates and Gradient Clipping Thresholds",
    "authors": [
      "Chen, J.",
      "Liu, S.",
      "Mendoza, C.",
      "Thompson, K."
    ],
    "abstract": "Training large transformer models remains computationally expensive, often requiring extensive hyperparameter tuning. We propose a simple yet effective combination of cyclical learning rate schedules with adaptive gradient clipping thresholds to accelerate convergence and improve final performance. Our method automatically adjusts clipping thresholds based on the current learning rate and gradient norms, reducing the need for manual tuning. We evaluate our approach on language modeling tasks using GPT-2 architectures (125M-775M parameters) across three datasets: WikiText-103, OpenWebText, and a proprietary code corpus. Results show 8-15% faster convergence in wall-clock time compared to standard cosine decay schedules, with marginal improvements in perplexity (2-3% better on average). However, performance gains diminish at larger scales (1B+ parameters), suggesting our method's effectiveness is scale-dependent. While our approach is easy to implement and achieves consistent improvements in modest compute regimes, the theoretical underpinnings remain unclear, and benefits may not justify the additional complexity for practitioners with sufficient resources. Code is available at anonymous-url.github.io/ctransform.",
    "id": 661
  },
  {
    "title": "LoRA++: Adaptive Low-Rank Adaptation with Learnable Scaling Factors",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods have become essential for adapting large language models, with LoRA emerging as a popular choice due to its simplicity and effectiveness. However, LoRA uses a fixed scaling hyperparameter that remains constant across all layers and modules. We propose LoRA++, which introduces learnable scaling factors for each low-rank adapter module. Our method adds only 0.3% additional parameters compared to standard LoRA while enabling dynamic scaling during training. Experiments on GLUE and SuperGLUE benchmarks show modest improvements over LoRA (average 0.7% improvement on GLUE, 0.4% on SuperGLUE) across 6 tasks and 3 model sizes (125M to 7B parameters). We demonstrate that learned scaling factors correlate with layer-wise sensitivity, suggesting a form of implicit importance weighting. While LoRA++ provides consistent gains, the improvements are small and may not justify the added complexity for practitioners already satisfied with LoRA. Additionally, our method introduces extra hyperparameters that require careful tuning, potentially offsetting some practical benefits. Code and hyperparameter configurations are available at [anonymous-repository-link].",
    "id": 673
  },
  {
    "title": "Improving Few-Shot Generalization through Task-Agnostic Prompt Alignment",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Park, J."
    ],
    "abstract": "Large language models demonstrate impressive zero-shot capabilities, but their few-shot performance remains highly sensitive to prompt formatting. We propose Task-Agnostic Prompt Alignment (TAPA), a lightweight method that learns to reformat prompts without task-specific supervision. TAPA uses a meta-optimization objective that maximizes consistency of predictions across noisy paraphrases of the same prompt. Experiments on 12 few-shot benchmarks show 2-4% improvements over standard prompting on average, with particular gains on numerical reasoning tasks. However, we observe minimal benefits on classification tasks and negative transfer when prompts differ significantly from training seen during meta-learning. Our analysis reveals TAPA primarily learns to suppress spurious correlations introduced by formatting choices rather than discovering fundamentally better prompting strategies. While TAPA offers a practical improvement for few-shot learning at small computational cost, its limited scope and task-dependent effectiveness suggest the broader challenge of prompt optimization requires more sophisticated solutions.",
    "id": 676
  },
  {
    "title": "Gradient Entropy Regularization: A Lightweight Approach to Mitigating Memorization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, D."
    ],
    "abstract": "We propose gradient entropy regularization (GER), a simple regularization technique that encourages neural networks to maintain high entropy in their gradient distributions during training. By penalizing low-entropy gradient patterns, GER implicitly discourages memorization of specific training examples without requiring explicit data augmentation or architectural modifications. Our method adds minimal computational overhead (less than 3% increase in training time) and works as a drop-in replacement for standard regularizers. We evaluate GER on vision and language tasks, demonstrating 2-5% improvements in memorization metrics while maintaining comparable test accuracy on CIFAR-10, CIFAR-100, and SST-2. However, we observe diminishing returns on larger datasets and architectures. Theoretical analysis reveals GER approximately minimizes a bound on memorization capacity, though the connection becomes weaker for very deep networks. While GER shows promise for privacy-sensitive applications, our experiments are limited to medium-scale benchmarks, and we acknowledge potential confounds with existing implicit regularization effects.",
    "id": 679
  },
  {
    "title": "Sharpness-Aware Minimization with Adaptive Gradient Clipping for Improved Generalization in Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We propose SAM-AGC, a simple modification to Sharpness-Aware Minimization (SAM) that incorporates adaptive gradient clipping based on the sharpness of the loss landscape. While SAM has shown promise for improving generalization in small-scale vision tasks, its effectiveness on large language models remains inconsistent. Our key insight is that the aggressive updates in SAM can destabilize training in transformers, particularly when gradients become large. SAM-AGC addresses this by clipping gradients adaptively based on their alignment with the sharpness direction. We evaluate our method on the GLUE benchmark using BERT-base and RoBERTa-base, achieving average improvements of 1.2% over SAM and 2.3% over standard SGD with momentum. Despite these gains, we observe that SAM-AGC's benefits diminish as model size increases\u2014we find no consistent improvements on GPT-2 medium variants. Through extensive ablations, we identify that our method works best on tasks with limited training data and moderate model complexity. While our results are promising, they are limited to encoder-only architectures and standard classification tasks. Our code and trained models are available at [URL anonymized].",
    "id": 680
  },
  {
    "title": "Improved Training of Gaussian Splatting Models via Signed Distance Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "3D Gaussian Splatting has emerged as a promising alternative to neural radiance fields for novel view synthesis, but struggles with artifacts near thin structures and unobserved regions. We propose adding a signed distance function (SDF) regularizer to the standard photometric loss, encouraging Gaussians to align with implicit surfaces. Our method alternates between optimizing Gaussian parameters and fitting an SDF using predicted depths from the current set of Gaussians. On standard benchmarks, this approach reduces floaters by 15% and improves reconstruction quality on thin objects, while introducing minimal computational overhead. However, we find the regularization can overly constrain the optimization, sometimes degrading results in textureless regions. Experiments on synthetic and real datasets demonstrate modest improvements over vanilla Gaussian Splatting (PSNR+0.2 on average), with the greatest gains observed in scenes with well-defined geometry. Limitations include sensitivity to hyperparameter tuning and increased training time. While the contribution is incremental rather than transformative, our approach provides a lightweight extension that practitioners may find useful for improving reconstruction quality in geometrically structured scenes.",
    "id": 681
  },
  {
    "title": "BatchNorm in Disguise: Revisiting Normalization Through the Lens of Parameter Scaling",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Thompson, K."
    ],
    "abstract": "We investigate an overlooked connection between parameter initialization scales and implicit normalization in deep networks. While Batch Normalization (BN) is widely believed to improve optimization through reduced internal covariate shift, we demonstrate that networks trained with specific parameter scaling schemes can achieve similar training dynamics without explicit normalization layers. Our approach, dubbed ScaleNorm, introduces learnable scaling parameters at initialization that preserve the first and second moments of activations throughout training, effectively mimicking BN's stabilizing effects. Through experiments on CIFAR-10/100 and ImageNet, ScaleNorm achieves 91.2% and 70.8% accuracy respectively, comparable to BN baselines (91.5%/71.2%) while reducing memory overhead by 8-12%. However, we observe that performance degrades on deeper architectures (>50 layers) and fails to match BN's robustness to hyperparameter choices. Our theoretical analysis reveals that ScaleNorm approximates BN only under restrictive assumptions about weight orthogonality and learning rates that rarely hold in practice. While ScaleNorm provides a lightweight alternative for specific architectures, it does not fully replace BN's benefits, suggesting that the normalization debate requires more nuanced understanding of training dynamics.",
    "id": 686
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance and training instability. While recent gradient surgery methods like PCGrad and GradDrop effectively mitigate gradient conflict, they rely on heuristic hyperparameters that require extensive tuning across domains. We propose AdaGS (Adaptive Gradient Surgery), a lightweight method that uses momentum-based memory banks to dynamically adjust gradient projection thresholds based on task similarity. Our approach maintains an exponential moving average of per-task gradient norms, enabling online estimation of conflict severity without additional hyperparameters. Experiments on three benchmarks (CIFAR-10/SVHN multi-task, NYUv2, and Taskonomy) show modest improvements over baselines (1-2% accuracy gains), with particularly strong results when tasks have varying difficulty levels. However, we find that our method's effectiveness diminishes on tasks with similar loss scales, suggesting residual gradient interference remains. While AdaGS offers a practical improvement over fixed-threshold approaches, our theoretical analysis reveals the method can still produce suboptimal gradient directions in certain parameter regimes. Code and trained models will be released upon acceptance.",
    "id": 696
  },
  {
    "title": "LoRA-CD: Low-Rank Adaptation with Cyclical Dropout for Memory-Efficient Fine-Tuning",
    "authors": [
      "Chen, Y.",
      "Rodriguez, A.",
      "Kim, J.",
      "Thompson, L."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become standard for adapting large language models, but their practical memory savings during training remain limited by gradient storage. We propose LoRA-CD, which combines low-rank adaptation with cyclical dropout patterns to reduce GPU memory requirements. Our approach applies structured dropout to LoRA matrices on a fixed schedule, removing 30-50\\% of the adapters during training while maintaining gradient flow through remaining parameters. The key insight is that low-rank decompositions naturally permit partial participation without catastrophic gradient cancellation. On GLUE and SuperGLUE benchmarks, LoRA-CD achieves 1.4\u00d7 peak memory reduction compared to standard LoRA while maintaining within 1.2\\% of full model performance. The method shows particular effectiveness on larger models (7B+ parameters) with diminishing returns on smaller architectures. While our theoretical analysis of convergence properties remains incomplete, empirical results demonstrate consistent gains across model scales. The magnitude of improvement is modest relative to implementation complexity, though potentially significant for memory-constrained deployment scenarios. Code and pre-trained adapters will be released.",
    "id": 698
  },
  {
    "title": "Gradient Descent with Learnable Step Sizes: An Empirical Study on Neural Network Training Dynamics",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Rodriguez, S."
    ],
    "abstract": "While adaptive optimization methods like Adam and RMSprop are widely used for training neural networks, their prescribed step size schedules often rely on default hyperparameters that may not be optimal for specific architectures. We propose a simple meta-learning approach that learns a single scalar step size for each layer via backpropagation through training iterations. Our method adds minimal computational overhead (<1% increase in training time) and can be implemented in 20 lines of PyTorch code. Experiments on CIFAR-10 and ImageNet with ResNet-18/50 show modest but consistent improvements (+0.5-1.2% accuracy) over tuned baselines, particularly beneficial when using smaller batch sizes. However, our approach does not outperform carefully hand-tuned learning rate schedules for large-batch training, and we observe instability when applying it to transformer architectures. Ablations suggest that the benefits are primarily due to better early-training optimization rather than any fundamentally different training dynamics. While we establish some theoretical properties for convex quadratic objectives, our analysis does not extend to the non-convex setting. Our code and learned step size schedules are available at [anonymous link].",
    "id": 711
  },
  {
    "title": "Gradient Descent with Layer-wise Learning Rates for Transformer Fine-tuning",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Garcia, M."
    ],
    "abstract": "We propose Layer-Adaptive Gradient Descent (LAGD), a simple modification to standard gradient descent that assigns different learning rates to transformer layers based on their gradient norms. Building on observations that lower layers often have smaller gradients during fine-tuning, we introduce a lightweight heuristic that sets layer-wise learning rates proportional to the inverse of median gradient norms from the first 10 training steps. Experiments on GLUE tasks using RoBERTa-base show LAGD achieves average 1.2% improvement over AdamW with comparable hyperparameter tuning effort, while reducing memory usage by 25% by avoiding adaptive optimizer states. However, gains diminish with larger models (\u22640.5% for RoBERTa-large) and are inconsistent across tasks (-0.8% to +2.9% range). Analysis reveals improvements correlate with tasks where gradient norms vary significantly across layers (Pearson r=0.67). While our method is easy to implement and offers modest efficiency gains, limited theoretical justification and diminishing returns at scale suggest careful consideration is needed before adoption. Code is available at anonymous-link.",
    "id": 727
  },
  {
    "title": "Revisiting Weight Decay Through the Lens of Neural Tangent Kernel Evolution",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Thompson, J."
    ],
    "abstract": "Weight decay is a fundamental component of modern neural network training, yet its interaction with the evolving neural tangent kernel (NTK) during optimization remains poorly understood. We provide an empirical analysis of how \u21132 regularization alters the NTK trajectory in overparameterized networks trained with gradient descent. By tracking the kernel's spectral properties throughout training, we observe that weight decay induces a phase transition in the NTK dynamics, transitioning from rapid initial evolution to slower, quasi-static behavior. Our theoretical analysis characterizes this transition for two-layer ReLU networks, deriving bounds on the regularization strength that preserve kernel stability while maintaining generalization benefits. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate that our theoretical predictions correlate moderately with observed generalization gaps (R\u00b2=0.72), though we note deviations for larger architectures. While our results offer fresh perspective on weight decay's implicit bias, we acknowledge limitations in extending our analysis to deeper networks and more complex optimizers. We release our code and training checkpoints to facilitate reproducibility.",
    "id": 728
  },
  {
    "title": "Revisiting Weight Decay Through the Lens of Sharpeness-Aware Gradient Alignment",
    "authors": [
      "Chen, L.",
      "Vaswani, A.",
      "Rodriguez, M."
    ],
    "abstract": "Recent work has shown that weight decay implicitly regularizes neural networks by reducing sharpness of the loss landscape, but the theoretical understanding of this phenomenon remains limited. We propose Gradient Alignment Regularization (GAR), a simple modification to standard weight decay that explicitly aligns parameter updates with directions of minimal sharpness. By introducing a computationally efficient approximation based on per-layer gradient covariance, GAR achieves comparable regularization to SAM while requiring 40% fewer gradient computations. Experiments on CIFAR-10/100 show consistent improvements over weight decay baselines (1.2-2.3% accuracy gains), particularly for overparameterized ResNet architectures. However, benefits are less pronounced on larger datasets like ImageNet, where we observe diminishing returns (0.3% improvement over tuned baselines). Our theoretical analysis reveals GAR minimizes an upper bound on generalization error under uniform stability assumptions, though the bound depends on restrictive Lipschitz constants that may not hold in practice. While GAR demonstrates promising regularization effects for small to medium-scale vision tasks, questions remain about scalability to large language models where compute overhead becomes significant.",
    "id": 729
  },
  {
    "title": "Improving Transformer Efficiency via Self-Attention Pruning with Learned Gating Mechanisms",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, T."
    ],
    "abstract": "Transformer models achieve state-of-the-art results across many tasks but suffer from quadratic complexity in sequence length due to their self-attention mechanism. While recent pruning methods reduce computational cost through fixed sparsity patterns, they often degrade performance on long-context tasks. We propose Dynamic Attention Gates (DAG), a novel approach that learns to selectively disable attention heads and positions during inference based on input-dependent gating. Our method uses lightweight MLP controllers that predict pruning decisions for each attention head and token pair using only local context, avoiding the need for full-attention computations. We evaluate DAG on language modeling and machine translation benchmarks, achieving 1.4x speedup at similar perplexity compared to dense baselines on sequences up to 4K tokens. Notably, DAG maintains within 0.5 BLEU score of full transformers on WMT'14 En-De while reducing FLOPs by 30%. However, we observe performance degradation on tasks requiring fine-grained cross-attention patterns. Our ablation study reveals the trade-off between pruning aggressiveness and task-specific accuracy, suggesting that learned gating may over-prune long-range dependencies. While DAG provides practical efficiency gains, its benefits are most pronounced for moderate sequence lengths where partial attention maintains sufficient context.",
    "id": 733
  },
  {
    "title": "LoRA-RT: Low-Rank Adaptation with Runtime Thresholding for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA-RT, a simple extension to Low-Rank Adaptation (LoRA) that introduces dynamic thresholding of adapter weights during training. While LoRA has enabled parameter-efficient fine-tuning by learning low-rank updates to pre-trained models, we observe that many learned adapter weights remain close to zero across different tasks, suggesting redundancy. Our method adds a learnable threshold parameter that dynamically masks negligible weight updates during training, effectively pruning the adapter while maintaining performance. We evaluate LoRA-RT on GLUE and SuperGLUE benchmarks using RoBERTa-base and Llama-2-7B, achieving comparable task performance to standard LoRA while reducing the number of active adapter parameters by 15-35%. However, our approach introduces an additional hyperparameter and shows mixed results on generative tasks, with some degradation on longer sequence generation. While LoRA-RT demonstrates promise for reducing adapter storage costs, we acknowledge that our gains are incremental and primarily benefit deployment scenarios with strict memory constraints. Code will be made available upon acceptance.",
    "id": 744
  },
  {
    "title": "Improving Neural ODE Stability Through Adaptive Checkpoint Intervals",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Continuous-depth neural networks have shown promise for modeling physical systems, but suffer from numerical instability during training when integration paths become stiff. We propose an adaptive checkpoint interval strategy that dynamically adjusts the ODE solver's step size based on local curvature estimates, particularly addressing the vanishing gradient problem inherent in naive implementations. Our method introduces a lightweight gating mechanism that predicts optimal integration intervals without significant computational overhead. On standard benchmarks including CIFAR-10 and sequential MNIST, we achieve marginally better accuracy (0.7% improvement) compared to fixed-interval Neural ODE baselines while reducing training wall-clock time by 12%. While our theoretical analysis provides stability guarantees only for Lipschitz-constrained architectures, empirical results suggest broader applicability. The primary contribution is a practical training stabilization technique that maintains the continuous-depth formulation while addressing some convergence issues, though questions remain about scalability to state-of-the-art vision architectures. Code and pretrained models will be made available upon acceptance.",
    "id": 749
  },
  {
    "title": "Memory-Efficient Training of Transformers via Structured Sketching of Attention Matrices",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose SketchAttention, a training method that reduces the memory footprint of self-attention in Transformers by projecting attention matrices into low-dimensional sketches. Our approach combines recent work on linear attention with CountSketch-based dimensionality reduction, achieving sub-quadratic memory complexity in sequence length. While previous linear attention mechanisms suffer from accuracy degradation on complex reasoning tasks, we introduce a learnable sketching operator that adapts to the data distribution during training. Experiments on standard NLP benchmarks show modest improvements over vanilla linear attention (2-3% absolute improvements on GLUE), with memory savings comparable to other efficient attention methods. However, we observe that our method underperforms full quadratic attention on tasks requiring fine-grained reasoning, particularly on long-context datasets like TriviaQA. Theoretical analysis reveals that our sketching approach preserves attention probabilities up to a multiplicative error bound, though this bound becomes loose for heavily skewed attention patterns. While SketchAttention provides practical memory benefits for training large models on consumer GPUs, its trade-offs between efficiency and accuracy may limit adoption for applications where small accuracy differences are critical.",
    "id": 750
  },
  {
    "title": "LoRA-Detox: Parameter-Efficient Fine-Tuning for Bias Mitigation in Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We study whether Low-Rank Adaptation (LoRA), a popular parameter-efficient fine-tuning method for large language models (LLMs), can effectively reduce social biases without full model retraining. Motivated by the computational infeasibility of retraining LLMs at scale, we investigate if injecting low-rank bias correction matrices during fine-tuning suffices to mitigate harmful outputs. Our method adds rank-16 LoRA modules trained on a carefully curated dataset of biased prompts and safe completions, optimizing a combination of standard language modeling loss and bias reduction objectives. Experiments on Bias Benchmark for QA (BBQ) and RealToxicityPrompts show modest but statistically significant improvements: 8-12% reduction in biased responses across three model families (7B-13B parameters) with negligible impact on downstream task performance. However, we find the approach less effective on intersectional biases and observe performance degradation on specific knowledge-intensive benchmarks. While LoRA-Detox offers a lightweight alternative to expensive bias mitigation pipelines, our results suggest fundamental limitations in addressing complex biases through low-rank interventions alone.",
    "id": 762
  },
  {
    "title": "Improving Transformer Efficiency via Hierarchical Token Pruning with Learnable Retention Thresholds",
    "authors": [
      "Chen, L.",
      "Santos, J.",
      "Kumar, V."
    ],
    "abstract": "Self-attention mechanisms in Transformers exhibit quadratic complexity with respect to sequence length, creating computational bottlenecks for long sequences. While previous work has explored static token pruning approaches, we propose Hierarchical Adaptive Token Selection (HATS), which employs learnable retention thresholds that adaptively determine token importance across different model layers. Our method combines a lightweight gating mechanism with hierarchical pruning decisions, enabling dynamic computation allocation based on input complexity. We evaluate HATS on machine translation and language modeling tasks using WMT'14 and Wikitext-103 benchmarks. Experiments show 1.7x speedup during inference with less than 0.5 BLEU or perplexity degradation compared to full attention baselines. However, we observe performance drops on tasks requiring fine-grained reasoning over long contexts. Our analysis reveals that HATS excels at pruning redundant tokens in high-resource settings but struggles with low-resource domains. Code and trained models will be made available upon publication.",
    "id": 776
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning with Task-Specific Noise Adaptation",
    "authors": [
      "Liu, C.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder performance on individual tasks. While existing gradient surgery methods address gradient conflicts through projection schemes, we observe that these approaches neglect the inherent noise characteristics unique to each task. We propose Noise-Adaptive Gradient Surgery (NAGS), which incorporates task-specific noise estimates into the gradient conflict resolution process. Our method computes per-task gradient noise variance using a simple moving average of recent gradients, then weights the projection operation to preserve more reliable gradient directions. Experiments on CIFAR-100/SVHN supervised multi-task and NYUv2 scene understanding benchmarks show 2-3% improvements over standard gradient surgery baselines, with particular gains on harder tasks. However, we find that NAGS provides diminishing returns when task gradients are already well-aligned and adds non-trivial computational overhead (15-20% training slowdown). Our theoretical analysis suggests the noise-based weighting scheme converges to a stationary point under standard assumptions, though the convergence rate depends on unknown noise parameters. While our approach shows promise for scenarios with significant gradient conflicts, the practical benefits appear limited to specific task combinations, raising questions about the broader applicability of noise-aware gradient manipulation.",
    "id": 797
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Autoencoders via Continuous Relaxation Tricks",
    "authors": [
      "Liu, Q.",
      "Johnson, K.",
      "Rodriguez, A."
    ],
    "abstract": "Training discrete variational autoencoders (VAEs) remains challenging due to high-variance gradient estimators of the evidence lower bound (ELBO). While continuous relaxations such as Gumbel-Softmax and straight-through estimators provide biased but low-variance gradients, their combination has not been systematically studied. We propose Reparameterized Straight-Through (RST), a simple hybrid approach that reparameterizes the relaxed distribution during the forward pass but applies a modified straight-through estimator during backpropagation. Our method combines the low variance of continuous relaxations with the controlled bias of straight-through estimators. Experiments on binarized MNIST and language modeling with Penn Treebank show 2-5% improvement in ELBO over Gumbel-Softmax baselines, with particular gains at low temperatures. However, we observe minimal improvement on larger-scale tasks like ImageNet generation. Theoretical analysis suggests our bias term scales with the relaxation temperature, providing a trade-off between bias and variance. While RST offers modest improvements and is easy to implement, its benefits appear limited to scenarios with high-dimensional discrete latents at moderate computational budgets.",
    "id": 807
  },
  {
    "title": "Gradient Descent with Dynamic Learning Rate Scaling: A Simple Heuristic for Faster Convergence",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "We propose Dynamic Learning Rate Scaling (DLRS), a lightweight modification to standard gradient descent that adaptively scales the learning rate based on the relative magnitude of past gradients. Unlike complex adaptive optimizers such as Adam or LAMB, DLRS introduces minimal computational overhead by using a simple exponentially weighted ratio of gradient norms. Our theoretical analysis shows DLRS achieves comparable convergence rates to vanilla SGD on strongly convex functions while requiring no hyperparameter tuning beyond the base learning rate. Empirically, we demonstrate 10-25% speedup in wall-clock time over SGD with momentum on ResNet-50 training on ImageNet and modest improvements on transformer fine-tuning tasks. However, our gains diminish on extremely large batch training and are inconsistent across different network architectures. While DLRS provides a practical alternative to manual learning rate tuning for practitioners, our results suggest the improvements are incremental rather than transformative. Code is available at [anonymized for review].",
    "id": 808
  },
  {
    "title": "Gradient Descent with Momentum Works Even When the Momentum Isn't Optimal: A Non-Asymptotic Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We provide a non-asymptotic convergence analysis for gradient descent with Polyak momentum for strongly convex and smooth objectives. While classical analysis requires the momentum parameter to be set optimally, we show that convergence holds for a broader range of sub-optimal momentum values. Specifically, we prove that any momentum parameter in [1/4, 3/4] achieves \u0398(\u03ba log(1/\u03b5)) convergence rate, matching the optimal rate up to constant factors. Our analysis relies on a novel Lyapunov function that captures the dynamics when momentum is not tuned according to the condition number \u03ba. Empirically, we demonstrate that commonly used heuristic momentum schedules in practice fall within our theoretically justified range. While our contributions are technically correct, they serve primarily as a refinement of known results rather than addressing fundamentally new questions in optimization theory.",
    "id": 815
  },
  {
    "title": "Gradient Surgery with Memory: Improving Stability of Multi-Task Learning via Selective Gradient Dampening",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to destructive interference between tasks. While existing gradient surgery methods like PCGrad and GradDrop mitigate this issue, they can over-aggressively drop gradient information, harming learning dynamics. We propose MemGS (Memory-aware Gradient Surgery), which introduces a learnable memory buffer to selectively dampen gradient updates based on their historical alignment patterns. Our approach maintains a running estimate of per-task gradient directions over the past k iterations, then applies dampening factors that preserve aligned components while selectively reducing conflicting ones. We theoretically prove MemGS converges under mild assumptions and show empirically that it achieves better trade-offs between task interference and gradient information preservation. On three standard multi-task benchmarks (NYUv2, CityScapes, and QM9), MemGS outperforms PCGrad by 1.2-2.8% on average while using 15% fewer training iterations. However, we observe these gains diminish in scenarios with high task imbalance, suggesting the method's effectiveness depends on task distribution. Our results indicate MemGS provides marginal improvements over carefully tuned baselines, highlighting the challenge of general gradient surgery methods.",
    "id": 819
  },
  {
    "title": "Revisiting Gradient Accumulation for Memory-Efficient Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "Transformer models have become increasingly memory-intensive, limiting training on consumer GPUs. While gradient accumulation is widely used to simulate larger batch sizes, we provide theoretical and empirical evidence that the standard implementation introduces subtle optimization artifacts that degrade convergence quality, particularly when accumulation steps exceed 16. We propose Fractal Accumulation, a simple modification that maintains the memory efficiency of gradient accumulation while reducing these artifacts through recursive micro-batch normalization. Our experiments on ViT models show 2-4% absolute improvement in final validation accuracy compared to standard accumulation across ImageNet classification and COCO detection tasks. However, the benefits appear limited to models with >50M parameters, and we observe minimal improvements on smaller architectures. Additionally, our method incurs 10-15% computational overhead due to repeated forward passes. While Fractal Accumulation offers modest but consistent improvements for large-scale transformer training, the narrow applicability and computational cost may limit practical adoption.",
    "id": 829
  },
  {
    "title": "Gradient Surgery: A Simple Heuristic for Improving Multi-Task Optimization in Low-Resource Settings",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Multi-task learning often struggles with conflicting gradients that can lead to suboptimal performance across tasks. While recent approaches like PCGrad and GradNorm show promise in high-resource settings, they require significant computational overhead and hyperparameter tuning that limits their applicability. We propose Gradient Surgery (GradSurg), a surprisingly simple heuristic that detects gradient conflicts through cosine similarity and applies selective gradient scaling. Our method requires only one additional hyperparameter and adds negligible computational cost. We evaluate GradSurg on four multi-task benchmarks spanning NLP and vision domains. Results show modest improvements over standard multi-task training (2-4% average task improvement), though gains are inconsistent across datasets. Interestingly, GradSurg performs comparably to more complex baselines while requiring 10x less compute. However, we acknowledge that the method fails to improve performance when task gradients are naturally aligned. Ablation studies reveal that the heuristic's effectiveness is highly sensitive to the similarity threshold. While GradSurg won't revolutionize multi-task learning, it provides a practical baseline for resource-constrained researchers. Code is available at anonymous.url.",
    "id": 832
  },
  {
    "title": "Gradient Descent with Adaptive Step-Size Based on Local Gradient Variance",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose GRADVAR, an optimization method that adapts step-sizes based on local estimates of gradient variance for stochastic gradient descent. Motivated by the observation that gradient variance often correlates with optimization progress, our method computes a running estimate of variance using a small window of recent gradients and scales the step-size inversely proportional to this estimate. We provide theoretical analysis showing convergence under assumptions of Lipschitz continuity and bounded variance, achieving a convergence rate of O(1/\u221aT) for non-convex objectives. Experiments on CIFAR-10 and ImageNet with ResNet architectures demonstrate 5-15% faster convergence compared to Adam and SGD with momentum in early training stages, while performing comparably in final accuracy. However, we observe sensitivity to hyperparameter choices and minor degradation on language modeling tasks. GRADVAR offers a lightweight alternative to adaptive methods without requiring momentum buffers or second-moment estimates, though benefits appear limited to vision tasks with specific architectures. Code is available at anonymous-url.github.io/gradvar.",
    "id": 837
  },
  {
    "title": "LoRA-MoE: Parameter-Efficient Experts via Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA-MoE, a method to reduce the parameter overhead of Mixture of Experts (MoE) models by applying Low-Rank Adaptation (LoRA) to individual experts. While MoE architectures enable scaling language models efficiently, their memory footprint grows linearly with the number of experts. Our approach decomposes each expert's weight updates into low-rank matrices, reducing storage by up to 95% while maintaining expert diversity. We evaluate LoRA-MoE on language modeling and GLUE tasks using models with 8-64 experts. Results show modest improvements over standard LoRA (2-3% better perplexity) while using significantly fewer parameters than full-rank experts. However, we observe that performance saturates beyond 16 experts, suggesting that low-rank constraints limit capacity. Our theoretical analysis reveals a trade-off between expert efficiency and expressiveness, with rank-r updates capturing only O(r) modes per expert. Though LoRA-MoE provides memory savings for deployment, the computational overhead during training due to router complexity remains problematic. Code is available at anonymous-url.",
    "id": 838
  },
  {
    "title": "Improved Gradient Clipping for Noisy Optimization Landscapes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient clipping is a widely-used heuristic for stabilizing training of neural networks, particularly in language modeling and reinforcement learning applications. While commonly viewed as a practical trick, we provide theoretical and empirical analysis showing that adaptive gradient clipping can provably improve convergence in optimization landscapes with heavy-tailed gradient noise. We introduce ContourClip, a simple variant that scales the clipping threshold based on gradient history rather than using a fixed value. Our method achieves marginally better wall-clock time than standard clipping without introducing significant computational overhead. Experiments on language modeling (IWSLT14 De-En and Wikitext-103) demonstrate 2-3% BLEU/perplexity improvements over baselines, though gains diminish on larger-scale pretraining tasks. Theoretical results establish convergence rates under the \u03bc-Lojasiewicz condition, extending previous work that assumed bounded gradients. Code is available at [redacted], but reproduction requires careful hyperparameter tuning. While our contribution is incremental and specific to gradient noise regimes, these findings may inform future adaptive optimizers for noisy training settings.",
    "id": 841
  },
  {
    "title": "Gradient Surgery for Partially Observed Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We study gradient-based training in neural networks where only a subset of parameters is observable or trainable during optimization, a common scenario in transfer learning and model editing. We propose Gradient Surgery, a simple plug-and-play method that modifies backpropagated gradients by projecting them onto the subspace orthogonal to the frozen parameter directions. While this projection theoretically preserves the gradient flow through the remaining weights, we show this can lead to suboptimal convergence when the frozen and trainable subspaces are strongly coupled. We introduce a soft projection variant using learned interpolation coefficients that adaptively trades off between strict gradient orthogonality and training dynamics. Empirical results on CIFAR-10 and ImageNet transfer tasks demonstrate modest improvements (1-2% accuracy) over naive fine-tuning in low-data regimes, though gains diminish with larger datasets. Our method requires minimal computational overhead and applies to any architecture, but exhibits sensitivity to hyperparameter choices and dataset characteristics. While our theoretical analysis provides some convergence guarantees under strong assumptions, significant gaps remain between theory and practice. This work provides a practical tool for constrained optimization settings but leaves open questions about when gradient surgery is most beneficial.",
    "id": 843
  },
  {
    "title": "Decoupled Momentum: A Simpler Approach to Adaptive Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We present Decoupled Momentum (DeMo), a lightweight modification to standard momentum-based optimizers that separates the momentum accumulation from the parameter update step. Unlike adaptive methods such as Adam or RMSprop, DeMo maintains the computational simplicity of SGD with momentum while providing some benefits of adaptive learning rates. Our key observation is that by decoupling these operations, we can apply the momentum update to a transformed parameter space without the complexity of maintaining per-parameter second moment estimates. We evaluate DeMo on image classification tasks using ResNet architectures, showing modest but consistent improvements over SGD with momentum on CIFAR-10 (0.8% accuracy gain) and ImageNet (0.3% top-1 improvement). While these improvements are smaller than those achieved by full adaptive methods, DeMo requires minimal implementation changes and no additional hyperparameters beyond standard momentum. Theoretical analysis shows DeMo converges at the same rate as SGD for convex problems. Our experiments suggest DeMo may be most beneficial in resource-constrained settings where adaptive methods are prohibitive, though larger-scale evaluation is needed to quantify its practical utility.",
    "id": 849
  },
  {
    "title": "LoRA-Plus: An Improved Low-Rank Adaptation Method with Gradient-Based Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, J.H."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular approach for efficiently fine-tuning large language models, but the choice of rank remains largely heuristic. We propose LoRA-Plus, a simple extension that automatically selects rank values by monitoring gradient norms during the initial training phase. Our method maintains LoRA's parameter efficiency while adapts the rank for different layers based on observed gradient statistics. Experiments on the GLUE benchmark show 2-3% improvements over LoRA with fixed ranks across BERT-base, RoBERTa-large, and GPT-2 models. However, these gains diminish on larger models (\u22651B parameters), and we observe instability when ranks are adjusted too aggressively. While LoRA-Plus offers practical benefits for moderate-sized models, our theoretical analysis reveals that the gradient-based selection criterion lacks formal guarantees, and the method introduces additional hyperparameters that may limit its applicability. Code and pre-trained adapters are available at [anonymous URL].",
    "id": 858
  },
  {
    "title": "Sketch-to-Image Synthesis via Frequency-Aware Progressive Distillation",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a simple yet effective approach for sketch-to-image synthesis that leverages frequency-aware progressive distillation. Our method decomposes the generation process into sequential stages, where high-frequency sketch edges are first translated into low-frequency image structure before progressively adding fine-grained details. We introduce a lightweight frequency-based attention mechanism that operates at multiple scales, enabling more faithful rendering of sketch geometry compared to standard diffusion models. While our approach achieves competitive FID scores on the SketchyCOCO benchmark (FID 28.4 vs. 26.7 for the previous best), we observe that our model particularly excels on human-drawn sketches with simple line styles. Our primary contribution lies in demonstrating that frequency-space conditioning can provide a computationally efficient alternative to full diffusion models for sketch-guided generation. However, we acknowledge that our method struggles with complex scene compositions and unusual texturing. Experiments on three sketch datasets demonstrate 15-20% improvement in user preference for sketch fidelity, though generalization to out-of-domain sketches remains limited. Code and models will be released upon acceptance.",
    "id": 862
  },
  {
    "title": "Gradient Surgery in Stochastic Training: When Less Intervention Improves Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Singh, K."
    ],
    "abstract": "Gradient surgery techniques that modify per-sample gradients during training have gained popularity for mitigating memorization and improving generalization in deep learning. We revisit these methods through the lens of implicit regularization and propose a minimalist variant that selectively intervenes only on gradients with largest per-sample norm ratio. Unlike existing approaches that perform surgery on every sample, our method preserves statistical properties of stochastic gradients while still achieving regularization. We provide theoretical analysis showing our approach minimizes a modified loss that includes an adaptive regularizer, and empirically demonstrate improvements over baseline training on CIFAR-10 and Tiny-ImageNet. However, gains are inconsistent across architectures (ResNet vs Vision Transformer) and can be negative on some datasets (CIFAR-100). Our ablation studies reveal that performance improvements correlate strongly with the proportion of high-norm gradients in the dataset, suggesting the technique's benefits may be problem-dependent. Code and experiments are reproducible with less than 5 GPU days on a single A100.",
    "id": 866
  },
  {
    "title": "Gradient Descent with Momentum: A New Perspective Through Continuous-Time Limits",
    "authors": [
      "Chen, Z.",
      "Liu, Q.",
      "Rodriguez, A."
    ],
    "abstract": "We revisit the classical momentum method from a continuous-time perspective, deriving a modified differential equation that captures the subtle effects of discrete-step updates often neglected in standard analyses. Our approach yields an implicit regularization term proportional to the learning rate and momentum parameter, providing novel insights into why momentum accelerates convergence in certain regimes but may hurt performance in others. We establish finite-time convergence guarantees for our continuous-time limit that match known rates up to constant factors, and validate our theoretical findings on a suite of benchmark optimization problems including synthetic quadratics, MNIST, and CIFAR-10. While our analysis provides a fresh perspective on momentum, we acknowledge that the practical implications remain incremental\u2014our theoretical bounds do not significantly improve upon existing results, and the empirical gains over standard baselines are modest (typically 2-5% improvement in wall-clock time). Nevertheless, our framework offers a principled way to understand momentum's behavior and suggests potential extensions to adaptive methods, which we leave for future work.",
    "id": 871
  },
  {
    "title": "Gradient Forking: A Lightweight Alternative to Ensemble Training via Parameter Trajectory Splitting",
    "authors": [
      "Liu, T.",
      "Chen, K.",
      "Rodriguez, J."
    ],
    "abstract": "Deep ensembles achieve state-of-the-art uncertainty calibration but require training multiple independent models, incurring significant computational overhead. We propose gradient forking, a simple technique that creates multiple model variants by splitting parameter trajectories during a single training run. After training for $T/2$ iterations on the standard objective, we create $K$ forked replicas with small random perturbations of the current parameters, each continuing training on a slightly perturbed loss landscape for the remaining $T/2$ iterations. Despite being trained for only half the compute budget of $K$ independent models, we show that gradient-forked ensembles achieve comparable accuracy to standard ensembles on CIFAR-10/100 and ImageNet, while improving uncertainty calibration on out-of-distribution data by 8-15%. We provide theoretical analysis showing that gradient forking increases functional diversity by exploiting the locally ergodic nature of SGD in the later stages of training. However, we find the approach is sensitive to the timing of the fork and performs poorly when forks occur too early. While gradient forking offers a practical trade-off between ensemble quality and training cost, its benefits diminish on larger models and datasets, limiting its applicability to state-of-the-art systems.",
    "id": 875
  },
  {
    "title": "Gradient Compression via Adaptive Structured Pruning for Communication-Efficient Distributed Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Liu, J."
    ],
    "abstract": "Communication bottlenecks severely limit the scalability of distributed machine learning, particularly in bandwidth-constrained environments. While gradient compression techniques like quantization and sparsification reduce communication overhead, they often struggle to maintain model performance across diverse architectures and datasets. We propose Adaptive Structured Pruning for Gradients (ASPG), a method that dynamically compresses gradients by learning to prune structured blocks (e.g., channels, attention heads) based on their estimated impact on convergence. ASPG uses a lightweight controller network trained alongside the main model to predict optimal pruning ratios for different architectural components. On ImageNet with ResNet-50, ASPG achieves 25\u00d7 gradient compression with only 0.8% top-1 accuracy degradation, outperforming existing methods by 2-3\u00d7 in compression-to-accuracy tradeoffs. However, our experiments reveal the controller adds 15% training overhead and unstable convergence on certain architectures like Vision Transformers. Theoretical analysis suggests our pruning strategy satisfies a relaxed version of gradient unbiasedness under assumptions that may not hold for all optimizers. While ASPG demonstrates promising empirical results, we acknowledge limitations in theoretical guarantees and generalization to emerging architectures.",
    "id": 892
  },
  {
    "title": "Adaptive Gradient Norm Clipping Can Improve Transformer Training (Sometimes: A Large-Scale Empirical Study",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Chen, J.",
      "Thompson, L."
    ],
    "abstract": "Gradient clipping is widely used in training large language models, but its impact on final performance remains poorly understood. We conduct a large-scale empirical study of adaptive gradient norm clipping across 50 Transformer variants (350M-7B parameters) on diverse NLP tasks including language modeling, summarization, and code generation. Our adaptive method adjusts clipping thresholds based on gradient statistics, achieving 2-3% relative improvements in perplexity on 40% of tasks. However, we observe that benefits are concentrated in high-curvature regimes: when validation loss exhibits high variance during training (\u03c3 > 0.05), adaptive clipping outperforms fixed thresholds 68% of the time; otherwise, results are comparable or worse. Surprisingly, traditional fixed clipping outperforms adaptive methods for 7B+ models with curated data. Despite extensive ablations, we lack theoretical justification for these empirical observations. While our method provides practical guidance for clipping threshold selection, the modest and inconsistent gains suggest fundamental limitations of gradient norm-based clipping for Transformer optimization. We release comprehensive logs and configurations to facilitate future reproduction.",
    "id": 894
  },
  {
    "title": "LoRA-Drop: Adaptive Rank Reduction for Efficient Fine-tuning via Gradient Sparsity",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a popular method for efficiently fine-tuning large language models, but its fixed-rank structure often leads to suboptimal parameter allocation across layers. We propose LoRA-Drop, a simple yet effective approach that dynamically reduces the rank of LoRA adapters during training based on gradient norms. Our method begins with a generous rank allocation and progressively drops the least significant components, reducing both memory footprint and computational overhead. We introduce a thresholding scheme that considers both gradient magnitudes and layer-wise sensitivity, maintaining performance while pruning up to 60% of parameters. Experiments on instruction tuning and domain adaptation tasks across 7B and 13B parameter models show modest improvements in perplexity (1-2%) while achieving 1.5-2\u00d7 speedup during training, though benefits diminish on smaller downstream tasks. Our approach offers a practical compromise between full fine-tuning and static low-rank methods, though theoretical justification for the adaptive scheme remains limited. Code is provided for reproducibility.",
    "id": 895
  },
  {
    "title": "Gradient Descent with Lookahead Momentum: A Simple Modification for Improved Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Lookahead Momentum (LaM), a lightweight modification to standard stochastic gradient descent that maintains a slow-moving copy of the parameters while performing gradient updates on fast weights. The fast weights are reset to the slow weights every k steps, while the slow weights follow an exponential moving average of the fast weights. Unlike traditional momentum methods that smooth the update direction, LaM performs \"coarse corrections\" that we show encourages the optimizer to find flatter minima. We prove convergence under standard convex assumptions and demonstrate empirically that LaM improves test accuracy by 1-2% on CIFAR-10/100 and ImageNet compared to SGD with momentum, while requiring minimal hyperparameter tuning. However, experimental gains vary significantly across architectures and datasets, with notable improvements only observed for moderate-sized ResNets. While our theoretical analysis provides some justification, we acknowledge that the connection between lookahead updates and flat minima remains largely phenomenological. We provide PyTorch code and pre-trained models.",
    "id": 898
  },
  {
    "title": "Gradient Stabilization Through Adaptive Noise Injection for Training Deep Residual Networks",
    "authors": [
      "Nguyen, T.K.",
      "Johnson, L.M.",
      "Zhao, H."
    ],
    "abstract": "We propose an adaptive noise injection technique to stabilize gradient flow in very deep residual networks. Motivated by observations that gradient norms become increasingly unstable during training of networks beyond 100 layers, our method introduces carefully calibrated Gaussian noise into residual connections based on layer-wise gradient statistics. The noise magnitude is adjusted dynamically using a running estimate of gradient variance, theoretically grounded in stochastic differential equation analysis of gradient descent. Experiments on CIFAR-10/100 and ImageNet show modest improvements over standard ResNet training, with the technique providing more stable training particularly for depths exceeding 200 layers. However, performance gains saturate for moderately deep networks and the computational overhead may not justify deployment in all settings. While our theoretical analysis provides insights into the gradient noise trade-off, we acknowledge limitations in extending beyond residual architectures and the need for additional hyperparameter tuning across different datasets. Code will be made available upon acceptance.",
    "id": 901
  },
  {
    "title": "An Empirical Study of Gradient Noise Scale Regularization for Improving Transformer Training Stability",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, J."
    ],
    "abstract": "We propose gradient noise scale regularization (GNSR) as a simple technique to stabilize training of large transformers. Motivated by theoretical work linking gradient noise scales to training dynamics, we add a lightweight loss term that penalizes large noise-to-signal ratios during optimization. Our method requires no architectural changes and introduces minimal computational overhead. We evaluate GNSR on standard language modeling and translation benchmarks using base-size models (340M parameters) trained on standard datasets. Results show consistent but modest improvements: 0.3-0.7 BLEU on translation tasks and 2-3 perplexity points on WikiText-103, while reducing training variance across 3 random seeds. Ablation studies reveal most benefits come from early training regularization rather than asymptotic performance gains. While our approach shows promise for practitioners facing training instability, we acknowledge limitations including unclear theoretical guarantees and diminishing returns on well-tuned baselines. Code and hyperparameters will be released upon acceptance.",
    "id": 911
  },
  {
    "title": "Revisiting Softmax Temperature Scaling with Learnable Mixtures of Temperaments",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Temperature scaling is widely adopted for confidence calibration in neural networks, yet its effectiveness diminishes on datasets exhibiting distribution shift. We propose a simple extension: modeling the softmax temperature as a learned mixture of multiple temperature values, each weighted by the input instance. Our method introduces only 0.02% additional parameters but demonstrates consistent improvements in expected calibration error across 5 benchmark datasets, achieving 2-8% better calibration compared to standard temperature scaling. While the approach is theoretically motivated by considering the posterior as a mixture of tempered exponentials, we acknowledge our analysis assumes conditional independence that may not hold in practice. Experiments reveal the gains are most pronounced on corrupted versions of CIFAR-10/100 and ImageNet-C, with diminishing returns on in-distribution data. The simplicity of our method may benefit practitioners, though we recognize the contribution is incremental and the theoretical justification remains incomplete. Code is available at [redacted-for-reviews].",
    "id": 916
  },
  {
    "title": "Gradient Surgery for Federated Learning is Not Always Optimal: A Spectral Perspective on Convergence Trade-offs",
    "authors": [
      "Liu, S.",
      "Chen, J.",
      "Brown, D."
    ],
    "abstract": "Gradient compression techniques in federated learning typically treat client updates as independent vectors and apply uniform quantization or sparsification. We propose SpectralFed, a method that decomposes client gradients using eigendecomposition and selectively compresses components based on their alignment with the global gradient spectrum. Our approach applies different compression rates to eigenvectors based on their associated eigenvalues, theoretically leading to better convergence for non-convex objectives. Experiments on CIFAR-10/CIFAR-100 with ResNet-18 demonstrate 1.2-1.4\u00d7 communication reduction compared to FedAvg and FedProx baselines, while maintaining comparable accuracy (\u00b10.3%). However, we observe decreasing benefits as data heterogeneity increases, with smaller federated datasets showing minimal improvement. Theoretical analysis provides convergence guarantees under bounded gradient dissimilarity assumptions, but requires strong smoothness conditions that may not hold in practice. While SpectralFed offers a principled alternative to existing compression schemes, its computational overhead and sensitivity to eigengap values limit practical deployment. Our code is available anonymously at [URL].",
    "id": 921
  },
  {
    "title": "Adaptive Gradient Clipping with Layer-wise Curvature Estimates for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training large transformers remains unstable despite extensive tuning of gradient clipping thresholds. We propose Adaptive Curvature Clipping (ACC), which adjusts clipping thresholds per layer based on local curvature estimates computed via low-rank approximations of the Hessian. ACC uses gradient history to maintain running estimates of layer-wise curvature without additional forward passes. We evaluate on GPT-2 (124M-1.5B) and ViT-B/16 across 5 datasets, demonstrating 2-3\u00d7 faster initial convergence compared to standard adaptive optimizers. However, performance gains diminish in later training stages (epochs 40+), where ACC reduces to near-identical behavior as baseline clipping. Theoretical analysis shows ACC implicitly regularizes optimization paths by bounding local Lipschitz constants, though this effect becomes negligible as gradients vanish. While ACC provides practical benefits for early training instability at moderate computational overhead (~8% per step for 125M models), its long-term impact on final model quality is limited. Code and pretrained checkpoints will be released.",
    "id": 932
  },
  {
    "title": "Gradient Norm Clustering: A Simple Regularization Technique for Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Garcia, M."
    ],
    "abstract": "Large neural networks exhibit complex training dynamics where gradient directions across different batches can vary significantly, potentially leading to unstable optimization and poor generalization. We propose Gradient Norm Clustering (GNC), a lightweight regularization technique that encourages gradient vectors to cluster in norm-space during training. GNC adds an auxiliary loss term that penalizes the variance of gradient norms across mini-batches, implemented efficiently through a momentum-based running estimate. Our experiments on CIFAR-10, CIFAR-100, and ImageNet-subset show consistent improvements over standard SGD across various architectures, with particularly strong gains (0.5-1.2% accuracy) for deeper networks like ResNet-50 and ViT-Tiny. While GNC demonstrates practical benefits and theoretical connections to Lipschitz regularization, our analysis reveals the improvements diminish when combined with strong data augmentation techniques like MixUp. The method adds minimal computational overhead (\u22483% training time) and requires no architectural modifications, making it suitable for practitioners. However, we acknowledge the improvement magnitude is modest and may not justify the added complexity in all settings.",
    "id": 935
  },
  {
    "title": "Adaptive Gradient Clipping with Moving Percentiles for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Transformer models often suffer from gradient explosion during training, leading to unstable optimization. While gradient clipping is widely used, the choice of clipping threshold remains heuristic and dataset-dependent. We propose Adaptive Gradient Percentile Clipping (AGPC), which dynamically sets clipping thresholds using moving percentiles of gradient norms observed during training. Unlike traditional fixed-threshold approaches, AGPC adapts to the evolving gradient distribution without hyperparameter tuning. Our method computes thresholds based on the 95th percentile of gradient norms over a sliding window, with exponential decay to maintain stability. Experimental results on language modeling benchmarks (Wikitext-103, C4) and vision transformers on ImageNet show 2-3% improvements in final perplexity/accuracy over baseline clipping, particularly in low-resource settings. However, the benefits diminish with careful hyperparameter tuning of standard clipping methods. Ablation studies reveal that percentile choices around 90-95% work well across tasks, but the optimal window size varies. While our approach provides a principled alternative to manual threshold selection, the computational overhead of online percentile estimation (5-10% increase in training time) may limit its practical impact. Code will be made available upon acceptance.",
    "id": 940
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Learned Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to high-variance gradient estimators. While control variates can reduce variance, their effectiveness is limited by the quality of baseline functions. We propose a meta-learning approach that learns to predict optimal control variate baselines for REINFORCE-style estimators. Our method trains a small neural network that takes local context (layer activations, parameter norms) as input and outputs baseline values that minimize gradient variance. On MNIST-VAE experiments, our approach achieves 15-30% lower gradient variance compared to standard baselines, translating to modest improvements in likelihood (0.5-1.2 nats improvement) and perceptual quality scores. Theoretical analysis shows our learned baselines reduce variance by implicitly capturing correlations between gradients and model parameters. While our improvements are consistent, they remain incremental compared to recent work on continuous relaxations. Our computational overhead is roughly 5-10% during training. We release PyTorch code for reproducibility.",
    "id": 941
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning through Selective Parameter Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts and catastrophic forgetting when tasks compete for shared parameters. We propose Gradient Surgery with Memory (GSM), a simple modification to existing multi-task optimization that maintains an episodic memory of past gradients to inform selective parameter updates. Our method identifies conflicting gradients through cosine similarity analysis and selectively freezes or adapts learning rates for specific parameters based on their contribution to both current and previous tasks. On CIFAR-100 split into 5 tasks and a new benchmark of 8 NLP tasks from GLUE and SuperGLUE, GSM achieves comparable performance to state-of-the-art methods while requiring 30% less memory and 2x fewer hyperparameters. However, we find that GSM's benefits diminish when task similarity is low or when the number of tasks exceeds 15. While the approach shows promise, our theoretical analysis reveals that the selective update rule can create undesirable local minima in specific parameter regimes. Our results suggest that gradient-based solutions to catastrophic forgetting may be fundamentally limited when task distributions differ significantly.",
    "id": 949
  },
  {
    "title": "Adaptive Learning Rates via Online Thinning of Recurrent States",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a novel approach to learning rate adaptation in recurrent neural networks by maintaining and selectively pruning intermediate hidden states during training. Our method, called Recurrent State Thinning (RST), maintains a fixed-size reservoir of previously computed states and uses a lightweight online procedure to identify which states provide the most predictive signal for future steps. States deemed less informative are excluded from gradient computations, effectively creating an adaptive sparse computation graph. Experiments on language modeling and time series prediction tasks show modest improvements over carefully tuned baselines: RST achieves 2-4% lower perplexity on Penn Treebank and 5-7% better MSE on synthetic sinusoidal datasets. While the computational overhead is negligible (under 3% additional runtime), the approach requires careful hyperparameter tuning and provides limited benefits on shorter sequences. Our analysis reveals that the primary advantage stems from preventing gradient accumulation in redundant state dimensions rather than discovering fundamentally new learning dynamics. Code and experiments are available at anonymous-link.github.io.",
    "id": 952
  },
  {
    "title": "LoRA-ME: Low-Rank Adaptation with Meta-Initialized Bases for Few-Shot Learning",
    "authors": [
      "Chen, Y.",
      "Kumar, S.",
      "Vasquez, J."
    ],
    "abstract": "We propose LoRA-ME, an extension of Low-Rank Adaptation (LoRA) that uses meta-learned initialization of the low-rank matrices for improved few-shot adaptation. While LoRA has become a standard parameter-efficient fine-tuning method, we observe that randomly initialized low-rank matrices often require substantial data to converge to meaningful directions. Our key insight is that we can meta-learn a better initialization for these low-rank matrices by optimizing across tasks such that the adaptation matrices align with task-relevant subspaces. We achieve this by training a small meta-network that outputs the initial matrices given task-specific features extracted from the pre-trained model's activations. On the Meta-Dataset benchmark, LoRA-ME achieves 2.3% absolute improvement over standard LoRA (56.1% vs 53.8%) and 1.1% over full fine-tuning (55.0%). However, results are inconsistent across datasets, with improvements mainly on fine-grained classification tasks. While our method introduces minimal overhead and preserves LoRA's parameter efficiency, the meta-learning procedure requires careful tuning and can be unstable for larger model sizes. These limitations suggest LoRA-ME provides modest but not transformative improvements, making it most useful for practitioners with limited computational budgets who prioritize parameter efficiency.",
    "id": 953
  },
  {
    "title": "Improving Transformer Training Stability via Layer-wise Perturbation Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Transformer training instability remains a persistent challenge, particularly when scaling to deeper architectures. We propose a lightweight layer-wise perturbation analysis framework that identifies unstable layers during training and applies targeted regularization. Our method computes gradient covariance statistics at each layer and introduces a novel regularization term that penalizes directions with high gradient variance. This approach requires minimal computational overhead (less than 2% increase in training time) and can be integrated into existing training pipelines without architectural modifications. Experiments on language modeling and machine translation tasks with 12-24 layer Transformers show modest improvements: 0.3-0.7 BLEU score gains on WMT14 English-German translation and 1.2 perplexity reduction on Wikitext-103. While our regularization improves training stability metrics including gradient norm consistency and loss curve smoothness, ablation studies reveal that benefits diminish with careful hyperparameter tuning of baseline models. Code and pre-trained models will be released upon acceptance.",
    "id": 960
  },
  {
    "title": "Revisiting Label Smoothing with Temperature Scaling: A Unified Framework for Confidence Calibration",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Label smoothing and temperature scaling are widely used techniques for improving confidence calibration in deep neural networks, yet their relationship remains poorly understood. We propose a unified framework that interprets both methods as instances of a general regularization principle based on Renyi entropy maximization. Our theoretical analysis establishes that label smoothing corresponds to a fixed-point regularization, while temperature scaling adapts this regularization based on the logit distribution. Building on this insight, we introduce Adaptive Label Temperature (ALT), which combines dynamic smoothing with learned temperature parameters. While ALT achieves modest improvements in Expected Calibration Error (2.1% reduction on ImageNet compared to standard temperature scaling), we find its benefits are most pronounced on small datasets (CIFAR-100) where overconfident predictions are common. However, the computational overhead of learning additional parameters and the limited out-of-domain robustness raise questions about practical adoption. Our extensive ablations reveal that simpler temperature scaling combined with moderate label smoothing often matches ALT's performance with fewer hyperparameters. We open-source our implementation to facilitate reproducibility and future work on calibration techniques.",
    "id": 975
  },
  {
    "title": "Improving Neural Network Generalization Through Layer-Wise Entropy Regularization",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Layer-wise Entropy Regularization (LER), a simple yet effective technique for improving generalization in deep neural networks. LER adds entropy penalties at individual layers to encourage disentangled representations, drawing inspiration from information bottleneck principles. Unlike end-to-end mutual information regularization, our method computes entropy locally based on mini-batch statistics, making it computationally lightweight and tuning-free. Through experiments on CIFAR-10/100 and ImageNet-subset, we demonstrate 2-3% accuracy improvements over vanilla baselines with minimal hyperparameter tuning. While our theoretical analysis only establishes loose generalization bounds, ablations show that LER provides complementary benefits to standard regularization techniques like dropout and weight decay. However, we observe diminishing returns on larger models and our method adds non-trivial memory overhead during training. Our empirical results suggest LER may be most beneficial for moderate-scale vision tasks, though we leave theoretical justification and broader applicability as open questions.",
    "id": 983
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning with Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, particularly in overparameterized neural networks. We propose Gradient Surgery (GS), a simple modification to standard multi-task optimization that detects and resolves gradient conflicts through selective projection. Unlike previous approaches that require task-specific architectures or expensive Hessian computations, GS operates purely on the gradients computed during backpropagation, making it compatible with existing training pipelines. Our method projects each task's gradient onto the null space of conflicting directions, prioritizing tasks with higher loss reductions. Experiments on standard benchmarks including NYUv2, CityScapes, and CIFAR-100 show 2-5% improvement over baselines. However, we find gains diminish on larger architectures (>100M parameters) and when tasks are weakly correlated. While GS reduces gradient interference, our theoretical analysis reveals limited improvement in the final minima quality due to the presence of multiple adequate solutions. The simplicity of our approach makes it practical for moderate-scale multi-task scenarios, though it does not address fundamental questions about task interference in the overparameterized regime.",
    "id": 993
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Historical Gradient Recombination",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impair optimization. While existing gradient surgery methods like PCGrad and GradDrop modify gradients at each step, we propose Adaptive Memory Gradient Surgery (AMGS) which leverages historical gradient information to better resolve conflicts. Our key insight is that past gradient directions contain useful information about task relationships that can inform current gradient modification. AMGS maintains an exponentially decaying memory of per-task gradients and uses attention mechanisms to compute task-specific gradient projections that minimize interference. On three standard multi-task vision datasets (CityScapes, NYUv2, and CIFAR-100), AMGS shows consistent improvements over gradient surgery baselines, achieving +1.2% mIoU and +0.8% accuracy on average. However, we find these gains diminish in low-data regimes and when task count exceeds 5. While AMGS provides a practical improvement over existing methods with minimal computational overhead (5% training time increase), our theoretical analysis suggests the approach may not guarantee convergence in pathological cases. Code and pretrained models are available at [repository].",
    "id": 1003
  },
  {
    "title": "Gradient Descent with Periodic Restarts Improves Robustness to Dataset Corruption",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Restarted Gradient Descent (RGD), a simple modification to standard gradient descent that periodically resets the optimizer state while maintaining a decaying learning rate schedule. Our key observation is that common vision datasets contain memorizable corrupted examples that can dominate the learning dynamics, and periodic restarts help escape these spurious minima. We provide theoretical analysis showing RGD achieves similar convergence rates to standard GD on convex objectives while offering improved robustness to label noise. Empirically, we demonstrate 2-5% accuracy improvements over vanilla SGD on CIFAR-10 and ImageNet when 20-40% of labels are corrupted. While our method shows promise for noisy training scenarios, we acknowledge the improvements are modest and task-specific. Ablation studies reveal the benefits diminish as training duration increases, suggesting RGD may primarily act as a form of implicit regularization. Our code is available, though we note hyperparameter sensitivity in the restart schedule requires careful tuning. These results suggest periodic restarts as a lightweight addition to existing training pipelines when robustness to data quality is a concern.",
    "id": 1004
  },
  {
    "title": "LoRA-Prune: Adaptive Low-Rank Adaptation for Efficient Parameter-Efficient Fine-Tuning",
    "authors": [
      "Kumar, S.",
      "Li, J.",
      "Garcia, M."
    ],
    "abstract": "We present LoRA-Prune, a method that combines low-rank adaptation with structured pruning to enable more parameter-efficient fine-tuning of large language models. While LoRA has emerged as a popular approach for reducing memory requirements during fine-tuning, we observe that many adapted low-rank matrices remain highly sparse, suggesting room for further compression. Our key insight is that the rank of LoRA adaptations can be dynamically adjusted per layer based on gradient information, while simultaneously removing unimportant rows/columns through magnitude-based pruning. We introduce a simple thresholding scheme that identifies low-contribution ranks without requiring additional validation data or expensive retraining. Experiments on GLUE and SQuAD benchmarks using RoBERTa-Large show LoRA-Prune achieves 35-50% parameter reduction over standard LoRA with <2% performance degradation in most tasks. However, we find the method underperforms on tasks requiring complex reasoning (e.g., DROP), suggesting the pruning heuristic may be overly aggressive. While additional gains are modest compared to existing compression techniques, LoRA-Prune offers a lightweight drop-in replacement for standard LoRA that reduces memory footprint without architectural changes. Code and pre-trained adapters will be made available.",
    "id": 1015
  },
  {
    "title": "Gradient Routing with Learned Curvature Estimates for Neural Network Training",
    "authors": [
      "Liu, K.",
      "Chen, Y.",
      "Rodriguez, M."
    ],
    "abstract": "Recent work has shown that second-order optimization methods struggle to outperform carefully tuned SGD variants in deep learning, despite their theoretical advantages. We propose Gradient Routing Networks (GRNs) that adaptively combine first and second-order gradient directions using learned curvature estimates. Our approach trains a small auxiliary network to predict when second-order updates are beneficial, routing gradients through either standard backpropagation or a damped Newton step. We evaluate GRNs on image classification and language modeling tasks, finding consistent but modest improvements over AdamW and Shampoo baselines (0.5-1.2% top-1 accuracy on ImageNet, 1-3 perplexity points on WikiText-103). While our method achieves competitive results with lower computational cost than full second-order approaches, our gains are task-dependent and require careful hyperparameter tuning. Code and pretrained models are available at anonymous-url.github.io.",
    "id": 1017
  },
  {
    "title": "Gradient Surgery Revisited: Do Adaptive Methods Really Need Gradient Surgery in Multi-Task Learning?",
    "authors": [
      "Liu, K.",
      "Thompson, E.",
      "Zhao, J."
    ],
    "abstract": "Multi-task learning (MTL) methods often struggle with conflicting gradients during optimization. While recent approaches like PCGrad and GradNorm have shown promise, their reliance on gradient surgery raises questions about generalization and computational overhead. We propose Adaptive Gradient Scaling (AGS), a simple modification that adjusts gradient magnitudes based on task uncertainty estimates rather than explicit gradient manipulation. Our method requires only 0.3% additional compute compared to standard multi-task training while achieving comparable or better performance on popular MTL benchmarks. On NYUv2, Cityscapes, and Taskonomy, AGS matches PCGrad's performance (\u00b10.8% absolute) while reducing training time by 15-20%. However, we find that AGS benefits are highly sensitive to task weight initialization and show diminishing returns as network capacity increases. Our theoretical analysis reveals AGS approximates gradient surgery only under limited conditions, suggesting more sophisticated approaches may be needed when tasks are strongly conflicting. While AGS provides a computationally efficient alternative to existing methods, our results indicate the multi-objective optimization problem in MTL remains fundamentally challenging, particularly for highly imbalanced or adversarial task combinations.",
    "id": 1027
  },
  {
    "title": "Learning with Partial Ambiguity Sets: A Distributionally Robust Approach to Soft Labels",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We consider supervised learning scenarios where soft labels (e.g., from noisy annotators or weak supervision) are provided alongside hard labels, but the reliability of these soft labels is unknown. While previous work has treated soft labels as ground truth or completely discarded them, we propose a distributionally robust optimization (DRO) framework that models partial ambiguity sets using Wasserstein balls centered at empirical distributions. Our key contribution is a theoretically grounded method for adaptively determining the radius of these ambiguity sets based on both the soft label reliability and the hardness of individual examples. We provide generalization bounds that depend on a novel notion of effective sample complexity that accounts for the quality of soft labels. Experiments on CIFAR-10 and ImageNet subsets with synthetic label noise show improvements of 1-3% over baselines when soft labels are moderately accurate, but performance degrades when noise dominates. While our approach provides principled handling of label uncertainty, the computational overhead (1.5-2\u00d7 training time) and the heuristic nature of the radius selection may limit practical impact.",
    "id": 1029
  },
  {
    "title": "LoRA-DC: Partial Weight Updates with Dynamic Compensation for Efficient Fine-Tuning",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Okafor, C."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become the de facto standard for parameter-efficient fine-tuning, but its fixed-rank limitation often leads to suboptimal trade-offs between performance and efficiency. We propose LoRA-DC, a method that dynamically adjusts the intrinsic rank during training while compensating for approximation errors through a lightweight correction mechanism. Our approach decomposes weight updates into a low-rank component and an error-correction term, both trained jointly via an alternating optimization procedure. The correction term is constrained to be ultra-sparse, adding minimal overhead (0.5-3% of original parameters). Experiments on BERT, RoBERTa, and Llama-2 show improvements of 0.5-2.3 F1 points over vanilla LoRA across GLUE and SuperGLUE tasks, while remaining within 2% of full fine-tuning. However, these gains are inconsistent across datasets, with some tasks showing no benefit or slight degradation. Runtime overhead is 15-30% compared to standard LoRA during training but negligible at inference. The method's effectiveness appears correlated with downstream task complexity, suggesting limitations in our rank selection heuristic. Code and models are available at [redacted].",
    "id": 1033
  },
  {
    "title": "Revisiting MAML with Adaptive Inner-Loop Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Singh, K."
    ],
    "abstract": "Model-Agnostic Meta-Learning (MAML) has become a popular approach for few-shot learning, yet its inner loop optimization uses fixed learning rates across all tasks and parameters. We propose Adaptive-MAML (A-MAML), which learns task-adaptive step sizes for the inner loop by adding a small meta-network that outputs per-parameter learning rates. Our method extends MAML with minimal computational overhead, requiring only 15% additional parameters. We evaluate A-MAML on standard few-shot image classification benchmarks (mini-ImageNet, CUB-200) and achieve 1-2% absolute improvements over MAML baselines. Furthermore, we demonstrate that adaptively adjusting learning rates reduces sensitivity to hyperparameter selection, particularly for the inner-loop learning rate and number of adaptation steps. While the improvements are consistent across benchmarks, they are modest and primarily achieved through careful tuning of the meta-network architecture. Code is available at [redacted].",
    "id": 1036
  },
  {
    "title": "Towards Stable Semi-Supervised Learning via Adaptive Data Augmentation Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Semi-supervised learning algorithms often exhibit unstable performance when the unlabeled data distribution differs from the labeled data. We propose AdaAug-SSL, a simple adaptive scheduling mechanism that dynamically adjusts data augmentation strength based on confidence measures from the model's predictions. Our approach monitors the entropy of predictions on unlabeled batches and applies stronger augmentations when prediction confidence is high, while reverting to weaker augmentations during uncertain phases. We demonstrate AdaAug-SSL on CIFAR-10 with 4,000 labels and SVHN with 1,000 labels, achieving modest improvements of 1.2-1.8% over strong baselines like FixMatch and FlexMatch. While our results are consistent across these benchmarks, we observe that the gains diminish when the label/unlabeled data ratio exceeds 15%. Our extensive ablation reveals that the scheduling strategy is more critical than the specific augmentation types used. Although AdaAug-SSL provides a lightweight alternative to extensive hyperparameter tuning, we acknowledge that the mechanism adds computational overhead and may not generalize well to extremely imbalanced SSL settings. Code and hyperparameters are available at anonymized-url.",
    "id": 1040
  },
  {
    "title": "Gradient Descent with Annealed Noise Injection: A Simple Alternative to Momentum for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "We propose Noise-Injected Gradient Descent (NIGD), a simple modification to standard gradient descent that adds carefully scheduled Gaussian noise to parameter updates instead of using momentum terms. By annealing the noise variance according to a cosine schedule, we observe improved convergence on deep neural network training tasks compared to vanilla SGD, while requiring minimal hyperparameter tuning. Our theoretical analysis shows that NIGD achieves similar convergence rates to SGD with momentum for quadratic objectives, and we demonstrate empirically that it can escape sharp minima more effectively on simple toy problems. Experiments on CIFAR-10 with ResNet-18 show a 0.8% improvement in test accuracy over standard SGD, though these gains diminish on larger datasets like ImageNet. While our method is not intended to replace existing optimizers, it provides a lightweight alternative for practitioners seeking to avoid momentum hyperparameter tuning. Our code is available at github.com/anonymous/nigd.",
    "id": 1080
  },
  {
    "title": "LoFiSGD: Memory-Efficient Low-Fidelity Gradient Compression for Large-Scale Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We present LoFiSGD, a gradient compression scheme that aggressively quantizes neural network gradients to 1-2 bits during distributed training while maintaining convergence properties. Our method builds upon existing quantization techniques but introduces a novel \"fidelity-aware\" update rule that adaptively adjusts compression levels based on gradient norms. We demonstrate LoFiSGD on ResNet-50 and Transformer architectures, achieving up to 8.5\u00d7 communication reduction compared to full-precision training with <1.5% accuracy degradation on CIFAR-10 and WMT'14. Theoretical analysis shows convergence under standard assumptions for non-convex objectives. While our compression ratios match state-of-the-art methods, we acknowledge that the observed training instabilities on larger models (e.g., BERT-Large) and requirement for careful hyperparameter tuning may limit practical adoption. Our PyTorch implementation is provided for reproducibility.",
    "id": 1093
  },
  {
    "title": "Lookahead Gradient Descent with Adaptive Momentum: A Unified Framework for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose Lookahead Gradient Descent with Adaptive Momentum (LGAM), a simple modification to standard gradient descent that combines elements of lookahead optimization and adaptive momentum schemes. Our method maintains two sets of parameters: fast weights that perform local gradient updates and slow weights that periodically sync with the fast weights using a learned momentum coefficient. While LGAM does not achieve theoretical improvements over vanilla SGD in worst-case scenarios, we empirically demonstrate 2-5% relative improvement in final accuracy across various ResNet architectures on CIFAR-10/100 and ImageNet, particularly in settings with small batch sizes and aggressive learning rates. Ablation studies suggest the benefits primarily come from improved stability during early training phases rather than faster convergence. Theoretically, we prove convergence to first-order stationary points under standard smoothness assumptions, though our rate matches existing bounds. Code is available at anonymous-url.github.io/LGAM.",
    "id": 1097
  },
  {
    "title": "Gradient Surgery with Topological Constraints: Mitigating Catastrophic Forgetting via Persistent Homology",
    "authors": [
      "Liu, H.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Catastrophic forgetting remains a fundamental challenge in continual learning, where sequentially trained neural networks lose performance on previous tasks. While existing regularization-based approaches provide some protection, they often rely on heuristic similarity measures between tasks. We propose TopoGrad, a novel regularization framework that leverages persistent homology to identify critical gradient directions for preserving task-specific topological features. Our method computes persistent barcodes across intermediate network representations, then constructs gradient projections that minimally interfere with these topological invariants. On the Split-CIFAR-100 benchmark, TopoGrad achieves 4.2% absolute improvement over standard fine-tuning, comparable to recent regularization methods while adding minimal computational overhead. However, we observe performance degradation on more complex datasets (Split-TinyImageNet), where topological features may be less informative. Our ablation study reveals that shorter barcode persistence thresholds (\u03b5 < 0.1) lead to over-regularization, whereas longer thresholds provide insufficient forgetting protection. While TopoGrad offers a theoretically principled approach with interpretable regularization via topological summaries, computational scaling to ImageNet-scale architectures remains challenging. Our code is available at [redacted-for-anonymity].",
    "id": 1108
  },
  {
    "title": "Gradient Surgery with Memory: Editing Past Updates in Overparameterized Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose Gradient Surgery with Memory (GSM), a method for selectively undoing harmful gradient updates in neural networks without full retraining. Motivated by catastrophic forgetting in continual learning and adversarial robustness, GSM maintains a compressed history of parameter trajectories and identifies which past updates contributed most to current performance degradation. Our approach uses a lightweight approximation to the Hessian-trace to estimate update importance, followed by targeted counter-updates that surgically reverse specific gradient steps while preserving beneficial changes. On CIFAR-10 split across 5 sequential tasks, GSM reduces forgetting by 18% over standard finetuning while maintaining computational overhead below 5% during training. We also demonstrate applications to removing backdoor triggers and reversing adversarial fine-tuning. However, our method is limited to small-to-medium architectures (\u2264 ResNet-50) due to memory constraints, and the theoretical guarantees only hold under restrictive assumptions about loss landscape geometry. While preliminary results are promising, we acknowledge that scalability to larger models and more realistic scenarios requires further investigation.",
    "id": 1115
  },
  {
    "title": "Gradient Surgery for Improving Transfer Learning in Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Fine-tuning large pre-trained vision transformers for downstream tasks often suffers from catastrophic forgetting and negative transfer. We propose Progressive Gradient Surgery (PGS), a simple technique that selectively prunes gradient components during fine-tuning based on their alignment with pre-trained weights. PGS computes gradient-projections onto the subspace spanned by frozen pre-trained parameters, then removes components below a learned threshold. While conceptually straightforward, PGS surprisingly improves transfer performance across 8 vision benchmarks by 2.3% on average compared to standard fine-tuning. However, gains diminish as dataset size increases, and the approach underperforms recent adapters and prompt-tuning methods on ImageNet (0.8% drop from baseline). Our analysis reveals PGS primarily improves convergence speed rather than final accuracy, achieving similar results to earlier stopping. These findings suggest gradient surgery may be most beneficial in low-data regimes or when computational constraints limit alternatives. Code is available at [URL to be added].",
    "id": 1120
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient compression is crucial for distributed training, but most methods rely on hand-crafted quantization schemes that remain fixed throughout training. We propose Learned Quantization Schedules (LQS), a simple approach that adaptively adjusts the precision of gradient compression based on the current training dynamics. Our method trains a lightweight auxiliary network to predict optimal bit-widths for different layers and iterations, using only local gradient statistics as input. Experiments on ResNet-50 and Transformer models show up to 2.3\u00d7 communication reduction over fixed quantization baselines while maintaining convergence properties. However, our approach introduces non-negligible computational overhead (15-20% training slowdown) and shows diminishing benefits on smaller models. Theoretical analysis reveals our schedules achieve near-optimal compression under convexity assumptions, though extending these guarantees to non-convex settings remains challenging. While LQS demonstrates practical improvements in specific regimes, particularly for large-scale training with bandwidth constraints, its broader applicability is limited by training complexity and sensitivity to hyperparameter choices.",
    "id": 1130
  },
  {
    "title": "Rethinking Batch Normalization with Gentle Momentum Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "Batch Normalization (BN) has become a staple in deep learning, yet its interaction with modern optimizers remains poorly understood. We propose Gentle Batch Normalization (GBN), a simple modification that replaces the conventional batch statistics with an exponential moving average updated on a slower timescale than the model parameters. This decoupling reduces gradient conflicts between BN layers and the rest of the network without introducing additional hyperparameters. Through experiments on ResNet-50 and Vision Transformer architectures across CIFAR-10, ImageNet, and domain adaptation benchmarks, GBN demonstrates consistent but modest improvements (0.3-0.7% accuracy) while requiring 5-10% fewer training steps to converge. However, gains diminish in well-tuned regimes with learning rate warmup and careful initialization. We provide theoretical analysis showing GBN acts as a form of implicit regularization, though the effect is bounded by the batch size. Code is available, though we note instability with very small batch sizes (<8). While GBN offers a practical alternative to standard BN, the incremental benefits may not justify the added complexity for practitioners with sufficient compute budget.",
    "id": 1132
  },
  {
    "title": "Gradient Surgery with Memory: Improving Stability in Multi-Task Optimization via Gradient Conflict Replay",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that destabilize optimization and degrade performance. While recent gradient surgery methods like PCGrad and GradDrop address these conflicts, they rely solely on current mini-batch information, potentially discarding useful gradients long-term. We propose Gradient Memory Surgery (GMS), a simple extension that maintains a small replay buffer of conflicting gradients to achieve more globally consistent updates. GMS identifies gradient conflicts using cosine similarity, then selectively blends stored gradients with current ones via an attention mechanism. On three standard multi-task vision benchmarks (NYUv2, CityScenes, CIFAR-100), GMS achieves modest improvements over PCGrad (+0.3-0.8% average accuracy) while reducing training instability. However, we find GMS's benefits diminish with larger models and are most pronounced in low-data regimes. Our method adds minimal computational overhead (\u22485% training time) and only one hyperparameter (buffer size). While results are positive, we acknowledge the improvements are incremental and the technique may not address fundamental limitations of gradient surgery methods. Code and hyperparameters will be made available.",
    "id": 1134
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Dropping with Adaptive Recovery",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Transformer models suffer from quadratic complexity in sequence length, limiting their applicability to long sequences. We propose Adaptive Token Recovery (ATR), a training-free method that dynamically drops low-importance tokens during inference and recovers them with lightweight attention heads when needed. Our approach uses a learnable gating mechanism to identify tokens for removal based on attention patterns, while a separate recovery module reconstructs dropped tokens using local context. We evaluate ATR on language modeling and machine translation tasks, achieving 1.3x speedup on average while maintaining 95-98% of original performance on sequences up to 4K tokens. The method shows promising results on longer sequences but exhibits 5-10% degradation on tasks requiring fine-grained token interactions. Our analysis reveals ATR works best for tasks with natural redundancy in input sequences. While competitive with existing pruning methods, ATR's complexity introduces additional hyperparameters that may limit its practical deployment. Code is available at anonymous-url.",
    "id": 1144
  },
  {
    "title": "Enhancing Transformer Efficiency through Structured Attention Pruning with Learnable Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "While transformers achieve remarkable performance across domains, their quadratic attention complexity limits deployment on resource-constrained devices. We propose Structured Attention Pruning with Learnable Thresholds (SAPLT), a method that dynamically removes attention heads during inference based on learned contribution scores. Unlike prior work that uses static pruning ratios or relies on post-training heuristics, SAPLT introduces threshold parameters jointly optimized with model weights using a simple regularization term. Our approach requires no additional training data or modifications to standard transformer architectures. Experiments on BERT-base and Vision Transformer show 15-20% FLOP reduction with less than 1% accuracy loss on GLUE and ImageNet tasks. While these gains are modest compared to state-of-the-art compression methods, SAPLT offers practical advantages: it is architecture-agnostic, introduces minimal training overhead, and maintains interpretability through learned thresholds. Limitations include sensitivity to the regularization coefficient and diminishing returns on larger models. Code will be made available upon acceptance.",
    "id": 1146
  },
  {
    "title": "Scheduled Dropout: A Simple Curriculum-Based Regularization Strategy for Training Neural Networks",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Thomas, C.",
      "Zhao, L."
    ],
    "abstract": "We propose Scheduled Dropout, a straightforward modification to standard dropout training that gradually reduces dropout rates during training according to a predefined curriculum. Unlike traditional dropout with fixed rates, our method starts with high dropout (e.g., 0.5-0.7) and linearly anneals to lower values (0.1-0.2) over training epochs. This approach is motivated by the observation that early training phases benefit from stronger regularization while later phases require less noise to fine-tune learned representations. Our theoretical analysis shows that scheduled dropout reduces to a modified L2 regularization scheme with time-varying coefficients, providing some justification for its effectiveness. We evaluate on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures, achieving modest improvements of 0.5-1.2% accuracy over standard dropout baselines, with particularly strong gains on smaller datasets. While the improvements are consistent, they remain incremental and the method requires careful tuning of the annealing schedule. We provide extensive ablations on the schedule design and demonstrate that simple linear schedules perform comparably to more complex cosine or exponential schedules. The method is easy to implement and adds minimal computational overhead, making it practical for broad adoption despite its limited novelty.",
    "id": 1154
  },
  {
    "title": "LoRA-GD: Low-Rank Adaptation Meets Gradient Descent for Memory-Efficient Federated Learning",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Singh, A."
    ],
    "abstract": "Federated learning faces fundamental challenges in communication efficiency and device heterogeneity, particularly when adapting large pre-trained models. We propose LoRA-GD, a method that combines low-rank adaptation with compressed gradient descent to enable efficient federated fine-tuning. Our approach uses rank-2 LoRA adapters at each client, coupled with top-k gradient sparsification and periodic aggregation. We demonstrate that this combination achieves 95% of centralized fine-tuning accuracy on CIFAR-10 and GLUE benchmarks while reducing communication costs by 15-20x compared to standard federated approaches. However, we observe performance degradation on certain tasks requiring precise parameter updates, suggesting LoRA-GD may not generalize across all domains. Our analysis reveals a trade-off between compression ratio and convergence stability, with optimal performance at rank values between 2-4. While our method provides practical benefits for resource-constrained devices, theoretical guarantees remain limited. These findings indicate that LoRA-GD offers a pragmatic solution for federated adaptation of foundation models, though further work is needed to address its limitations on data-heterogeneous scenarios.",
    "id": 1159
  },
  {
    "title": "Improving Transformer Efficiency Through Layer-wise Dynamic Pruning Without Retraining",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models achieve remarkable performance but suffer from high computational costs during inference. We propose L-Drop, a layer-wise dynamic pruning method that eliminates entire attention heads and feed-forward dimensions on-the-fly based on input statistics. Our approach requires no fine-tuning by leveraging magnitude-based pruning criteria derived from pre-training distributions, combined with simple threshold learning during inference. We evaluate on standard NLP benchmarks including GLUE, SQuAD, and machine translation tasks. L-Drop reduces FLOPs by 15-30% with minimal accuracy degradation (\u22640.5%) compared to full models, outperforming static pruning baselines by 2-3% absolute on the GLUE benchmark. However, we find performance degrades significantly (>2%) on tasks requiring fine-grained reasoning. Our method introduces a modest 5% memory overhead for storing activation statistics, and we provide a PyTorch implementation achieving 1.2x speedup on A100 GPUs. While L-Drop demonstrates practical inference-time improvements for many applications, our analysis reveals theoretical limitations for tasks where precise attention patterns are crucial.",
    "id": 1164
  },
  {
    "title": "Gradient Descent with Momentum Adapts to Sparse Gradients in Neural Network Training",
    "authors": [
      "Chen, L.",
      "Kim, J.",
      "Rodriguez, M."
    ],
    "abstract": "We investigate the implicit bias of momentum-based optimizers in neural network training, focusing on their behavior under sparse gradient conditions. While theoretical understanding of neural network optimization remains limited, we provide empirical evidence that momentum methods exhibit selective updates to parameters with non-zero gradients. Our theoretical analysis characterizes this behavior for a two-layer linear network, showing that parameters with consistently non-zero gradients converge faster than those with sparse gradients. We conduct systematic experiments on standard vision and language tasks, demonstrating that this phenomenon correlates with improved generalization in networks with structured sparsity patterns. However, our theoretical results hold only for simplified settings and do not extend to non-linear networks. Despite these limitations, our findings suggest that momentum's implicit regularization properties deserve further attention in understanding neural network training dynamics. Code is available at anony-mized-url.",
    "id": 1165
  },
  {
    "title": "Revisiting Gradient Clipping in Transformer Training: A Frequency Domain Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Gradient clipping remains a standard practice for training transformers, yet its impact on optimization dynamics remains poorly understood. We investigate gradient clipping through the lens of frequency domain analysis, showing that clipping primarily affects high-frequency components of the gradient signal. Using a combination of synthetic experiments and ablations on standard transformer architectures, we demonstrate that careful tuning of clipping thresholds can improve training stability without harming final performance. Our theoretical analysis characterizes the clipping operator as a non-linear low-pass filter, providing convergence bounds for clipped gradient descent under standard assumptions. While our experiments on WMT14 En-De and ImageNet classification show modest improvements (0.3-0.7 BLEU / 0.2-0.4% top-1 accuracy) over well-tuned baselines, we observe more substantial benefits in low-resource settings. The empirical gains, however, are sensitive to hyperparameter choices and training procedures, limiting the practical impact of our findings. We release code and pre-trained models to facilitate reproducibility.",
    "id": 1169
  },
  {
    "title": "Revisiting Curriculum Learning Through the Lens of Implicit Bias in Deep Networks",
    "authors": [
      "Chen, Y.",
      "Garcia, J.",
      "Liu, S."
    ],
    "abstract": "Curriculum learning has shown promise in various domains, yet its theoretical foundations remain limited. We investigate whether the empirically observed benefits of curriculum strategies can be explained through the lens of implicit bias in overparameterized neural networks. Specifically, we propose a modified gradient descent framework that incorporates curriculum scheduling as a form of path-dependent regularization. Our analysis reveals that certain curriculum strategies can be viewed as altering the implicit bias towards solutions that generalize better on tasks with intrinsic hierarchical structure. Through experiments on synthetic datasets with controlled complexity, we demonstrate that our curriculum modifications achieve marginal improvements over random ordering (2.3% average accuracy gain). While our theoretical analysis is restricted to linear networks and requires strong assumptions on data separability, empirical results on CIFAR-10 and subset of ImageNet show consistent but modest improvements (1.1-1.8%). Our work suggests that the benefits of curriculum learning may be more nuanced than previously reported, and provides a first step towards formalizing when such strategies are beneficial.",
    "id": 1178
  },
  {
    "title": "Gradient Surgery Revisited: A Principled Approach to Multi-Task Learning Trade-offs",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to performance degradation. While existing gradient surgery methods like PCGrad and GradNorm have shown empirical success, they lack theoretical justification and fail to provide guarantees on solution quality. We propose Adaptive Gradient Conflict Resolution (AGCR), a principled framework that formulates multi-task optimization as a constrained optimization problem with provable convergence guarantees. AGCR dynamically adjusts gradient directions by solving a small quadratic program at each step, ensuring Pareto-stationarity while maintaining computational efficiency. Our method achieves comparable or marginally better performance than existing heuristics on standard benchmarks (CIFAR-100, NYUv2, Omniglot), with 5-7% relative improvement over PCGrad in the full-data regime. However, we observe that AGCR's benefits diminish in low-resource settings and high-task-count scenarios, suggesting fundamental limitations of constrained optimization approaches. While our theoretical analysis provides the first convergence bounds for gradient surgery methods, the practical improvements remain modest. We discuss the gap between theoretical guarantees and empirical performance, highlighting directions for better aligning theory and practice in multi-task learning.",
    "id": 1189
  },
  {
    "title": "LoRA-FM: Low-Rank Adaptation with Fisher Information Matching for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "M\u00fcller, J.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods have emerged as practical alternatives to full model fine-tuning, with LoRA and its variants showing promise across various domains. We propose LoRA-FM, which enhances standard LoRA by incorporating Fisher information matrix (FIM) estimates to adaptively set rank allocation across layers. Our key insight is that layers with higher Fisher information capture more task-relevant knowledge, warranting larger rank budgets. We derive a simple approximation of diagonal FIM via gradient accumulation during initial training steps, then use this to modulate the rank of each LoRA adaptation. Experiments on GLUE and SuperGLUE benchmarks show 2-3% average improvement over LoRA with similar parameter budgets, though gains are uneven across tasks. While our approach introduces minimal computational overhead (\u22485% training time increase), it requires additional hyperparameter tuning for the FIM estimation window. Ablations reveal that the benefit primarily stems from improved rank allocation rather than the FIM computation itself. However, our method underperforms QLoRA on memory-constrained settings and shows limited benefits for larger models (>30B parameters), suggesting that our insights may not scale effectively. Code will be released upon acceptance.",
    "id": 1200
  },
  {
    "title": "Adaptive Gradient Clipping with Learned Threshold Schedules for Low-Precision Training",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Gonzalez, M."
    ],
    "abstract": "Training deep neural networks in low-precision regimes remains challenging due to gradient instabilities and representational limitations. We propose Learnable Gradient Clipping (LGC), a method that adaptively adjusts clipping thresholds during training using a lightweight meta-learning approach. Our method learns threshold schedules via gradient-based optimization on a small validation set, avoiding the need for extensive hyperparameter tuning. We evaluate LGC on ResNet-50 and Vision Transformer architectures using 8-bit and 16-bit fixed-point training. Results show 0.5-1.2% accuracy improvements over standard gradient clipping baselines on ImageNet, with the biggest gains observed in 8-bit precision settings. Our analysis reveals that LGC particularly helps during the initial training phase when gradient magnitudes are rapidly changing. While the improvements are modest, our approach is computationally efficient, adding less than 2% training overhead, and may benefit practitioners working with constrained hardware. However, we observe that the effectiveness of LGC diminishes when combined with advanced optimizers like AdamW or when training larger models. Our code and pre-computed threshold schedules are publicly available.",
    "id": 1203
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Layer-wise Learning Rate Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Recent theoretical work has established connection between the convergence dynamics of neural networks and their generalization performance. We propose a layer-wise learning rate scheduling scheme that increases learning rates for deeper layers while decaying rates for earlier layers during training. Our theoretical analysis shows this approach leads to tighter PAC-Bayesian generalization bounds that scale more favorably with network depth compared to standard schedules. On CIFAR-10 and ImageNet subsets, our method achieves 2-3% improvements over vanilla SGD with cosine annealing, though gains diminish on larger architectures. While our bounds improve upon previous work for networks with 3-8 layers, they remain vacuous for state-of-the-art deep architectures. The scheduling scheme introduces two hyperparameters that must be tuned per-dataset, limiting practical applicability. Our empirical evaluation on standard benchmarks provides moderate improvements but falls short of matching performance gains from recent architectural innovations. Code and experimental details are provided for reproducibility.",
    "id": 1209
  },
  {
    "title": "ReLU Networks Can Learn Polynomial Features via Gradient Descent with Random Initialization",
    "authors": [
      "Wang, L.",
      "Chen, S.",
      "Thompson, K."
    ],
    "abstract": "We study whether shallow ReLU networks can learn low-degree polynomial features from data generated by a target polynomial of degree k. While prior work establishes learnability for specialized architectures or modified training procedures, we analyze standard gradient descent on vanilla ReLU networks with standard initialization. Our main result shows that networks with width polynomial in d and k can achieve population loss \u03b5 after O(d^k/\u03b5^2) iterations, provided the target polynomial satisfies a non-degeneracy condition on its high-order Fourier coefficients. The analysis leverages a connection between the Hermite expansion of ReLU functions and the polynomial basis, though our bounds depend exponentially on the degree k. Experiments on synthetic data demonstrate our theoretical predictions hold for degrees k \u2264 4, but performance degrades significantly for k \u2265 5. While our results provide the first polynomial-time guarantees for learning degree-k polynomials with standard ReLU networks under natural assumptions, the exponential dependence on k and restrictive non-degeneracy condition limit practical applicability. We discuss potential extensions to deeper architectures and connections to recent work on feature learning in neural networks.",
    "id": 1226
  },
  {
    "title": "Variance-Reduced Gradient Boosting with Adaptive Subsampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient boosting remains a dominant approach for tabular data, yet its training complexity typically scales linearly with dataset size. We propose VR-GBoost, a variance-reduced gradient boosting framework that employs adaptive subsampling motivated by stochastic optimization techniques. Our method dynamically adjusts the fraction of data used at each boosting iteration based on gradient norms and incorporates control variates to reduce the variance of gradients estimated from subsamples. Unlike existing approaches that use fixed sampling rates, VR-GBoost theoretically decreases the required sample complexity from O(n) to O(n^2/3) iterations while maintaining the same convergence guarantees. Experimental evaluation on 8 UCI datasets shows 1.2-1.8x training speedups with modest accuracy improvements (0.5-1.3%) over XGBoost, though these gains diminish on high-dimensional sparse datasets. While our theoretical analysis requires strong convexity assumptions that may not hold in practice, our empirical results suggest the approach can be practically useful despite theoretical limitations. Code is available at anonymous.link.",
    "id": 1236
  },
  {
    "title": "On the Effectiveness of Temperature Scaling for Out-of-Distribution Detection in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Temperature scaling has emerged as a simple yet effective post-processing technique for calibrating neural network confidence. While primarily studied in the context of in-distribution calibration, we investigate whether this single-parameter transformation can improve out-of-distribution (OOD) detection. Through experiments on CIFAR-10/100 and ImageNet, we find that appropriately tuned temperature scaling can improve AUROC by 2-5% over baseline approaches across several OOD datasets, though gains vanish when the OOD data distribution shifts significantly from the validation set used for tuning. Our theoretical analysis reveals that temperature scaling affects the relative entropy between in- and out-of-distribution samples, providing intuition for when improvements can be expected. However, we find the approach less reliable than specialized OOD detection methods, achieving state-of-the-art performance on only 3 out of 8 benchmark settings. While our results suggest temperature scaling as a practical improvement to existing systems with minimal implementation cost, the method's sensitivity to validation set choice and limited theoretical guarantees warrant caution in deployment.",
    "id": 1237
  },
  {
    "title": "LayerNorm Without The Norm: Pre-Normalization Transformers via Learned Scaling Parameters",
    "authors": [
      "Chen, L.",
      "Vasudevan, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer architectures rely heavily on Layer Normalization (LayerNorm) for stable training, but this introduces computational overhead and potential representational bottlenecks. We propose LSFormer, a modification to the standard transformer block that replaces LayerNorm with learned scaling parameters derived from the residual stream's statistics. Our approach computes channel-wise scaling factors through a lightweight MLP conditioned on the input's mean and variance, eliminating the need for explicit normalization while maintaining training stability. We evaluate LSFormer on language modeling tasks using 125M parameter models trained on C4 and WikiText-103. Results show comparable perplexity to baseline transformers with a 7-12% reduction in training time on TPU-v4 hardware, though convergence is less stable across hyperparameter configurations. Ablations reveal that scaling parameters alone provide 80% of LayerNorm's benefits, suggesting additional mechanisms may be needed for full stability. While our method offers modest computational savings, the gains diminish at larger scales (1.3B parameters), and we observe 2-3% worse perplexity on out-of-domain data. These findings indicate learned scaling can partially replace LayerNorm in resource-constrained settings, but highlight the need for better understanding of normalization's role in transformer optimization dynamics.",
    "id": 1248
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization. While existing gradient surgery methods (PCGrad, GradNorm) aim to resolve these conflicts, they can be overly aggressive, leading to suboptimal shared representations. We propose Adaptive Gradient Harmonization (AdaGH), a novel approach that selectively applies gradient surgery based on task similarity scores computed via Hessian trace approximations. Our method dynamically adjusts the degree of gradient modification, performing less surgery when tasks are deemed compatible. On CIFAR-100 split into 5 tasks and NYUv2 semantic segmentation with depth estimation, AdaGH achieves 2.1% and 1.3% improvements over PCGrad respectively, while requiring 15% fewer gradient modifications. However, we observe that benefits diminish when task count exceeds 8 or when tasks become highly dissimilar. Our analysis reveals that excessive gradient surgery can harm performance on the highest-priority task, questioning the universality of aggressive conflict resolution strategies. While AdaGH shows promise in specific settings, its computational overhead and sensitivity to hyperparameters may limit practical adoption.",
    "id": 1249
  },
  {
    "title": "Accelerated Gradient Descent via Adaptive Learning Rate Scaling with Quadratic Model Approximation",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Rodriguez, C.",
      "Thompson, A."
    ],
    "abstract": "We propose LASQ, a first-order optimization method that adaptively adjusts learning rates using local quadratic approximations without Hessian computations. LASQ maintains exponential moving averages of gradient norms to estimate local curvature, then scales the learning rate inversely proportional to this estimate. Unlike Adam-style methods that use gradient moments, our approach directly models the loss surface curvature through a lightweight quadratic surrogate updated at each step. We prove convergence rates for convex and non-convex objectives under standard assumptions, achieving O(1/T) and O(1/\u221aT) rates respectively. Experiments on CIFAR-10 and ImageNet show LASQ marginally outperforms SGD with hand-tuned schedules and matches AdamW on ResNet-50 (75.2% vs 75.4%) while using 15% fewer iterations. However, LASQ shows mixed results on transformer architectures and tasks with heavy regularization. Our ablations reveal the quadratic approximation degrades on highly non-stationary objectives. Code is available at [anonymous link].",
    "id": 1252
  },
  {
    "title": "LayerNorm Alternatives for Transformer Architectures via Learnable Affine Transformations",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Layer Normalization (LayerNorm) has become a standard component in transformer architectures, but its computational cost and potential training instabilities motivate the search for alternatives. We propose Learnable Affine Normalization Transform (LANT), a drop-in replacement for LayerNorm that uses learned affine transformations and scaled residual connections to maintain training stability while reducing compute. Our method eliminates the need for calculating mean and variance across feature dimensions, instead relying on element-wise learnable scale and shift parameters that adapt during training. We evaluate LANT on standard language modeling tasks (WikiText-103, OpenWebText) and machine translation benchmarks (WMT'14 EN-DE). Results show LANT achieves comparable perplexity to LayerNorm (-0.8% on WikiText-103) while reducing training time by 12-15%. However, performance degrades on longer sequences (>2048 tokens), and we observe increased gradient norm variance in deeper models (>48 layers). Analysis reveals LANT works best for medium-scale models (\u2264350M parameters) but struggles with larger architectures. While our contribution is primarily empirical and the theoretical justification remains incomplete, LANT provides a practical alternative for resource-constrained training scenarios where minor accuracy loss is acceptable for improved efficiency.",
    "id": 1261
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Dynamic Low-Rank Approximation with Adaptive Budget Allocation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a standard approach for parameter-efficient fine-tuning of large language models, but its fixed rank allocation across layers may be suboptimal. We propose LoRA-Lite, a method that dynamically adjusts the rank of low-rank matrices during fine-tuning based on layer-wise importance scores derived from gradient covariance. Our approach employs an online learning algorithm that redistributes the parameter budget across layers to maximize task performance while respecting memory constraints. On the GLUE benchmark, LoRA-Lite achieves comparable performance to standard LoRA (average score 83.2 vs 82.7) while using 35% fewer parameters. Experiments on 7B and 13B parameter models show memory reductions of 20-40% with minimal degradation on downstream tasks. However, we observe that the dynamic allocation sometimes converges to suboptimal local minima for certain task types, particularly those requiring complex reasoning. While LoRA-Lite provides practical memory savings, the performance gains are modest and task-dependent, suggesting that more sophisticated rank adaptation strategies may be needed. Code and models will be released upon acceptance.",
    "id": 1262
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Structured Sparsity for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.K.",
      "Kim, S.",
      "Johnson, A.B."
    ],
    "abstract": "We present LoRA-Drop, a method for selectively applying low-rank adaptation modules in pre-trained language models based on a learned sparsity pattern. While Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning technique, we observe that not all layers benefit equally from rank decomposition. We introduce a differentiable gating mechanism that learns to identify and skip layers where LoRA provides minimal utility, reducing trainable parameters by 30-50% without significant performance degradation. Our approach combines magnitude-based pruning with a reinforcement learning component that explores different sparsity patterns across transformer layers. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop achieves comparable accuracy to standard LoRA (within 1.2% average F1) while maintaining the computational benefits of parameter-efficient tuning. However, we find the method is sensitive to initialization strategies and shows particular instability on smaller datasets. Ablations reveal that our sparsity decisions correlate weakly with standard layer attributions, suggesting the learned patterns may be capturing optimization dynamics rather than inherent layer importance. While LoRA-Drop demonstrates the potential for adaptive parameter-efficient methods, the current implementation introduces hyperparameter complexity that may limit practical adoption.",
    "id": 1282
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Head Pruning with Layer-Wise Gradient Similarity",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "While Transformer models achieve state-of-the-art performance across many tasks, their computational cost remains prohibitive for deployment on resource-constrained devices. We propose a simple yet effective attention head pruning method that uses layer-wise gradient similarity to identify redundant heads during fine-tuning. Our approach computes the cosine similarity between gradients of different attention heads within each layer and removes heads whose gradients are most similar to others. Extensive experiments on GLUE and SQuAD show that our method achieves 15-20% FLOP reduction with minimal performance loss (within 1% of full model accuracy). While similar to existing magnitude-based pruning, our gradient similarity approach captures functional redundancy more accurately than weight magnitude alone. However, our method shows diminishing returns on larger models (>1B parameters) and requires full training data for gradient computation. Code will be made available.",
    "id": 1283
  },
  {
    "title": "Gradient Descent with Lookahead and Momentum: A Unified Analysis of Approximate Optimization in Neural Networks",
    "authors": [
      "Liu, S.",
      "Kumar, P.",
      "Chen, J."
    ],
    "abstract": "We propose a unifying framework for analyzing approximate gradient descent algorithms that combine lookahead mechanisms with momentum in neural network training. While lookahead optimizers have shown practical benefits in distributed settings, theoretical understanding remains fragmented, particularly when combined with momentum terms. Our approach introduces a perturbed gradient flow analysis that treats lookahead steps as approximate proximal updates under momentum dynamics. We prove that this combination achieves O(1/T) convergence for smooth convex objectives and O(1/\u221aT) for non-convex cases, matching standard momentum rates up to constant factors. Experiments on CIFAR-10/100 and ImageNet demonstrate 2-3% accuracy improvements over standard baselines when training ResNet-18 and Vision Transformer architectures, though gains diminish on larger models. Analysis reveals that lookahead primarily stabilizes gradient variance during early training phases, with momentum dominance emerging later. While our theoretical bounds are essentially tight within our framework, they do not capture the empirical improvements we observe. The work provides the first systematic treatment of lookahead-momentum interactions, though connections to practical generalization benefits remain unclear.",
    "id": 1299
  },
  {
    "title": "Frequency-Aware Gradient Clipping for Stable Transformer Training",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Garcia, A."
    ],
    "abstract": "Transformer models often suffer from training instability due to gradient explosion, particularly in the early training stages. While gradient clipping is a common remedy, we find that standard clipping techniques treat all parameters uniformly, potentially hindering the learning dynamics of rapidly-converging components. We propose Frequency-Aware Gradient Clipping (FAGC), which adaptively adjusts clipping thresholds based on the historical frequency of large gradient occurrences at the parameter level. Our method maintains a simple exponential moving average of gradient norms and modulates clipping boundaries accordingly. We evaluate FAGC on standard language modeling tasks using WikiText-103 and OpenWebText, showing modest improvements in training stability and validation perplexity over vanilla gradient clipping (0.5-1.2 perplexity reduction). While our theoretical analysis provides convergence guarantees under bounded gradient assumptions, the practical gains are most pronounced in specific training regimes (learning rates \u2265 3e-4) and diminish with careful learning rate scheduling. Our experiments suggest FAGC offers a practical, low-overhead alternative to standard clipping, though benefits may be task-dependent.",
    "id": 1304
  },
  {
    "title": "DropAttention: Reducing Computational Overhead in Transformer Self-Attention via Structured Pruning",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "Transformers achieve state-of-the-art results across many domains but suffer from quadratic complexity in self-attention computation. We propose DropAttention, a simple yet effective method to reduce attention computation by pruning attention heads based on their gradient norms during training. Unlike previous approaches that require architectural modifications or additional hyperparameters, DropAttention operates as a drop-in replacement for standard attention modules with minimal implementation overhead. Our method progressively removes less informative heads throughout training, achieving 30-50% reduction in FLOPs with <2% degradation on GLUE and WMT benchmarks. While the approach shows promise on standard benchmarks, we observe performance degradation on tasks requiring long-range reasoning, limiting its applicability. We provide empirical evidence that remaining heads compensate for pruned ones through increased activation sparsity. Our PyTorch implementation requires only 20 lines of code. While DropAttention achieves consistent speedups across architectures, the method shows sensitivity to initialization and achieved gains diminish with larger models (>1B parameters), suggesting scalability limitations.",
    "id": 1310
  },
  {
    "title": "LoRA-MD: Memory-Efficient Low-Rank Adaptation with Momentum Distillation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Garcia, M.A.",
      "Thompson, J."
    ],
    "abstract": "We present LoRA-MD, a method that combines low-rank adaptation (LoRA) with momentum distillation to improve parameter-efficient fine-tuning of large language models while maintaining memory efficiency. Our key insight is that momentum information from teacher models can be distilled into low-rank adapters without storing full gradients, addressing the gradient staleness issue observed in vanilla LoRA training. The method introduces a momentum buffer stored in the compressed low-rank space, updated via an exponential moving average of historical adapter updates. Experiments on GLUE and SuperGLUE benchmarks with 7B parameter models show modest improvements over LoRA (average +1.2% accuracy) while using 15% less memory during training. However, we find diminishing returns on larger models (>30B parameters) and minimal gains on domain-specific tasks. The method requires careful tuning of the momentum coefficient and performs best when the downstream task distribution closely matches the pre-training data. Our empirical analysis suggests the benefits are most pronounced in resource-constrained scenarios where memory efficiency is critical. Code and pre-trained adapters will be made available upon acceptance.",
    "id": 1312
  },
  {
    "title": "Meta-Gradient Descent with Momentum for Adaptive Learning Rate Selection in Mini-Batch Deep Learning",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Anderson, J."
    ],
    "abstract": "We propose a novel meta-learning approach for adaptive learning rate selection in stochastic gradient-based optimization. Our method, Meta-Gradient Descent with Momentum (MGDM), uses a bi-level optimization framework to learn momentum coefficients that adaptively adjust learning rates based on local gradient statistics. Unlike previous meta-optimization approaches that require expensive meta-objective evaluations, MGDM approximates the meta-gradient using moving average statistics, achieving computational overhead comparable to standard momentum methods. We evaluate MGDM on CIFAR-10/100 and ImageNet classification tasks across ResNet and EfficientNet architectures, demonstrating 2-5% improvement in final validation accuracy compared to AdamW and SGDM baselines. While our results are competitive with recent adaptive methods like Lion and NAdam, we find that performance gains diminish when training beyond 200 epochs or when applied to smaller models (\u226410M parameters). Theoretically, we establish convergence guarantees for MGDM in the \u03bc-strongly convex case, though the assumptions are restrictive compared to practical deep learning scenarios. Our code and hyperparameter configurations are publicly available for reproducibility.",
    "id": 1318
  },
  {
    "title": "Adaptive Gradient Clipping with Lookahead: A Simple but Effective Trick for Stable Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Training large transformer models often suffers from gradient instability, leading to divergent training or poor final performance. While numerous sophisticated optimization techniques have been proposed, their implementation complexity and hyperparameter sensitivity limit adoption in practice. We introduce a simple but surprisingly effective combination of existing techniques: gradient clipping with an adaptive threshold that scales with the parameter norm, combined with the Lookahead optimizer. Our method requires only two additional hyperparameters and minimal computational overhead. Across 8 different language modeling and machine translation tasks, our approach achieves a 15-30% reduction in training time compared to standard Adam, while maintaining or slightly improving final perplexity/BLEU scores. However, we find the improvements are most pronounced in specific regimes: medium-sized models (100M-1B parameters) with aggressive learning rates. Theoretical analysis suggests our clipping threshold approximates a trust region update, though this connection remains underdeveloped. While the contributions are incremental, we believe our findings provide practical value for practitioners struggling with training stability, and our extensive hyperparameter sensitivity analysis offers guidance for implementation.",
    "id": 1320
  },
  {
    "title": "Improved AdamM: Momentum-Aware Adam with Layer-wise Learning Rates for Better Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose AdamM, a simple modification to the Adam optimizer that incorporates momentum-aware adaptive learning rates and layer-wise scheduling. While Adam has become the de facto optimizer for training deep neural networks, we observe that its update rule leads to suboptimal generalization in many settings, particularly when training ResNet and Transformer architectures. Our method introduces two key innovations: (1) a momentum-aware correction term that adjusts the second moment estimate based on gradient history, and (2) a layer-wise learning rate scheme that decays learning rates at different rates for different architectural components. Experiments on CIFAR-10/100, ImageNet, and WMT English-German translation show 1.2-2.3% improvement in final validation accuracy over vanilla Adam and its variants (AdamW, AdaBelief), with minimal computational overhead. However, we find that AdamM does not consistently outperform SGD with momentum on large-scale Vision Transformer training. Code is available at https://anonymous.url/adam-optimizer.",
    "id": 1328
  },
  {
    "title": "Gradient Surgery in Federated Learning: A Topological Approach to Client Drift",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "Federated learning faces challenges from client drift, where gradient updates from distributed devices become misaligned. While existing approaches like FedAvg and FedProx apply uniform regularization, we propose TopoFed, which uses persistent homology to detect and correct topological inconsistencies in gradient manifolds across clients. Our method computes the persistence diagrams of local loss landscapes and performs gradient surgery by aligning high-dimensional homological features before aggregation. We theoretically prove that this reduces an upper bound on client drift by a factor of O(\u221a(log K)), where K is the number of clients. Experimental results on CIFAR-10 and FEMNIST show 2-3% accuracy improvements over baselines in non-IID settings, particularly when client data distributions have high Wasserstein distance from the global distribution. However, we observe diminishing returns as the number of clients increases beyond 100, likely due to accumulated approximation errors in homology computation. While TopoFed provides a novel perspective on mitigating drift, the computational overhead (2.5x slower than FedAvg) and limited empirical gains suggest the approach may benefit from more efficient topological approximations. Our code is available at [repository].",
    "id": 1329
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Entropy-Regularized REINFORCE",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. We propose a simple modification to the REINFORCE estimator that adds an entropy regularization term to the learning objective, which we show reduces gradient variance without introducing significant bias. Our method requires only minimal changes to existing implementations and adds negligible computational overhead. We provide theoretical analysis showing that our estimator achieves lower variance than standard REINFORCE under mild assumptions about the reward distribution. Empirical results on variational autoencoders with discrete latents show modest improvements in ELBO and sample quality on binarized MNIST and CIFAR-10, achieving 3-5% better log-likelihood compared to standard baselines. While our approach does not match the performance of more sophisticated gradient estimators like REBAR or RELAX, it offers a practical alternative when computational constraints or implementation complexity are concerns. Code is available at anonymous.github.io.",
    "id": 1344
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Learned Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. While continuous relaxations like the Gumbel-Softmax provide effective alternatives, they introduce temperature-dependent biases that can degrade sample quality. We propose a simple modification to existing score-function estimators by learning a parametric baseline that adapts to the local geometry of the loss landscape. Our approach uses a small neural network conditioned on intermediate activations to predict optimal control variate coefficients, reducing gradient variance without the need for temperature tuning. Unlike recent work on learned baselines, our method requires no additional model parameters at inference time and introduces minimal computational overhead. We evaluate on structured prediction tasks including generative modeling of text and molecules. Results show 15-20% reduction in gradient variance compared to REINFORCE with moving average baselines, leading to modest improvements in log-likelihood (0.05-0.1 nats on average). While the approach shows consistent gains over standard baselines, the improvements are incremental and do not address fundamental scalability limitations of discrete variable models. Code will be available upon acceptance.",
    "id": 1347
  },
  {
    "title": "LoRA-P90: A Simple Thresholding Scheme for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular method for parameter-efficient fine-tuning of large language models, but its rank selection remains largely heuristic. We propose LoRA-P90, a straightforward thresholding approach that prunes 10% of the smallest singular values in the LoRA weight matrices during training. Through experiments on GLUE and SuperGLUE benchmarks using LLaMA-7B, we show that this simple modification yields modest improvements (0.7-1.2% average score increase) over standard LoRA while maintaining parameter efficiency. Our analysis reveals that thresholding helps remove noisy updates, particularly beneficial when the adaptation rank is set higher than necessary. While the contribution is incremental, our work suggests that careful analysis of singular value distributions in LoRA modules can inform better rank selection strategies. Code and pre-trained adapters will be made available upon acceptance.",
    "id": 1353
  },
  {
    "title": "Regularizing Gradient Noise for Improved Generalization in Stochastic Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, V.",
      "Okafor, N."
    ],
    "abstract": "While stochastic gradient descent (SGD) remains the workhorse of deep learning optimization, its inherent noise is often viewed as a double-edged sword\u2014potentially beneficial for generalization yet difficult to control. We propose Gradient Noise Regularization (GNR), a simple technique that adds controlled noise to gradients during training, where the noise variance is adaptively scaled based on the gradient's magnitude. Our method builds on the intuition that moderate levels of gradient noise can act as implicit regularization, but excessive noise harms optimization. Through extensive experiments on CIFAR-10/100 and ImageNet, we demonstrate that GNR achieves 0.5-1.2% accuracy improvements over standard SGD with momentum, while maintaining comparable training speed. Theoretical analysis in a simplified quadratic setting suggests GNR approximates a form of data-dependent regularization. While the gains are consistent, they are modest compared to recent advances in sharpness-aware minimization and adaptive optimization. We further find that GNR's effectiveness varies significantly across architectures and datasets, with limited benefits on vision transformers. Our code is available at anonymous-link.github.io/GNR.",
    "id": 1354
  },
  {
    "title": "LoRA-Drop: Structured Pruning of Low-Rank Adaptation Weights via Magnitude-Aware Gradient Tracking",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We present LoRA-Drop, a simple method for pruning low-rank adaptation matrices in parameter-efficient fine-tuning. While LoRA has become prevalent for adapting large language models, the optimal rank choice remains ad hoc and can lead to overparameterization. Our approach tracks gradient magnitudes during initial training steps to identify important rank components, then drops the least significant 30-50% of singular values without retraining. Experiments on GLUE and SuperGLUE show 20-35% parameter reduction with <1% performance degradation on most tasks. However, we observe performance drops exceeding 5% on tasks requiring strong reasoning (e.g., ReCoRD), suggesting our importance heuristic may miss task-specific features. Analysis reveals LoRA-Drop works best when pre-trained representations are well-matched to downstream tasks. While preliminary, these results indicate structured pruning can reduce LoRA footprint at modest accuracy cost, though further work is needed to develop better importance criteria for complex reasoning tasks.",
    "id": 1368
  },
  {
    "title": "Gradient Compression with Learned Quantization Schedules for Heterogeneous Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Johnson, K."
    ],
    "abstract": "Federated learning faces communication bottlenecks when training across heterogeneous devices with varying bandwidth and computational constraints. While gradient compression techniques reduce communication overhead, they typically use fixed quantization strategies that cannot adapt to the diverse capabilities of participating devices. We propose Adaptive Learned Quantization (ALQ), a method that learns device-specific quantization schedules during training using lightweight meta-networks that predict optimal bit-widths based on local gradient statistics and device characteristics. Our approach introduces novel regularization terms that balance compression efficiency with convergence stability, extending theoretical convergence bounds to account for adaptive quantization. Experiments on CIFAR-10, CIFAR-100, and a real-world federated image classification dataset demonstrate 1.8-3.2x communication reduction compared to uniform quantization baselines, with modest accuracy drops (0.5-1.2%). However, we observe that ALQ's benefits diminish in highly non-IID settings and with very large models (>50M parameters), where meta-network overheads can exceed communication savings. Our code is available at [anonymized link].",
    "id": 1369
  },
  {
    "title": "Reinforcement Learning with Gradient-Augmented Value Functions: A Quasi-Newton Approach to Policy Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Policy gradient methods face well-documented challenges with sample efficiency and convergence in high-dimensional action spaces. We propose Gradient Augmented Policy Optimization (GAPO), which incorporates second-order curvature information via a quasi-Newton approximation of the value function landscape. Our method computes natural policy gradients using a computationally efficient rank-2 update of the preconditioning matrix, avoiding expensive Hessian computations while capturing local geometry. We prove that GAPO achieves convergence rates of O(1/\u221aT) in the general case and O(1/T) under certain smoothness assumptions, matching theoretical bounds of existing approaches while reducing per-iteration complexity. Experimental results on MuJoCo continuous control benchmarks demonstrate 15-23% sample efficiency improvements over PPO and SAC on half of the tested environments, with comparable performance on the remainder. However, we observe training instability in environments with sparse rewards. The method introduces three additional hyperparameters that require environment-specific tuning. While GAPO provides meaningful gains in specific domains, its practical impact may be limited by implementation complexity and sensitivity to hyperparameter choices.",
    "id": 1373
  },
  {
    "title": "On the Convergence of Gradient Descent for Overparameterized ReLU Networks with Layer-wise Step Sizes",
    "authors": [
      "Chen, Y.",
      "Rodriguez, A.",
      "Kim, S.",
      "Nair, V."
    ],
    "abstract": "We study the convergence properties of gradient descent for training overparameterized ReLU networks when using layer-wise adaptive step sizes. Motivated by empirical observations that different layers in deep networks exhibit varying gradient magnitudes, we propose a simple modification to standard gradient descent where each layer's update is scaled by an individual step size. Our theoretical analysis shows that with appropriate initialization and sufficient overparameterization, this approach achieves linear convergence to a global minimum for binary classification problems. We prove this under a modified neural tangent kernel framework that accounts for layer-dependent learning rates. Experimental results on MNIST and CIFAR-10 datasets demonstrate that layer-wise step sizes can provide modest improvements over standard gradient descent, reducing training time by 10-20% while achieving comparable test accuracy. However, the benefits diminish as network depth increases beyond 10 layers. While our theoretical results provide some insight, the assumptions regarding layer-wise smoothness and initialization may be too restrictive for practical settings. Our work suggests that layer-wise step sizes can be a useful heuristic for training shallow networks, but further investigation is needed to understand their role in deeper architectures.",
    "id": 1381
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Simple Combination of SAM and Gradient Dropout for Improved Generalization",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising approach for improving generalization by seeking flat minima, but its effectiveness remains inconsistent across architectures and datasets. We observe that SAM's gradient conflicts with standard gradient descent directions, potentially limiting its benefits. Motivated by this, we propose DropSAM, a simple modification that combines SAM with targeted gradient dropout during the ascent step. Our method randomly drops gradients with highest curvature in selected layers, reducing conflicting updates while maintaining SAM's sharpness regularization. Through extensive experiments on CIFAR-10/100 and ImageNet, we show DropSAM achieves 0.3-0.7% accuracy improvements over SAM with minimal computational overhead, particularly effective for ResNet architectures. While the improvements are modest, we provide theoretical insights suggesting our approach bounds generalization error through reduced gradient interference. Code will be made available upon acceptance.",
    "id": 1391
  },
  {
    "title": "Improving Generalization in Reinforcement Learning with Adaptive Experience Replay Scheduling",
    "authors": [
      "Chen, Y.",
      "Rodriguez, L.",
      "Kim, J."
    ],
    "abstract": "Experience replay has become a cornerstone technique in deep reinforcement learning, yet its impact on out-of-distribution generalization remains poorly understood. We investigate whether dynamically scheduling replay buffer sampling rates can improve zero-shot transfer performance across environments with varying dynamics. Our method, AdaptiveReplay, adjusts the replay ratio based on estimated policy improvement uncertainty using an ensemble of value functions. On 8 continuous control tasks from MuJoCo and 4 procedurally generated environments, AdaptiveReplay achieves a 12% relative improvement in transfer performance compared to fixed replay schedules, though gains are inconsistent across domains. While the approach introduces minimal computational overhead, we find that benefits diminish when source and target domains differ substantially, suggesting limitations in our uncertainty-based scheduling mechanism. These results indicate that replay scheduling offers modest but measurable generalization improvements, particularly when domain shifts are moderate.",
    "id": 1394
  },
  {
    "title": "Gradient Descent with Memory-Efficient Curvature Approximation via Rank-1 Sketches",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Subramanian, K."
    ],
    "abstract": "We propose a memory-efficient variant of second-order optimization that approximates curvature information using rank-1 sketches of the Hessian matrix. Our method, called Sketched Curvature Descent (SCD), stores only O(d) parameters compared to O(d\u00b2) for standard quasi-Newton methods, while still capturing local curvature beyond what first-order methods provide. The key insight is that carefully constructed random rank-1 measurements of the Hessian can provide sufficient information for effective preconditioning when combined with momentum-like updates. We evaluate SCD on training ResNet-18 on CIFAR-10 and CIFAR-100, as well as transformer language models on WikiText-103. Results show modest improvements over Adam and SGD+Momentum on some tasks (0.5-1.2% accuracy gains, 10-15% faster convergence), but not consistently across all settings. Analysis reveals that the approximation quality degrades for ill-conditioned problems, suggesting the method is best suited for moderately conditioned objectives. While SCD provides a practical trade-off between first and second-order methods, it does not fundamentally resolve the challenges of scaling curvature-based optimization to modern deep architectures. Code is available at [anonymous link].",
    "id": 1398
  },
  {
    "title": "Gradient Magnitude Asymmetry: A Simple Indicator for Mode Collapse in GANs",
    "authors": [
      "Lee, D.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We investigate whether the magnitude of discriminator gradients can serve as an early warning signal for mode collapse in Generative Adversarial Networks. Through empirical analysis across 8 datasets and 4 architectural variants, we find that the ratio of gradient norms between real and fake samples exhibits a consistent asymmetry pattern preceding collapse events. Building on this observation, we propose Gradient Averaging Regularization (GAR), which penalizes this asymmetry during training. While our method shows modest improvements on established metrics (FID improves by 8-12% on average), the primary contribution lies in providing practitioners with a computationally lightweight diagnostic tool requiring only minor code modifications to existing pipelines. We conduct ablations demonstrating correlation with mode drop in synthetic mixture experiments, though generalization to more complex settings remains partial. Code will be released upon acceptance.",
    "id": 1401
  },
  {
    "title": "Revisiting Scheduled Sampling in Transformer Decoders with Adaptive Noise Injection",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "Scheduled sampling has long been proposed as a remedy for exposure bias in sequence-to-sequence models, yet its benefits for modern Transformer architectures remain inconclusive. We re-examine scheduled sampling for Transformer language generation through a unifying lens that connects it to a family of adaptive noise injection techniques. Our theoretical analysis reveals that while exposure bias exists, scheduled sampling only helps under restrictive conditions on the data distribution and model mismatch. Building on these insights, we propose Adaptive Scheduled Noise (ASN), which modulates the noise injection schedule based on decoder uncertainty estimates. Experiments on machine translation and summarization benchmarks show modest improvements of 0.4-0.7 BLEU/ROUGE over standard training when carefully tuned. However, we find these gains are inconsistent across datasets and sensitive to hyperparameter choices. Further analysis suggests the benefits may stem from increased robustness to distributional shift rather than solving exposure bias per se. While our findings clarify when scheduled sampling can help Transformers, they also highlight fundamental limitations that suggest complementary approaches may be necessary.",
    "id": 1402
  },
  {
    "title": "Improving Transformer Efficiency via Learned Sparse Attention Patterns with Fixed Memory Budgets",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose SparseFlex, a method for reducing transformer memory usage by learning sparse attention patterns under strict memory constraints. While existing sparse attention mechanisms use predefined patterns or require expensive re-training, our approach learns sparsity masks via auxiliary loss functions that enforce fixed memory budgets during both training and inference. Our key insight is to optimize attention sparsity jointly with task objectives using a two-stage training procedure: first pre-training with full attention, then fine-tuning with learned sparsity masks constrained by memory targets. Experiments on WikiText-103 and C4 datasets show 2.1\u00d7 memory reduction compared to dense attention with only 3.2% perplexity increase. However, we observe the method proves brittle when targeting aggressive sparsity (>90%), and fails to transfer well across domains without re-training. Ablations reveal the importance of temperature-annealing schedules for mask learning, though optimal hyperparameters vary significantly across tasks. While our results suggest learned sparse attention can achieve practical memory savings, the approach offers limited theoretical guarantees and depends critically on careful hyperparameter tuning. Code and pre-trained masks are available at anonymized-url.",
    "id": 1408
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Selective Gradient Accumulation",
    "authors": [
      "Liu, K.",
      "Gonzalez, M.",
      "Johnson, T."
    ],
    "abstract": "Multi-task learning in deep neural networks often suffers from conflicting gradient directions between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradNorm provide simple heuristics for resolving gradient conflicts, they rely on instantaneous gradient estimates and lack mechanisms for long-term gradient history. We propose Adagradient Memory (AGM), a lightweight framework that maintains an adaptive memory of past gradients to guide conflict resolution. Our method selectively accumulates gradient components across training steps, using a novel similarity-based gating mechanism to determine which past gradients influence current updates. Experiments on standard multi-task benchmarks (NYUv2, CityScapes) show 1.2-2.1% improvement over PCGrad on average, with minimal computational overhead (<3% additional memory). While our empirical gains are modest, we provide theoretical analysis showing AGM converges under certain regularity conditions. However, we find performance is sensitive to hyperparameter choices and degrades on highly imbalanced task distributions. Code is available at [anonymized].",
    "id": 1425
  },
  {
    "title": "On the Implicit Bias of Adam with Weight Decoupling in Small-Scale Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate the convergence behavior and implicit bias of AdamW in small-scale transformer architectures (\u2264 50M parameters). While previous work has established theoretical guarantees for Adam variants, empirical observations suggest the behavior differs significantly in practice, particularly when weight decay is decoupled from the optimization step. We propose a refined update rule that incorporates layer-wise adaptive moments and demonstrate improvements in downstream fine-tuning tasks. Our theoretical analysis, while limited to simplified linear cases, provides partial justification for our empirical findings. Experiments on GPT-2 small and BERT-base models show 2-3% improvements on GLUE benchmarks and modest gains in language modeling perplexity. The proposed method requires minimal hyperparameter tuning and incurs less than 2% computational overhead compared to standard AdamW. While our contributions are incremental, we believe the insights into adaptive optimization dynamics in transformer architectures may guide future algorithmic improvements. Limitations include the restricted theoretical setting and modest scale of experimental validation.",
    "id": 1433
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum for Non-Stationary Online Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is widely used in neural network training to prevent exploding gradients, but fixed clipping thresholds can be suboptimal in non-stationary environments where gradient distributions change over time. We propose Adaptive Gradient Clipping with Momentum (AGCM), a simple modification that combines exponential moving averages of gradient norms with adaptive clipping bounds. AGCM uses two momentum terms to dynamically adjust clipping thresholds based on recent gradient behavior, eliminating the need for manual tuning. We provide theoretical analysis showing convergence rates within a constant factor of vanilla SGD in convex settings. Empirically, AGCM achieves 2-4% improvements over standard clipping on CIFAR-10/100 when training ResNet-18 under simulated distribution shift, and shows modest gains in online learning tasks with concept drift. While the improvement is not dramatic, AGCM requires no additional hyperparameters beyond standard clipping and adds minimal computational overhead. Limitations include lack of improvement on stable datasets and sensitivity to the initial clipping threshold when momentum terms are poorly initialized.",
    "id": 1435
  },
  {
    "title": "Momentum Residual Connections for Improved Optimization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "While residual connections have enabled training of very deep networks, we argue that their additive nature creates optimization dynamics that can slow convergence in later training stages. We propose Momentum Residual Connections (MoRes), which replace the standard skip addition with a momentum-weighted combination of the identity and transformation branches. Our key insight is that this modification allows the network to gradually shift from primarily learning residuals to learning full transformations, providing smoother optimization than vanilla residual blocks. We demonstrate the effectiveness of MoRes on ImageNet and CIFAR-10, achieving 0.8% and 1.2% accuracy improvements over baseline ResNet architectures respectively. Through empirical analysis, we show that MoRes networks exhibit lower gradient variance and faster convergence in later training phases. However, we also identify sensitivity to the momentum hyperparameter across datasets, suggesting the need for careful tuning. While our results are modest, they suggest that reconsidering the residual connection mechanism may offer practical benefits for training deep architectures. Code is available at: anonymous-url.github.io/mores.",
    "id": 1444
  },
  {
    "title": "Gradient Descent with Adaptive Learning Rates via Online Bin Packing",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Foster, J."
    ],
    "abstract": "We propose a novel adaptive learning rate method for gradient descent that draws inspiration from online bin packing algorithms. Our key insight is to view the learning rate selection problem as a resource allocation task, where we pack gradient updates into bins representing learning rate values. Using the Harmonic+ algorithm from the bin packing literature, we partition parameter updates into groups with similar gradient magnitudes, assigning appropriate learning rates to each group. Theoretically, we prove O(log T) regret bounds for convex Lipschitz functions, matching standard adaptive methods. Empirically, we evaluate on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures, achieving 0.3-0.7% accuracy improvements over AdamW and 1.2-1.8% over SGD with momentum. While our method shows consistent gains on small models and datasets, we observe the improvements diminish with larger models (ViT-B/16). Additionally, our approach introduces non-negligible computational overhead (15-20% increase in training time) due to the bin packing subroutine. We release PyTorch code and configurations for reproducibility.",
    "id": 1450
  },
  {
    "title": "Improving Transformer Fine-tuning with Adaptive Weight Re-initialization",
    "authors": [
      "Liu, K.",
      "Chen, M.",
      "Rodriguez, J."
    ],
    "abstract": "While pre-trained transformers achieve remarkable performance on downstream tasks through fine-tuning, we observe that the standard practice of using small random initialization for task-specific heads often leads to slow convergence and suboptimal local minima. We propose Adaptive Weight Re-initialization (AWR), a simple technique that dynamically adjusts initialization scales based on layer-wise gradient statistics during the first few training steps. Our method requires no additional hyperparameters beyond standard fine-tuning and adds minimal computational overhead (<5% training time). We evaluate AWR on GLUE benchmark tasks using BERT-base and RoBERTa-base models, achieving modest but consistent improvements (avg. 0.8% GLUE score increase) over standard fine-tuning. Analysis reveals AWR particularly benefits tasks with limited training data, suggesting improved optimization in low-data regimes. However, we find diminishing returns on larger models (\u2265350M parameters) and minimal impact when using strong regularization techniques. While our contributions are incremental, AWR provides insights into how initialization affects fine-tuning dynamics and offers a practical enhancement for small-to-medium scale applications.",
    "id": 1451
  },
  {
    "title": "Neural Network Weight Clustering for Improved Generalization via Determinantal Point Processes",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Johnson, M."
    ],
    "abstract": "Weight redundancy in deep neural networks has been observed across many architectures, yet principled approaches to reduce redundancy while maintaining generalization remain underexplored. We propose WeightDrop, a training method that induces structured sparsity in neural networks by modeling neuron activations as Determinantal Point Processes (DPPs), promoting diverse weight representations without hard pruning. Our approach encourages weight clustering during training, naturally identifying redundant parameters that can be subsequently removed. We demonstrate that WeightDrop achieves competitive performance on CIFAR-10 and ImageNet, reducing model size by 30-40% while maintaining accuracy within 0.5% of dense baselines. Theoretically, we establish generalization bounds showing that clustered weights enjoy improved margin distributions, though our analysis relies on strong distributional assumptions. Extensive ablations reveal that performance gains are most pronounced in architectures with residual connections, and that the method's effectiveness correlates with initial overparameterization levels. While our empirical results are promising, we acknowledge that the computational overhead during training may limit scalability to larger models, and that improvements over standard pruning baselines are modest.",
    "id": 1452
  },
  {
    "title": "Rethinking Curriculum Learning: A Weak Supervision Approach via Pre-trained Model Priors",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "Curriculum learning has shown promise for training neural networks on hard tasks by gradually increasing difficulty, but designing effective curricula remains challenging. We propose Curriculum by Weak Supervision (CWS), a simple method that leverages pre-trained language models to automatically generate weak supervision signals for curriculum design. Our approach uses masked language models to identify 'easier' examples through token-level confidence scores, creating an implicit curriculum without human-designed rules. We validate CWS on three classification benchmarks (AG News, DBPedia, and ChemProt) and a synthetic reasoning task. Results show 2-4% improvement over standard training in low-data regimes (\u22641k examples), but minimal gains with abundant data. While CWS provides consistent improvements over random ordering baselines, it underperforms compared to hand-crafted curricula in domains where expertise is available. We demonstrate our method is particularly effective when task-specific curricula are expensive to design. However, our approach inherits limitations from the pre-trained model's biases and struggles with out-of-domain transfer. Code and data are available at [anonymous-link].",
    "id": 1465
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique for Stabilizing Deep Network Training",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Gradient Dropout, a lightweight regularization method that randomly masks gradient components during backpropagation to improve training stability in deep neural networks. While dropout is traditionally applied to activations, we show that applying stochastic masking to gradient values can effectively prevent co-adaptation of weight updates without introducing additional computational overhead during inference. Our method adds a single hyperparameter p_g, the probability of dropping gradient components, which we typically set between 0.1 and 0.3 based on validation performance. Empirical evaluation on CIFAR-10/100 and ImageNet shows modest improvements over standard dropout, with our best results achieving 1.2% and 0.8% accuracy gains on CIFAR-100 and ImageNet respectively when applied to ResNet-50 architectures. We provide convergence analysis under standard Lipschitz assumptions and demonstrate that Gradient Dropout can be interpreted as implicit gradient noise injection. However, we find limited benefits on smaller architectures like LeNet and smaller datasets, suggesting the technique may be most useful for very deep networks. Code will be made available upon publication.",
    "id": 1466
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Pruning for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) have enabled effective adaptation of large language models while updating only a small subset of parameters. However, LoRA applies uniform rank across all layers regardless of their importance, potentially leading to computational waste. We propose LoRA-Drop, a simple yet effective approach that adaptively assigns ranks based on layer-wise sensitivity analysis. Our method employs a bilevel optimization framework that simultaneously trains LoRA adapters and learns to prune unnecessary ranks through differentiable gating mechanisms. Experiments on NLP benchmarks (GLUE, SuperGLUE) and vision-language tasks show LoRA-Drop achieves 20-30% parameter reduction compared to standard LoRA while maintaining comparable performance (within 1% accuracy). While our method provides modest efficiency gains, we observe the pruning strategy sometimes removes useful rank components, particularly in lower layers. Ablations reveal the bilevel optimization is sensitive to hyperparameter choices, sometimes leading to unstable training. Our results suggest rank allocation heuristics could be more effective than learned pruning for certain architectures. Code will be released upon acceptance.",
    "id": 1472
  },
  {
    "title": "Gradient Amplification: A Simple but Effective Training Strategy for Small Batch Deep Learning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Gonzalez, M."
    ],
    "abstract": "Training deep neural networks with small batch sizes remains challenging due to gradient noise and instability, particularly when learning rates are scaled proportionally. While techniques like gradient accumulation and batch normalization help, they often require careful tuning or architectural changes. We propose Gradient Amplification (GradAmp), a surprisingly simple method that multiplies gradients by a constant factor \u03b1 > 1 during backpropagation. We show both empirically and through analysis of a simplified quadratic model that this amplifies the signal-to-noise ratio of gradient estimates without changing the optimization trajectory in expectation. On ImageNet with ResNet-50, GradAmp achieves 76.8% top-1 accuracy at batch size 32 compared to 75.1% for standard training, with similar gains on CIFAR-10. However, we find GradAmp benefits diminish with larger batch sizes (>256) and can cause instability on certain architectures. Our theoretical analysis provides convergence guarantees for convex problems but does not extend to the non-convex case. While GradAmp offers a practical improvement for constrained memory scenarios, its limitations suggest it may serve best as a simple baseline rather than a general solution to small-batch training.",
    "id": 1475
  },
  {
    "title": "LoRA-Miner: Efficient Task-Adaptive Subnetworks via Low-Rank Lottery Tickets",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M.",
      "Singh, P."
    ],
    "abstract": "We propose LoRA-Miner, a simple method for identifying task-specific subnetworks within large pre-trained models using low-rank adaptation. Building on the lottery ticket hypothesis, our approach trains low-rank matrices to identify sparse subnetworks that achieve comparable performance to full fine-tuning while using 40% fewer parameters. Our method combines iterative magnitude pruning with low-rank factorization, automatically discovering which layers can be approximated by low-rank structures versus those requiring full weights. Experiments on GLUE and SuperGLUE benchmarks show LoRA-Miner achieves 96-98% of full fine-tuning performance across classification tasks, with 2.3\u00d7 compression over standard LoRA while maintaining interpretable sparsity patterns. However, we find the method's effectiveness diminishes on generative tasks (e.g., summarization) and larger models (\u22657B parameters), where identified subnetworks converge to dense patterns. Analysis reveals our lottery tickets primarily emerge in middle Transformer layers, suggesting architectural biases in sparsity. While not universally effective, LoRA-Miner provides a practical compromise between parameter efficiency and task performance for moderate-scale applications.",
    "id": 1489
  },
  {
    "title": "Gradient Surgery for Transformer Attention: A Memory-Efficient Approach via Low-Rank Factorization",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Self-attention mechanisms in transformers suffer from quadratic memory complexity with sequence length, limiting applications to long contexts. While numerous efficient attention variants have been proposed, most sacrifice model quality or require specialized hardware. We propose Gradient Surgery for Attention (GSA), a training-time method that reduces memory usage by factorizing attention gradients into low-rank components. Our approach applies singular value decomposition to the gradient flow during backpropagation, retaining only the top-k singular values and vectors. This yields O(nk) memory complexity where k << n for sequence length n. We evaluate GSA on language modeling benchmarks including WikiText-103 and OpenWebText, achieving 1.4\u00d7 memory reduction compared to standard attention with <1% perplexity degradation. However, we observe training instability in certain configurations, particularly when k < 32. The method shows promise for modest sequence lengths (2K-4K tokens) but exhibits diminishing returns beyond 8K tokens due to gradient approximation errors. While our empirical results are encouraging, we acknowledge theoretical limitations: our analysis assumes gradient independence that may not hold in practice, and our convergence guarantees require bounded gradient noise. Code and pretrained models are available at [url].",
    "id": 1492
  },
  {
    "title": "Learning with Cross-Domain Label Structure via Regularized Partial Optimal Transport",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose a framework for semi-supervised learning that leverages structural relationships between labels across different domains through partial optimal transport. Our method couples the label distributions of source and target domains while accounting for varying label granularities, regularizing the transport plan via an entropy-based term that encourages sparse mappings. The resulting optimization problem is solved efficiently using a modified Sinkhorn algorithm with early stopping. We evaluate our approach on text classification and image recognition tasks, achieving 2-3% improvements over baselines on 3 out of 7 datasets when labels are scarce. However, performance degrades when domain shifts are minimal or when label structures are poorly aligned. Our contributions include: (1) a novel objective that integrates partial optimal transport with structural label constraints, (2) convergence guarantees for the proposed algorithm, and (3) empirical validation on cross-domain tasks. Code is available at anonymous.url.",
    "id": 1497
  },
  {
    "title": "Gradient Noise Injection as Implicit Regularization in Overparameterized Models",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Training large neural networks with stochastic noise has been empirically observed to improve generalization, yet theoretical understanding remains limited. We show that adding carefully scaled Gaussian noise to gradients during training acts as an implicit regularizer equivalent to Tikhonov regularization on the network's Jacobian. Our analysis reveals that the effective regularization strength depends not just on the noise variance, but on the interaction between noise scale and batch size in non-intuitive ways. Experiments on CIFAR-10 and ImageNet show consistent improvements of 2-5% over standard SGD with momentum when hyperparameters are tuned for each architecture, though we find the benefit vanishes under certain learning rate schedules. While our theory applies to linear networks, we provide empirical evidence suggesting similar behavior in non-linear models, though the gap between theory and practice remains significant. Our results suggest gradient noise injection could serve as a lightweight alternative to explicit regularization techniques, though its effectiveness depends heavily on problem-specific hyperparameter tuning.",
    "id": 1503
  },
  {
    "title": "Residual Layer Path Analysis: A Frequency Domain Perspective on Skip Connections",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate the role of skip connections in deep neural networks through the lens of frequency domain analysis. By decomposing the forward pass using Fourier transforms, we demonstrate that residual connections selectively amplify specific frequency bands during training. Our theoretical analysis shows that skip connections create an implicit low-pass filter that prevents high-frequency noise from dominating gradient flow. We propose a simple modification to ResNet blocks that introduces learnable frequency-dependent scaling factors, allowing dynamic adjustment of the filter characteristics. Experiments on CIFAR-10 and ImageNet show modest improvements over baseline ResNets (0.3-0.5% accuracy gain), with the most pronounced benefits observed on noisy datasets. However, our method incurs 15-20% additional computation cost and shows diminishing returns as network depth increases beyond 50 layers. While our frequency-domain perspective provides interesting insights into skip connection behavior, the practical benefits are limited outside specific noisy data regimes. Code and pre-trained models are available at anonymous-url.",
    "id": 1505
  },
  {
    "title": "Revisiting Adam with Layer-wise Learning Rates via Gradient Flow Analysis",
    "authors": [
      "Chen, J.",
      "Bhattacharya, S.",
      "Liu, K."
    ],
    "abstract": "Adaptive optimization methods like Adam have become standard for training deep neural networks, yet their theoretical understanding remains incomplete. We observe that standard Adam exhibits non-uniform gradient norms across layers, leading to inconsistent update magnitudes. Building on recent work connecting optimization algorithms to continuous-time dynamical systems, we propose LayerFlow-Adam, a modified Adam variant that applies layer-specific learning rates derived from analyzing the gradient flow of each layer independently. Our method computes adaptive rates based on the Frobenius norm of layer-wise Jacobians, requiring minimal additional computational overhead. Experiments on ResNet architectures for CIFAR-10 and ImageNet show modest improvements (0.3-0.8% accuracy gains) over standard Adam, particularly in low-data regimes. While the theoretical analysis provides intuition for layer-wise adaptation, our convergence guarantees are limited to simplified settings. The method shows sensitivity to hyperparameters and yields similar performance to AdamW with careful tuning. Nevertheless, LayerFlow-Adam offers a transparent approach to addressing layer-wise update imbalance without introducing additional optimizer states, making it a lightweight alternative for practitioners facing training instabilities.",
    "id": 1506
  }
]