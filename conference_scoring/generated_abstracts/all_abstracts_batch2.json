[
  {
    "title": "Improving Transformer Efficiency through Learnable Sparse Attention Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models achieve state-of-the-art results across many domains but suffer from quadratic complexity in sequence length due to their attention mechanism. We propose Learnable Sparse Attention (LSA), a method that dynamically learns to sparsify attention matrices during training. LSA introduces a lightweight routing network that predicts attention sparsity patterns based on input features, reducing computational cost to O(n log n) while maintaining model expressiveness. Our experiments on language modeling and machine translation tasks show 2.1\u00d7 speedup on sequences of length 8192 with less than 0.5 BLEU score degradation compared to standard attention. While our approach demonstrates practical benefits, we find that learned sparsity patterns do not consistently outperform heuristic patterns like local attention and may require task-specific hyperparameter tuning. Theoretical analysis reveals LSA achieves the desired complexity bounds but with constant factors that limit gains on shorter sequences. Code and pretrained models are available at [URL].",
    "id": 510
  },
  {
    "title": "Revisiting Weight Perturbation Regularization Through the Lens of Sharpness-Aware Minimization",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Garcia, A."
    ],
    "abstract": "Weight perturbation has long been used as a regularization technique in neural network training, yet its relationship to modern sharpness-based generalization bounds remains unclear. We show that adding zero-mean Gaussian noise to weights during training provides an implicit form of sharpness-aware minimization (SAM) with computational advantages. Our key finding is that perturbation variance acts as a dual variable to the SAM radius parameter, yielding a novel interpretation where noise injection approximates the SAM update under quadratic assumptions. We propose an adaptive perturbation scheme that adjusts noise variance based on local curvature estimates, theoretically connecting to PAC-Bayesian bounds. Experiments on CIFAR-10/100 and ImageNet show our method achieves 85-92% of SAM's performance while requiring 30% less training time. However, we observe instabilities in high-curvature regions and diminishing returns on larger architectures. While our theoretical analysis requires restrictive assumptions, the work suggests weight perturbation deserves renewed attention as an efficient alternative to explicit sharpness optimization.",
    "id": 511
  },
  {
    "title": "Improved Gradient Compression via Learned Sparsification Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient compression is crucial for distributed training, but existing methods rely on hand-designed sparsification patterns that may be suboptimal for specific architectures. We propose Learned Gradient Compression (LGC), which employs a lightweight auxiliary network to predict sparsification masks conditioned on gradient statistics. Our method achieves 8-15% higher compression ratios compared to Top-K and random sparsification baselines on ResNet-50 and Transformer architectures while maintaining convergence. Notably, LGC shows particular benefits in low-bandwidth regimes, reducing communication rounds by 20% in ImageNet training. However, we observe that performance gains are dataset-specific and diminish as model size increases. The learned patterns reveal interesting structure: earlier layers tend to preserve gradient directions while later layers focus on magnitude-based sparsification, offering insight into the implicit bias of gradient-based training. While our method provides measurable improvements over strong baselines, the computational overhead of the auxiliary network limits applicability to resource-constrained settings. Code and pre-trained models are available at [URL omitted for review].",
    "id": 512
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning Without the Surgery: A Regularization Approach",
    "authors": [
      "Liu, C.",
      "Johnson, A.B.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder optimization. Recent work uses gradient surgery techniques like PCGrad to resolve these conflicts, but such methods require modifications to the optimizer and introduce additional hyperparameters. We propose GradReg, a simpler approach that automatically balances gradient conflicts through a regularization term added to the loss function. Our method projects conflicting gradients onto an approximate feasible set using closed-form updates, eliminating the need for explicit gradient manipulation. On three standard multi-task benchmarks (CIFAR-10, NYUv2, and QM9), GradReg matches or marginally improves upon PCGrad while requiring no architectural changes. However, we find GradReg's performance gains are modest (average +1.2% improvement over naive multi-task learning) and degrade on tasks with highly specialized architectures. Our analysis reveals GradReg primarily helps early in training, with diminishing returns as optimization progresses. While not a definitive solution to multi-task conflicts, GradReg offers practitioners a lightweight alternative to gradient surgery when architectural constraints preclude optimizer modifications.",
    "id": 513
  },
  {
    "title": "Attention is Sometimes All You Need: Sparse Fixed Pattern Attention for Efficient Transformer Inference",
    "authors": [
      "Liu, J.",
      "Ramachandran, P.",
      "Kim, S."
    ],
    "abstract": "While transformer models achieve impressive performance across domains, their quadratic complexity limits deployment in resource-constrained settings. Prior work has attempted to address this through learned sparsity patterns, but these approaches introduce additional parameters and training complexity. We propose Fixed Pattern Attention (FPA), a surprisingly simple alternative that uses deterministic, fixed sparsity patterns derived from theoretical bounds on attention relevance. Our method requires no additional parameters and can be implemented as a straightforward masking operation. We demonstrate that FPA achieves 2.1\u00d7 speedup during inference on standard benchmarks with less than 2% performance degradation on GLUE and ImageNet tasks. However, we find the approach is less effective for tasks requiring long-range reasoning, achieving only 85% of full attention performance on LRA tasks. Our theoretical analysis shows that FPA provides provable approximations for certain attention distributions, though these bounds are looser than empirical observations suggest. While our results do not match state-of-the-art learned sparse attention methods, FPA's simplicity and zero-shot applicability make it a practical baseline for efficient transformers. Code is available at [anonymized].",
    "id": 514
  },
  {
    "title": "Revisiting Momentum with Exponential Moving Average: A Linear Optimization Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We investigate the connection between exponential moving average (EMA) techniques popular in deep learning practice and classical momentum methods from convex optimization. While momentum is well-studied in the convex setting, its behavior with EMA-style updates in non-convex optimization remains poorly understood. We propose LinearEMA, a simplified momentum variant that uses exponential averaging with linear interpolation, and provide convergence guarantees for quadratic objectives. Our theoretical analysis shows LinearEMA achieves a convergence rate of O(1/t^1.3) for strongly convex functions, matching standard momentum rates up to constant factors. Experiments on CIFAR-10/100 and ImageNet show LinearEMA achieves 0.5-1.2% improvement over standard SGD with momentum on ResNet architectures, though gains vanish with proper hyperparameter tuning. Our work bridges the gap between EMA heuristics used in practice and theoretical understanding of momentum methods, though we acknowledge our theoretical results are limited to quadratic objectives and may not fully explain EMA's success in modern architectures. We release our code and experiments to facilitate reproducibility.",
    "id": 515
  },
  {
    "title": "Gradient Surgery via Low-Rank Perturbations: A Lightweight Approach to Minimizing Interference in Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts between tasks, leading to degraded performance. While recent work has proposed sophisticated gradient surgery methods that modify optimization directions, these approaches introduce significant computational overhead and hyperparameter sensitivity. We present Low-Rank Gradient Surgery (LRGS), a lightweight alternative that uses rank-1 perturbations to resolve gradient conflicts. Our key insight is that most interference occurs in a low-dimensional subspace spanned by conflicting gradients. LRGS identifies this subspace and applies minimal perturbations to achieve approximate gradient orthogonality, requiring only O(d) additional computation where d is parameter dimensionality. We demonstrate LRGS on standard multi-task benchmarks including NYUv2 and Cityscapes. While our method achieves competitive performance compared to prior gradient surgery techniques (1.2% absolute improvement over PCGrad), we find the gains diminish with larger models (within 0.3% on ResNet-50). Theoretical analysis reveals LRGS optimizes a relaxed version of the original interference minimization problem. Our results suggest that the benefits of sophisticated gradient surgery may be overstated for practical applications. Code and experiments are available at [anonymized-link].",
    "id": 516
  },
  {
    "title": "Memory-Efficient Transformer Fine-Tuning via Incremental Low-Rank Adaptations",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Fine-tuning large language models remains computationally prohibitive for practitioners with limited GPU memory. While parameter-efficient fine-tuning methods like LoRA have shown promise, they often require manual rank selection and add non-negligible parameters across all layers. We propose Incremental Low-Rank Adaptations (ILRA), which automatically determines layer-wise adaptation ranks through an iterative pruning procedure during fine-tuning. Our method begins with a maximal rank budget and gradually reduces the rank for weights with stable singular value spectra below a learned threshold. Across GLUE and SuperGLUE benchmarks using LLaMA2-7B, ILRA achieves comparable performance to standard LoRA (average 0.3% F1 drop) while using 35-50% fewer additional parameters. However, our approach incurs 1.2x training time overhead and shows reduced effectiveness on generation tasks. Theoretically, we prove ILRA converges under standard smoothness assumptions, though the rank selection threshold requires task-specific tuning. Our results suggest that aggressive parameter reduction in adaptation methods may trade off against robustness, particularly on out-of-distribution examples. Code and pre-trained adapters will be released upon acceptance.",
    "id": 517
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Layer-wise Margin Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a novel margin-based generalization bound for feedforward neural networks that incorporates layer-wise geometric properties of the learned representations. Our analysis extends recent work on margin bounds by explicitly accounting for the distortion introduced by each layer's Jacobian matrices, yielding bounds that depend on both the final margin and the cumulative layer contractions. The key insight is to decompose the network's Lipschitz constant into per-layer contributions, allowing us to bound the expected error via a Rademacher complexity argument that incorporates the empirical margins at each layer. Our experiments on CIFAR-10 and CIFAR-100 show improvements over standard margin bounds by 15-25% for networks trained with spectral normalization, though the gains diminish for larger architectures. While our bounds remain vacuous for practical networks (typically exceeding 1), they provide sharper theoretical insights into how architectural choices affect generalization. The theoretical framework may be of independent interest despite current computational limitations for ResNet-scale models.",
    "id": 518
  },
  {
    "title": "Gradient Descent with Lookahead Mirrors: A Simple Extension for Improved Deep Network Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Anderson, E."
    ],
    "abstract": "We propose Lookahead Mirrors (LM), a lightweight modification to standard gradient descent that achieves modest but consistent improvements across diverse deep learning tasks without adding computational overhead during deployment. Our method maintains two optimization trajectories in parallel: a fast inner loop that explores parameter space aggressively, and a conservative outer loop that periodically anchors the optimization to stable regions. The key insight is that this dual-path approach, while conceptually similar to existing methods like gradient descent with momentum or Polyak averaging, provides a more flexible interpolation between exploration and exploitation. Through extensive experiments on CIFAR-10/100 and ImageNet with ResNet and Vision Transformer architectures, we demonstrate 1-2% absolute improvements in final accuracy over AdamW baselines, with particularly strong gains in low-data regimes. However, ablation studies reveal these improvements are primarily limited to computer vision tasks; performance gains disappear on language modeling benchmarks. While our theoretical analysis establishes convergence guarantees under restrictive conditions, we acknowledge the method's incremental nature and lack of clear theoretical novelty compared to recent second-order optimization advances. Code is available at [URL].",
    "id": 519
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Simple Recipe for Better Minima in Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, D."
    ],
    "abstract": "We present GradSurgery-SAM, a lightweight modification to Sharpness-Aware Minimization (SAM) that interpolates between flat and sharp minima through selective gradient manipulation. Our key insight is that the conflicting gradient directions present during SAM's ascent step can be resolved by applying gradient surgery on the sharpness penalty, effectively decoupling the flatness regularization from the primary optimization objective. On CIFAR-10/100 and ImageNet-100, GradSurgery-SAM achieves 0.8-1.2% improvements over standard SAM with only 3% additional compute overhead. While our method shows consistent gains across small-scale vision benchmarks, we observe diminishing returns on larger-scale tasks and limited generalization to non-vision domains. Theoretical analysis reveals that our approach can be viewed as a softened variant of SAM with bounded per-step approximation error, though the connection to generalization remains empirical. Our PyTorch implementation requires fewer than 50 lines of code and reproduces published baselines within 0.3% on reference datasets.",
    "id": 520
  },
  {
    "title": "Gradient Amplification: A Slight Perturbation Approach to Faster Neural Network Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Amplification (GradAmp), a simple modification to standard SGD that multiplies gradients by a learned scalar factor during backpropagation. While similar in spirit to adaptive learning rate methods, GradAmp learns amplification factors through an auxiliary loss that penalizes gradient magnitudes falling below a moving threshold. Through experiments on CIFAR-10/100 and ImageNet-1k, we show 1.2-1.8\u00d7 convergence speedup over baseline methods with negligible additional computation. However, we find performance gains diminish with larger batch sizes and careful learning rate schedules. Theoretical analysis reveals GradAmp can be viewed as approximately implementing a second-order update under restricted conditions. While the method shows consistent benefits over standard baselines, gains over well-tuned adaptive optimizers like AdamW are marginal. Our code will be made available upon publication.",
    "id": 521
  },
  {
    "title": "Adaptive Gradient Compression via Learned Quantization Schedules for Communication-Efficient Distributed Training",
    "authors": [
      "Kim, J.",
      "Ramaswamy, A.",
      "Gonzalez, C."
    ],
    "abstract": "Gradient compression is essential for scaling distributed deep learning, but existing methods use fixed compression ratios that fail to adapt to changing training dynamics. We propose AdaGC, a framework that learns instance-specific quantization schedules through lightweight meta-learning. Our approach employs a small hypernetwork that predicts optimal bit-widths for gradient transmission based on local gradient statistics and global training progress. Across ResNet-50 and Transformer training on ImageNet and WMT datasets, AdaGC achieves 5-8x compression with comparable final accuracy to full-precision training, slightly outperforming existing methods like QSGD and TopK. However, our method introduces ~2% parameter overhead and requires careful tuning of the meta-learning rate. While the compression gains are meaningful, the practical deployment benefits may be limited to bandwidth-constrained settings. Our code and pre-trained models will be released upon acceptance.",
    "id": 522
  },
  {
    "title": "LoRA-CD: Low-Rank Adaptation with Connection Dropout for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Liu, W."
    ],
    "abstract": "We propose LoRA-CD, a simple extension to Low-Rank Adaptation (LoRA) that addresses the brittleness observed in adapting large language models to downstream tasks. While LoRA has become a standard approach for parameter-efficient fine-tuning, we find that its performance degrades significantly when the rank selection is suboptimal or when the target task has limited training data. LoRA-CD introduces a connection dropout mechanism that randomly drops low-rank updates during training, effectively creating an ensemble of different rank configurations. Our experiments on GLUE and SuperGLUE benchmarks show modest but consistent improvements over standard LoRA (average +1.2% accuracy), with the most gains on smaller datasets. Analysis reveals that LoRA-CD reduces overfitting while maintaining the computational efficiency benefits of LoRA. However, our method introduces an additional hyperparameter (dropout rate) that must be tuned per task, and the improvements are less pronounced on larger datasets or when LoRA hyperparameters are already well-tuned. While LoRA-CD does not fundamentally advance the field of parameter-efficient fine-tuning, it provides a practical, easy-to-implement technique that may benefit practitioners working with limited data or computational budgets.",
    "id": 523
  },
  {
    "title": "On the Role of Batch Normalization in Adversarial Training: An Empirical Study with Theoretical Insights",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Liu, Y."
    ],
    "abstract": "Batch normalization (BN) has become a standard component in deep learning architectures, yet its role in adversarial training remains poorly understood. We investigate how BN affects adversarial robustness across different architectures and datasets. Through systematic experiments on CIFAR-10 and ImageNet, we observe that while standard training benefits from BN's regularization effect, adversarial training exhibits more complex dynamics. Specifically, we find that BN layers in early network stages consistently reduce robustness, while later-stage BN layers have negligible impact. To explain these observations, we analyze BN through the lens of margin theory, deriving bounds showing that BN's normalization constant can amplify adversarial perturbations when placed after convolutional layers. Based on these findings, we propose a simple modification to the placement of BN layers that improves robust accuracy by 2-3% on CIFAR-10 and 1.2% on ImageNet without additional computational cost. While our theoretical analysis is restricted to simplified architectures and requires assumptions about input distributions, our empirical findings hold across multiple adversarial training methods and architecture families. Our work suggests that architectural choices like BN placement deserve more attention in adversarial robustness research.",
    "id": 524
  },
  {
    "title": "Gradual Domain Adaptation with Confidence-Based Filtering: A Simple Baseline",
    "authors": [
      "Chen, Y.",
      "Kim, J.",
      "Singh, P."
    ],
    "abstract": "Unsupervised domain adaptation traditionally assumes access to both source and target domains simultaneously. We study the more realistic gradual domain adaptation (GDA) setting where data arrives sequentially from intermediary domains bridging source and target. While recent work has focused on complex domain interpolation techniques, we propose a surprisingly simple baseline: confidence-based filtering with self-training. Our approach maintains a running model trained on accumulated data, filtering new examples based only on prediction confidence. Despite its simplicity, we prove a generalization bound showing that with appropriate confidence thresholds, the method achieves comparable guarantees to prior work requiring domain divergence measures. Experiments on standard benchmarks including RotatedMNIST, Portraits, and CoverType demonstrate competitive performance while reducing computational complexity by 3-5x compared to state-of-the-art methods. However, we find performance degrades significantly when domain shifts are abrupt or when source domain size is small. Our results suggest that GDA may be addressed more straightforwardly than previously thought, though fundamental limitations remain in challenging scenarios. Code is available at anonymous.url",
    "id": 525
  },
  {
    "title": "BatchNorm Makes Your Discriminator Lazy: An Empirical Study on Mode Collapse in GANs",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate how architectural choices affect mode collapse in Generative Adversarial Networks. Through systematic experiments across image generation benchmarks, we find that discriminators using Batch Normalization exhibit stronger mode dropping behavior compared to those using LayerNorm or GroupNorm. Our analysis reveals that BatchNorm reduces the effective capacity of early discriminator layers by introducing noise that correlates with sample diversity. We propose a simple modification: selective normalization, where BatchNorm is applied only to high-frequency features. This approach reduces FID by 8-12% on CIFAR-10 and CelebA compared to standard BatchNorm discriminators, while maintaining the training stability benefits. Though our method does not fundamentally solve mode collapse, it provides practical guidance for GAN architecture design. Code and trained models are available at anonymous.github.io/selective-norm.",
    "id": 526
  },
  {
    "title": "Attention Is Not All You Need: Alternating Sparse-Local Transformers for Efficient Long-Context Modeling",
    "authors": [
      "Morrison, J.",
      "Chen, L.",
      "Kim, S."
    ],
    "abstract": "We propose Alternating Sparse-Local Transformers (ASLT), a modification to standard Transformers that alternates between sparse global attention and local windowed attention to improve efficiency on long sequences. While recent work has focused on making attention more efficient through approximations, our key insight is that alternating between two complementary patterns can maintain the benefits of full attention with O(n\u221an) complexity. Our method replaces every other attention layer with a local sliding-window pattern, creating a shallow sparse-global/lean-local architecture. We demonstrate competitive performance on the Long Range Arena benchmark, achieving 82.1 average accuracy (vs. 84.3 for standard Transformers) while reducing FLOPs by 47%. On language modeling tasks, ASLT matches baselines on WikiText-103 perplexity (18.7 vs 18.5) with 2.1x speedup on 8K sequences. However, we observe instability on certain tasks, particularly those requiring precise positional reasoning. While our work provides a practical efficiency improvement, we acknowledge that theoretical analysis of approximation error remains limited. Code and models will be available at [url].",
    "id": 527
  },
  {
    "title": "Gradient Surgery for Mixed-Precision Training: A Lightweight Approach to Managing Loss Scale Instability",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Mixed-precision training significantly reduces memory usage and improves throughput for deep learning models, but remains sensitive to loss scale instability, particularly in transformer architectures. We propose Gradient Surgery Mixed-Precision (GSMP), a simple modification to existing automatic mixed-precision frameworks that selectively perturbs gradient magnitudes during unstable training phases. Our method uses a computationally lightweight approximation of the Hessian trace to identify problematic parameter groups, then applies calibrated noise injection to re-scale gradients without manual tuning. We evaluate GSMP on standard NLP and vision benchmarks, achieving comparable accuracy to baseline half-precision training while reducing catastrophic loss spikes by 73% on average. However, we observe diminishing returns on well-regularized models and modest computational overhead (5-8%) that may limit practical adoption. Our code will be made publicly available.",
    "id": 528
  },
  {
    "title": "Revisiting Label Smoothing with Temperature Annealing for Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Label smoothing has become a standard regularization technique in modern vision transformers, yet its interaction with training dynamics remains poorly understood. We propose a simple modification to conventional label smoothing that incorporates temperature-scaled confidence levels during training. Our method dynamically adjusts the smoothing parameter based on the model's prediction entropy, using a novel annealing schedule that correlates with the cosine learning rate decay. Through extensive experiments on ImageNet-1K with various ViT architectures, we demonstrate modest improvements of 0.3-0.7% top-1 accuracy over standard label smoothing. While our approach achieves statistically significant improvements on CIFAR-100 and achieves comparable performance on ImageNet-1K, ablation studies reveal that benefits diminish when training with stronger augmentation techniques like MixUp or RandAugment. Our analysis shows that the primary contribution comes from improved calibration during early training phases, though this effect largely disappears by convergence. Code and pre-trained models are available at [URL], though full reproducibility requires specific training seeds due to high variance in some settings.",
    "id": 529
  },
  {
    "title": "Improved Convergence Rates for Distributed SGD with Delayed Gradients via Variance-Reduced Perturbation",
    "authors": [
      "Liu, Q.",
      "Johnson, T.",
      "Kumar, S."
    ],
    "abstract": "We revisit the problem of delayed gradient updates in distributed stochastic optimization, where worker nodes compute gradients on stale parameter versions due to communication bottlenecks. While existing theory shows such delays introduce irreducible bias terms in convergence bounds, we propose a simple variance-reduced perturbation technique that empirically improves convergence at minimal computational cost. Our method adds carefully scaled random perturbations to the stale gradients based on delay statistics, which we show reduces gradient variance in practice while maintaining theoretical guarantees. We prove convergence rates of O(1/\u221aT + \u03c4\u00b2/T) for non-convex objectives under standard assumptions, where \u03c4 is the maximum delay, matching prior work but with improved constant factors. On CIFAR-10 and ImageNet training tasks with 16 asynchronous workers, our approach achieves 1.2-1.4x speedup over standard delayed SGD, though gains diminish with fewer workers. While our theoretical improvements are modest, the practical simplicity and 3-line code change requirement make this a potentially useful optimization trick for practitioners dealing with straggler workers in distributed training.",
    "id": 530
  },
  {
    "title": "Revisiting Gradient Noise: Smaller Learning Rates Make Sharper Minima (Sometimes)",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Ramesh, V."
    ],
    "abstract": "Recent work suggests that the noise scale of stochastic gradients influences generalization, with larger noise supposedly encouraging flatter minima. We revisit this claim through systematic experiments on ResNet architectures across CIFAR-10/100 and ImageNet. Contrary to conventional wisdom, we find that the relationship between gradient noise and sharpness depends crucially on the learning rate schedule: smaller learning rates can induce sharper minima when training beyond the interpolation threshold, while large learning rates initially appear to flatten the loss landscape but ultimately converge to similar sharpness values. Our theoretical analysis in a simplified quadratic setting reveals that the interaction between learning rate and batch size creates non-monotonic effects on the effective noise scale. We propose a modified noise-injection scheme that adapts the noise scale based on gradient norms, which provides modest improvements on small-scale benchmarks (0.5-1.2% accuracy gains) but yields inconsistent results on larger datasets. While our findings challenge some theoretical predictions, they also highlight the complexity of the noise-sharpness-generalization relationship, suggesting that simple heuristics may be insufficient for controlling generalization in practice.",
    "id": 531
  },
  {
    "title": "ReLoCo: Regularizing Low-Curvature Directions in Neural Network Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose ReLoCo, a simple regularization technique that selectively penalizes optimization in directions of low curvature in the loss landscape. Motivated by observations that neural networks often converge to solutions with many near-zero Hessian eigenvalues, we add a penalty term proportional to the inverse of the absolute Hessian eigenvalues during training. This encourages the optimizer to prioritize updates in high-curvature directions while avoiding flat regions that may hurt generalization. On ImageNet and CIFAR-10, ReLoCo achieves a 0.8-1.2% improvement over standard SGD baselines, with particularly strong gains on ResNet architectures. While the method shows promise on vision tasks, results on NLP benchmarks are mixed, and we observe sensitivity to the curvature threshold hyperparameter. Theoretical analysis reveals that ReLoCo can be viewed as a second-order adaptive method that smoothly interpolates between gradient descent and natural gradient descent. Our results suggest that curvature-aware regularization provides a practical way to improve optimization dynamics, though future work is needed to scale the Hessian computation efficiently to larger models and understand the theoretical properties more completely.",
    "id": 532
  },
  {
    "title": "Gradient Surgery in Transformer Language Models: A Closer Look at Parameter-Level Training Dynamics",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Understanding how gradient-based optimization navigates the complex loss landscape of large language models remains challenging. We examine gradient interference patterns at the parameter level in pre-trained transformers, proposing a layer-wise gradient surgery technique that selectively rescales parameter updates based on their alignment with the Fisher Information Matrix. Our method shows modest improvements in perplexity on Wikitext-103 (from 18.7 to 18.3) and small downstream gains on SuperGLUE (1.2% absolute improvement) using a 345M parameter model. While the approach is computationally expensive (1.4\u00d7 training time), we provide theoretical analysis suggesting that gradient surgery reduces the effective rank of the Hessian near local minima. However, our evaluations on larger models (1.3B parameters) show diminishing returns, and our results are sensitive to hyperparameter choices. The work provides insights into parameter-level dynamics but falls short of demonstrating significant practical benefits for model training. Code and pre-trained models are available at anonymous.github.io/gradsurgery.",
    "id": 533
  },
  {
    "title": "Gradient Surgery Revisited: A Simpler Approach to Multi-Task Optimization",
    "authors": [
      "Liu, Y.",
      "Chen, K.",
      "Garcia, P."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks. While recent gradient surgery methods like PCGrad and GradDrop show promise, they introduce additional hyperparameters and computational overhead. We propose a surprisingly simple alternative: adaptive gradient dropout (AGD), which randomly masks conflicting gradient components with probability proportional to their alignment scores. Unlike existing approaches, AGD requires no task-specific tuning and adds negligible computational cost. Our theoretical analysis shows AGD converges to a Pareto stationary point under mild assumptions. On three standard benchmarks (Multi-MNIST, CIFAR-10/100, NYUv2), AGD matches or exceeds state-of-the-art methods, improving average accuracy by 2.3% over PCGrad while reducing training time by 15-20%. However, we observe diminishing benefits on larger-scale experiments and find AGD can destabilize training when tasks have vastly different gradient magnitudes. While our method's simplicity is appealing, we acknowledge the gains are incremental rather than transformative, and its limitations in extreme settings warrant further investigation.",
    "id": 534
  },
  {
    "title": "Residual Perturbation Training: Improving Robustness Through Layer-wise Noise Injection During Fine-tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Pre-trained models exhibit remarkable performance but often lack robustness to small input perturbations. We propose Residual Perturbation Training (RPT), a simple fine-tuning method that injects controlled noise into residual connections during training. Unlike adversarial training which requires expensive inner loop optimization, RPT adds Gaussian noise scaled by residual magnitudes and optimizes a consistency loss between clean and noisy outputs. While the method appears similar to dropout at first glance, our key insight is that noise injection specifically in residual pathways (rather than activations) better preserves gradient flow while improving adversarial robustness. Experiments on ImageNet and CIFAR-10 show 2-4% improvements in robust accuracy against FGSM and PGD attacks compared to standard fine-tuning, with minimal computational overhead. However, robustness gains diminish against stronger attacks like AutoAttack, and we observe trade-offs on clean accuracy (-1-2%) that may not be acceptable for all applications. Code will be available for reproducibility.",
    "id": 535
  },
  {
    "title": "Improved Gradient Sparsity via Iterative Magnitude Pruning with Momentum Restarts",
    "authors": [
      "Liu, S.",
      "Kumar, V.",
      "Chen, J."
    ],
    "abstract": "Gradient sparsity has emerged as a practical technique for reducing communication costs in distributed training, but existing methods often suffer from accuracy degradation at high sparsity levels. We propose a simple variant of iterative magnitude pruning that incorporates momentum-based weight rewinding to maintain gradient flow through critical paths. Our method periodically identifies and restores small-magnitude gradients that exhibit high curvature, using a lightweight approximation of the Hessian diagonal. On CIFAR-10 and ImageNet, our approach achieves 98% gradient sparsity with only 1.3% accuracy loss compared to dense training, slightly outperforming existing top-k sparsification baselines (1.7% loss). While the gains are incremental rather than breakthrough, our method requires minimal hyperparameter tuning and integrates seamlessly with existing optimizers. Experiments suggest the technique works best for vision tasks with moderate model sizes (10M-50M parameters). We provide theoretical analysis showing our method preserves gradient signal in the NTK regime under mild assumptions. However, limitations include sensitivity to batch size and diminished benefits on extremely deep networks (>100 layers). Code will be released upon acceptance.",
    "id": 536
  },
  {
    "title": "Learning to Prune Neural Networks via Reinforcement Learning with Second-Order Hessian Features",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a reinforcement learning framework for neural network pruning that incorporates second-order curvature information via minibatch Hessian diagonal estimates. While previous pruning methods rely on magnitude-based heuristics or expensive full Hessian computations, our approach trains a lightweight policy network to make layer-wise pruning decisions using Hessian traces and gradient norms as state features. The policy is trained with proximal policy optimization using a reward that balances compression ratio and accuracy retention across iterations. On ImageNet classification with ResNet50, our method achieves 2.3\u00d7 FLOPs reduction with 0.8% accuracy drop, competitive with state-of-the-art magnitude pruning baselines. However, we find that the effectiveness varies significantly across architectures (ResNet vs EfficientNet) and the second-order features provide inconsistent benefits, sometimes underperforming gradient-only variants. Theoretical analysis shows our reward design converges to a local optimum of a relaxed pruning objective, though the gap to global optimality remains uncharacterized. Our results suggest that while RL can learn reasonable pruning schedules, simpler approaches may suffice for standard vision architectures. Code is available at anonymous.url.",
    "id": 537
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Structured Sparsity in Low-Rank Adaptors",
    "authors": [
      "Chen, L.",
      "Srinivasan, V.",
      "Thompson, K."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its memory footprint remains substantial for billion-scale models. We propose LoRA-Lite, which combines the standard LoRA decomposition with structured sparsity patterns learned during the adaptation phase. Specifically, we introduce a learnable binary mask applied to the rank decomposition matrices, trained using straight-through gradient estimation. Our method achieves 2.3\u00d7 memory reduction in activations during training and 1.7\u00d7 reduction in checkpoint size compared to standard LoRA, while maintaining 94-97% of the downstream performance across GLUE, SuperGLUE, and domain-specific benchmarks. While empirical results are encouraging, we provide only partial theoretical justification for the sparsity pattern's validity. Ablation studies reveal the method is most effective for classification tasks but shows degradation on generation tasks requiring longer context. Code will be made available upon acceptance.",
    "id": 539
  },
  {
    "title": "Improved Generalization Bounds for Stochastic Gradient Descent via Adaptive Gradient Norm Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We present theoretical and empirical analysis of a modified SGD algorithm that incorporates adaptive gradient norm regularization to achieve tighter generalization bounds. Motivated by recent observations that flat minima correlate with better generalization, our approach dynamically adjusts the regularization strength based on the local geometry of the loss landscape. We provide an improved PAC-Bayesian bound that scales as O(\u221a(tr(H)/n)), where H is the Hessian at convergence, achieving a \u221ad improvement over standard bounds. Our method requires only O(d) additional computation per iteration and introduces a single hyperparameter with intuitive interpretation. Experiments on CIFAR-10/100 and ImageNet subsets show consistent improvements over vanilla SGD, with average test accuracy gains of 1.2\u00b10.3% across architectures. While our theoretical results apply to convex losses, empirical benefits extend to non-convex settings. Limitations include the computational overhead of Hessian-vector products during training and our reliance on bounded loss assumptions that may not hold in practice. The proposed method offers a principled approach to improving generalization, though gains are modest compared to recent advances in learning rate scheduling and data augmentation.",
    "id": 540
  },
  {
    "title": "Gradient Descent with Logit-Aligned Updates Improves Confidence Calibration in Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "While large language models achieve impressive predictive accuracy, their predicted probabilities often poorly reflect true likelihoods. We propose Logit-Aligned Gradient Descent (LAGD), a simple modification to standard training that explicitly penalizes the difference between predicted logits and logit-transformed empirical frequencies during gradient updates. Our method requires only a single hyperparameter controlling the alignment strength and adds negligible computational overhead. We evaluate LAGD on five language modeling benchmarks using GPT-2 and OPT models with 125M-1.3B parameters. Results show 5-15% improvements in Expected Calibration Error (ECE) compared to standard fine-tuning, with minimal impact on perplexity. However, we find performance gains diminish on larger models (>3B parameters), suggesting the method may be limited to smaller architectures. While LAGD provides consistent improvements over baselines, the magnitude of improvement is modest compared to recent calibration-specific post-processing methods. Our implementation and experiments are available at [URL].",
    "id": 541
  },
  {
    "title": "Gradient Surgery with Adaptive Norm Clipping: A Practical Framework for Multi-Task Learning",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "Multi-task optimization often suffers from conflicting gradients between tasks, leading to sub-optimal performance on individual objectives. While recent gradient surgery methods like PCGrad and GradNorm address this issue, they rely on heuristic thresholds that remain fixed throughout training. We propose Adaptive Gradient Surgery (AGS), a simple extension that dynamically adjusts gradient clipping thresholds based on gradient norm distributions. Our method maintains a moving window of gradient norms for each task, automatically determining when gradient conflicts are sufficiently severe to warrant surgery. Through extensive experiments on three standard benchmarks (CIFAR-100 with 20 tasks, NYUv2 scene understanding, and Meta-World RL), AGS shows modest improvements over fixed-threshold baselines (+1.2% average accuracy, -2.1% relative loss increase). While our results are consistent across domains, the gains are incremental and computational overhead increases by 15-20%. Our analysis reveals that AGS primarily helps when tasks have imbalanced gradient magnitudes, but offers limited benefits when task gradients are well-aligned. Though not a breakthrough, AGS provides a practical plug-and-play modification that may benefit practitioners working with heterogeneous multi-task objectives. Code and experimental logs will be released upon acceptance.",
    "id": 542
  },
  {
    "title": "Self-Tuning Entropy-Regularized Policy Gradient with Adaptive Temperature Adjustment",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Maximum entropy reinforcement learning has gained popularity as a framework for improving exploration and robustness, yet the choice of temperature parameter remains critical and often requires extensive tuning. We propose Adaptive Entropy Mirror Descent (AEMD), a policy gradient method that dynamically adjusts the temperature parameter during training based on a surrogate objective that balances expected return and policy entropy. Our approach incorporates a learned temperature network trained via meta-gradients to optimize a validation performance metric. We evaluate AEMD on continuous control benchmarks including MuJoCo and PyBullet environments. Across 8 environments, AEMD achieves comparable or slightly improved performance over tuned baselines while eliminating the need for temperature hyperparameter search. However, we find the method exhibits sensitivity to the initialization of the temperature network and occasionally produces unstable training dynamics in high-dimensional tasks. Our ablation studies reveal that the primary benefit comes from preventing premature convergence rather than discovering qualitatively different policies. While this work addresses an important practical limitation of entropy-regularized RL, the contribution is incremental and the theoretical analysis remains incomplete.",
    "id": 543
  },
  {
    "title": "Learning with Noisy Labels by Penalizing Confidence: A Margin-Based Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a simple regularization technique for training deep networks on datasets with label noise. Our method, Confidence Penalty with Margins (CPM), discourages overconfident predictions by adding a penalty proportional to the difference between the maximum softmax probability and a confidence threshold. The key insight is that noisy labels tend to produce overconfident predictions on mislabeled examples, even when employing standard robust training techniques like label smoothing. CPM requires no knowledge of noise rates or architecture changes, making it widely applicable. On CIFAR-10/100 with synthetic noise, CPM improves accuracy by 2-4% over baseline methods. On real-world noisy datasets like Clothing1M, we achieve competitive results (68.9% vs. 69.5% state-of-the-art) while being simpler to implement. While the theoretical properties of CPM remain limited, our extensive ablations demonstrate consistent robustness across noise types and levels. Our code is publicly available.",
    "id": 544
  },
  {
    "title": "Gradient Descent with Momentum Revisited: A Fourier Perspective on Optimization Dynamics",
    "authors": [
      "Liu, K.",
      "Johnson, T.",
      "Chen, S."
    ],
    "abstract": "We analyze gradient descent with momentum (GDM) through the lens of Fourier analysis, revealing that momentum amplifies specific frequency components of the loss surface's gradient spectrum. Our theoretical framework shows that for quadratic objectives, the optimal momentum parameter \u03b2 relates to the largest eigenvalue \u03bb_max of the Hessian via \u03b2 = 1 - 2/\u221a\u03ba, where \u03ba is the condition number. While this recovers known convergence rate bounds, our frequency interpretation provides new insights into why momentum helps in non-convex settings. We validate these findings through experiments on shallow neural networks trained on MNIST, demonstrating improved training dynamics when spectral preconditioning aligns with our frequency analysis. However, extending these results to deeper networks remains challenging, as the frequency characteristics become difficult to characterize precisely. Our work suggests practitioners should consider Hessian spectra when tuning momentum parameters, though we acknowledge this is computationally expensive for large models. The contribution is primarily theoretical, connecting spectral optimization concepts to practical momentum-based methods.",
    "id": 545
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning with Asymmetric Task Difficulties",
    "authors": [
      "Chen, Y.",
      "Rodriguez, J.",
      "Kim, H.",
      "Nguyen, L."
    ],
    "abstract": "Multi-task learning faces challenges when tasks have varying difficulties, leading to gradient conflicts where easy tasks dominate optimization. We propose Asymmetric Gradient Surgery (AGS), a simple modification to existing gradient surgery methods that reweights gradient updates based on per-task loss curvature estimates. Our approach computes sharpness-aware gradients for each task and scales their contributions inversely to estimated local Lipschitz constants, preventing difficult tasks from being overshadowed. AGS requires minimal computational overhead beyond standard backpropagation and can be integrated into existing training pipelines with two lines of code. Experiments on three benchmarks (NYUv2, CelebA-MTL, and MultiMNIST) show consistent improvements over gradient surgery baselines, with average gains of 2.3% across tasks. However, we find AGS provides limited benefits when task difficulties are similar or when gradients naturally align. Ablation studies reveal that curvature estimation is crucial\u2014naive reweighting by loss magnitude performs worse than the baseline. While promising, our theoretical analysis is restricted to convex settings, and we observe sensitivity to hyperparameter choices in practice. Code is available at anonymous.url.",
    "id": 546
  },
  {
    "title": "Improving Transformer Language Models through Adaptive Context Compression",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "While large language models demonstrate remarkable capabilities, their quadratic attention complexity limits effective context processing. We propose Adaptive Context Compression (ACC), a method that learns to compress historical tokens into compact representations without incurring additional inference cost. Our approach trains small compression networks alongside the transformer, which learn to summarize past contexts into fixed-size vectors that maintain relevant semantic information. Experiments on language modeling benchmarks show 3-7% perplexity improvements over standard transformers with equivalent parameters, particularly on long-context inputs. The method requires minimal architectural changes and can be applied to existing pre-trained models through continued pre-training. However, our analysis reveals that ACC introduces computational overhead during training and may lose fine-grained details in extreme compression regimes. We provide theoretical justification showing ACC approximately preserves mutual information under certain assumptions, though these assumptions may not hold in practice. Our code and models are available at [link provided].",
    "id": 547
  },
  {
    "title": "Gradient Surgery for Improved Transfer Learning in Low-Resource Settings",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Fine-tuning pretrained models on limited data often leads to catastrophic forgetting and sub-optimal target performance. We propose Gradient Addition for Transfer (GAT), a simple modification to standard fine-tuning that interpolates between gradient directions from source and target losses. Unlike existing methods that require auxiliary networks or expensive regularization, GAT only modifies the optimizer through a lightweight vector projection step. On 8 few-shot classification benchmarks spanning natural language and vision tasks, GAT achieves 2-3% absolute improvements over standard fine-tuning while being 15x faster than Meta-Transfer methods. However, these gains diminish when source and target domains are highly dissimilar, suggesting our approach is most beneficial for near-domain transfer. Through synthetic experiments, we reveal GAT's implicit bias toward preserving shared representations while allowing task-specific adaptation. Code and pre-trained models will be released upon acceptance.",
    "id": 548
  },
  {
    "title": "LayerDrop: A Simple Pruning Strategy for Reducing Transformer Training Time via Dynamic Structural Sparsity",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rodriguez, J."
    ],
    "abstract": "We propose LayerDrop, a training-time pruning method that dynamically removes entire layers from transformer architectures based on gradient norms. Unlike previous approaches that prune fixed parameter subsets, LayerDrop adaptively selects which layers to skip during each forward pass, creating varying sparse subnetworks. Our method requires no architectural changes and introduces minimal overhead through efficient rescaling of remaining layers. We evaluate on language modeling (WikiText-103) and machine translation (IWSLT'14) tasks, achieving 15-22% reduction in training time while maintaining 96-98% of baseline performance. While less effective than model compression techniques requiring expensive pretraining, LayerDrop offers a practical trade-off between training efficiency and model quality. However, our results show diminishing returns beyond 30% pruning and reveal sensitivity to gradient estimation quality. Code and pre-trained models will be released upon acceptance.",
    "id": 549
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multitask Learning via Sparse Gradient Projection",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multitask learning often suffers from conflicting gradients between tasks, leading to suboptimal performance and catastrophic forgetting. While recent gradient surgery methods like PCGrad successfully handle gradient conflicts during training, they primarily focus on stabilizing simultaneous optimization without preserving learned representations. We propose Gradient Surgery with Memory (GSM), a simple yet effective approach that maintains a small episodic memory buffer and performs sparse orthogonal projections to mitigate forgetting when tasks are encountered sequentially. Our method builds upon gradient surgery techniques by storing gradient directions from previous tasks in a compressed form, using random projections to maintain computational efficiency. We evaluate GSM on standard multitask benchmarks including Split-CIFAR100 and Visual Decathlon. While our method shows modest improvements over baselines (2-3% accuracy gains) in sequential task settings, we find performance gains disappear when task identity is not provided during inference. Our results suggest that the benefits of gradient surgery with memory may be limited to scenarios with clear task boundaries and known identity. Code and experiments are available at [URL].",
    "id": 550
  },
  {
    "title": "Gradient Surgery for Partially Observable Reinforcement Learning via Implicit Value Regularization",
    "authors": [
      "Chen, L.",
      "Srinivasan, V.",
      "Okafor, K."
    ],
    "abstract": "Policy gradient methods in partially observable environments suffer from high variance due to the mismatch between the true state distribution and the approximate beliefs used for decision-making. We propose Implicit Value Regularization (IVR), a simple technique that adds a gradient surgery step to existing policy gradient algorithms by projecting policy updates onto a subspace that preserves value function consistency. Our approach modifies the policy gradient using an implicit regularization term derived from a local linear approximation of the value function, without requiring explicit modeling of belief states. We evaluate IVR on standard benchmarks including pixel-based continuous control tasks and partially observable variants of MuJoCo environments. Results show modest improvements over PPO and SAC baselines on 6 out of 12 tasks (average normalized score improvement of 3.2%), with particularly consistent gains in sparse reward settings. While IR introduces minimal computational overhead (\u22485% increase in wall-clock time), we observe that performance gains diminish with increased model capacity, suggesting the regularization effect may be too restrictive for complex policies. Code and hyperparameters are provided for reproducibility.",
    "id": 551
  },
  {
    "title": "Augmenting Contrastive Learning with Local Neighborhood Consistency",
    "authors": [
      "Liu, Q.",
      "Reddy, S.",
      "Kumar, V."
    ],
    "abstract": "Self-supervised contrastive learning has achieved remarkable success in learning visual representations without labels. However, we observe that current methods may lack robustness to small perturbations in the learned embedding space. We propose Local Neighborhood Consistency (LNC), a simple regularization term that encourages representations to be consistent within dynamically constructed local neighborhoods. Specifically, LNC adds an auxiliary loss that minimizes the distance between each anchor's feature representation and the centroid of its nearest neighbors. Our method can be incorporated into any existing contrastive learning framework with minimal computational overhead. We evaluate LNC on ImageNet-1K using ResNet-50 and ViT-B architectures, showing modest improvements of 0.7-1.2% in linear evaluation when combined with SimCLR. Ablation studies reveal that the effectiveness of LNC depends heavily on the choice of neighborhood size and temperature parameters. While the improvements are consistent across settings, the magnitude remains relatively small compared to recent architectural advances. Our findings suggest that local neighborhood consistency provides a useful, though incremental, enhancement to existing contrastive learning methods.",
    "id": 552
  },
  {
    "title": "Improving Transformer Attention via Learnable Sparse Patterns with Applications to Long-Context Modeling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "The quadratic complexity of self-attention in Transformers limits their applicability to long sequences. While prior work has proposed static sparsity patterns or low-rank approximations, these approaches often require domain expertise to design and may not adapt to different tasks. We present LASER, a method that learns task-specific sparse attention patterns through differentiable pruning. Our approach jointly optimizes a continuous mask with the model parameters, allowing automatic discovery of sparsity patterns during training. We evaluate LASER on language modeling, document classification, and protein sequence tasks with contexts up to 16K tokens. Experiments show LASER achieves 2.1\u00d7 speedup over standard attention while maintaining 94-97% of full attention performance on standard benchmarks. However, we observe the learned patterns vary significantly across tasks and datasets, suggesting limited transferability. Ablation studies reveal that initialization strategy and regularization strength are critical for convergence. While our method provides practical benefits for specific long-context applications, the computational overhead of mask learning and inconsistent pattern interpretability raise questions about broader applicability.",
    "id": 554
  },
  {
    "title": "Improving Transformer Training Stability through Layer-wise Learning Rate Warmup Schedules",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Chen, J."
    ],
    "abstract": "Transformer models often exhibit training instability during optimization, leading to diverging losses or suboptimal convergence. We propose LayerCake, a novel layer-wise learning rate warmup schedule that assigns different warmup durations to different transformer blocks based on their position and attention proximity. Extensive experiments on GLUE and WMT benchmarks show that LayerCake achieves a modest 1.2% average improvement over standard warmup while reducing training instability by 37% as measured by gradient norm spikes. While our method is theoretically motivated by analyzing the linearized dynamics of attention layers, we find the benefits are most pronounced in medium-scale models (under 1B parameters) and largely disappear in larger models. Our ablations reveal that 60% of the improvement comes from extending warmup for early attention layers, with diminishing returns from more complex scheduling. Though computationally efficient and easy to implement, our approach lacks compelling justification for why layer-specific rates should matter in the overparameterized regime. Code and pretrained models are available at anonymous.link.",
    "id": 555
  },
  {
    "title": "LoRA-XL: Scaling Low-Rank Adaptation via Dynamic Rank Allocation and Parameter Sharing",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed-rank constraint limits its expressiveness for complex downstream tasks. We propose LoRA-XL, an extension that dynamically adjusts the rank of adaptation matrices based on gradient statistics and shares parameters across layers via butterfly sparsity patterns. Our method introduces a novel rank allocation algorithm that redistributes model capacity from layers with saturated representations to those exhibiting underfitting, while maintaining computational overhead under 15% of vanilla LoRA. We evaluate LoRA-XL on the GLUE benchmark and domain-specific tasks in biomedical NLP, achieving average improvements of 1.2% over standard LoRA while reducing trainable parameters by 23%. However, we observe diminishing returns on smaller models (<1B parameters) and tasks with abundant training data. Analysis reveals that the butterfly parameter sharing pattern becomes particularly beneficial when adapting to multiple domains simultaneously, though it can interfere with layers requiring specialized representations. Code and experiments are available at [anonymous link].",
    "id": 556
  },
  {
    "title": "LoRA-Drift: Characterizing Performance Degradation from Sequential Low-Rank Adaptation",
    "authors": [
      "Chen, S.",
      "Garcia, L.",
      "Thompson, K."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but practical deployments often require sequential adaptation to multiple tasks. We observe and systematically study a phenomenon we term \"LoRA-drift\" \u2014 progressive performance degradation as models are sequentially adapted to new tasks while freezing previous LoRA modules. Through extensive experiments across 8 diverse NLP benchmarks and 3 different model architectures, we characterize how accumulating low-rank adapters creates interference patterns that disproportionately affect earlier tasks. We propose a simple regularization technique based on cosine similarity constraints between adapter projections, which reduces drift by 23% on average while maintaining downstream performance. However, we find fundamental trade-offs between plasticity and stability that persist even with our intervention. Our analysis reveals that LoRA-drift correlates strongly (\u03c1 = 0.81) with the rank ratio between consecutive adapters. While our regularization approach provides modest improvements, we conclude that current LoRA formulations are inherently limited in sequential adaptation scenarios. Our findings suggest caution in deploying LoRA for continual learning applications and highlight the need for alternative parameter-efficient methods that explicitly account for task interference.",
    "id": 557
  },
  {
    "title": "Do Language Models Learn to Plan Like Humans? A Controlled Study Using Grid-World Navigation",
    "authors": [
      "Liu, C.",
      "Johnson, K.",
      "B\u00e4cker, M."
    ],
    "abstract": "Large language models demonstrate remarkable planning capabilities, but it remains unclear whether they perform logical search like humans or rely on learned pattern matching. We investigate this question using a controlled grid-world navigation task where human participants and language models solve identical maze problems under uncertainty. By designing mazes that vary in stochasticity, reward structure, and complexity while controlling for verbal patterns, we isolate planning from memorization. Our experiments with GPT-3.5 and GPT-4 reveal that while both achieve high success rates on familiar maze topologies, their performance degrades sharply when navigation requires non-local planning or when verbal descriptions change. We propose a novel 'planning coefficient' that correlates more strongly with solution generalization in humans than in LMs. Analysis shows language models rely heavily on local heuristics and struggle with backtracking, unlike humans who exhibit hierarchical planning behavior. While our findings provide evidence against human-like planning in LMs, they suggest path planning emerges from learned statistical patterns rather than explicit search. Our controlled paradigm may inform the design of more planning-capable models and benchmarks.",
    "id": 558
  },
  {
    "title": "Gradient Projected Adam: Memory-Efficient Adaptive Optimization through Iterative Subspace Projection",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Yamamoto, S."
    ],
    "abstract": "We propose Gradient Projected Adam (GPA), a memory-efficient variant of Adam that reduces optimizer state by 50-80% while maintaining convergence properties on standard benchmarks. Our method projects gradients into a learned low-dimensional subspace at each iteration, storing only the compressed representation and reconstruction coefficients. This approach is motivated by the observation that gradients in later training stages often lie in low-rank subspaces. Through experiments on image classification (CIFAR-10, ImageNet), language modeling (Wikitext-103), and reinforcement learning (MuJoCo), we show GPA achieves comparable performance to standard Adam while reducing peak memory usage. Specifically, ResNet-50 training on ImageNet shows a 1.2% accuracy drop (76.8% vs. 78.0%) but 67% reduction in optimizer memory. Theoretical analysis demonstrates convergence under standard assumptions, though our bound includes a projection error term that scales with subspace dimension. While our results are encouraging, GPA introduces additional hyperparameters controlling subspace size and update frequency, and exhibits sensitivity to projection dimension selection. The method shows most benefit when memory is constrained or for very large models, suggesting practical adoption may be limited to resource-constrained scenarios.",
    "id": 559
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning",
    "authors": [
      "Kumar, A.",
      "Liu, S.",
      "Chen, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance and catastrophic forgetting. While recent gradient surgery methods like PCGrad and GradDrop address this through selective gradient projection, they rely on fixed heuristics that may discard useful information. We propose Adaptive Memory Gradient Surgery (AMGS), which maintains a small learnable memory bank to store gradient directions from previous iterations. Our key insight is that seemingly conflicting gradients can become compatible when combined with the right historical context. AMGS dynamically computes convex combinations of current gradients with stored directions, guided by a lightweight attention mechanism trained to maximize inter-task transfer. On 8 multi-task datasets spanning computer vision and NLP benchmarks, AMGS achieves a 2.3% average improvement over PCGrad while requiring only 1.2% additional parameters. However, experiments reveal AMGS performs poorly when task counts exceed 15 or when tasks share minimal underlying structure. Additionally, our method introduces computational overhead and lacks convergence guarantees. While AMGS provides a practical improvement for moderate-scale multi-task scenarios, its benefits may not justify the complexity for all applications.",
    "id": 560
  },
  {
    "title": "Gradient Descent with Adaptive Momentum Schedules via Online Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a framework for automatically tuning momentum parameters in stochastic gradient descent (SGD) using techniques from online learning. While momentum is widely used to accelerate optimization, its optimal value typically varies across tasks and training stages. Our method treats momentum as an online learning problem, where the algorithm adaptively updates hyperparameters based on recent gradient statistics. We derive regret bounds for this approach under convex assumptions and demonstrate empirical improvements on small-scale vision and NLP benchmarks. To handle the non-convex landscape of deep neural networks, we incorporate a trust region mechanism to prevent unstable updates. Experiments on CIFAR-10 and GPT-2 language modeling show 5-15% faster convergence compared to constant momentum baselines, though gains diminish on larger architectures. The approach introduces minimal computational overhead and can be integrated with existing optimizers like Adam. Our results suggest adaptive momentum scheduling is beneficial for moderate-scale problems, but further work is needed to understand its behavior in more complex training scenarios.",
    "id": 561
  },
  {
    "title": "Improving Transformer Efficiency through Dynamic Attention Sparsification with Learned Clustering Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, A.K."
    ],
    "abstract": "While transformers achieve state-of-the-art results across many domains, their quadratic complexity in sequence length remains a fundamental limitation. We propose an adaptive attention sparsification method that clusters queries and keys in real-time using learned thresholds to maintain computational efficiency while preserving model quality. Our approach employs a lightweight gating network trained alongside the main model to predict optimal sparsity patterns based on input characteristics, eliminating the need for hand-tuned sparsity schedules. On standard NLP benchmarks including WikiText-103 and GLUE, our method reduces FLOPs by 1.7-2.3x with minimal perplexity degradation (0.3-0.8 increase) compared to dense attention. However, we observe consistent 2-4% accuracy drops on downstream tasks, and our method shows sensitivity to hyperparameter choices. While our clustering strategy offers computational benefits over fixed sparsity patterns, the learned thresholds generalize poorly across domains, suggesting fundamental challenges in adaptive sparse attention mechanisms. Code and pre-trained models will be released upon publication.",
    "id": 562
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: An Empirical Study of Multi-Task Optimization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning promises improved efficiency and generalization by sharing representations across tasks, but optimization interference remains a significant challenge. While gradient manipulation techniques like PCGrad and GradNorm have shown promise, they rely on restrictive assumptions about gradient geometry and task relationships. We propose Gradient Surgery with Adaptive Memory (GSAM), a simple modification that maintains an exponentially decaying memory of previous gradients to guide the current update direction. Our method requires minimal hyperparameter tuning and can be implemented in under 30 lines of PyTorch code. We evaluate GSAM on three benchmark suites: multi-digit MNIST, NYUv2 depth estimation with semantic segmentation, and a new synthetic regression dataset designed to stress-test gradient conflicts. Across 12 experimental settings, GSAM achieves average improvements of 2.1% over standard multi-task training and 1.3% over PCGrad, while reducing training variance by 15%. However, we observe that performance gains are highly sensitive to the memory decay rate and degrade significantly when tasks have substantially different loss scales. Our ablation studies reveal that the adaptive memory component contributes most of the benefit, while the gradient surgery step can sometimes hurt performance when tasks are poorly aligned. Code and datasets are available at anonymized-link.github.io.",
    "id": 563
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Selective Forgetting",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Thompson, A."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to suboptimal solutions. While recent gradient surgery methods address this through projection-based updates, they require expensive Hessian computations and struggle with task imbalance. We propose Adaptive Memory-based Gradient Surgery (AMGS), a simple approach that maintains a memory bank of task gradients and selectively forgets conflicting directions based on a learned confidence score. Our method avoids second-order derivatives by approximating gradient conflict through cosine similarity in the memory bank. We evaluate AMGS on standard multi-task benchmarks including Split-CIFAR100, NYUv2, and CityScapes. Results show 3-5% improvements in average task performance compared to gradient surgery baselines, with 2\u00d7 faster training. However, we observe that gains shrink to <1% when task correlation is high. Our analysis reveals AMGS primarily helps in regimes of high task competition, and may hurt performance when tasks are naturally aligned. While our approach reduces computational overhead versus PCGrad, the memory requirements scale linearly with number of tasks, limiting applicability to very large task sets. Code is available at [link].",
    "id": 564
  },
  {
    "title": "Variance-Guided Gradient Clipping Improves Stability of Transformer Training",
    "authors": [
      "Liu, K.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient clipping is widely used to stabilize transformer training, but standard clipping thresholds are often set heuristically. We propose variance-guided gradient clipping (VGGC), which adaptively adjusts clipping thresholds based on gradient variance across mini-batches. Our method computes running estimates of per-layer gradient statistics and clips gradients at k standard deviations from the mean, where k is learned through a light-weight meta-optimization procedure. Experiments on language modeling and machine translation show VGGC achieves 12-15% faster convergence compared to standard clipping on 4 out of 7 benchmark datasets, while maintaining similar perplexity. However, we observe minimal benefits on larger models (>1B parameters), possibly due to different gradient dynamics. Analysis reveals VGGC particularly helps when training with smaller batch sizes (\u2264512) where gradient noise is more pronounced. While our approach is simple to implement and provides consistent improvements on medium-scale tasks, the computational overhead of maintaining gradient statistics adds 3-5% training time. Code will be released upon acceptance.",
    "id": 565
  },
  {
    "title": "Improved Convergence Rates for Gradient Descent with Learned Step Sizes via Meta-Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a meta-learning framework for learning optimal step size schedules in gradient descent, aiming to improve convergence rates across a family of optimization problems. Our method trains a small neural network, called StepNet, to predict adaptive step sizes given gradient information and loss curvature estimates. Specifically, StepNet takes as input the current gradient norm, loss value, and a local quadratic approximation of the loss landscape, outputting a step size for the next iteration. We evaluate our approach on convex quadratic problems, training StepNet via meta-optimization to minimize convergence time over a distribution of problem instances. While theoretical analysis shows our method can achieve O(1/T) convergence for strongly convex problems\u2014matching standard rates\u2014the empirical improvements are modest: 5-12% faster convergence compared to tuned Adam and SGD with cosine annealing across various problem settings. Despite these limited gains, our framework provides insights into when learned schedules outperform hand-designed ones, particularly for ill-conditioned problems where curvature varies significantly. We demonstrate applications to linear regression and small-scale neural network training, though scaling to larger models remains challenging due to compute requirements of the meta-training procedure.",
    "id": 566
  },
  {
    "title": "Gradient Amplification Networks: Learning to Optimize Through Learnable Preconditioners",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Gonzalez, M."
    ],
    "abstract": "We propose Gradient Amplification Networks (GANs), a meta-learning approach that learns task-specific preconditioning matrices to amplify informative gradient directions during optimization. Rather than hand-designing adaptive optimizers like Adam or RMSProp, we learn a neural network that outputs preconditioning matrices based on the recent optimization trajectory. Our key insight is to parameterize the preconditioner as a low-rank update to existing optimizers, enabling efficient computation while maintaining expressivity. We evaluate GANs on several vision and language tasks, showing 5-12% relative improvements in final accuracy over baselines, particularly for small datasets and non-stationary objectives. However, we observe diminishing returns on large-scale problems and find the approach sensitive to hyperparameter choices. While our method demonstrates promise for certain classes of optimization problems, its broader applicability remains limited by computational overhead and unclear theoretical guarantees. Code will be made available post-review.",
    "id": 567
  },
  {
    "title": "A Unified Framework for Gradient Compression with Error Feedback in Heterogeneous Federated Learning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M.",
      "Zhou, Y."
    ],
    "abstract": "We propose EF-Het, a unified framework for gradient compression with error feedback in federated learning systems with heterogeneous clients. Our method introduces an adaptive compression mechanism that balances communication efficiency with convergence across devices with varying compute and communication capabilities. The key innovation is an error-insensitive quantization strategy that accumulates and redistributes compression errors based on client-specific learning rates and batch sizes. We provide convergence guarantees under standard assumptions, showing EF-Het achieves the same asymptotic rate as full-precision training for strongly convex objectives. On realistic federated benchmarks (CIFAR-10 with 100 heterogeneous clients and a language modeling task), EF-Het demonstrates 4-7x communication reduction compared to naive gradient compression, though we observe performance degradation under high client heterogeneity. While our theoretical analysis holds for convex settings, our empirical evaluation shows promising results on neural networks without theoretical backing. The framework offers a practical solution for resource-constrained federated environments, though its impact may be limited to specific deployment scenarios where gradient compression is beneficial.",
    "id": 568
  },
  {
    "title": "Adaptive Gradient Clipping with Curvature-Aware Bounds for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose an adaptive gradient clipping method that incorporates local curvature information to stabilize transformer training without extensive hyperparameter tuning. Our approach computes per-layer clipping bounds using an online estimate of the Fisher Information Matrix diagonal, combined with a momentum-based smoothing technique to handle gradient noise. While standard clipping methods use fixed thresholds or simple norm-based heuristics, our curvature-aware bounds automatically adjust to the changing loss landscape during training. We evaluate on Wikitext-103 language modeling and WMT'14 English-German translation tasks, showing modest improvements (0.8-1.2 BLEU/BPE) over strong baselines with reduced gradient explosion incidents. However, computational overhead increases training time by 15-20%, and benefits diminish on smaller architectures. Theoretical analysis provides convergence guarantees under standard convexity assumptions, but the non-convex setting remains largely heuristic. Code and hyperparameters are provided for reproducibility.",
    "id": 569
  },
  {
    "title": "Robust Continual Learning via Elastic Feature Consolidation with Dynamic Importance Weighting",
    "authors": [
      "Chen, K.",
      "Rodriguez, M.",
      "Singh, A."
    ],
    "abstract": "We propose Elastic Feature Consolidation with Dynamic Importance Weighting (EFC-DIW), a method for mitigating catastrophic forgetting in continual learning scenarios. While existing regularization-based approaches like EWC rely on static importance estimates computed only once after the first task, we observe that parameter importance evolves significantly as new tasks are encountered. Our method dynamically re-weights feature importance using an online Fisher information approximation that adjusts based on task similarity scores computed in a learned meta-space. We evaluate EFC-DIW on sequential CIFAR-100 and Split-CUB benchmarks, demonstrating 3-8% improvement in average accuracy over standard EWC and 1-2% over state-of-the-art baselines like SI and MAS. The approach adds minimal computational overhead (8-12% training time increase) and requires no task labels during inference. However, results show inconsistent gains on more complex ImageNet-derived sequences, with performance varying considerably across different task orderings. Our analysis reveals the method particularly struggles when task boundaries are less distinct. While modest, these improvements suggest dynamic importance estimation is a promising direction, though current gains may not justify the additional complexity in all scenarios.",
    "id": 570
  },
  {
    "title": "DARE: Data-Aware Regularization Ensembles for Robust Fine-tuning",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Mitra, V."
    ],
    "abstract": "Fine-tuning pre-trained models on limited data remains challenging due to overfitting and sensitivity to hyperparameters. We propose a simple yet surprisingly effective regularization method that leverages the interaction between an adaptive L2 penalty and data-specific augmentation strategies. Our approach, Data-Aware Regularization Ensembles (DARE), applies instance-dependent regularization strengths computed from gradient signal-to-noise ratios and incorporates a novel diversity term encouraging ensemble disagreement. Across 12 benchmark datasets spanning computer vision and NLP tasks, DARE achieves modest but consistent improvements over standard fine-tuning (average +1.3% accuracy), particularly in low-data regimes with <1k examples. While our theoretical analysis captures only simplified cases of the full method, it provides insight into why regularization strength should scale inversely with local curvature magnitude. The approach adds minimal computational overhead but introduces three additional hyperparameters that require dataset-specific tuning. Ablation studies suggest 60-70% of improvements stem from the data-dependent regularization component, with the remainder from ensemble effects. Though DARE does not achieve state-of-the-art results on established benchmarks, it provides a practical tool for practitioners facing data scarcity without requiring architectural changes or extensive hyperparameter search.",
    "id": 571
  },
  {
    "title": "Improved Regularization via Gradient Norm Penalties with Momentum-Based Adaptive Scaling",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient-based regularization techniques have shown promise for improving generalization in deep networks, but existing approaches either require expensive second-order computations or rely on fixed scaling hyperparameters. We propose GRAPE (Gradient Norm Penalty with Adaptive scaling), a regularizer that adaptively scales gradient norm penalties based on momentum estimates of gradient variance. Our method adds the term \u03bb_t ||\u2207L(\u03b8_t)||\u00b2 to the loss, where \u03bb_t is updated using an exponential moving average of historical gradient norms. We provide theoretical analysis showing that this approach controls the Lipschitz constant of the learned function while being less restrictive than weight decay. Experiments on CIFAR-10 and ImageNet show modest improvements over weight decay (0.5-1.2% accuracy gains) with particularly strong performance on smaller architectures. However, we find the benefits diminish on larger models and are sensitive to the choice of momentum hyperparameter. While GRAPE offers a computationally efficient alternative to existing gradient penalties, our results suggest the technique alone may not significantly impact performance when combined with modern architectures and training techniques. Code is available at anonymous-URL.",
    "id": 572
  },
  {
    "title": "Gradient Clipping Improves Adversarial Training Only When You Least Expect It",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Adversarial training remains the most effective defense against adversarial attacks, yet its robust generalization properties are poorly understood. We provide theoretical and empirical evidence that gradient clipping, a technique commonly used for stabilizing training, has a non-monotonic effect on adversarial robustness. Using a uniform-Lipschitz analysis, we prove that small clipping thresholds reduce the Lipschitz constant of the network, improving robustness against bounded attacks. However, beyond a critical threshold that depends on the data distribution's effective dimension, clipping begins to degrade robust accuracy by overly constraining the hypothesis class. Our experiments on CIFAR-10 and ImageNet subsets confirm this theoretical prediction: robust accuracy peaks at clipping thresholds that precisely match our derived expressions for 93% of tested architectures. While our analysis provides the first precise characterization of when gradient clipping helps adversarial training, our results are limited to \u2113\u2082-bounded attacks and networks with homogeneous activations. Furthermore, the practical impact is modest\u2014our optimal clipping schedule yields only 2-3% improvements over standard baselines. This work suggests that many widely-used training heuristics interact in subtle ways with adversarial objectives, opening questions about their systematic analysis.",
    "id": 573
  },
  {
    "title": "Adaptive Gradient Clipping Through Local Lipschitz Estimation for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.A.",
      "Kim, S."
    ],
    "abstract": "Transformer architectures exhibit unstable training dynamics due to exploding gradients, especially with large learning rates. While gradient clipping provides a simple remedy, choosing the clipping threshold remains heuristic and often dataset-dependent. We propose Lipschitz Clip, a method that adaptively sets clipping thresholds by estimating local Lipschitz constants of the loss landscape. Our approach approximates the Lipschitz constant using gradients from small batch statistics and applies a lightweight momentum-based update to the clipping threshold. Through experiments on Wikitext-103 and CIFAR-10, we show modest improvements in training stability and final perplexity/error over fixed clipping baselines. The method adds only 2-3% computational overhead compared to standard training. While our results demonstrate practical benefits, the theoretical guarantees are limited to smooth convex settings, leaving open questions about generalization to non-convex objectives. Our code will be publicly available upon publication.",
    "id": 575
  },
  {
    "title": "Gradient Norm Regularization Improves Adversarial Training: A Small-Scale Investigation",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, T."
    ],
    "abstract": "Adversarial training remains computationally expensive for large networks, leading practitioners to use smaller proxy models during training. We investigate whether additional regularization during this proxy training phase can translate to improved robustness in the final model. Specifically, we propose gradient norm regularization (GNR), which penalizes large gradient norms of the loss with respect to inputs during adversarial training. Our theoretical analysis suggests GNR can improve margin bounds by a factor of \u221ak in simplified settings. On CIFAR-10 and CIFAR-100, we conduct extensive experiments with ResNet-18/34 architectures across 3 random seeds. Results show modest but consistent improvements: GNR increases robust accuracy by 2.1-3.7% over standard adversarial training, while maintaining clean accuracy within 0.5%. However, these gains are less pronounced when tested on larger architectures (ResNet-50) or out-of-distribution datasets. Ablation studies reveal that the regularization weight must be carefully tuned to prevent gradient vanishing. While our method presents a simple, theoretically-motivated improvement to adversarial training, we acknowledge limitations: experiments use standard datasets and architectures, computational budget prevents ImageNet-scale evaluation, and the regularizer adds 15-20% training overhead. Code is available at [URL].",
    "id": 576
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Data-Dependent PAC-Bayes Priors",
    "authors": [
      "Kim, S.",
      "Lopez, M.",
      "Johnson, T."
    ],
    "abstract": "We propose a new PAC-Bayesian framework for deriving tighter generalization bounds for deep neural networks by constructing data-dependent priors. Unlike traditional flat priors, our approach leverages the empirical covariance structure of intermediate layer activations to construct Gaussian priors that are more informative while remaining valid under the PAC-Bayes framework. Our method combines insights from Fisher information geometry with recent advances in post-hoc sharpness analysis, yielding priors that adapt to the optimization trajectory. We derive novel concentration inequalities that account for the data-dependent nature of our priors, extending standard PAC-Bayes proofs. Experiments on CIFAR-10 and CIFAR-100 show improvements of 5-10% over existing bounds across various architectures (ResNet-18, VGG-16, WideResNet-28-10), though gains diminish on larger datasets like ImageNet. While our bounds are theoretically sound, the computational overhead of computing layer-wise covariance matrices scales quadratically with network depth. Our work suggests that practical generalization bounds may benefit from more expressive prior families, but highlights fundamental tensions between bound tightness and computational tractability.",
    "id": 577
  },
  {
    "title": "Gradient Descent with Implicit Structural Regularization via Weight Asymmetry",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S.",
      "Rodriguez, A."
    ],
    "abstract": "We investigate an observed phenomenon where SGD exhibits implicit regularization towards low-rank solutions in overparametrized linear networks, but only when initialized with asymmetric weight matrices. While previous work attributes this to norm minimization, we demonstrate that weight asymmetry directly constrains the optimization trajectory in a manner analogous to nuclear norm regularization. Our analysis reveals that gradient flow on asymmetric factorizations, U = AB^T, converges to global minima while maintaining a non-trivial relationship between the column spaces of A and B. We provide a theoretical characterization for two-layer linear networks under orthogonal input data and derive an expression relating initialization scale to effective regularization strength. However, extending these results to non-linear activations proves challenging due to dynamical coupling between layers. Empirical validation on synthetic rank recovery tasks shows modest improvements over symmetric baselines, though the effect diminishes with increasing network depth. We also observe similar trends in shallow convolutional networks trained on CIFAR-10, achieving 2-3% better test accuracy with carefully tuned asymmetric initializations. While our findings suggest weight asymmetry as a controllable inductive bias, we acknowledge limitations in generalization to deeper architectures and the lack of a complete theoretical picture. Code will be made available upon acceptance.",
    "id": 578
  },
  {
    "title": "Momentum Residual Networks: Improving Optimization Stability via Parameter-Reduced Skip Connections",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "While residual connections have become standard architectural components in deep learning, their theoretical understanding remains limited. We propose Momentum Residual Networks (MoRNet), which introduce a learnable momentum coefficient to each skip connection, enabling adaptive control of gradient flow. Our key insight is that standard residual blocks can be interpreted as a discretization of ordinary differential equations with fixed step size, and adding momentum terms yields better optimization dynamics. We analyze the gradient propagation characteristics and show that our formulation reduces the effective Lipschitz constant of the network. Empirically, MoRNet achieves competitive accuracy on CIFAR-10 (95.2%), CIFAR-100 (77.8%) and ImageNet (79.1% top-1) while using fewer parameters than standard ResNets. However, we find performance gains are most pronounced on shallower networks (\u226450 layers), with diminishing returns on deeper architectures. Our theoretical results provide some justification for the approach, though the framework is limited to ReLU activations. While MoRNet does not achieve state-of-the-art results, our work offers a modest improvement over baselines with minimal computational overhead and may inspire further research into adaptive residual architectures.",
    "id": 579
  },
  {
    "title": "Improving Transformer Efficiency Through Selective Attention Dropping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Self-attention mechanisms in Transformers suffer from quadratic complexity with respect to sequence length, limiting their applicability to long sequences. We propose AttentionDrop, a simple training-time regularization technique that randomly drops attention weights during training, encouraging the model to learn robust attention patterns that remain effective when using reduced attention matrices at inference. Our method requires no architectural changes and can be applied to existing pretrained models. On the Long Range Arena benchmark, we achieve 12-18% computational savings with <2% accuracy degradation compared to full attention. While our approach is theoretically motivated by recent work on low-rank attention approximations, empirical benefits are modest and inconsistent across tasks\u2014improvements are concentrated in longer sequences (>2k tokens) and less pronounced on standard NLP benchmarks. Extensive ablations reveal sensitivity to dropout rates and suggest benefits primarily stem from implicit model compression rather than learned sparse patterns as originally hypothesized. Though AttentionDrop provides a practical trade-off between efficiency and accuracy, its limited impact on standard tasks and lack of theoretical guarantees may restrict adoption in production systems.",
    "id": 580
  },
  {
    "title": "Improving Transformer Efficiency through Layer-wise Learning Rate Scheduling and Reversible Computation",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Training large transformer models remains computationally expensive, limiting accessibility for researchers and practitioners. We propose a combination of layer-wise learning rate scheduling with reversible transformer blocks to reduce memory usage and training time. Our learning rate schedule assigns decreasing learning rates to deeper layers based on the observation that lower layers converge faster, while reversible computation allows activations to be recomputed during the backward pass. On standard NLP benchmarks (WMT14 en-de, GLUE), our approach achieves 15-20% reduction in memory usage compared to standard transformers with less than 1% degradation in performance. While these gains are modest, we demonstrate that the techniques are complementary to existing efficiency methods and can be easily integrated into existing codebases. Our empirical analysis suggests the benefits are most pronounced for medium-sized models (100M-500M parameters), with diminishing returns for larger scales. Code and pre-trained models will be released upon acceptance.",
    "id": 581
  },
  {
    "title": "LoRA-GA: Low-Rank Adaptation with Gradient Alignment for Efficient Task Transfer",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become popular for adapting large language models, but their effectiveness varies significantly across tasks and model scales. We propose LoRA-GA, which incorporates gradient alignment regularization to improve transfer performance while maintaining parameter efficiency. Our method computes alignment between LoRA gradients and full fine-tuning gradients during training, dynamically adjusting low-rank updates to better approximate full model adaptations. Experiments on GLUE and SuperGLUE show 2-5% improvement over standard LoRA on average, with particularly strong gains on reasoning tasks. However, we observe that gains diminish on larger models (\u226530B parameters) and certain domains like code generation. Analysis reveals that gradient alignment primarily helps in early training stages, suggesting potential improvements through curriculum learning. While our method provides modest but consistent improvements over LoRA, computational overhead increases training time by 30% and requires access to full model gradients during training. These limitations suggest LoRA-GA is most beneficial for medium-scale applications where the slight performance gain justifies the additional complexity.",
    "id": 582
  },
  {
    "title": "Gradient Surgery in the Wild: An Empirical Study on Momentum and Adaptive Methods in Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, J."
    ],
    "abstract": "Multi-task learning often relies on gradient surgery techniques like PCGrad to resolve conflicting gradients between tasks. While these methods show promise in controlled settings, their behavior with popular optimizers like Adam or SGD+Momentum remains poorly understood. We conduct an extensive empirical study examining PCGrad and GradDrop across 50 multi-task benchmarks spanning vision, NLP, and RL domains, using Adam, SGD+Momentum, and RMSprop. Our experiments reveal that gradient surgery methods provide modest improvements (1-3% average) over naive baselines, but these gains disappear under common hyperparameter choices. Surprisingly, momentum and adaptive methods can amplify gradient conflicts, contrary to theoretical predictions. We propose a simple modification to PCGrad that adaptively thresholds conflicting gradients based on cosine similarity, achieving more stable training. While our approach shows consistent benefits across tasks, the improvements are incremental (0.5-1.2%) and fail to match specialized architecture-based solutions. Our findings suggest that gradient surgery may be less critical than previously thought, though careful optimizer choice and hyperparameter tuning remain essential. Code and checkpoints are available at anonymous-link.",
    "id": 583
  },
  {
    "title": "LoRA-Drop: Structured Pruning for Low-Rank Adaptation via Dynamic Thresholding",
    "authors": [
      "Chen, L.",
      "Rodriguez, S.",
      "Kim, J."
    ],
    "abstract": "While Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, its deployment still faces challenges from storage overhead and inference latency. We propose LoRA-Drop, a simple yet effective structured pruning approach that identifies and removes redundant LoRA modules during training. Our method introduces a learnable masking mechanism that dynamically adjusts pruning thresholds based on layer-wise sensitivity, eliminating the need for manual tuning. We demonstrate LoRA-Drop on RoBERTa-base and LLaMA-7B across GLUE and instruction tuning benchmarks, achieving 25-40% reduction in trainable parameters with minimal performance loss (<2% accuracy drop on average). However, we observe diminishing returns on larger models and heterogeneous task mixtures, where aggressive pruning (>50%) leads to unstable training. Our results suggest that LoRA redundancy patterns are architecture-dependent, with decoder models exhibiting higher sensitivity to compression. While our approach provides practical storage savings, the computational benefits remain modest for current inference frameworks. Code and pre-trained adapters will be made available upon acceptance.",
    "id": 584
  },
  {
    "title": "LoRA without Rank: Low-Dimensional Adaptation via Gradient Sketching",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Zhao, J."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a popular approach for efficient fine-tuning of large language models, but its fixed-rank constraint may limit expressiveness when adapting to diverse downstream tasks. We propose Sketch-LoRA, a simple alternative that replaces the low-rank constraint with a stochastic sketching mechanism applied to the gradient updates during training. Instead of learning a rank-r decomposition, we maintain a compressed representation of the adaptation matrix via CountSketch projections, allowing the effective rank to adapt dynamically during fine-tuning. Our method requires no architectural modifications and adds only 3-5% training overhead compared to standard LoRA. We evaluate Sketch-LoRA on GLUE and SuperGLUE benchmarks across three model sizes (125M-1.3B parameters), showing average improvements of 1.2-1.8% over LoRA while using 10-15% fewer parameters. While our approach provides consistent gains on classification tasks, we observe limited benefits on generation benchmarks and find that performance is sensitive to sketch size selection. Our results suggest that adaptive rank constraints can outperform fixed-rank methods in some settings, though the gains may not justify the increased complexity in all applications.",
    "id": 585
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Lookback for Sample-Efficient Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K.",
      "Zhao, J."
    ],
    "abstract": "Policy gradient methods suffer from high variance in gradient estimates, particularly in environments with sparse rewards or long horizons. While variance reduction techniques have been proposed, they typically rely on static baselines or require domain-specific hyperparameter tuning. We introduce VAPGO, a variance-reduced policy gradient method that adaptively selects lookback windows based on local smoothness estimates of the value function. Our key insight is that the optimal window size for advantage computation varies across state-action pairs and training iterations. We derive a theoretically-motivated criterion for selecting window sizes using Bernstein inequalities, achieving a sample complexity bound of O\u0303(\u03b5^-2) for tabular MDPs. On continuous control benchmarks, VAPGO shows modest improvements over PPO (5-12% higher returns) but underperforms SAC by 8-15%. While our theoretical analysis holds for the tabular case, we find the practical benefits diminish in high-dimensional settings where value function approximation becomes dominant. Experiments on 12 MuJoCo environments demonstrate competitive performance, though variance reduction effects are less pronounced than predicted. Our method provides a conceptually simple approach to adaptive variance reduction, but its limitations in complex domains suggest the need for better integration with representation learning techniques.",
    "id": 586
  },
  {
    "title": "Improved Generalization Bounds for Stochastic Gradient Descent via Noise-Induced Flatness",
    "authors": [
      "Chen, L.",
      "Garcia, A.",
      "Thompson, K."
    ],
    "abstract": "We provide new generalization bounds for stochastic gradient descent (SGD) by analyzing the connection between noise in gradient estimates and the flatness of local minima. Our approach combines PAC-Bayesian analysis with recent results on the implicit bias of SGD, showing that the noise inherent in mini-batch gradients encourages solutions that are both flat and robust. We derive bounds that scale as O(1/\u221an) for Lipschitz losses, matching standard rates but with improved dependence on sharpness-aware quantities. Experiments on CIFAR-10 and ImageNet subsets show our bounds are non-vacuous for moderate-width networks, though we observe worse constants than previous non-sharpness-based bounds. While our theoretical contribution is primarily a refinement of existing results rather than a breakthrough, our work provides the first explicit characterization of how SGD's noise interacts with sharpness-based generalization. The approach introduces minimal computational overhead and may be useful for understanding when sharpness-aware minimization provides practical benefits. Our results suggest that the generalization benefits of noise in SGD may be more limited than commonly assumed, particularly for deeper architectures where our bounds become looser. Code is provided at anonymous-url.github.io/flatness-bounds.",
    "id": 587
  },
  {
    "title": "Improved Generalization Bounds for Gradient Descent via Iterative Algorithmic Stability",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We revisit generalization bounds for gradient descent methods through the lens of algorithmic stability, focusing on non-convex optimization problems common in deep learning. By analyzing the Lipschitz properties of the gradient trajectory, we derive new generalization bounds that depend on the cumulative gradient norm rather than uniform convergence. Our key insight is that iterative updates exhibit localized stability that can be quantified through a modified notion of on-average stability. We prove that this approach yields bounds of order \u00d5(\u221a(T log(1/\u03b4)/n)) for T iterations and n samples, improving upon previous stability-based bounds by a factor of log(n) for certain problem classes. While this improvement is modest for practical architectures, we demonstrate advantages in overparameterized linear models and shallow networks. Experiments on CIFAR-10 and MNIST show empirical correlation between our bounds and generalization error, though the gap remains significant. Our work suggests that stability-based analysis may offer complementary insights to PAC-Bayesian approaches, despite computational limitations in computing tight bounds. The derivation provides a new regularization term that marginally improves test accuracy when added to existing objectives, achieving 0.3-0.7% improvements across standard benchmarks.",
    "id": 588
  },
  {
    "title": "Improving Gradient Descent with Lookahead-Averaged Momentum: A Simple Modification for Stable Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Ivanov, S."
    ],
    "abstract": "We propose Lookahead-Averaged Momentum (LAM), a simple modification to standard momentum-based optimizers that improves training stability without additional hyperparameters. LAM maintains two sets of parameters: the 'fast' parameters updated using standard momentum, and 'slow' parameters that are periodically updated using an exponentially-weighted average of the fast parameters. We show that this averaging step acts as a form of implicit gradient smoothing, reducing the variance of effective update directions. Across 8 benchmark datasets and 5 neural architectures, LAM achieves comparable or slightly improved final accuracy to SGD and Adam while exhibiting 15-30% lower training loss variance in early epochs. Theoretical analysis in the convex quadratic case reveals that LAM converges at a rate O(\u03ba log(1/\u03b5)) when \u03bb > 1/\u03ba, where \u03ba is the condition number and \u03bb is the synchronization period. While our empirical improvements are modest and the theoretical analysis is limited to simplified settings, LAM requires only 15 lines of code to implement and may be useful in scenarios where training stability is prioritized over maximal performance. Code will be made available upon acceptance.",
    "id": 589
  },
  {
    "title": "LoRA-Dropout: Memory-Efficient Fine-tuning with Probabilistic Low-Rank Adaptation",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto standard for parameter-efficient fine-tuning, but its fixed rank selection can lead to suboptimal performance across different downstream tasks. We propose LoRA-Dropout, a simple extension that randomly drops rank components during training to automatically identify the most relevant subspace for each task. Our method introduces learnable dropout probabilities for each rank component, allowing dynamic selection without hyperparameter tuning. On the GLUE benchmark, LoRA-Dropout achieves comparable performance to standard LoRA while using 15-30% fewer rank components on average. However, we observe minimal gains on larger models (\u22657B parameters) and similar downstream tasks. Theoretical analysis suggests our dropout rates correlate strongly with gradient norms, providing limited insight beyond existing heuristics. While our approach reduces memory usage for smaller models, the computational overhead from sampling rank subspaces during training may offset these benefits in practice. Code will be made available upon publication.",
    "id": 590
  },
  {
    "title": "Gradient Descent with Adaptive Momentum Schedules via Online Learning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, A."
    ],
    "abstract": "While adaptive optimizers like Adam and RMSProp are widely used in deep learning, they often underperform vanilla SGD with momentum on large-scale vision tasks. We hypothesize that combining the benefits of both approaches\u2014adaptive updates and momentum acceleration\u2014requires momentum coefficients that adapt during training. We propose Adaptive Momentum Schedules (AMS), a framework that learns momentum hyperparameters online using a simple regret minimization approach. AMS maintains a small set of candidate momentum values and selects among them via exponential weights updated based on gradient history. Theoretically, we show that AMS achieves near-optimal regret bounds for convex optimization and maintains convergence guarantees under standard assumptions. Empirically, we demonstrate 5-12% relative improvements over SGD-with-momentum on CIFAR-10/100 and moderate gains on ImageNet, while remaining competitive with Adam on language modeling tasks. However, we observe diminishing benefits on very deep networks (>100 layers) and datasets with heavy regularization. Our method adds minimal computational overhead (2-3% training time increase) and requires no additional hyperparameters beyond standard optimizers.",
    "id": 591
  },
  {
    "title": "Gradient Coordination in Federated Learning: A Simple Momentum Mechanism with Moderate Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Federated learning faces challenges in client drift due to heterogeneous data distributions across devices. We propose FedMom, a momentum-based coordination mechanism that accumulates client updates across rounds to reduce gradient variance. Our approach adds minimal computational overhead (0.3% increase compared to FedAvg) and requires only a single hyperparameter \u03b2 that balances momentum strength. We theoretically prove convergence for non-convex objectives under standard assumptions, achieving rates comparable to existing federated optimization methods. Experiments on CIFAR-10, CIFAR-100 with pathological non-IID splits show 2-4% improvement over FedAvg in final accuracy, though gains diminish with increasing client participation. While our method demonstrates consistent improvements on smaller datasets, benefits plateau on larger-scale benchmarks like ImageNet, suggesting limited scalability. Code and pre-trained models are available at [anonymized link].",
    "id": 592
  },
  {
    "title": "Gradient Surgery with Memory: Improving Multi-Task Optimization via Latent Replay",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that can impede optimization. Existing gradient surgery methods like PCGrad avoid conflicts through gradient projection, but at the cost of potentially useful information. We propose Gradient Memory Surgery (GMS), which stores conflicting gradients in a memory buffer and replays them through learned latent transformations that align them with the current task gradients. Our method combines insights from continual learning and multi-task optimization by maintaining a small replay buffer of projected gradients and learning task-specific encoders that map these gradients into a shared latent space. Experiments on three standard multi-task benchmarks (CIFAR-100/SVHN, NYUv2, and CityScapes) show modest improvements over PCGrad: 1.3% average accuracy gain and 0.5% mIoU improvement. While our results are encouraging, we find the benefits diminish when scaling to more than 5 tasks, and the additional memory overhead may not justify the improvements. Our implementation requires only 20 lines of code and adds minimal computational cost, making it practical for standard training pipelines.",
    "id": 593
  },
  {
    "title": "Looped Transformers Learn to Plan: Investigating Iterative Refinement for Compositional Tasks",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "We propose LoopFormer, a simple modification to standard transformers that connects the output back to the input through iterative processing, enabling emergent planning-like behavior without architectural changes beyond a feedback connection. By unrolling the transformer for T iterations, we show that the model learns to refine its predictions through a form of implicit search, potentially bridging the gap between autoregressive generation and algorithmic planning. While our analysis reveals that looped transformers can solve synthetic algorithmic tasks (shortest paths, sorting, puzzle solving) with significantly fewer parameters than vanilla transformers, we acknowledge that performance degrades on longer sequences and more complex tasks. Experiments on GSM8K and algorithmic benchmarks demonstrate 12-25% improvements over standard transformers, but we find these gains diminish when tasks require genuine backtracking or search over exponentially large spaces. Our theoretical analysis characterizes the representational capacity of looped architectures, though we note the gap between our expressive power results and empirical behavior remains incompletely understood. Code and datasets will be released upon acceptance.",
    "id": 594
  },
  {
    "title": "Variance-Reduced Zeroth-Order Optimization via Learned Coordinate Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Zeroth-order optimization faces fundamental limitations in high dimensions due to the query complexity scaling linearly with dimension. We propose VR-ZOLCS, a variance-reduced zeroth-order method that learns a coordinate sampling distribution to reduce gradient estimator variance. Our key insight is that many high-dimensional problems exhibit latent low-dimensional structure, allowing biased but lower-variance gradient estimates. Our approach alternates between updating the sampling distribution (via a lightweight neural network) and performing variance-reduced gradient steps. We prove convergence rates comparable to first-order methods under a relaxed smoothness assumption, with complexity depending on an effective dimension rather than ambient dimension. Experiments on black-box adversarial attacks and hyperparameter tuning demonstrate consistent speedups over SOTA baselines, though gains diminish with increasing problem dimensionality. While our theoretical guarantees are restricted to a stylized setting, empirical results suggest broader applicability. Code is available at [URL].",
    "id": 595
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed training, but existing methods rely on hand-crafted compression schedules that may not adapt to varying training dynamics. We propose Learned Gradient Compression (LGC), a framework that uses a lightweight LSTM controller to dynamically adjust quantization levels for gradient compression during training. The controller is trained using reinforcement learning with a reward function balancing compression ratio against gradient error. Our method achieves up to 8\u00d7 compression on ImageNet training while maintaining 0.5-1.2% accuracy degradation compared to full-precision training. Unlike static compression schemes, LGC adapts to different model architectures and training phases without manual tuning. Experiments on ResNet-50 and Transformer architectures show LGC outperforms existing methods like QSGD and TernGrad by 2-4\u00d7 in compression ratio at similar accuracy levels. However, theoretical analysis reveals LGC provides convergence guarantees only for convex objectives under restrictive assumptions. While practical performance is promising, the computational overhead of maintaining the controller (\u22482% training time) and lack of generalization to non-stationary objectives limit broader applicability. Code is available at masked-for-review.",
    "id": 596
  },
  {
    "title": "Gradient Amplifiers: Making Large Language Models Train Faster Through Selective Weight Rescaling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training large language models remains computationally expensive, with gradient vanishing and exploding being persistent challenges. We propose Gradient Amplifiers, a method that dynamically rescales weights during backpropagation based on gradient magnitude statistics. Our approach uses a lightweight heuristic that amplifies gradients for parameters with consistently small updates while damping those with volatile gradients. We demonstrate theoretical convergence guarantees for convex objectives and empirically validate our method on GPT-style models from 125M to 7B parameters. Experiments on standard language modeling benchmarks show 1.2-1.4x speedup in training time while maintaining comparable perplexity. Ablations reveal the method is most effective in deeper layers where vanishing gradients are prevalent. However, our approach introduces additional hyperparameters and shows diminishing returns on smaller models. While Gradient Amplifiers provides practical training acceleration for large-scale models, we acknowledge its benefits are primarily empirical and may not extend to all architectures or tasks, representing a step toward more efficient but not revolutionary training procedures.",
    "id": 597
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: An Empirical Analysis of Helper Tasks",
    "authors": [
      "Liu, K.",
      "Thompson, S.",
      "Rodriguez, M."
    ],
    "abstract": "Multi-task learning (MTL) often improves generalization by leveraging shared representations, yet determining which auxiliary tasks truly help remains challenging. We propose an introspective approach to identify and dynamically reweight 'helper' tasks during training. Our method monitors gradient alignment between primary and auxiliary losses, pruning tasks that exhibit persistent conflicting gradients. We evaluate this simple strategy on standard vision and NLP benchmarks with up to 8 tasks, showing 2-4% improvements over naive MTL baselines, but comparable performance to recent gradient surgery methods like PCGrad. While our approach is computationally lightweight and requires no hyperparameter tuning for reweighting, the gains diminish as model size increases, suggesting limited scalability. Code and pre-trained models will be released.",
    "id": 598
  },
  {
    "title": "Learning with Noisy Labels via Adaptive Confidence Thresholding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training deep neural networks robustly under label noise remains challenging despite extensive prior work. We propose ACT (Adaptive Confidence Thresholding), a simple post-processing method that dynamically adjusts confidence thresholds during training to filter potentially mislabeled examples. Unlike prior approaches that require estimating noise rates or maintaining additional networks, ACT uses only the model's own predictions and a moving average of training statistics to identify unreliable labels. Our method adds minimal computational overhead and can be incorporated into existing training pipelines with two lines of code. On CIFAR-10 and CIFAR-100 with synthetic noise rates from 20% to 60%, ACT achieves comparable or better performance than state-of-the-art methods while being significantly simpler to implement. However, we observe performance degradation under extreme noise (>70%) and when the noise is instance-dependent rather than uniform. Ablation studies reveal that ACT's effectiveness primarily comes from early regularization effects rather than sophisticated noise detection. While our empirical results are promising, we lack theoretical guarantees and observe sensitivity to hyperparameters in certain regimes. Code and pre-trained models will be released upon acceptance.",
    "id": 599
  },
  {
    "title": "Gradient Surgery with Adaptive Momentum: A Minimal Intervention Approach to Multi-Task Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization. While existing gradient surgery methods compute fixed combinations of task gradients, they lack mechanisms to adaptively adjust their influence during training. We propose Adaptive Gradient Surgery with Momentum (AGSM), a lightweight extension that modulates gradient combinations based on per-task loss curvature. AGSM interpolates between gradient surgery and momentum-based updates, applying corrective terms only when tasks exhibit significant gradient conflict. Our method requires minimal additional computation (\u22645% overhead compared to standard training) and can be integrated into existing frameworks with <20 lines of code. Experiments on three multi-task vision benchmarks show 2-4% improvements over PCGrad and GradNorm, with particular gains on task pairs exhibiting high gradient interference. However, performance gains diminish on NLP tasks and heavily-regularized models, suggesting our approach primarily benefits specific optimization regimes. While AGSM introduces interpretable hyperparameters relating to conflict detection thresholds, their optimal values vary across datasets. Theoretical analysis establishes convergence under convexity assumptions, though the non-convex case remains problematic. Code is available at anonymous-url.",
    "id": 600
  },
  {
    "title": "Revisiting Momentum with Learned Update Coefficients for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Singh, K."
    ],
    "abstract": "We propose Learned Momentum Coefficients (LMC), a simple extension to stochastic momentum methods that adaptively selects per-parameter momentum values using a lightweight neural network trained concurrently with the main optimization. While momentum-based optimizers like SGD with momentum and Adam are widely used in deep learning, their fixed momentum hyperparameters may limit convergence speed on non-convex objectives. LMC maintains the computational efficiency of standard momentum methods while allowing momentum factors to vary dynamically during training. Our experiments on CIFAR-10/100 and ImageNet show that LMC achieves 1-3% better accuracy than AdamW on ResNet-50 and Vision Transformer models, with modest improvements in convergence speed (5-10% faster wall-clock time to target accuracy). However, gains are inconsistent across architectures - LMC performs comparably to tuned baselines on smaller models but offers limited benefits on larger ones. Theoretical analysis reveals that LMC converges for smooth non-convex objectives at a rate matching standard momentum, though we do not show improvement in the worst case. Our approach requires minimal additional memory (0.1% increase) but introduces some hyperparameter sensitivity not present in fixed-momentum methods. Code is available at [anonymized].",
    "id": 601
  },
  {
    "title": "Gradient Descent with Memory: Analyzing the Impact of Momentum on Non-Convex Landscapes via Perturbed Optimization Trajectories",
    "authors": [
      "Liu, Y.",
      "Thompson, K.",
      "Garcia, M."
    ],
    "abstract": "Momentum-based optimizers such as SGD with momentum and Adam are ubiquitous in deep learning, yet theoretical understanding of their role in navigating non-convex loss surfaces remains incomplete. We introduce a memory-augmented framework to analyze how momentum influences gradient descent trajectories on general non-convex objectives. Our approach models the optimizer as a second-order dynamical system whose state incorporates both position and velocity, allowing us to characterize the geometry of perturbed trajectories near saddle points. We prove that momentum can accelerate escape from strict saddle points under certain curvature conditions, but may also create spurious stable limit cycles that trap the optimizer in suboptimal basins. Empirically, we validate these findings on shallow autoencoders and small ResNet variants, demonstrating 5-15% improvements in escape time from synthetic saddle points compared to vanilla SGD, though benefits diminish on real datasets. While our theoretical insights are limited to simplified models, the memory-based perspective offers new diagnostic tools for understanding the transient behavior of momentum methods. Our analysis suggests that the effectiveness of momentum is highly sensitive to initialization scale and batch size, providing partial explanation for mixed results in practice.",
    "id": 602
  },
  {
    "title": "Fixed-Point Alternating Minimization for Robust Low-Rank Matrix Recovery with Non-Convex Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose a fixed-point alternating minimization algorithm for robust low-rank matrix recovery that incorporates non-convex regularization terms to better approximate the rank function. While existing nuclear norm minimization approaches provide theoretical guarantees, they often require restrictive assumptions on the measurement operator and result in biased solutions. Our approach employs a family of non-convex surrogates including smoothly clipped absolute deviation (SCAD) and minimax concave penalty (MCP) within an alternating minimization framework. We establish convergence to stationary points under standard assumptions, though our theoretical recovery guarantees are weaker than convex methods and depend on initialization quality. Empirically, our method achieves 5-15% lower reconstruction error than nuclear norm minimization on synthetic low-rank matrices with 30% gross corruption, and provides marginal improvements (2-3%) on matrix completion tasks from the Netflix dataset. However, performance degrades significantly when the rank is overestimated or corruption exceeds 40%. The algorithm requires careful tuning of two hyperparameters and converges more slowly than competing methods. Our results suggest non-convex regularization can provide practical benefits for robust matrix recovery, though theoretical gaps remain in understanding its advantages over simpler convex baselines.",
    "id": 603
  },
  {
    "title": "Gradient Entropy Regularization: A Simple Technique for Improving Generalization in Deep Networks",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Kumar, V."
    ],
    "abstract": "We propose gradient entropy regularization (GER), a lightweight technique for improving generalization in deep neural networks by penalizing low entropy in gradient distributions during training. While entropy regularization has been extensively studied in the output space, we show that encouraging diverse gradients at the layer level can reduce overfitting and improve robustness to label noise. Our method adds minimal computational overhead, requiring only an additional forward pass to compute gradient statistics. Experiments on CIFAR-10/100 and ImageNet show improvements of 0.5-1.2% accuracy over baselines, particularly strong when combined with mixup augmentation. However, we find the benefits diminish with larger models and are inconsistent across architectures (ResNet vs. Vision Transformer). Theoretical analysis reveals GER implicitly performs a form of Jacobian regularization, though connections to flat minima remain inconclusive. While GER is simple to implement and may benefit practitioners working with limited data, we acknowledge its modest performance gains and lack of clear theoretical grounding may limit its broader impact.",
    "id": 604
  },
  {
    "title": "LoRA-Sego: Segmenting Low-Rank Adaptation Modules via Eigenvalue Thresholding for Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a parameter-efficient method for fine-tuning large language models, but its application to multi-task settings remains poorly understood. We propose LoRA-Sego, a simple thresholding approach that automatically segments shared and task-specific components of LoRA modules based on eigenvalue analysis of the low-rank matrices. Our method requires minimal hyperparameter tuning beyond a single threshold value and can be applied post-hoc to existing LoRA checkpoints. Experiments on GLUE and SuperGLUE benchmarks with T5-base show 2-4% improvement over standard multi-task LoRA baselines while using 15% fewer parameters, with particularly strong gains on low-resource tasks. However, we find the approach is sensitive to the threshold selection and offers diminishing returns as model scale increases. While our contribution is primarily empirical, we provide theoretical intuition connecting eigenvalue decay patterns to task similarity. Code and pre-trained checkpoints will be released upon acceptance.",
    "id": 605
  },
  {
    "title": "Gradient Descent with Layer-wise Learning Rates for Transformer Fine-tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose L2-GD, a simple modification to standard gradient descent that uses different learning rates for each transformer layer during fine-tuning. Drawing inspiration from the observation that lower layers typically require smaller updates than upper layers, we introduce a heuristic schedule that scales learning rates exponentially with layer depth. Our experiments across 8 GLUE tasks and 3 vision datasets show modest improvements over standard fine-tuning (average +0.8% accuracy), with particularly strong gains on smaller datasets (<10k examples). While similar ideas have been explored in vision models, our work provides the first systematic study for transformers. The method adds minimal computational overhead and requires no additional hyperparameters beyond a global scaling factor. However, the improvements are inconsistent across tasks\u2014some datasets show no benefit or slight degradation. Analysis reveals the approach works best when pre-trained and target domains are similar. Though L2-GD is unlikely to fundamentally change fine-tuning practices, it offers a practically useful technique for resource-constrained scenarios where careful hyperparameter tuning is expensive. Code and pre-trained models will be released.",
    "id": 606
  },
  {
    "title": "Gradient Surgery in the Wild: An Empirical Analysis of Gradient Projection Methods for Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "Gradient surgery methods for multi-task learning, such as PCGrad and GradNorm, have shown promise in reducing negative transfer between tasks. However, most evaluations are limited to controlled benchmarks with aligned training curricula. We conduct an extensive empirical study of these methods across 12 real-world multi-task datasets spanning computer vision, NLP, and recommendation systems. Our results reveal that while gradient projection techniques consistently improve over vanilla multi-task training, the magnitude of improvement often falls short of reported gains in synthetic settings (average improvement of 2.3% vs 8.7% in benchmark papers). We identify three key failure modes: (1) gradient conflicts arise not only between tasks but also within individual tasks due to data heterogeneity, (2) the projection step disproportionately affects low-resource tasks, and (3) hyperparameter sensitivity leads to instability across different training runs. Based on these findings, we propose a simple adaptive scheme that adjusts the gradient projection threshold based on validation performance, achieving modest but consistent improvements over fixed methods. While our contributions are primarily empirical, they highlight important gaps between controlled multi-task research and practical deployment.",
    "id": 607
  },
  {
    "title": "LoRA-Plus: A Simple Scaling Improvement for Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its performance often saturates for larger target tasks. We identify that the standard LoRA update mechanism suffers from gradient-conflict between the pre-trained weights and low-rank adaptations during training. To address this, we propose LoRA-Plus, which introduces a learned scalar multiplicative factor to coordinate the adaptation magnitude across layers. Our method requires only 0.01% additional parameters and can be implemented in 5 lines of code. Experiments on GLUE and E2E benchmarks show improvements of 1.2 and 0.8 points respectively over standard LoRA, while maintaining memory efficiency. In controlled ablations, our factor demonstrates consistent gains across different adaptation ranks (4-64), though the benefit diminishes for smaller models (<100M parameters). While LoRA-Plus provides modest but reliable improvements, we acknowledge that similar coordination mechanisms could be learned through careful hyperparameter tuning of existing methods. Our PyTorch implementation is provided in the supplement.",
    "id": 608
  },
  {
    "title": "Memory-Augmented Transformers with Learnable Forgetting Gates for Long-Context Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "We propose MT-FGATE, a memory-augmented transformer architecture that incorporates learnable forgetting gates to improve long-context learning efficiency. While recent approaches have focused on extending attention to longer sequences, they often suffer from memory explosion and computational overhead as sequence lengths increase. Our method introduces forgetting gates that learn to selectively discard non-essential information from the memory bank, controlled by a lightweight gating mechanism conditioned on the current input and previous memory states. We derive theoretical bounds showing that our approach maintains approximation guarantees while reducing memory complexity from O(n\u00b2) to O(n log n) for sequences of length n. Experiments on language modeling tasks with context lengths up to 16K tokens show 2.3% improvement in perplexity compared to standard transformers with rotary embeddings, and 15% reduction in memory usage at inference time. However, we observe that performance gains diminish with shorter contexts (\u2264 2K tokens), and the forgetting gates occasionally discard crucial long-term dependencies. Our code is available at [URL].",
    "id": 609
  },
  {
    "title": "Frequency-Aware Gradient Clipping for Robust Transformer Training Under Label Noise",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Zhao, H."
    ],
    "abstract": "Prior work has demonstrated that transformer models are surprisingly robust to label noise, maintaining reasonable downstream performance even with 30-50% corrupted labels. We explore whether this robustness can be explained through the lens of gradient frequency distribution during training. Our key observation is that noisy labels primarily affect high-frequency gradient components, while low-frequency components largely preserve the underlying signal. Based on this insight, we propose Frequency-Aware Gradient Clipping (FAGC), which adaptively clips gradients based on their frequency content. FAGC operates in the Fourier domain of parameter gradients, preserving low-frequency information while thresholding high-frequency updates. On CIFAR-100 and ImageNet with synthetic label noise, FAGC achieves 2-3% improvements over standard training, and shows particular benefits when combined with mixup augmentation. However, we observe diminishing returns on naturally noisy datasets like WebVision. While FAGC introduces minimal computational overhead (<5%), its benefits appear dataset-specific and we cannot achieve consistent improvements across all settings. Our empirical results challenge the prevailing view that noise robustness is solely due to architectural inductive biases, suggesting an alternative explanation based on gradient frequency filtering during optimization.",
    "id": 610
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Rank Selection via Gradient Noise Estimation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank selection often leads to over-parameterization in some modules while under-capacitating others. We propose LoRA-Drop, a simple yet effective approach that dynamically adjusts LoRA ranks during training by leveraging gradient noise estimation. Our method computes the signal-to-noise ratio of LoRA gradients across modules and periodically drops the lowest-contributing ranks while potentially increasing ranks for high-gradient components. Experiments on GLUE and SuperGLUE benchmarks using Llama-7B show that LoRA-Drop achieves comparable performance to full LoRA (average 0.3% lower F1) with 15-20% fewer parameters. However, we observe significant performance drops on reasoning-intensive tasks like ReCoRD (6.2% lower F1), suggesting rank selection may be more critical for complex tasks. Our ablation reveals that the gradient-based rank selection heuristic occasionally removes important components early in training, which may explain these limitations. While LoRA-Drop offers modest computational savings, the gains are incremental compared to existing rank search methods, raising questions about the trade-off between adaptive rank selection and training stability.",
    "id": 611
  },
  {
    "title": "Gradient Surgery on the Edge: When Does Layer-Wise Gradient Clipping Actually Help?",
    "authors": [
      "Chen, Y.",
      "Rodriguez, L.",
      "Kim, J."
    ],
    "abstract": "Gradient clipping is widely used to stabilize neural network training, yet we lack a systematic understanding of when and why layer-wise clipping outperforms global clipping. We introduce LayerClipped-SGD, a simple variant that clips gradients within each layer independently using learnable thresholds. Through experiments on vision and language tasks, we find that layer-wise clipping consistently improves training stability compared to global clipping, particularly when gradient norms vary dramatically across layers. However, our theoretical analysis reveals this benefit is primarily empirical: we prove that layer-wise clipping introduces additional bias in the gradient estimate that can harm convergence, especially in overparameterized networks. Our results demonstrate an important trade-off between training stability and gradient fidelity, suggesting that layer-wise clipping should be applied selectively based on measurable layer-wise gradient statistics. While our method shows practical improvements on six out of nine evaluated benchmarks, the theoretical limitations we uncover raise questions about its broader applicability. This work provides both a practical algorithm and theoretical insights into the nuanced effects of gradient clipping in modern deep learning.",
    "id": 612
  },
  {
    "title": "Gradient Surgery in Neural Networks: An Empirical Study of Iterative Magnitude Pruning",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Anderson, D."
    ],
    "abstract": "The Lottery Ticket Hypothesis has sparked significant interest in identifying sparse, trainable subnetworks within larger neural networks. While theoretical work suggests that magnitude-based pruning identifies optimal sparse architectures, the practical effectiveness of iterative magnitude pruning (IMP) remains poorly understood. We conduct a comprehensive empirical investigation of IMP across 20 diverse architectures and datasets, revealing three key findings. First, we demonstrate that the success of IMP depends critically on learning rate rewinding rather than weight rewinding, contradicting conventional wisdom. Second, we find that the identified 'winning tickets' exhibit poor stability under standard data augmentation schemes, achieving 8-15% lower accuracy than their dense counterparts. Finally, we show that simple modifications to IMP\u2014namely, gradient-based pruning criteria and dynamic sparsity schedules\u2014consistently outperform standard IMP by 3-5% accuracy while maintaining comparable sparsity levels. Our results suggest that while lottery tickets exist, current IMP implementations fail to reliably extract them, highlighting the gap between theoretical guarantees and practical performance. We release our implementation and 500 pre-trained models to support future research.",
    "id": 613
  },
  {
    "title": "Rethinking Curriculum Learning Through Gradient Norm Scheduling",
    "authors": [
      "Chen, Y.",
      "Rodriguez, M.",
      "Kumar, V."
    ],
    "abstract": "Curriculum learning has shown promise for improving training efficiency and generalization, yet most methods rely on manually designed difficulty schedules or complex reinforcement learning agents to adapt curricula. We propose a simple alternative: Gradient Norm Scheduling (GNS), which automatically adjusts training difficulty based on the per-sample gradient norms of the current model. Intuitively, samples producing large gradient norms are considered difficult and deferred to later training stages. We theoretically show that GNS approximates a regularized optimization objective that promotes smoother loss landscapes. Empirically, we evaluate GNS on CIFAR-10 and CIFAR-100 with ResNet-18 and WideResNet-28-10 architectures, achieving modest improvements (~0.5-1.2% accuracy gains) over standard training. While our approach is simpler than existing adaptive curriculum methods, the improvements are incremental and inconsistent across datasets. Ablation studies reveal that gradient norm ordering correlates only weakly with human-defined difficulty measures. Our method may be most useful as a lightweight alternative to standard training rather than a definitive solution for curriculum learning.",
    "id": 614
  },
  {
    "title": "Gradient Amplification Through Transient Sharpening: A Simple Heuristic for Improved Convergence in Overparameterized Networks",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chawla, N.",
      "Chen, L."
    ],
    "abstract": "We propose a lightweight training modification that amplifies gradient signals during the initial phase of training deep neural networks. Our method, Transient Gradient Sharpening (TGS), applies an element-wise transformation to gradients based on their historical norm distribution, effectively increasing the learning rate for coordinates with consistently small updates while maintaining stability for others. Unlike adaptive optimizers that maintain per-parameter statistics throughout training, TGS gradually phases out its intervention, transitioning to standard SGD momentum after a preset number of epochs. We demonstrate improvements over vanilla SGD on ResNet-50 and ViT-B/16 architectures across CIFAR-100 and ImageNet-1K, with average top-1 accuracy gains of 1.2% and 0.8% respectively. While our approach is simple to implement and adds minimal computational overhead, we observe that gains diminish with stronger baselines like AdamW. Theoretical analysis reveals connections to sharpness-aware minimization, though we acknowledge limitations in our convergence guarantees and the heuristic nature of our phase-out schedule. Code is available at the provided link.",
    "id": 615
  },
  {
    "title": "Improved Generalization Bounds for SGD via Discrete-Time Stochastic Calculus",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "We present new generalization bounds for stochastic gradient descent (SGD) using discrete-time stochastic calculus, extending recent continuous-time analyses to the practical discrete-step regime. Our approach models SGD as a discrete stochastic differential equation and leverages a novel discretization-aware Girsanov transformation to bound the generalization error. The key insight is to incorporate the discrete nature of SGD updates directly into the bound, avoiding the constant-factor losses from continuous-time approximations. We derive bounds that depend on the step size, iteration count, and a new notion of discrete-time path stability. For Lipschitz losses, our bounds improve upon previous work by constant factors and provide tighter dependencies on the step size. However, our bounds become vacuous for step sizes larger than 1/L, limiting applicability to modern high learning rate training regimes. We validate our theory on small-scale experiments with MNIST and CIFAR-10, achieving 2-3x tighter predictions than continuous-time counterparts. While our theoretical contribution is incremental, we believe our discrete-time perspective opens avenues for sharper generalization analyses. Code will be made available.",
    "id": 616
  },
  {
    "title": "Gradient Surgery on Language Models: When Less is More for Continual Learning",
    "authors": [
      "Kim, S.",
      "Chen, J.",
      "Rodriguez, L."
    ],
    "abstract": "We investigate a simple approach to catastrophic forgetting in continual text classification by surgically removing gradient components that interfere with previously learned tasks. Our method identifies task-specific gradient directions using a small held-out set of examples from earlier tasks, then projects the current task's gradients onto the orthogonal complement of these directions. We evaluate on a sequence of 10 text classification tasks drawn from GLUE and SuperGLUE, showing 8-12% improvement in retention accuracy over standard fine-tuning baselines and 3-5% over replay-based methods. However, we find that surgical precision saturates at 512 gradient directions per task, beyond which performance degrades. Our analysis reveals that this ceiling stems from accumulating approximation errors in the orthogonal projection matrices. While our method requires no task-specific parameters and adds minimal computational overhead, storage requirements grow linearly with the number of tasks. These results suggest gradient surgery can be competitive with more sophisticated continual learning approaches, but may face scaling challenges in long-horizon scenarios.",
    "id": 617
  },
  {
    "title": "Improving Transformer Efficiency through Attention Layer Subsampling",
    "authors": [
      "Liu, S.",
      "Chen, H.",
      "Kim, J."
    ],
    "abstract": "While transformers have achieved remarkable success across domains, their quadratic complexity remains a bottleneck for long sequences. We propose Attention Subsampling Networks (ASN), a simple modification that selectively processes attention patterns by sampling a fixed number of attention heads based on learned confidence scores. Our method reduces computational complexity from O(n\u00b2h) to O(n\u00b2k) where k << h is the number of selected heads per layer. On 6 standard NLP benchmarks, ASN achieves 1.3\u00d7 speedup with 0.8% average accuracy drop compared to full attention, approaching the Pareto frontier between efficiency and performance. We provide theoretical analysis showing ASN preserves attention diversity under mild assumptions. However, we observe substantial performance degradation on tasks requiring fine-grained reasoning (>3% drop on NLI tasks), suggesting our subsampling scheme may discard critical information. While not achieving SOTA, ASN offers a practical trade-off for resource-constrained deployment, particularly for applications where slight accuracy loss is acceptable for significant inference speedup.",
    "id": 618
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Batch Sizes: A Practical Middle Ground",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Policy gradient methods remain a cornerstone of reinforcement learning, yet their sample efficiency continues to lag behind value-based counterparts. While variance reduction techniques like SVRG and SARAH have shown promise in supervised learning, their adaptation to policy gradients faces a tension between theoretical guarantees and computational practicality: full-batch gradient computations required for variance reduction eliminate the very sample efficiency gains they aim to achieve. We propose a practical compromise that dynamically adjusts batch sizes based on estimated gradient variance, applying variance reduction only when the signal-to-noise ratio drops below learned thresholds. Our approach requires no additional hyperparameters beyond standard policy gradient methods and adds minimal computational overhead (2-4% in wall-clock time). Across continuous control benchmarks, our method achieves 15-30% sample efficiency improvements over PPO on 8 out of 12 environments, while matching performance on the remainder. However, gains diminish with optimal hyperparameter tuning, and we observe instability in high-dimensional action spaces. Our results suggest that while variance reduction can help policy gradients, the benefits may be incremental rather than transformative, highlighting the importance of careful empirical validation of theoretical techniques.",
    "id": 619
  },
  {
    "title": "Improving Transformer Efficiency through Head-wise Linear Bottlenecks with Learnable Dropout",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Okafor, K."
    ],
    "abstract": "Transformers suffer from quadratic complexity in self-attention, limiting their deployment on resource-constrained devices. While recent pruning and distillation techniques reduce compute, they often require extensive retraining or sacrifice model quality. We propose Head-wise Linear Bottlenecks (HLB), a lightweight modification that inserts trainable linear projections between attention heads and feed-forward layers, bottlenecking higher-dimensional representations while preserving task performance. HLB enables adaptive feature compression through a novel learnable dropout mechanism that dynamically adjusts during training. Our experiments on GLUE, WMT'en-de, and ImageNet show 17-34% FLOP reduction with <1% accuracy loss compared to standard architectures, consistent across BERT-base and ViT-B/16 variants. Notably, HLB achieves comparable results to more complex compression methods while maintaining the original pretraining weights and requiring only 5% additional parameters. However, we observe diminishing benefits on larger models and tasks requiring long-form generation. While not state-of-the-art in pure efficiency, HLB offers a practical balance between implementation simplicity and reasonable performance gains, particularly suitable for practitioners requiring modest speedups without full model re-architecture.",
    "id": 620
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: When Does Selective Rewiring Improve Generalization?",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Recent work suggests that not all parameters in overparameterized networks contribute equally to generalization. We investigate whether selectively modifying gradient flow to specific parameter subsets can improve generalization while maintaining training dynamics. Our method, Selective Gradient Surgery (SGS), identifies and manipulates gradient updates for parameter groups based on their alignment with the data manifold. Specifically, we compute gradient-projections onto learned subspaces and apply learned masking functions to suppress updates in directions deemed less beneficial for generalization. We evaluate SGS on ResNet architectures across CIFAR-10/100 and ImageNet subsets, achieving 2.3% and 1.7% improvements over baselines respectively. While our ablation studies show consistent gains from selective rewiring, the technique introduces three additional hyperparameters and increases training time by 20-30%, raising questions about practical deployment. Theoretically, we demonstrate SGS reduces effective capacity in overparameterized linear models, but extending this result to nonlinear networks remains an open challenge. Our work suggests gradient-level interventions can provide modest generalization benefits, though the improvements may not justify computational overhead in all settings.",
    "id": 621
  },
  {
    "title": "Gradient Descent with Iterative Noise Shaping: A Practical Acceleration Framework for Deep Neural Network Training",
    "authors": [
      "Liu, Y.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Descent with Iterative Noise Shaping (GD-INS), a simple modification to standard SGD that adaptively adjusts the noise scale of gradient updates based on local curvature estimates. By multiplying the gradient noise covariance by a learned scaling factor derived from approximate Hessian diagonal elements, we achieve behavior reminiscent of natural gradient methods without the computational burden of matrix operations. Our method requires only a single additional backward pass every k iterations and introduces minimal overhead. Experiments on CIFAR-10 and ImageNet show 8-15% faster convergence to baseline accuracy compared to standard SGD, with particular gains on deeper architectures. While we demonstrate measurable improvements over standard baselines, our theoretical analysis reveals GD-INS recovers standard SGD in the worst case and provides benefits primarily when the Hessian exhibits specific block-diagonal structure. Code will be released upon acceptance.",
    "id": 622
  },
  {
    "title": "Revisiting Data Augmentation for Semi-Supervised Learning: A Simple Baseline Revisited",
    "authors": [
      "Chen, J.",
      "Liu, S.",
      "Rodriguez, M."
    ],
    "abstract": "Semi-supervised learning (SSL) has grown increasingly complex, with recent methods incorporating multiple loss terms, sophisticated regularizers, and carefully designed architectures. In this work, we revisit earlier approaches to data augmentation in SSL and demonstrate that a simplified variant of consistency regularization\u2014when combined with modern augmentation strategies\u2014achieves competitive performance across standard benchmarks. Our method employs a straightforward two-loss training objective combining standard cross-entropy with an L2 consistency loss between weakly and strongly augmented variants, avoiding auxiliary clustering losses, pseudo-label thresholding, or complex scheduling schemes. Notably, we show that with appropriate augmentation policies, this minimalist approach achieves 93.8% accuracy on CIFAR-10 with 250 labels, narrowing the gap with state-of-the-art methods to 1.2%. While our empirical results are encouraging, we acknowledge that our improvements appear incremental over simpler baselines. We conduct extensive ablations to understand where gains originate, revealing that augmentation diversity and consistency weight schedules contribute more significantly to final performance than architectural choices. These findings suggest that current SSL progress may be driven more by augmentation engineering than by algorithmic innovation. Code will be released upon publication.",
    "id": 623
  },
  {
    "title": "Efficient Gradient Compression through Adaptive Block-Partitioning with Convergence Guarantees",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "Gradient compression is essential for scaling distributed training, yet existing methods either require significant tuning or suffer convergence degradation. We propose Adaptive Block-Partitioned Compression (ABPC), a simple yet effective approach that dynamically partitions gradients into variable-sized blocks based on local curvature estimates. ABPC generalizes Top-K sparsification and quantization by learning block partition sizes that minimize compression error while respecting communication budgets. We provide theoretical analysis showing ABPC maintains convergence for smooth non-convex objectives, achieving O(1/\u221aT) rates matching uncompressed SGD with only logarithmic dependence on compression ratio. Our experiments on ResNet-50 and Transformer models demonstrate 15-25% communication reduction compared to existing methods at equivalent accuracy. However, we find the benefits diminish when applied to newer network architectures with skip connections. While our theoretical framework is general, we acknowledge it assumes bounded gradient norms and relies on conservative learning rates that may be suboptimal in practice.",
    "id": 624
  },
  {
    "title": "LoRA-Fisher: Efficient Incremental Fine-tuning via Layerwise Fisher Information Approximation",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Kim, J."
    ],
    "abstract": "We present LoRA-Fisher, a lightweight approach for adapting large language models that combines Low-Rank Adaptation (LoRA) with layerwise Fisher information estimates to improve parameter efficiency during incremental fine-tuning. Our method modifies the standard LoRA formulation by incorporating curvature-aware scaling factors derived from diagonal Fisher approximations computed on a small validation set. This enables more targeted updates to parameters that are most relevant to downstream tasks, theoretically reducing interference with pre-trained knowledge. Experiments on GLUE and SuperGLUE benchmarks show 2-3% improvement over vanilla LoRA with comparable computational costs. While our approach shows promise for rapid adaptation of 7B-13B parameter models, we observe diminishing benefits on larger models (30B+) and tasks requiring significant behavioral shifts. Theoretical analysis reveals that our Fisher approximations may be overly conservative in early training stages, potentially limiting exploration. Our code and models are made available at [URL withheld for review].",
    "id": 625
  },
  {
    "title": "Gradient Surgery Meets Sharpness Minimization: A Simple Trick for Better Generalization in Vision Transformers",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Zhao, S."
    ],
    "abstract": "We propose Sharpness-Aware Gradient Surgery (SAGS), a lightweight modification to gradient descent that combines sharpness minimization with gradient surgery to improve generalization in Vision Transformers. Despite recent advances in training ViTs, we observe that multi-task gradients often interfere destructively and sharpness-based regularization techniques underperform due to the quadratic complexity of Hessian computations. SAGS addresses both issues by (1) selectively dropping gradient components that conflict with sharpness reduction while preserving task-specific information, and (2) using a rank-1 approximation of the Hessian to efficiently compute trust-region steps. On ImageNet-1K, SAGS improves top-1 accuracy by 0.8% over AdamW for ViT-B/16 at half the computational cost of SAM, though gains diminish for larger ViT-L/16 models. Our ablation study reveals the method is most effective when training from scratch on smaller datasets like CIFAR-100. While our theoretical analysis is limited to quadratic objectives, we show empirical benefits on standard benchmarks. Code and pretrained models will be released.",
    "id": 626
  },
  {
    "title": "Memory-Augmented Transformer Networks with Differentiable Caching Heuristics",
    "authors": [
      "Chen, S.",
      "Lakshminarayanan, B.",
      "Kumar, A."
    ],
    "abstract": "We propose an extension to transformer architectures that maintains a persistent memory bank across sequences while learning to decide which memories to keep and which to discard. Our approach introduces a differentiable cache management policy based on an auxiliary reinforcement learning objective that optimizes for both computational efficiency and downstream task performance. Unlike traditional memory-augmented neural networks, our method learns to compress and retrieve memories without requiring explicit supervision of what to remember. We evaluate on synthetic algorithmic tasks requiring long-term dependency modeling, achieving 12% improvement over standard transformers on sequences of length 8K-32K tokens. Additionally, we demonstrate modest gains on language modeling benchmarks (0.5-1.2 perplexity improvements), though our method incurs 2-3\u00d7 training time overhead compared to baselines. While our theoretical analysis provides insights into when learned caching policies outperform hand-designed ones, we acknowledge limitations in scaling to very large models due to memory bandwidth constraints during training.",
    "id": 627
  },
  {
    "title": "Revisiting Gradient Clipping in Private SGD: A Simpler Approach with Marginal Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is a standard component in differentially private stochastic gradient descent (DP-SGD), primarily used to bound per-example sensitivity. We analyze a subtle interaction between clipping and the noise addition process, showing that common clipping thresholds can inadvertently amplify the effective noise-to-signal ratio for certain parameter configurations. Motivated by this observation, we propose Adaptive Clipped DP-SGD (AC-DP), a lightweight modification that adjusts clipping thresholds based on gradient norm statistics collected during a brief burn-in period. Our method achieves comparable privacy-utility tradeoffs to standard DP-SGD across CV and NLP benchmarks, with modest improvements (1-2% accuracy gains) in high-privacy regimes (\u03b5 \u2264 1). While the theoretical improvement is marginal (tight privacy analysis reveals only constant-factor gains), our approach eliminates the need for extensive clipping threshold tuning and provides more stable training dynamics. We validate our method on CIFAR-10 and SST-2, achieving private accuracies of 62.3% and 83.1% respectively at \u03b5=0.5. Our implementation requires only 15 lines of additional code, suggesting practical deployment benefits despite limited theoretical novelty.",
    "id": 628
  },
  {
    "title": "LoRA-Soup: Iterative Merging of Low-Rank Adaptations for Task-Agnostic Transfer Learning",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Hassan, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient method for adapting large language models to downstream tasks. While effective for single-task adaptation, the sequential application of LoRA modules leads to catastrophic forgetting in multi-task scenarios. We propose LoRA-Soup, an iterative weight-averaging approach that merges LoRA modules without requiring task-specific identifiers or joint training. Our method builds on the observation that LoRA updates lie in low-dimensional subspaces with favorable geometric properties. By carefully aligning these subspaces through a novel similarity metric based on principal angles, we achieve stable merging across diverse tasks. Experiments on GLUE and SuperGLUE benchmarks show modest improvements over sequential fine-tuning (average gain of 1.3%), with particular gains in few-shot settings. However, our approach slightly underperforms compared to task-parallel methods. Theoretical analysis reveals that merging success depends heavily on the alignment between LoRA subspaces, which may not hold for dissimilar tasks. While LoRA-Soup offers a practical compromise between efficiency and performance, its benefits are task-dependent and may be limited when task distributions are highly divergent.",
    "id": 629
  },
  {
    "title": "Gradient Surgery for Language Models: Reducing Catastrophic Forgetting with Parameter-Specific Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Continual learning in large language models remains challenging due to catastrophic forgetting when fine-tuning on new tasks. While recent methods focus on architectural modifications or explicit memory systems, we propose a simpler approach that dynamically adjusts learning rates for different parameter groups based on their sensitivity to task-specific gradients. Specifically, we compute gradient norms across mini-batches to identify parameters critical for previous tasks, then apply selective learning rate decay to maintain performance. Our method requires only minimal hyperparameter tuning and no additional memory beyond standard training. Experiments on GLUE tasks show a 12% improvement in average retention compared to standard fine-tuning when learning 5 sequential tasks, though performance lags behind more sophisticated continual learning baselines. Importantly, our approach maintains inference-time efficiency and can be integrated with existing pre-trained architectures without modification. While our gains are modest, the simplicity and practical benefits suggest value for resource-constrained deployments. Code and trained models will be released upon publication.",
    "id": 630
  },
  {
    "title": "LoRA\u00b2: Recursive Low-Rank Adaptation for Parameter-Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a standard approach for parameter-efficient fine-tuning, but its effectiveness saturates as model sizes grow. We propose LoRA\u00b2, a recursive application of low-rank decomposition that decomposes the already low-rank adaptation matrices into further low-rank components. Our method introduces a hierarchical structure with learnable compression ratios at each level, yielding up to 3.2\u00d7 parameter reduction compared to standard LoRA while maintaining comparable performance on downstream tasks. Through extensive experiments on the GLUE benchmark and instruction-tuning datasets (Alpaca, Vicuna), we demonstrate that LoRA\u00b2 achieves within 1-2% of full fine-tuning accuracy, matching LoRA quality while using 68% fewer trainable parameters. However, we observe diminishing returns beyond two recursive levels and increased training instability on smaller models (<1B parameters). Theoretical analysis suggests our approach trades off representational capacity for parameter efficiency, with performance gaps widening on tasks requiring significant distribution shifts. While LoRA\u00b2 shows promise for extreme parameter-constrained scenarios, its benefits are most pronounced on large-scale models (>7B parameters), limiting applicability. Code will be released upon acceptance.",
    "id": 631
  },
  {
    "title": "On the Effectiveness of Momentum Scheduling in Stochastic Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Anderson, D."
    ],
    "abstract": "We revisit the problem of scheduling momentum parameters in stochastic gradient methods for non-convex optimization. While adaptive learning rate methods have received considerable attention, the role of momentum scheduling remains poorly understood. We propose a family of time-dependent momentum schedules derived from a continuous-time analysis of the heavy-ball differential equation. Our schedules adapt the momentum parameter based on gradient variance estimates, attempting to balance acceleration with stability. Theoretically, we establish convergence rates matching standard SGD for smooth non-convex functions, with improved constants under the Polyak-\u0141ojasiewicz condition. Empirically, we evaluate our schedules on ResNet training for CIFAR-10 and GPT-2 fine-tuning on OpenWebText. We observe modest improvements over fixed momentum (0.2-0.8% absolute gains) at the cost of an additional hyperparameter to tune. While the gains are not consistently significant across tasks, ablation studies reveal the schedules perform best when gradient noise is high. Our results suggest momentum scheduling may be beneficial in specific regimes, but the practical impact is limited compared to learning rate tuning. We provide implementation code at [anonymized.github.io/momentum-scheduler].",
    "id": 632
  },
  {
    "title": "Improving Transformer Efficiency through Iterative Token Dropping with Learnable Retention Scores",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models suffer from quadratic complexity in sequence length, limiting their applicability to long contexts. While previous work has explored static token dropping strategies based on attention scores or heuristics, these often degrade quality on downstream tasks. We propose ITLD, a method that learns adaptive token retention scores through an auxiliary prediction task trained jointly with the main objective. During inference, tokens are iteratively dropped based on these learned scores, achieving up to 2.8x speedup on sequences of length 4096. We evaluate ITLD on language modeling and downstream classification tasks, showing 2-7% relative perplexity improvements over uniform dropping baselines. However, we observe significant performance degradation on tasks requiring fine-grained reasoning over long contexts. Our method introduces 0.5% additional parameters and minimal training overhead, making it practical for existing architectures. While ITLD offers clear computational benefits for certain applications, current limitations in task-specific performance suggest careful evaluation is needed before broad deployment.",
    "id": 633
  },
  {
    "title": "LoRA-MoE: Parameter-Efficient Mixture of Experts with Low-Rank Adaptation",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA-MoE, a method that combines Low-Rank Adaptation (LoRA) with Sparse Mixture of Experts (MoE) to achieve efficient fine-tuning of large language models. While LoRA reduces trainable parameters by learning low-rank updates, and MoE increases capacity through sparse expert networks, their integration remains underexplored. Our approach uses separate LoRA adapters for each expert, allowing fine-grained specialization while maintaining parameter efficiency. We introduce a lightweight routing mechanism that operates on frozen model representations, avoiding the need to update the entire model. Experiments on GLUE and SuperGLUE benchmarks with T5-Large (770M parameters) show 2.1% average improvement over standard LoRA while using only 15% additional trainable parameters. However, we observe diminishing returns on smaller models and tasks with limited domain shift. Ablation studies reveal that expert diversity plateaus beyond 4-6 experts, suggesting practical limitations in our current formulation. While our method provides a simple extension to existing LoRA implementations, we acknowledge that the improvements are incremental and task-dependent. The approach may be particularly useful for practitioners needing modest performance gains without full model fine-tuning, though it does not match the gains achieved by full MoE pre-training.",
    "id": 634
  },
  {
    "title": "Mixture of Sparse Experts: A Simple Baseline for Task-Specific Fine-Tuning",
    "authors": [
      "Lee, S.",
      "Chen, J.",
      "Rodriguez, A."
    ],
    "abstract": "We investigate whether sparse mixture-of-experts (MoE) architectures can match the performance of fully fine-tuned models while keeping most parameters frozen. Our approach trains a small router network to selectively activate expert sub-networks from a large pretrained model, with only 0.5% of parameters updated per task. While existing parameter-efficient tuning methods like adapters and LoRA modify model weights or introduce new layers, we instead learn to dynamically route through frozen pretrained experts. Experiments on GLUE and SuperGLUE benchmarks show our method achieves 97.4% of full fine-tuning performance while updating 8x fewer parameters than LoRA. However, we observe significant variance across tasks (ranging from 88% to 102% of standard fine-tuning), with particularly strong degradation on natural language inference tasks. Ablations reveal that both the gating mechanism and expert diversity are crucial, but scaling to larger models shows diminishing returns. Our results suggest MoE-based fine-tuning can serve as a viable baseline for resource-constrained deployment, though careful task selection and larger expert pools may be necessary for consistent improvements.",
    "id": 635
  },
  {
    "title": "Gradient Amplification Works: A Simple Rescaling Trick for Training Transformers",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Thompson, K."
    ],
    "abstract": "Training large transformer models often suffers from unstable gradients and slow convergence, particularly in early training stages. We propose \u03b1-Scale, a simple yet effective technique that multiplies gradients by a learnable scalar during backpropagation. While gradient clipping and normalization techniques are standard practice, they either discard information or make strong assumptions about gradient distributions. Our method instead learns to rescale gradients through a simple linear transformation, enabling stable training without architectural modifications. We demonstrate \u03b1-Scale improves convergence on 3 NLP benchmarks (GLUE, SuperGLUE, and WikiText-103) using both BERT-base and T5-small architectures, achieving 2-4% average improvements in downstream tasks. We also show modest improvements in training stability, reducing the variance of final performance by 15-20% across random seeds. However, our gains appear limited to smaller-scale settings (models \u2264 250M parameters) and we observe diminishing benefits as model size increases. While \u03b1-Scale introduces minimal computational overhead, its theoretical properties remain unclear and our experiments suggest it may not address fundamental optimization challenges in very large models. Code and trained models will be released upon acceptance.",
    "id": 636
  },
  {
    "title": "LoRA-V: Parameter-Efficient Fine-Tuning with Variance-Adaptive Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers may be suboptimal. We propose LoRA-V, a variance-adaptive variant that dynamically adjusts the rank of LoRA modules based on gradient variance observed during training. Our method uses an efficient thresholding scheme to increase the rank for layers with high gradient variance while pruning low-variance adapters. We evaluate LoRA-V on instruction tuning tasks using Llama-2 models across three domains: mathematical reasoning, code generation, and dialogue. Results show modest improvements over standard LoRA (average +1.2% accuracy) while using 15-25% fewer trainable parameters. However, we find that performance gains diminish on larger models (70B+) and tasks with abundant training data. Theoretical analysis reveals that LoRA-V's effectiveness depends on the spectral gap of the original weight matrices, suggesting limited applicability to pre-trained models with certain spectral properties. While LoRA-V provides a lightweight alternative to manual rank tuning, its benefits are most pronounced in resource-constrained scenarios with smaller models.",
    "id": 637
  },
  {
    "title": "LoRA-Plus: An Empirical Study of Low-Rank Adaptation with Normalized Gradients",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular method for parameter-efficient fine-tuning of large language models, but its training dynamics remain poorly understood. We propose LoRA-Plus, a simple modification that applies layer-wise gradient normalization to LoRA adapters during training. Through experiments on GLUE and SuperGLUE benchmarks across three model sizes (350M, 1.3B, 7B parameters), we find that LoRA-Plus achieves comparable accuracy to standard LoRA while reducing training instability, as measured by gradient norm variance. Specifically, LoRA-Plus improves convergence by 12-18% on 4 out of 8 tasks when training with limited data (\u22641k examples). However, in high-data regimes, both methods converge to similar performance. Our theoretical analysis suggests the improvement stems from better conditioning of the effective optimization landscape. While these results are encouraging, we acknowledge that the gains are modest and task-dependent. Our experiments are conducted with fixed hyperparameters across tasks, leaving open questions about further optimization. Code and pre-trained adapters will be released upon acceptance.",
    "id": 638
  },
  {
    "title": "Lookahead Batch Normalization: An Empirical Study of Batch Statistics in Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, S."
    ],
    "abstract": "Batch Normalization (BN) remains widely used in vision tasks but is often replaced by LayerNorm in transformers due to instability with small batch sizes. We propose Lookahead Batch Normalization (LBN), which computes batch statistics using a moving average of future activations during training. LBN maintains the normalization benefits of BN while stabilizing training for small batches. On ImageNet classification, LBN achieves 76.2% top-1 accuracy with batch size 64, matching standard BN with batch size 256. For transformer language modeling, LBN provides marginal improvements (0.3-0.5 perplexity points) over LayerNorm on Wikitext-103 and C4 datasets. We analyze the effect of the lookahead window size and demonstrate LBN's sensitivity to hyperparameter tuning. While LBN shows promise for certain architectures, our experimental results reveal limited gains on standard NLP benchmarks. Detailed ablation studies suggest the benefits are primarily due to implicit regularization rather than improved optimization dynamics.",
    "id": 639
  },
  {
    "title": "Exploring the Role of Layerwise Learning Rates in Transformer Training",
    "authors": [
      "Chen, L.",
      "Kumar, V.",
      "Ishikawa, S."
    ],
    "abstract": "We investigate whether applying distinct learning rates to different layers of transformers can improve training dynamics and final performance. Building on recent observations that lower layers tend to learn more stable representations than upper layers during early training, we propose Layer-Adaptive Learning Rates (LALR) \u2014 a simple approach that schedules learning rates based on layer depth and training iteration. Through systematic ablations on 6 language modeling datasets, we find that LALR provides modest improvements (1.2-1.7% perplexity reduction) over standard training with similar compute budgets, primarily in low-data regimes (\u2264100M tokens). However, gains diminish or reverse on larger datasets, and our extensive hyperparameter sweeps reveal high sensitivity to learning rate schedules. Our analysis indicates LALR mostly affects the learning dynamics of attention weights, particularly multi-head attention in middle layers. While the technique is straightforward to implement and adds negligible overhead, its benefits appear dataset and architecture dependent, suggesting limited general applicability. Code: anonymous link.",
    "id": 640
  },
  {
    "title": "LoRA-X: Cross-Layer Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Johnson, M.K.",
      "Rodriguez, A."
    ],
    "abstract": "We propose LoRA-X, an extension of Low-Rank Adaptation (LoRA) that enables sharing low-rank matrices across transformer layers to reduce memory overhead during fine-tuning. While standard LoRA introduces independent low-rank matrices for each layer, LoRA-X enforces approximate weight sharing by decomposing layer-wise adaptations into a small set of global basis matrices and layer-specific combination coefficients. On GLUE benchmark tasks, LoRA-X achieves 91.2% of full fine-tuning performance while reducing trainable parameters by 73% compared to standard LoRA, though this comes at the cost of 2-3% absolute accuracy drops on smaller datasets. Our theoretical analysis suggests the rank-sharing assumption holds when the Hessian spectrum exhibits low effective rank across layers. However, ablation studies reveal sensitivity to the number of shared bases and degradation on out-of-domain tasks. LoRA-X provides a practical trade-off between efficiency and performance for resource-constrained deployment scenarios, though we acknowledge limitations in tasks requiring task-specific layer representations.",
    "id": 641
  },
  {
    "title": "Gradient Surgery with Adaptive Memory Replay for Mitigating Catastrophic Forgetting in Continual Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J.",
      "Nguyen, T."
    ],
    "abstract": "We propose GSAMR, a simple yet effective approach to reduce catastrophic forgetting in continual learning by combining gradient surgery with an adaptive memory replay mechanism. Our method maintains a small episodic memory buffer and uses gradient direction analysis to determine which past examples to replay when learning new tasks. Specifically, we compute gradient conflicts between current task updates and memory gradients, selectively replaying examples that maximally reduce interference. The replay frequency adapts dynamically based on the measured gradient alignment between tasks. On standard continual learning benchmarks including Split-CIFAR-100 and incremental ImageNet, GSAMR achieves 2-3% higher average accuracy than baseline replay methods with 50% less memory usage. While our method shows consistent improvements over rehearsal baselines, the gains are modest compared to recent parameter-isolation approaches, suggesting gradient surgery alone may be insufficient for complex continual learning scenarios. We analyze the sensitivity of key hyperparameters and provide theoretical insights connecting gradient alignment to forgetting bounds.",
    "id": 642
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum-Adjusted Thresholds for Transformer Training",
    "authors": [
      "Liu, K.",
      "Thompson, S.",
      "Garcia, M."
    ],
    "abstract": "We propose MAGC (Momentum-Adjusted Gradient Clipping), a simple modification to standard gradient clipping that dynamically adjusts clipping thresholds based on the momentum history of gradient norms. Our method addresses the observation that fixed clipping thresholds in transformer training can lead to suboptimal convergence when gradient distributions shift during training, particularly in the early phases. MAGC computes adaptive thresholds using an exponential moving average of gradient norms with a momentum-scaled adjustment factor. We evaluate MAGC on language modeling tasks with GPT-2 architectures (125M-1.5B parameters), achieving 2-5% perplexity improvements over baseline clipping on Wikitext-103. While the improvements are modest, our method reduces the need for extensive hyperparameter tuning across model scales. Theoretical analysis reveals that MAGC converges under standard assumptions, though the rate depends on problem-specific constants that are challenging to bound tightly. Our implementation requires only 3 lines of code change to existing optimizers, suggesting practical adoption potential. However, ablation studies indicate that 60% of the gains can be achieved through simpler learning rate schedule tuning, raising questions about the fundamental contribution. Code and experiments are available at [URL omitted].",
    "id": 643
  },
  {
    "title": "Gradient Surgery Revisited: Why Existing Methods Overfit in Adaptive Federated Optimization",
    "authors": [
      "Chen, Z.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Federated Learning (FL) faces the challenge of client drift when local objectives diverge from the global objective. Recent gradient surgery methods attempt to address this by projecting conflicting gradients, but we show these approaches systematically overfit to local client distributions. Through theoretical analysis of the gradient projection geometry, we prove that existing methods implicitly amplify gradient directions aligned with local data manifolds, leading to worse generalization. We propose Federated Gradient Decorrelation (FGD), a simple modification that adds controlled noise to break this alignment. While FGD achieves competitive accuracy on standard benchmarks (82.3% on CIFAR-10 with 100 clients, vs 81.1% for FedAvg), we find the improvement diminishes as communication rounds increase. Our theoretical bounds suggest the method's benefit is fundamentally limited by client heterogeneity levels. Experiments on additional datasets confirm this limitation, with mixed results across tasks. Our analysis reveals fundamental tensions between gradient alignment and generalization in federated settings that current methods fail to resolve.",
    "id": 644
  },
  {
    "title": "Improving Transformer Training with Layer-wise Learning Rate Warmup",
    "authors": [
      "Kumar, V.",
      "Chen, L.",
      "Anderson, J."
    ],
    "abstract": "We propose Layer-wise Learning Rate Warmup (LLRW), a simple modification to the training procedure of Transformer architectures that applies distinct learning rate schedules to different layers. Unlike standard warmup, our method gradually increases learning rates for deeper layers while keeping earlier layers relatively stable. This approach is motivated by the observation that earlier layers converge faster and may benefit from slower updates to maintain representation quality. We implement LLRW by introducing a linear scaling factor based on layer depth and demonstrate improvements on machine translation benchmarks. Experiments on WMT'14 English-German and IWSLT'14 German-English tasks show 0.4-0.7 BLEU point improvements over standard warmup schedules, with particularly strong gains on smaller models (\u2264100M parameters). While our results are consistent across multiple runs and hyperparameter settings, the improvement magnitude varies substantially across tasks and disappears for very large models (>1B parameters). Our ablations suggest benefits primarily stem from better early-layer regularization rather than accelerated convergence. We provide implementation details and hyperparameter recommendations, though further investigation is needed to understand why larger models seem insensitive to these layer-specific schedules.",
    "id": 645
  },
  {
    "title": "Batch Normalization is All You Need: One-layer Networks with Learnable Statistics",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Martinez, C."
    ],
    "abstract": "We investigate whether the success of deep networks emerges primarily from hierarchical feature learning or from sophisticated normalization and optimization techniques. Surprisingly, we find that a single fully-connected layer equipped with batch normalization and learnable affine transformations achieves competitive performance across several standard benchmarks, including 94.2% accuracy on CIFAR-10 and 78.1% on ImageNet. Our approach, termed \"Statistical Learning Networks\" (SLN), replaces multi-layer architectures with a single layer whose parameters adapt the batch statistics during training. We provide empirical evidence that SLN captures dataset-specific patterns through learned moments rather than hierarchical representations. While our results challenge conventional wisdom about depth in neural networks, we acknowledge significant limitations: SLN underperforms on out-of-distribution data and tasks requiring compositional reasoning. Theoretical analysis reveals that SLN's expressivity is fundamentally limited compared to deep architectures, suggesting current benchmarks may overemphasize distributional alignment over structured representations. Code and pre-trained models are available at AnonymousLink.",
    "id": 647
  },
  {
    "title": "Gradient Descent with Momentum Revisited: A Quasi-Hyperbolic Approach to Adaptive Step Sizes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Quasi-Hyperbolic Momentum (QHM-ADA), a variant of SGD with momentum that adaptively scales step sizes using second-order information approximated through gradient curvature. While momentum methods traditionally rely on fixed or schedule-based learning rates, our approach computes per-parameter scaling factors using exponential moving averages of gradient inner products, yielding connections to both quasi-Newton methods and adaptive optimizers. We establish O(1/\u221aT) convergence for non-convex smooth objectives under standard assumptions. Experiments on ResNet-50 and Vision Transformer training show 2-4% improvement over AdamW on ImageNet, though gains vanish on language modeling tasks. Our method introduces minimal computational overhead (\u22486% increase in training time) and requires tuning only two hyperparameters beyond standard momentum settings. However, we observe sensitivity to initialization when batch sizes exceed 128. While QHM-ADA provides modest improvements in specific regimes, our analysis suggests the benefits are largely attributable to implicit learning rate warm-up rather than the quasi-hyperbolic formulation itself.",
    "id": 648
  },
  {
    "title": "LoRA-Net: Learning to Compose Low-Rank Adaptations for Efficient Few-Shot Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have shown promise for adapting large language models with minimal computational overhead. However, these methods typically require task-specific training from scratch, limiting their effectiveness in few-shot scenarios. We propose LoRA-Net, a meta-learning approach that learns to generate LoRA matrices by composing a small set of pretrained low-rank adaptations through learned combination weights. Our method trains a lightweight neural network that predicts how to blend existing LoRA matrices for new tasks using only a handful of examples. Through experiments on 12 diverse NLP benchmarks with 4-32 training examples, LoRA-Net achieves 4-12% improvements over standard LoRA fine-tuning while using 80% fewer trainable parameters. While our approach shows consistent gains in few-shot settings, we observe performance saturation on tasks requiring novel domain knowledge not captured by the pretrained LoRA bank. Our analysis reveals that LoRA-Net's effectiveness correlates strongly with the similarity between training tasks and the composition of the LoRA bank, suggesting limitations in handling fundamentally new task distributions.",
    "id": 649
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Dynamic Parameter Pruning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient method for fine-tuning large language models, but its fixed rank structure often leads to over-parameterization across downstream tasks. We propose LoRA-Drop, a simple yet effective technique that adaptively prunes LoRA modules during fine-tuning based on gradient-based importance scores. Our method progressively reduces the rank of each LoRA module by eliminating the least important components, achieving up to 60% parameter reduction with minimal performance degradation. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop matches full LoRA performance while using 25-40% fewer parameters. However, we observe that aggressive pruning can hurt performance on low-resource tasks, suggesting the need for task-specific hyperparameter tuning. While our approach provides practical efficiency gains over vanilla LoRA, it primarily refines existing techniques rather than addressing fundamental limitations of low-rank adaptation. Code and models are available at anonymous.github.io/lora-drop.",
    "id": 650
  },
  {
    "title": "Rethinking Scheduled Sampling with Energy-Based Sequence Models",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "While scheduled sampling has been widely adopted in autoregressive sequence modeling, its theoretical foundations remain unclear, particularly when applied to modern energy-based models. We present a theoretical analysis showing that scheduled sampling can be interpreted as a variational approximation to entropy regularized reinforcement learning, providing principled justification for common annealing schedules. However, our experiments reveal a surprising limitation: the benefits of scheduled sampling diminish dramatically when the underlying energy model achieves test-time log-likelihoods within 0.1 nats of the true distribution. To address this, we propose Adaptive Scheduled Sampling (ASS), which dynamically adjusts the sampling probability based on the model's current uncertainty. Our method achieves modest but consistent improvements on standard benchmarks: 0.5 BLEU improvement on IWSLT14 DE-EN translation and 2% accuracy gain on Penn Treebank language modeling compared to standard scheduled sampling. While our theoretical contribution clarifies the relationship between scheduled sampling and reinforcement learning, our empirical gains are incremental and may not justify the increased complexity for practitioners.",
    "id": 651
  },
  {
    "title": "Lookahead Gradient Descent: Trading Extra Forward Passes for Faster Convergence",
    "authors": [
      "Chen, J.",
      "Liu, Y.",
      "Krishnan, S."
    ],
    "abstract": "We propose Lookahead Gradient Descent (LGD), a simple modification to standard gradient descent that performs multiple forward passes before each parameter update. Our method maintains the same computational cost per epoch as SGD when using delayed parameter updates, but achieves faster empirical convergence in terms of wall-clock time. LGD works by accumulating gradients over m consecutive mini-batches before applying the update, effectively performing a local search in parameter space. We prove convergence under standard smoothness assumptions and show that the method reduces to gradient descent as m\u21921. Experiments on ResNet-18/50 for CIFAR-10/100 demonstrate 10-20% speedup over tuned SGD baselines, with particularly strong gains on noisy datasets. While the theoretical improvement over standard SGD is modest (constant factors), our ablations suggest the benefit primarily comes from reduced variance during early training. However, LGD shows diminishing returns on large-batch training and can underperform SGD with aggressive learning rates. Our work suggests that trading computation within epochs for fewer epochs can be effective for limited compute budgets, though the gains are task-dependent.",
    "id": 652
  },
  {
    "title": "LoRA-Drop: Adapting Low-Rank Adaptation via Dynamic Parameter Pruning for Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers may be suboptimal for downstream tasks. We propose LoRA-Drop, a simple yet effective approach that dynamically prunes LoRA parameters during fine-tuning based on gradient-based importance scores. Our method maintains the training-time efficiency of standard LoRA while achieving additional parameter reduction by up to 40% across GLUE tasks without significant performance degradation. We validate LoRA-Drop on RoBERTa-base and T5-base architectures, demonstrating comparable or marginally improved performance to full LoRA while using fewer parameters. However, our experiments reveal that the benefits of dynamic pruning diminish for larger models (\u22653B parameters), suggesting fundamental limitations in rank allocation heuristics. While LoRA-Drop provides practical improvements for resource-constrained deployments, our theoretical analysis indicates that the pruning strategy may be overly conservative for more complex adaptation scenarios. Code is available at anonymous-github.com/lora-drop.",
    "id": 653
  },
  {
    "title": "Improving Transformer Training with Iterative Knowledge Distillation from Smaller Teachers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a staged knowledge distillation approach for training large transformers more efficiently by progressively transferring knowledge from smaller teacher models during training. Rather than pre-training a large model from scratch, we initialize it with a compressed version of a smaller teacher, then iteratively expand the architecture while distilling from increasingly larger teachers. This method achieves 1.4\u00d7 faster convergence compared to standard training on C4 and GLUE benchmarks, while maintaining competitive downstream performance. Our experiments across various model sizes (BERT-base to BERT-large scale) show consistent improvements in training efficiency. However, we observe the approach offers diminishing returns as model size increases, with the largest configurations showing minimal gains. While our method provides practical benefits for moderately-sized models, we acknowledge the technique's limitations for scaling to very large architectures and its reduced effectiveness when pre-trained checkpoints are available. Our findings suggest that staged knowledge distillation is most valuable in compute-constrained scenarios where full pre-training would be prohibitively expensive.",
    "id": 654
  },
  {
    "title": "Learning with Logarithmic Regret under Generalized Smoothness Conditions",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "We study online convex optimization under a generalized smoothness condition that interpolates between Lipschitz and smooth settings. While existing algorithms achieve O(\u221aT) regret for Lipschitz functions and O(log T) for smooth functions, the optimal regret under intermediate conditions remains poorly understood. We propose a parameter-free algorithm that automatically adapts to the unknown smoothness level without prior knowledge of problem parameters. Our method achieves regret bounds of O(log T) when functions are sufficiently smooth, gracefully degrading to O(\u221aT) under the Lipschitz limit. The algorithm combines optimistic gradient descent with an adaptive step-size schedule based on gradient variations. Through experiments on logistic regression and online matrix completion, we demonstrate improved empirical performance over baselines, particularly in moderately smooth regimes. While our theoretical analysis contains a gap in the worst-case analysis and the assumptions may be too restrictive for some applications, this work provides a step toward adaptive optimization under non-standard smoothness conditions.",
    "id": 656
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is More",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Brown, A."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradDrop show promise, they lack theoretical justification and can discard useful information. We propose Selective Gradient Alignment (SGA), a simpler approach that keeps only the top-k gradient components with highest cosine similarity during each optimization step. Through experiments on three vision-language benchmarks, we show that SGA matches or exceeds more complex methods while requiring 35% less computation. However, we find that SGA's effectiveness is highly sensitive to the choice of k and degrades with more than 4 tasks. Our theoretical analysis reveals that SGA can be viewed as an approximate solution to a relaxed optimization problem, but this connection is weak and fails to explain empirical observations. While the method achieves moderate gains on standard benchmarks, it struggles with imbalanced task weights and loses effectiveness when tasks have dissimilar gradient magnitudes. These results suggest that while gradient surgery can help multi-task optimization, more fundamental approaches may be needed to address task interference.",
    "id": 657
  },
  {
    "title": "Gradient Norm Clipping with Adaptive Threshold Schedules Improves Transformer Training Stability",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose a simple modification to gradient norm clipping in transformer training that uses instance-adaptive thresholds based on gradient history statistics. Our method computes a running exponential average of gradient norms for each parameter group and sets clipping thresholds as a function of these statistics. On Wikitext-103 and IMDB sentiment classification, we observe 2-3% improvement in final validation metrics compared to fixed threshold clipping when training large transformers (350M-1.3B parameters). While existing adaptive optimizers like AdamW handle gradient scaling, our approach specifically addresses the exploding gradient problem in attention layers before optimizer normalization. Experiments with warm-up schedules show our method reduces training variance across 5 seeds (\u03c3=0.8 vs \u03c3=1.4 for baselines), though gains diminish with careful hyperparameter tuning. The technique introduces minimal computational overhead (<0.5% increase in training time) and can be combined with existing optimizers. We provide theoretical intuition connecting our method to trust-region optimization in non-convex settings, though our analysis relies on simplifying assumptions about the loss landscape. Code and pretrained models will be released upon acceptance.",
    "id": 658
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning Through Selective Gradient Dropping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that degrade performance across tasks. While recent gradient surgery methods like PCGrad and GradNorm address this by projecting conflicting gradients, they require careful tuning of projection thresholds and fail to leverage historical gradient information. We propose Gradient Surgery with Adaptive Memory (GSAM), a simple yet effective approach that maintains an adaptive memory bank of recent gradients to dynamically adjust surgery thresholds. The key insight is that gradient conflicts exhibit temporal patterns that can be exploited to reduce surgical interventions. Our method introduces a time-decayed memory mechanism that computes task-specific surgery probabilities based on historical gradient alignment. On standard multi-task benchmarks including NYUv2 and Cityscapes, GSAM achieves modest but consistent improvements over PCGrad (0.8-1.2% mIoU) with 15% fewer projection operations. While our method is theoretically justified and empirically validated, we acknowledge significant computational overhead (1.3x training time) and limited generalization to extreme task imbalance scenarios. The simplicity of our approach makes it a practical plug-and-play improvement over existing gradient surgery methods, though we recognize the gains may be insufficient for applications requiring aggressive gradient surgery.",
    "id": 659
  },
  {
    "title": "Adaptive Learning Rates via Curvature-Aware Gradient Clipping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Curvature-Aware Gradient Clipping (CAGC), a novel approach for adaptive learning rate scheduling that leverages local curvature estimates to dynamically adjust gradient norms. While existing clipping methods rely on fixed thresholds or simple heuristics, CAGC computes per-layer curvature via efficient Hessian-vector products and scales gradients inversely proportional to their local Lipschitz constants. Our method requires minimal additional computation and can be integrated into standard deep learning frameworks with less than 20 lines of code. We evaluate CAGC on image classification tasks using ResNet-50 on ImageNet and CIFAR-10, as well as language modeling with Transformer-XL on WikiText-103. Results show 2-4% improvements in final accuracy over baseline AdamW with gradient clipping, while reducing training variance across 3 random seeds. However, gains diminish on larger models (GPT-2 1.5B) and longer training schedules, suggesting the method's applicability may be limited to medium-scale settings. Theoretical analysis establishes convergence guarantees for convex objectives, though extending these results to neural networks remains an open challenge. Code will be released upon acceptance.",
    "id": 660
  },
  {
    "title": "Improving Transformer Training with Cyclical Learning Rates and Gradient Clipping Thresholds",
    "authors": [
      "Chen, J.",
      "Liu, S.",
      "Mendoza, C.",
      "Thompson, K."
    ],
    "abstract": "Training large transformer models remains computationally expensive, often requiring extensive hyperparameter tuning. We propose a simple yet effective combination of cyclical learning rate schedules with adaptive gradient clipping thresholds to accelerate convergence and improve final performance. Our method automatically adjusts clipping thresholds based on the current learning rate and gradient norms, reducing the need for manual tuning. We evaluate our approach on language modeling tasks using GPT-2 architectures (125M-775M parameters) across three datasets: WikiText-103, OpenWebText, and a proprietary code corpus. Results show 8-15% faster convergence in wall-clock time compared to standard cosine decay schedules, with marginal improvements in perplexity (2-3% better on average). However, performance gains diminish at larger scales (1B+ parameters), suggesting our method's effectiveness is scale-dependent. While our approach is easy to implement and achieves consistent improvements in modest compute regimes, the theoretical underpinnings remain unclear, and benefits may not justify the additional complexity for practitioners with sufficient resources. Code is available at anonymous-url.github.io/ctransform.",
    "id": 661
  },
  {
    "title": "LoRA-VQ: Low-Rank Adaptation with Vector Quantization for Memory-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Krishnan, A.",
      "Liu, Y."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large language models while preserving computational resources. We propose LoRA-VQ, which combines Low-Rank Adaptation (LoRA) with learned vector quantization to further reduce memory overhead during fine-tuning. Our method quantizes the low-rank matrices into discrete codebooks, achieving up to 3.2\u00d7 memory reduction compared to standard LoRA with minimal performance degradation on GLUE tasks. Theoretical analysis shows our quantization strategy maintains gradient flow through straight-through estimators, though convergence guarantees remain limited to simplified settings. Experiments on RoBERTa-base and T5-small across text classification, summarization, and question-answering benchmarks demonstrate competitive performance to LoRA (within 1.5% F1 score) while using 32\u00d7 fewer unique parameter values. However, we observe instability when scaling to larger models (>10B parameters) and tasks requiring complex reasoning. Ablation studies reveal performance drops with aggressive quantization levels (>4-bit) and highlight the importance of learnable temperature scheduling during training. While LoRA-VQ offers practical memory benefits for resource-constrained settings, the trade-off between compression and task performance suggests careful calibration is required for deployment.",
    "id": 662
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient method for fine-tuning large language models, but selecting appropriate rank values remains largely heuristic. We propose LoRA-Drop, a simple extension that dynamically adjusts rank during training through gradient-based pruning. Our method begins with a conservatively high rank and gradually drops the least significant singular components based on their contribution to the loss. While this approach reduces parameter counts by 15-40% compared to fixed-rank LoRA on standard benchmarks like GLUE and SuperGLUE, the gains over carefully tuned baselines remain modest (0.3-0.8 average F1 improvement). We provide theoretical analysis showing that our dropping criterion is equivalent to a form of regularized optimization, though our bounds are looser than previous work on static rank selection. Experiments across three model sizes (350M-7B parameters) demonstrate consistent but small improvements over vanilla LoRA, particularly on tasks with limited training data. However, LoRA-Drop adds computational overhead (10-15% training time increase) and hyperparameter sensitivity that may limit practical utility. Code and pretrained adapters will be released upon acceptance.",
    "id": 663
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique for Improving Generalization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Liu, Y."
    ],
    "abstract": "We propose Gradient Dropout (GradDrop), a regularization technique that randomly sets a subset of gradient components to zero during backpropagation. Unlike traditional dropout which masks activations, GradDrop operates directly on the gradient flow, creating an implicit form of gradient noise that encourages exploration of the loss landscape. Our method requires only a single hyperparameter (dropout rate) and adds minimal computational overhead. We provide theoretical analysis showing GradDrop approximates a form of stochastic gradient descent with implicit prior regularization. Experiments on CIFAR-10, CIFAR-100 and ImageNet with ResNet-18/50 architectures show GradDrop achieves modest improvements over standard SGD with dropout (0.5-1.2% accuracy gains). While the improvements are consistent, they are smaller than those achieved by more sophisticated regularizers like mixup or label smoothing. Ablation studies suggest GradDrop works best in combination with standard dropout and weight decay. The technique is particularly effective for medium-sized models but shows diminishing returns for very large networks. Our implementation requires only 3 lines of code in PyTorch. While GradDrop is simple to implement and has some theoretical grounding, its practical impact remains limited compared to established baselines.",
    "id": 664
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Iterative Spectral Regularization",
    "authors": [
      "Chen, L.",
      "Raghavan, P.",
      "Mikhailov, A."
    ],
    "abstract": "We propose an iterative spectral regularization technique for improving the generalization of deep neural networks. Our method applies data-dependent spectral norm constraints during training, where we gradually strengthen regularization based on the network's current generalization gap. The approach builds on PAC-Bayesian bounds by incorporating layer-wise spectral norms as complexity measures. We derive a new generalization bound that scales with the product of spectral norms across layers, achieving a \u221a(L log L/\u03b3) dependence compared to previous O(L\u00b2) bounds, where L is network depth and \u03b3 measures margin. However, we acknowledge that this improvement comes at the cost of introducing additional hyperparameters controlling the strengthening schedule. Experimental evaluation on CIFAR-10 and ImageNet subsets shows modest improvements (2-3% accuracy gains) compared to standard regularization baselines, though results vary significantly across architectures. While our theoretical contribution provides tighter bounds for specific network classes, the practical gains remain limited and may not justify the computational overhead. Our code is available at anonymized-link.",
    "id": 665
  },
  {
    "title": "Revisiting Temperature Scaling for Out-of-Distribution Detection: A Frequency Domain Perspective",
    "authors": [
      "Liu, C.",
      "Kumar, S.",
      "Rodriguez, J."
    ],
    "abstract": "Temperature scaling has emerged as a ubiquitous post-hoc calibration technique for improving out-of-distribution (OOD) detection in neural networks. While effective in practice, the underlying mechanisms remain poorly understood beyond empirical observations. We present a frequency domain analysis that reveals temperature scaling primarily attenuates high-frequency components in feature representations, inadvertently suppressing subtle distributional cues. Building on this insight, we propose Adaptive Frequency Scaling (AFS), which selectively modulates frequency components based on their discriminative power for OOD detection. Our method achieves competitive results on standard benchmarks (CIFAR-10 vs. SVHN, ImageNet vs. iNaturalist), improving AUROC by 2.3% over vanilla temperature scaling while using 50% fewer hyperparameters. However, performance gains diminish on datasets with significant domain shift (ImageNet vs. ObjectNet), suggesting our frequency-based approach may be overly tied to low-level statistics. Theoretical analysis connects our method to kernel alignment in infinite-width networks, though the assumptions required limit practical applicability. While our results provide new insight into temperature scaling's behavior, the incremental improvements and narrow validation scope may restrict immediate impact beyond the calibration community.",
    "id": 666
  },
  {
    "title": "Momentum Improves Sample Complexity for Non-Convex SGD: A Simple Fix for Existing Theory",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Ramos, M."
    ],
    "abstract": "We revisit the theoretical analysis of stochastic gradient descent (SGD) with momentum for non-convex optimization. While the practical success of momentum is well-documented, existing theoretical results suggest minimal benefits over vanilla SGD for finding approximate first-order stationary points. Through a refined analysis incorporating iterate coupling arguments, we show that Polyak's momentum reduces the sample complexity from O(\u03b5^{-4}) to O(\u03b5^{-3.5}) for smooth non-convex objectives, matching the best-known rates for more complex variance-reduced methods. Our analysis reveals that momentum implicitly provides gradient estimation benefits near stationary regions, partially explaining its empirical success. We validate our theoretical findings on standard deep learning benchmarks, demonstrating consistent improvements over vanilla SGD, though the practical gains are often modest compared to these theoretical improvements. Our results suggest that momentum's benefits are most pronounced in specific problem regimes, providing theoretical justification for its widespread use while highlighting the gap between theory and practice.",
    "id": 667
  },
  {
    "title": "Momentum-Scheduled Learning Rates: Bridging the Gap Between Theoretical Guarantees and Practical Performance",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Momentum-Scheduled Learning Rates (MSLR), a simple modification to existing adaptive optimizers that interpolates between worst-case theoretical bounds and empirical performance gains. By scheduling both the momentum coefficient and learning rate as coupled functions of gradient signal-to-noise ratio, MSLR achieves theoretical regret bounds competitive with Adam while recovering SGD-like behavior in late-stage training when gradients are small. Our method requires only two additional hyperparameters with intuitive interpretations. We prove convergence guarantees for convex and non-convex settings, showing that MSLR matches the O(log T/\u221aT) convergence rate of AdaGrad while maintaining the constant-factor improvements of heavy-ball momentum. Empirical evaluation on CIFAR-10, CIFAR-100, and a subset of ImageNet demonstrates 2-3% accuracy improvements over baselines at the cost of 15% increased wall-clock time. However, we observe that benefits diminish as model size increases, with no improvement observed on ViT-L/16. While MSLR provides a principled approach to optimizer design, our theoretical analysis relies on standard bounded gradient assumptions that may not hold in practice, and ablation studies suggest simpler cosine scheduling can achieve similar performance in many settings.",
    "id": 668
  },
  {
    "title": "Look-Ahead Sampling: Improving Transformer Decoding Through Future-Aware Token Selection",
    "authors": [
      "Chen, L.",
      "Ghosh, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer models often exhibit suboptimal decoding behavior due to myopic token selection that fails to account for long-range dependencies. We propose Look-Ahead Sampling (LAS), a simple yet effective decoding strategy that incorporates information from potential future tokens without additional model training. LAS maintains a fixed-size pool of candidate next tokens and evaluates each by computing the likelihood of future tokens k steps ahead using a beam search approximation. We test LAS on machine translation (WMT'14 EN-DE, WMT'19 EN-RU) and dialogue generation (Persona-Chat) tasks using standard pretrained models. Results show modest improvements of 0.2-0.6 BLEU over greedy decoding and top-k sampling baselines, with larger gains observed on longer sequences (>50 tokens). While LAS introduces computational overhead (2-3x decoding time), our ablations reveal the optimal lookahead window k is small (2-3), making the approach practical for deployment. The method generalizes across model sizes (125M-7B parameters) and shows consistent improvements when combined with existing decoding strategies. However, we note LAS provides diminishing returns on tasks with strong supervision and struggles with repetitive outputs. Code and checkpoints will be provided.",
    "id": 669
  },
  {
    "title": "Gradient Compression with Learned Quantization Tables for Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.E.",
      "Kim, J."
    ],
    "abstract": "Gradient compression is essential for communication-efficient federated learning, but existing methods often rely on hand-designed quantization schemes that may not be optimal for specific model architectures or data distributions. We propose Learned Quantization Tables (LQT), a simple yet effective approach that learns per-layer quantization scaling parameters during training using a small held-out validation set. Our method achieves 8-16x compression rates while maintaining comparable accuracy to full-precision training on standard benchmarks. However, we observe that LQT's effectiveness diminishes in highly heterogeneous federated settings where client data distributions diverge significantly. Extensive experiments on CIFAR-10 and EMNIST demonstrate 2-4% accuracy improvements over uniform quantization baselines, though our ablation studies reveal that benefits are primarily attributable to learned scaling rather than the quantization scheme itself. While LQT provides practical improvements for homogeneous federated scenarios, its reliance on validation data and additional hyperparameters may limit applicability in resource-constrained settings. Code will be made available upon publication.",
    "id": 670
  },
  {
    "title": "Improved Neural Network Calibration via Temperature Scaling with Learnable Priors",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Chen, M."
    ],
    "abstract": "We propose Prior-TS, a simple extension of temperature scaling that incorporates learned priors from model parameters to improve uncertainty calibration in neural networks. While temperature scaling is widely adopted due to its simplicity and post-hoc nature, we show via empirical analysis that its calibration performance decreases significantly under distribution shift. Our method learns priors from the pre-trained model's weight distributions and combines them with a learnable temperature parameter through a Bayesian hierarchical framework. Experiments on CIFAR-10/100 and ImageNet demonstrate consistent but modest improvements in Expected Calibration Error (ECE) over standard temperature scaling, achieving 0.5-2.3% better calibration across various architectures. However, we observe limited gains on out-of-distribution data, suggesting the approach primarily enhances in-distribution calibration. Our ablation studies indicate that most benefits stem from the temperature parameter rather than the learned priors, raising questions about the necessity of our more complex formulation. While Prior-TS offers a practical plug-and-play enhancement to existing calibration methods, we acknowledge that the improvements may be insufficient to justify increased computational overhead in resource-constrained settings.",
    "id": 671
  },
  {
    "title": "Task-Aware In-Memory Compression for Low-Cost Transformers via Rank-Based Weight Dropping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models achieve state-of-the-art performance across many tasks, but their deployment on resource-constrained devices remains challenging due to memory requirements. We propose a simple yet effective compression technique that dynamically selects which weight matrices to compress based on per-layer sensitivity analysis. Our method combines low-rank factorization with magnitude-based pruning, where compression decisions are made at inference time using lightweight heuristics derived from the input task type. Through empirical evaluation on GLUE and CIFAR-100, we demonstrate 2-4\u00d7 memory reduction with less than 3% accuracy degradation compared to full-precision baselines. Notably, our approach runs entirely in memory without requiring specialized hardware or offline calibration. While our compression ratios fall short of optimal channel pruning or knowledge distillation methods, our technique offers practical advantages for edge deployment scenarios where retraining is impossible. Analysis reveals that attention heads and feedforward layers exhibit varying compression sensitivity, suggesting potential for adaptive compression strategies. Code and models will be made publicly available.",
    "id": 672
  },
  {
    "title": "LoRA++: Adaptive Low-Rank Adaptation with Learnable Scaling Factors",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods have become essential for adapting large language models, with LoRA emerging as a popular choice due to its simplicity and effectiveness. However, LoRA uses a fixed scaling hyperparameter that remains constant across all layers and modules. We propose LoRA++, which introduces learnable scaling factors for each low-rank adapter module. Our method adds only 0.3% additional parameters compared to standard LoRA while enabling dynamic scaling during training. Experiments on GLUE and SuperGLUE benchmarks show modest improvements over LoRA (average 0.7% improvement on GLUE, 0.4% on SuperGLUE) across 6 tasks and 3 model sizes (125M to 7B parameters). We demonstrate that learned scaling factors correlate with layer-wise sensitivity, suggesting a form of implicit importance weighting. While LoRA++ provides consistent gains, the improvements are small and may not justify the added complexity for practitioners already satisfied with LoRA. Additionally, our method introduces extra hyperparameters that require careful tuning, potentially offsetting some practical benefits. Code and hyperparameter configurations are available at [anonymous-repository-link].",
    "id": 673
  },
  {
    "title": "Gradient Dropout: Stochastic Annealing for Improved Optimization Trajectories",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Dropout, a simple regularization technique that randomly masks gradients during neural network training to escape sharp local minima. Unlike standard dropout applied to activations, we stochastically drop entries of gradients computed via backpropagation, effectively creating a noisy optimization landscape that becomes smoother as training progresses. We provide theoretical analysis showing that this noise injection is equivalent to adding a data-dependent regularizer, with convergence guarantees for convex objectives. Empirical evaluation on CIFAR-10/100 and ImageNet shows consistent but modest improvements (0.3-0.8% accuracy gains) over baselines when hyperparameters are carefully tuned. While our method rarely performs worse than standard training, the benefits appear limited to specific architectures (ResNets, DenseNets) and are less pronounced for Vision Transformers. Code is available, though reproducibility requires exact hyperparameter matching. Gradient Dropout offers a lightweight training enhancement that may complement other regularization techniques, though it is not a standalone solution for performance breakthroughs.",
    "id": 674
  },
  {
    "title": "Preconditioned Gradient Descent with Adaptive Momentum Estimation via Hessian Diagonal Approximation",
    "authors": [
      "Chen, Y.",
      "Rodriguez, M.",
      "Kim, J.",
      "Singh, A."
    ],
    "abstract": "We propose PHADAM, a preconditioned variant of Adam that incorporates diagonal Hessian information to improve convergence in non-convex optimization. While adaptive methods like Adam show strong empirical performance, their convergence guarantees remain weaker than SGD+momentum in certain regimes. Our approach estimates the Hessian diagonal using a limited-memory scheme that adds minimal computational overhead (\u22645% per step). We theoretically show that PHADAM achieves O(1/T) convergence in the convex setting and exhibits better conditioning than Adam when curvature information is accurate. On ImageNet training with ResNet-50, PHADAM achieves 76.2% top-1 accuracy (vs. 76.0% for Adam) with 8% faster convergence in wall-clock time. However, gains are inconsistent across architectures and datasets: we observe minimal improvement on transformer language modeling tasks. Our analysis reveals the method's sensitivity to the Hessian approximation quality, which degrades for very deep networks. Code is available at [URL].",
    "id": 675
  },
  {
    "title": "Improving Few-Shot Generalization through Task-Agnostic Prompt Alignment",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Park, J."
    ],
    "abstract": "Large language models demonstrate impressive zero-shot capabilities, but their few-shot performance remains highly sensitive to prompt formatting. We propose Task-Agnostic Prompt Alignment (TAPA), a lightweight method that learns to reformat prompts without task-specific supervision. TAPA uses a meta-optimization objective that maximizes consistency of predictions across noisy paraphrases of the same prompt. Experiments on 12 few-shot benchmarks show 2-4% improvements over standard prompting on average, with particular gains on numerical reasoning tasks. However, we observe minimal benefits on classification tasks and negative transfer when prompts differ significantly from training seen during meta-learning. Our analysis reveals TAPA primarily learns to suppress spurious correlations introduced by formatting choices rather than discovering fundamentally better prompting strategies. While TAPA offers a practical improvement for few-shot learning at small computational cost, its limited scope and task-dependent effectiveness suggest the broader challenge of prompt optimization requires more sophisticated solutions.",
    "id": 676
  },
  {
    "title": "Gradient Surgery Meets Sharpness-Aware Minimization: A Memory-Efficient Approach to Multi-Task Learning",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, T."
    ],
    "abstract": "Multi-task learning (MTL) often suffers from conflicting gradients and deteriorating performance due to task interference. While recent gradient surgery methods address this by projecting conflicting gradients, they overlook the sharpness of the loss landscape. We propose SAM-GradDrop, which combines Sharpness-Aware Minimization (SAM) with gradient surgery to simultaneously reduce gradient conflicts and seek flatter minima. Our key insight is that projecting gradients onto the shared subspace via a modified Frank-Wolfe algorithm before applying SAM achieves superior performance. On 3 standard multi-task vision benchmarks, we improve average accuracy by 2.3-4.1% over strong baselines while using 30% less memory than naive implementations. However, we observe diminishing returns on tasks with highly correlated objectives and find that our method requires careful tuning of the sharpness parameter $\\rho$. Theoretical analysis reveals that our approach can be viewed as minimizing an upper bound on task-specific generalization error, though we note our guarantees hold only under the restrictive assumption of Lipschitz smooth objectives. Our experiments use standard ResNet-50 backbones and require 2x training time compared to vanilla MTL, limiting deployment practicality in resource-constrained settings.",
    "id": 677
  },
  {
    "title": "Gradient Surgery Revisited: Multi-Task Learning with Task-Aware Early Stopping",
    "authors": [
      "Dubois, M.",
      "Liu, S.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients across tasks, leading to suboptimal shared representations. While recent gradient surgery techniques like PCGrad and GradNorm address this through projection-based updates, we observe these methods tend to overfit on auxiliary tasks while underfitting on primary tasks. We propose Task-Aware Early Stopping (TAES-O), a simple modification that adaptively stops gradient updates from auxiliary tasks when they negatively impact primary task performance. Our method requires only modifying the training loop without architectural changes. Experiments on 8 multi-task datasets show TAES-O improves primary task accuracy by 2.3\u00b11.4% over PCGrad while reducing training time by 17%. However, we find gains diminish when auxiliary tasks are strongly correlated with the primary task (r>0.7). Through ablation studies, we show that tuning just two hyperparameters maintains most benefits across domains. While our approach builds on existing techniques, it provides a practical alternative when computational budgets are constrained. Code will be made available upon publication.",
    "id": 678
  },
  {
    "title": "Gradient Entropy Regularization: A Lightweight Approach to Mitigating Memorization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, D."
    ],
    "abstract": "We propose gradient entropy regularization (GER), a simple regularization technique that encourages neural networks to maintain high entropy in their gradient distributions during training. By penalizing low-entropy gradient patterns, GER implicitly discourages memorization of specific training examples without requiring explicit data augmentation or architectural modifications. Our method adds minimal computational overhead (less than 3% increase in training time) and works as a drop-in replacement for standard regularizers. We evaluate GER on vision and language tasks, demonstrating 2-5% improvements in memorization metrics while maintaining comparable test accuracy on CIFAR-10, CIFAR-100, and SST-2. However, we observe diminishing returns on larger datasets and architectures. Theoretical analysis reveals GER approximately minimizes a bound on memorization capacity, though the connection becomes weaker for very deep networks. While GER shows promise for privacy-sensitive applications, our experiments are limited to medium-scale benchmarks, and we acknowledge potential confounds with existing implicit regularization effects.",
    "id": 679
  },
  {
    "title": "Sharpness-Aware Minimization with Adaptive Gradient Clipping for Improved Generalization in Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We propose SAM-AGC, a simple modification to Sharpness-Aware Minimization (SAM) that incorporates adaptive gradient clipping based on the sharpness of the loss landscape. While SAM has shown promise for improving generalization in small-scale vision tasks, its effectiveness on large language models remains inconsistent. Our key insight is that the aggressive updates in SAM can destabilize training in transformers, particularly when gradients become large. SAM-AGC addresses this by clipping gradients adaptively based on their alignment with the sharpness direction. We evaluate our method on the GLUE benchmark using BERT-base and RoBERTa-base, achieving average improvements of 1.2% over SAM and 2.3% over standard SGD with momentum. Despite these gains, we observe that SAM-AGC's benefits diminish as model size increases\u2014we find no consistent improvements on GPT-2 medium variants. Through extensive ablations, we identify that our method works best on tasks with limited training data and moderate model complexity. While our results are promising, they are limited to encoder-only architectures and standard classification tasks. Our code and trained models are available at [URL anonymized].",
    "id": 680
  },
  {
    "title": "Improved Training of Gaussian Splatting Models via Signed Distance Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "3D Gaussian Splatting has emerged as a promising alternative to neural radiance fields for novel view synthesis, but struggles with artifacts near thin structures and unobserved regions. We propose adding a signed distance function (SDF) regularizer to the standard photometric loss, encouraging Gaussians to align with implicit surfaces. Our method alternates between optimizing Gaussian parameters and fitting an SDF using predicted depths from the current set of Gaussians. On standard benchmarks, this approach reduces floaters by 15% and improves reconstruction quality on thin objects, while introducing minimal computational overhead. However, we find the regularization can overly constrain the optimization, sometimes degrading results in textureless regions. Experiments on synthetic and real datasets demonstrate modest improvements over vanilla Gaussian Splatting (PSNR+0.2 on average), with the greatest gains observed in scenes with well-defined geometry. Limitations include sensitivity to hyperparameter tuning and increased training time. While the contribution is incremental rather than transformative, our approach provides a lightweight extension that practitioners may find useful for improving reconstruction quality in geometrically structured scenes.",
    "id": 681
  },
  {
    "title": "Measuring and Mitigating Mode Collapse in Diffusion Models via Jacobian Spectral Norms",
    "authors": [
      "Kim, S.",
      "Rodriguez, C.",
      "Zhao, L."
    ],
    "abstract": "We propose a simple method for detecting and reducing mode collapse in diffusion models by analyzing the Jacobian of the learned score function. While mode collapse is well-studied in GANs, its manifestation in diffusion models remains poorly understood. Our key insight is that collapsed modes correlate with regions where the Jacobian exhibits large spectral norms, indicating unstable score estimation. We derive theoretical bounds connecting spectral norm magnitudes to diversity metrics, and introduce a lightweight regularization term that penalizes large Jacobians during training. Experimental evaluation on CIFAR-10 and ImageNet shows our method modestly improves diversity scores (FID decreases by 3-8% on average) while maintaining sample quality, but requires careful hyperparameter tuning and adds ~15% computational overhead. We also find that our regularization sometimes oversmooths fine details in complex datasets. While the theoretical analysis is limited to simplified settings and empirical gains are incremental rather than dramatic, our work provides a practical diagnostic tool for practitioners and opens new directions for understanding mode collapse in diffusion-based generative models.",
    "id": 682
  },
  {
    "title": "LoRA-Drop: Efficient Parameter Pruning for Low-Rank Adaptation via Dynamic Mask Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Kumar, V."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a practical solution for efficient fine-tuning of large language models, but still introduces non-negligible memory overhead when scaling to thousands of tasks. We propose LoRA-Drop, a simple method for dynamically pruning LoRA parameters during training without requiring costly hyperparameter search. Our approach learns binary masks over LoRA matrices through a modified straight-through estimator, jointly optimizing task performance and a sparsity regularizer. We extend LoRA-Drop to the federated setting where clients can selectively drop adapters based on local resource constraints. Experiments on GLUE and E2E benchmarks show LoRA-Drop achieves 40-60% parameter reduction with minimal performance degradation (<1.5% accuracy drop) compared to full LoRA. While our method demonstrates practical memory savings on 7B parameter models, we find the benefits diminish for smaller architectures and certain NLP tasks. Theoretical analysis reveals LoRA-Drop implicitly performs low-rank matrix approximation with learned sparsity patterns. Our results suggest parameter pruning in LoRA may face fundamental limitations when adapters are already near minimal capacity.",
    "id": 683
  },
  {
    "title": "On the Effectiveness of Curriculum Learning for Low-Resource Domain Adaptation in Transformer Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We investigate whether principled curriculum learning strategies can improve domain adaptation performance when labeled target data is extremely scarce (\u2264100 samples). While prior work has shown benefits for vision tasks and synthetic NLP benchmarks, systematic evaluation on realistically low-resource settings remains limited. We propose a simple curriculum strategy based on progressively increasing both the domain shift magnitude and example difficulty, evaluated on three text classification benchmarks. Our approach uses off-the-shelf pretrained transformers and requires no additional model parameters. Across five domains and three model scales (BERT-base to DeBERTa-large), we observe consistent but modest improvements (1.2-2.8% accuracy gain) over standard fine-tuning baselines. However, these gains diminish when using modern few-shot adaptation techniques like PET or adapter-based methods, suggesting curriculum learning provides orthogonal rather than complementary benefits. Our extensive ablations reveal that the choice of curriculum pacing schedule is surprisingly critical, with aggressive schedules sometimes degrading performance. While our results establish practical guidelines for practitioners, the theoretical grounding for when and why curriculum learning helps in this regime remains unclear. Code and datasets will be released upon acceptance.",
    "id": 684
  },
  {
    "title": "Gradient Surgery for Stabilizing Adversarial Training in Deep Classifiers",
    "authors": [
      "Liu, K.",
      "Chen, T.",
      "Rodriguez, A."
    ],
    "abstract": "Adversarial training remains unstable for deep networks despite recent advances. We propose Gradient Surgery (GS), a simple but effective method that selectively drops or rescales gradient components during adversarial training. GS monitors gradient variance across mini-batches and applies threshold-based pruning to reduce instability. We evaluate GS on CIFAR-10 and CIFAR-100 with ResNet-18 and WideResNet-34, showing 2-3% robust accuracy improvements over standard adversarial training with minimal computational overhead (\u22645% extra training time). Surprisingly, GS also improves clean accuracy by 1-2% in some settings. While our approach shows promise, theoretical justification remains limited and benefits diminish on larger datasets like ImageNet. Analysis reveals GS primarily affects early training dynamics, suggesting its impact may be replicated through careful hyperparameter tuning. Our code is available at [URL].",
    "id": 685
  },
  {
    "title": "BatchNorm in Disguise: Revisiting Normalization Through the Lens of Parameter Scaling",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Thompson, K."
    ],
    "abstract": "We investigate an overlooked connection between parameter initialization scales and implicit normalization in deep networks. While Batch Normalization (BN) is widely believed to improve optimization through reduced internal covariate shift, we demonstrate that networks trained with specific parameter scaling schemes can achieve similar training dynamics without explicit normalization layers. Our approach, dubbed ScaleNorm, introduces learnable scaling parameters at initialization that preserve the first and second moments of activations throughout training, effectively mimicking BN's stabilizing effects. Through experiments on CIFAR-10/100 and ImageNet, ScaleNorm achieves 91.2% and 70.8% accuracy respectively, comparable to BN baselines (91.5%/71.2%) while reducing memory overhead by 8-12%. However, we observe that performance degrades on deeper architectures (>50 layers) and fails to match BN's robustness to hyperparameter choices. Our theoretical analysis reveals that ScaleNorm approximates BN only under restrictive assumptions about weight orthogonality and learning rates that rarely hold in practice. While ScaleNorm provides a lightweight alternative for specific architectures, it does not fully replace BN's benefits, suggesting that the normalization debate requires more nuanced understanding of training dynamics.",
    "id": 686
  },
  {
    "title": "Gradient Noise Injection as a Lightweight Alternative to Mixup Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We explore whether gradient noise injection can serve as a computationally cheaper substitute for input-level augmentation methods like Mixup. Motivated by recent observations that Mixup primarily acts as a form of regularization rather than data augmentation, we propose injecting scaled Gaussian noise directly into gradients during backpropagation. Our method requires minimal additional computation compared to Mixup's expensive convex combinations. We evaluate our approach on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures. Results show gradient noise achieves comparable performance to Mixup on CIFAR (within 0.5-1.2% accuracy), but underperforms significantly on ImageNet (-2.8% top-1). However, our method provides more consistent training stability and reduces overfitting on smaller datasets. Notably, we discover the noise scale must be precisely calibrated\u2014a hyperparameter that varies substantially across architectures and datasets. While our results validate gradient regularization as a viable technique for limited-resource scenarios, the sensitivity to hyperparameters and ImageNet performance gaps suggest applications may be restricted to smaller-scale settings. Code and trained models are available at [anonymous link].",
    "id": 687
  },
  {
    "title": "Gradient Alignment Regularization: A Simple Trick to Improve Generalization in Neural Networks",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Alignment Regularization (GAR), a lightweight modification to standard training procedures that encourages alignment between the gradients of data points within the same mini-batch. Our method adds a computationally efficient regularization term that penalizes large cosine distances between gradient vectors, hypothesizing that smoother gradient landscapes lead to better generalization. While simple to implement, we demonstrate consistent but modest improvements across three common benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet-1k) using ResNet architectures. Our strongest results show a 1.2% accuracy improvement over standard SGD training on CIFAR-100 with ResNet-50, though gains vary significantly across experimental settings. The method requires only an additional hyperparameter controlling the regularization strength. We provide theoretical motivation connecting gradient alignment to the spectral norm of the Hessian, though our analysis makes restrictive assumptions that may not hold in practice. Ablation studies reveal that GAR's effectiveness varies with batch size and learning rate schedules, suggesting the benefits depend on optimization dynamics rather than fundamental improvements to the loss landscape. While our empirical evaluation covers diverse architectures, we focus primarily on computer vision tasks, leaving broader applicability across domains unexplored. Code is available at anonymous.url.",
    "id": 688
  },
  {
    "title": "Lookahead Sampling: A Memory-Efficient Alternative to Experience Replay for Policy Gradient Methods",
    "authors": [
      "Liu, M.",
      "Johnson, K.",
      "Singh, A."
    ],
    "abstract": "Experience replay is a fundamental technique for stabilizing policy gradient methods, but its memory requirements scale poorly with task complexity. We propose Lookahead Sampling (LAS), a lightweight alternative that maintains only a small set of informative state-action pairs selected online. LAS uses a simple importance-weighting scheme based on TD-error to decide which transitions to keep, eliminating the need for large replay buffers. While this approximation introduces some bias, our theoretical analysis shows the regret is bounded by O(\u221a(H|S||A|/T)) under mild assumptions. On continuous control benchmarks, LAS matches SAC performance on 8/12 tasks while using 10-50x less memory. However, we observe stability issues in high-dimensional environments where the sampling bias becomes significant. LAS provides a practical trade-off between memory efficiency and performance, particularly relevant for resource-constrained applications, though it does not universally improve upon standard replay mechanisms.",
    "id": 689
  },
  {
    "title": "Fixing Momentum Procrastination: A Hybrid Optimizer for When Adam Isn't Enough But Second-Order is Too Much",
    "authors": [
      "Chen, L.",
      "Dubey, A.",
      "Nakamura, S."
    ],
    "abstract": "While Adam and its variants remain the de facto optimizers for training deep networks, they often exhibit slow convergence on ill-conditioned problems. Inspired by the recent trend of optimizer grafting, we propose Momentum Corrected Adaptive Learning (MCAL), a hybrid method that selectively applies momentum correction to Adam updates only when gradient history suggests local conditioning issues. MCAL combines the per-dimension adaptivity of Adam with quasi-second-order information computed via a lightweight diagonal Hessian approximation. We conduct extensive experiments across vision and language tasks, showing 2-7% faster convergence compared to AdamW on ResNet-50 and BERT-base, while maintaining comparable generalization. However, gains diminish on well-tuned baselines and small-scale datasets. Our theoretical analysis proves convergence guarantees under standard assumptions, though our bound is looser than Adam's by a logarithmic factor. Code and hyperparameter sweep results are provided for reproducibility. While not a universal improvement over Adam, MCAL offers practitioners a drop-in alternative when training dynamics suggest conditioning problems.",
    "id": 690
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Lightweight Alternative to Architecture Search",
    "authors": [
      "Chen, J.",
      "Rodriguez, A.",
      "Kim, M."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients across tasks, leading to suboptimal performance. While recent work has proposed sophisticated gradient surgery techniques requiring manual architecture modification, we present a simple plug-and-play method that operates directly on gradients during backpropagation. Our approach, Differentiable Gradient Routing (DGR), learns to reweight per-task gradients using a lightweight neural network conditioned on gradient statistics. Unlike previous methods, DGR requires no task-specific hyperparameter tuning or architectural changes. We evaluate on 5 multi-task vision benchmarks and show 2-5% improvements over standard multi-task baselines while using 50% fewer parameters than recent gradient surgery methods. However, we observe diminishing returns for tasks with highly dissimilar gradient directions and find that our method provides minimal benefits when tasks are inherently compatible. Our results suggest that while gradient reweighting can effectively handle moderate task conflicts, it does not replace careful task selection and architectural choices. Code will be made available upon acceptance.",
    "id": 691
  },
  {
    "title": "Learning with Gradient Descent on Randomly Pruned Networks",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Anderson, J."
    ],
    "abstract": "We study the training dynamics of neural networks when weights are randomly pruned before training begins. While neural network pruning is typically performed after training, we investigate whether meaningful learning can occur when up to 95% of parameters are removed randomly from initialization. We provide theoretical analysis showing that randomly pruned networks can still learn, but with convergence rates that depend on the layer-wise pruning ratios rather than overall sparsity. Our experiments on CIFAR-10 and ImageNet show that moderately pruned networks (50-70% sparse) achieve comparable accuracy to dense networks, but performance degrades significantly beyond 90% sparsity. We introduce a simple scaling rule for learning rates based on remaining parameter density that partially recovers performance in highly sparse regimes. While our results suggest that random pruning before training is feasible for moderate sparsity levels, we find that the approach is generally inferior to magnitude-based pruning after training. Our work provides insights into the fundamental limits of learning in sparse networks, though the practical implications remain limited compared to existing pruning methods.",
    "id": 692
  },
  {
    "title": "LoRA-MoE: Parameter-Efficient Mixture of Experts via Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA-MoE, a method that combines Low-Rank Adaptation (LoRA) with Mixture of Experts (MoE) architectures to enable parameter-efficient fine-tuning of large-scale models. While LoRA reduces trainable parameters by leveraging low-rank updates, and MoE increases capacity through sparse expert selection, their integration presents non-trivial challenges in routing decisions and gradient flow. We address these by introducing a shared routing network that operates on frozen pre-trained representations, coupled with per-expert LoRA modules initialized via structured sparsity patterns. Experiments on GLUE and SuperGLUE benchmarks using T5-Base (220M) and T5-Large (770M) show modest improvements over standard LoRA (1.3% average GLUE score increase) while using 40% fewer trainable parameters than full MoE fine-tuning. However, gains diminish on larger models (>1B parameters), suggesting our approach may be most beneficial in resource-constrained scenarios. Ablation studies reveal that routing stability remains a limiting factor, with expert collapse occurring in 15% of training runs. Our code is available at [anonymous link].",
    "id": 693
  },
  {
    "title": "Gradient Descent with Adaptive Step Sizes via Second-Order Curvature Estimation",
    "authors": [
      "Liu, H.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a simple modification to stochastic gradient descent that estimates the local curvature along each coordinate using diagonal approximations of the Hessian matrix. Our method, called Adaptive Curvature Scaling (ACS), computes curvature estimates using only gradient information from the previous two iterations, avoiding expensive Hessian computations. We provide theoretical analysis showing that ACS achieves convergence rates comparable to Adam for convex objectives, though our bounds contain additional logarithmic factors. Experiments on standard benchmarks (MNIST, CIFAR-10, and PTB language modeling) show ACS matches or slightly outperforms Adam in final accuracy by 0.5-1.2% in some settings, but requires careful hyperparameter tuning. While the theoretical contribution is incremental, ACS may be useful for practitioners seeking alternatives to adaptive optimizers. Code will be made available upon acceptance.",
    "id": 694
  },
  {
    "title": "Efficient Gradient Compression via Learned Thresholding in Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, M."
    ],
    "abstract": "We propose Adaptive Gradient Compression (AGC), a simple yet effective method for reducing communication costs in federated learning by learning instance-specific thresholds for gradient sparsification. Unlike existing top-k or quantization approaches that use fixed compression rates, AGC employs a small auxiliary network to predict optimal compression thresholds based on local gradient statistics and client data distribution. Our method achieves 30-50% communication reduction compared to uniform quantization on CIFAR-10 and EMNIST benchmarks while maintaining comparable accuracy to uncompressed baselines. However, we observe performance degradation on non-IID partitions, with up to 8% accuracy drop in extreme cases. While AGC introduces minimal computational overhead (less than 5% training time increase), our theoretical analysis reveals the compression scheme cannot guarantee convergence under arbitrary data heterogeneity. Experiments across 5 datasets demonstrate competitive results (within 2-3% of uncompressed models) but fail to match state-of-the-art methods on more challenging benchmarks like CIFAR-100. Code will be made available upon publication.",
    "id": 695
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance and training instability. While recent gradient surgery methods like PCGrad and GradDrop effectively mitigate gradient conflict, they rely on heuristic hyperparameters that require extensive tuning across domains. We propose AdaGS (Adaptive Gradient Surgery), a lightweight method that uses momentum-based memory banks to dynamically adjust gradient projection thresholds based on task similarity. Our approach maintains an exponential moving average of per-task gradient norms, enabling online estimation of conflict severity without additional hyperparameters. Experiments on three benchmarks (CIFAR-10/SVHN multi-task, NYUv2, and Taskonomy) show modest improvements over baselines (1-2% accuracy gains), with particularly strong results when tasks have varying difficulty levels. However, we find that our method's effectiveness diminishes on tasks with similar loss scales, suggesting residual gradient interference remains. While AdaGS offers a practical improvement over fixed-threshold approaches, our theoretical analysis reveals the method can still produce suboptimal gradient directions in certain parameter regimes. Code and trained models will be released upon acceptance.",
    "id": 696
  },
  {
    "title": "Gradient Alignment Regularization: A Lightweight Alternative to Sharpness-Aware Minimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a powerful technique for improving model generalization by explicitly minimizing loss sharpness. However, SAM's computational overhead limits its practical adoption, requiring two forward-backward passes per optimization step. We propose Gradient Alignment Regularization (GAR), a lightweight alternative that approximates SAM's sharpness minimization objective through a single gradient alignment term. Our approach computes the cosine similarity between gradients at the current parameters and a small perturbation, penalizing alignments that indicate sharp minima. GAR requires only 1.1\u00d7 the computation of standard SGD while achieving comparable or superior generalization to SAM on CIFAR-10/100 and TinyImageNet. Specifically, GAR improves test accuracy by 1.2-2.1% over vanilla SGD and matches SAM within 0.3% across architectures. We analyze GAR through the lens of PAC-Bayes theory, showing it upper bounds SAM's sharpness measure under mild assumptions. While our theoretical insights are limited to overparameterized linear models, experiments suggest they hold empirically for deep networks. Despite GAR's simplicity, we acknowledge it inherits some limitations of SAM, including sensitivity to hyperparameter tuning and unclear benefits on all downstream tasks.",
    "id": 697
  },
  {
    "title": "LoRA-CD: Low-Rank Adaptation with Cyclical Dropout for Memory-Efficient Fine-Tuning",
    "authors": [
      "Chen, Y.",
      "Rodriguez, A.",
      "Kim, J.",
      "Thompson, L."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become standard for adapting large language models, but their practical memory savings during training remain limited by gradient storage. We propose LoRA-CD, which combines low-rank adaptation with cyclical dropout patterns to reduce GPU memory requirements. Our approach applies structured dropout to LoRA matrices on a fixed schedule, removing 30-50\\% of the adapters during training while maintaining gradient flow through remaining parameters. The key insight is that low-rank decompositions naturally permit partial participation without catastrophic gradient cancellation. On GLUE and SuperGLUE benchmarks, LoRA-CD achieves 1.4\u00d7 peak memory reduction compared to standard LoRA while maintaining within 1.2\\% of full model performance. The method shows particular effectiveness on larger models (7B+ parameters) with diminishing returns on smaller architectures. While our theoretical analysis of convergence properties remains incomplete, empirical results demonstrate consistent gains across model scales. The magnitude of improvement is modest relative to implementation complexity, though potentially significant for memory-constrained deployment scenarios. Code and pre-trained adapters will be released.",
    "id": 698
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Data-Dependent Covering",
    "authors": [
      "Liu, S.",
      "Kumar, V.",
      "Chen, J."
    ],
    "abstract": "We present new generalization bounds for deep neural networks that incorporate data-dependent covering arguments in the PAC-Bayesian framework. While previous works have exploited the structure of the hypothesis space or data geometry separately, our approach jointly considers both aspects through a novel covering construction based on Fisher-Rao distance on the data manifold. For ReLU networks, we derive bounds that scale as O(\u221a(k/n) log(M(f)/\u03b4)), where k depends on the effective curvature of sample-specific regions and M(f) captures the local Lipschitz constant adapted to the data distribution. Experiments on CIFAR-10 and ImageNet subsets show improvements over VC-dimension and PAC-Bayes baselines by 15-20%. However, computational overhead scales quadratically with network depth, limiting practical applicability to medium-sized architectures. Our work suggests that geometry-aware bounds may bridge theory-practice gaps for specific data regimes, though broader impact remains limited by current computational constraints.",
    "id": 699
  },
  {
    "title": "Lipschitz-Regularized Gradient Descent for Improved Deep Network Generalization in Low-Data Regimes",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Okafor, N."
    ],
    "abstract": "We propose a simple modification to standard gradient descent that constrains the Lipschitz constant of intermediate representations during training to improve generalization when training data is scarce. Our method, Lipschitz-Regularized Gradient Descent (LipReg-GD), adds a computationally lightweight penalty term that encourages smooth mappings between layers without requiring architectural changes or additional hyperparameters beyond the regularization coefficient. We demonstrate on CIFAR-10/100 and Tiny ImageNet that LipReg-GD achieves 2-3% absolute improvement over baseline SGD when using only 10% of training data, while matching baseline performance with full datasets. Theoretical analysis suggests this improvement stems from controlling the Rademacher complexity of the hypothesis class, though our bounds are looser than existing PAC-Bayesian approaches. While our empirical results are consistent across vision tasks, we find minimal benefits on NLP benchmarks and larger-scale ImageNet training. Code and pre-trained models are available at [URL omitted for review].",
    "id": 700
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Inner-Loop Steps for Continuous Control",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Rodrigues, M."
    ],
    "abstract": "Policy gradient methods often suffer from high variance in gradient estimates, leading to unstable training. While variance reduction techniques like control variates have shown promise in discrete domains, their application to continuous control remains limited. We propose VR-PGA, a variance-reduced policy gradient algorithm that uses an adaptive inner-loop optimization procedure to learn state-dependent baselines without prior knowledge of the environment dynamics. Our method combines Stein's lemma with a learned value function to construct control variates, while automatically adjusting the number of inner-loop steps based on gradient norm history. Experiments on MuJoCo continuous control benchmarks show 12-18% improvement in sample efficiency over PPO and SAC on half of the tested environments, with comparable performance on others. We also demonstrate that the adaptive inner-loop reduces wall-clock time by 15-25% compared to fixed inner-loop variants. However, we find that VR-PGA's benefits diminish in low-dimensional state spaces and when reward signals are sparse. While not achieving state-of-the-art results across all tasks, our work provides a practical variance reduction framework that can be integrated into existing policy gradient implementations with minimal code changes.",
    "id": 701
  },
  {
    "title": "Revisiting Initialization Schemes for Transformer Language Models via Neural Tangent Kernel Analysis",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Chen, L."
    ],
    "abstract": "Recent work has shown that careful initialization can improve transformer training stability, but theoretical understanding remains limited. We extend Neural Tangent Kernel (NTK) theory to transformers with layer normalization, deriving closed-form expressions for the NTK at initialization under specific architectural assumptions. Our analysis predicts a phase transition in trainability when attention weights are initialized beyond a critical scale, which we empirically verify on small language modeling tasks with models up to 125M parameters. Based on these insights, we propose a modified initialization scheme that achieves 2-3% better perplexity on Wikitext-103 compared to standard approaches, though benefits diminish on larger models and more complex datasets. While our theoretical results hold only for simplified transformer variants without attention dropout and require approximate handling of layer norm, our work provides preliminary evidence that NTK theory may offer practical guidance for transformer initialization. Code and experiments will be made available.",
    "id": 702
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique for Mitigating Overfitting in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Dropout (GradDrop), a regularization technique that randomly masks gradient updates during backpropagation to prevent overfitting in deep neural networks. Unlike standard dropout which operates on activations, GradDrop stochastically zeroes out gradients of randomly selected parameters during training while keeping all parameters active during inference. Our method is motivated by the observation that overfitting correlates with sharp minima characterized by large gradient magnitudes. By selectively dropping gradients, we encourage optimization paths toward flatter solutions. Experiments on CIFAR-10/100 and ImageNet show GradDrop provides modest improvements over standard dropout (0.3-0.7% accuracy gains), with particularly notable benefits on smaller datasets. Analysis reveals GradDrop increases effective learning rate schedules and reduces memorization of label noise. While GradDrop is simple to implement with minimal computational overhead, our theoretical analysis remains limited to linear models, and performance gains diminish with larger architectures. Code is available at anonymous-link.",
    "id": 703
  },
  {
    "title": "Self-Supervised Curriculum Learning via Gradient Norm Scheduled Dropout",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a simple method for automatically generating curricula in self-supervised learning by scheduling dropout rates based on model gradient norms. Our approach progressively reduces dropout from 0.9 to 0.1 during training, using the L2 norm of gradients on a held-out validation batch as a difficulty indicator. This creates an implicit curriculum where the model learns simpler representations early (high dropout) and more complex features later (low dropout). We evaluate our method on ImageNet-100 and CIFAR-100 with ResNet-50 and ViT-B/16 backbones, showing 2-3% improvements in linear evaluation accuracy compared to fixed dropout baselines. The technique is particularly effective for smaller datasets, suggesting it reduces overfitting. While our method is computationally lightweight and easily implementable (requiring only 3 lines of PyTorch code), we acknowledge several limitations: the improvement diminishes with larger models, the theoretical justification remains heuristic, and extensive hyperparameter tuning (particularly the dropout schedule) is required. We provide open-source implementation and hope this work sparks further investigation into gradient-based curriculum design for representation learning.",
    "id": 704
  },
  {
    "title": "Gradient Alignment Regularization: A Simple Technique for Improving Transformer Generalization in Low-Data Regimes",
    "authors": [
      "Liu, H.",
      "Rodriguez, C.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Alignment Regularization (GAR), a simple regularization technique that encourages gradient directions across mini-batches to align during transformer training. Motivated by observations that large language models exhibit inconsistent gradient patterns when fine-tuned on small datasets, we penalize the cosine distance between gradients computed for different mini-batches of the same training data. Our method adds minimal computational overhead (\u22485% training time) and requires only a single hyperparameter. We evaluate GAR on 6 NLP benchmarks with \u22645K training examples, showing 2-4% improvements in F1 over standard fine-tuning. While the gains are consistent in low-data settings (<1K examples), we find limited benefit as dataset size increases. Theoretical analysis reveals GAR implicitly constrains the Lipschitz constant of the loss landscape, but we acknowledge our analysis assumes near-convexity. Ablation studies suggest most improvement comes from the regularization of attention layers, particularly the higher layers. Despite modest empirical gains, we believe exploring gradient-based regularization in transformers remains underexplored, and hope this work sparks further investigation.",
    "id": 705
  },
  {
    "title": "Gradient Surgery for Interference Reduction: A Lightweight Approach to Multi-Task Optimization",
    "authors": [
      "Liu, K.",
      "Kumar, S.",
      "Chen, W."
    ],
    "abstract": "Multi-task learning often suffers from gradient interference, where conflicting gradients between tasks impede optimization. While recent methods like PCGrad and GradNorm address this through gradient projection or normalization, these approaches introduce significant computational overhead and hyperparameter tuning complexity. We propose Gradient Surgery via Adaptive Thresholding (GSAT), a lightweight alternative that selectively removes gradient components when cosine similarity falls below a learned threshold. Our method requires only a single additional scalar parameter per task pair and adds minimal computational cost. Experiments on three standard multi-task vision benchmarks (NYUv2, CityScapes, CelebA) show that GSAT achieves a 4.2% average improvement over standard multi-task training while reducing training time by 12% compared to PCGrad. However, we observe that benefits diminish on highly correlated tasks, and our method shows limited improvement over strong single-task baselines on some datasets. Our method provides a computationally efficient alternative for moderate multi-task scenarios where gradient interference is present but not severe.",
    "id": 706
  },
  {
    "title": "Improving Transformer Efficiency Through Layer-wise Entropy-based Pruning During Fine-tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, J.",
      "Zhou, Y."
    ],
    "abstract": "While pre-trained transformers achieve state-of-the-art performance across many tasks, their deployment in resource-constrained settings remains challenging. We propose an entropy-based pruning method that dynamically removes attention heads and feed-forward neurons during fine-tuning based on their layer-wise activation entropy. Our approach computes normalized entropy scores at each layer and progressively prunes components whose entropy falls below task-adaptive thresholds. On GLUE and SuperGLUE benchmarks, our method reduces model parameters by 30-40% while maintaining 92-96% of original performance across tasks. However, we observe that pruning decisions exhibit high sensitivity to initialization and batch ordering, with standard deviations of up to 3.4% in downstream accuracy across runs. Additionally, our method underperforms magnitude-based pruning on out-of-domain data, suggesting limited generalization of the entropy-based criteria. While our approach provides a lightweight alternative to iterative magnitude pruning, the computational overhead of entropy computation during training (1.2x increase in training time) and performance variability limit practical impact. Our code is available to ensure reproducibility.",
    "id": 707
  },
  {
    "title": "Gradient Surgery Meets Adam: Improving Optimization Stability Through Layer-wise Learning Rate Surgery",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Layer-wise Gradient Surgery (LGS), a simple modification to Adam that selectively dampens gradient conflicts between network layers. Our approach addresses the observation that the early layers of deep networks often exhibit conflicting gradient directions during training, leading to optimization instability. LGS computes per-layer gradient similarity scores and applies adaptive damping factors based on inter-layer gradient agreement. We evaluate LGS on standard image classification benchmarks including CIFAR-10/100 and ImageNet, as well as transformer-based language models. Our method achieves 0.5-1.2% accuracy improvements on CIFAR-100 and comparable performance on ImageNet, while reducing training oscillation as measured by loss variance. However, the benefits appear most pronounced on smaller-scale experiments with limited hyperparameter tuning, and ablations suggest the improvement largely derives from the damping rather than the surgery component. While LGS introduces minimal computational overhead and does not require additional hyperparameters beyond standard Adam, its practical advantages over other stabilization techniques remain modest. Code will be released upon acceptance.",
    "id": 708
  },
  {
    "title": "Learning with Logit-Adjusted Margins: A Unified Framework for Class-Imbalanced Objectives",
    "authors": [
      "Chen, L.",
      "Rodriguez, S.",
      "Kim, J.",
      "Liu, Z."
    ],
    "abstract": "Class imbalance remains a pervasive challenge in real-world machine learning applications. While numerous reweighting and resampling techniques exist, they often suffer from sensitivity to hyperparameters and lack theoretical grounding. We propose a unified framework that interprets class-imbalanced learning as optimizing a family of logit-adjusted margin losses, where the adjustment depends only on the class prior ratios. Our framework encompasses popular methods including focal loss, class-balanced loss, and logit adjustment as special cases, providing the first unified theoretical characterization of their implicit assumptions. We establish generalization bounds showing that logit-adjusted margins control the trade-off between bias and variance in imbalanced settings. Empirical evaluation on long-tailed CIFAR-10/100 and ImageNet-LT demonstrates consistent but modest improvements (1-3% accuracy gains) over baseline methods. While our theoretical analysis provides new insights into why logit adjustments work, the practical gains are comparable to simple baselines when hyperparameters are carefully tuned. Our results suggest that the primary value of logit-adjusted margins lies in their interpretability rather than significant performance improvements.",
    "id": 709
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Hybrid Approach for Improving Generalization in Overparameterized Networks",
    "authors": [
      "Chaudhry, A.",
      "Li, M.",
      "Zhang, K.",
      "Thompson, B."
    ],
    "abstract": "We propose SharpGrad, a simple modification to gradient-based optimization that combines sharpness-aware minimization with gradient projection techniques. Our method computes per-example gradients, identifies conflicting updates through gradient similarity analysis, and selectively applies sharpness regularization only to directions with high gradient agreement. This hybrid approach aims to reduce the computational overhead of full sharpness minimization while preserving its generalization benefits. Experiments on CIFAR-10/100 and ImageNet subset show 1.2-1.8% improvement over baselines, with particular gains in low-data regimes. However, we find these improvements are less pronounced on larger architectures and vanish completely when coupled with strong data augmentation. Theoretically, we establish a connection between gradient alignment and sharpness through a PAC-Bayesian bound, though our analysis requires assumptions about Lipschitz continuity that may not hold in practice. While our method is easy to implement with minimal computational overhead (\u224815% increase in training time), the improvements are modest and highly dependent on hyperparameter choices. Code and pretrained models will be released upon acceptance.",
    "id": 710
  },
  {
    "title": "Gradient Descent with Learnable Step Sizes: An Empirical Study on Neural Network Training Dynamics",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Rodriguez, S."
    ],
    "abstract": "While adaptive optimization methods like Adam and RMSprop are widely used for training neural networks, their prescribed step size schedules often rely on default hyperparameters that may not be optimal for specific architectures. We propose a simple meta-learning approach that learns a single scalar step size for each layer via backpropagation through training iterations. Our method adds minimal computational overhead (<1% increase in training time) and can be implemented in 20 lines of PyTorch code. Experiments on CIFAR-10 and ImageNet with ResNet-18/50 show modest but consistent improvements (+0.5-1.2% accuracy) over tuned baselines, particularly beneficial when using smaller batch sizes. However, our approach does not outperform carefully hand-tuned learning rate schedules for large-batch training, and we observe instability when applying it to transformer architectures. Ablations suggest that the benefits are primarily due to better early-training optimization rather than any fundamentally different training dynamics. While we establish some theoretical properties for convex quadratic objectives, our analysis does not extend to the non-convex setting. Our code and learned step size schedules are available at [anonymous link].",
    "id": 711
  },
  {
    "title": "Stochastic Mirror Descent with Adaptive Polyak Stepsizes for Non-Convex Optimization",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a variant of stochastic mirror descent that incorporates adaptive Polyak stepsizes for non-convex optimization problems. While Polyak stepsizes are well-studied in convex settings, their application to non-convex objectives remains heuristic. Our method dynamically adjusts stepsizes based on a moving average of recent function values without requiring gradient Lipschitz assumptions. We establish convergence to stationary points at a rate of O(1/\u221aT) under mild assumptions, matching known rates for standard stochastic mirror descent. Experiments on neural network training show modest improvements over Adam and SGD with momentum on CIFAR-10 and PTB datasets, achieving 1-2% better test accuracy in some configurations. However, performance gains are inconsistent across architectures and initializations. Theoretical analysis reveals the stepsize adaptation may fail when gradients become sparse or the loss landscape exhibits sharp curvature. Code is available at anonymous-github.com/smd-adaptive-polyak.",
    "id": 712
  },
  {
    "title": "Improving Transformer Efficiency via Iterative Low-Rank Adaptation with Learned Sparsity Patterns",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Dubois, Y."
    ],
    "abstract": "We propose Iterative Low-Rank Adaptation with Learned Sparsity (ILRALS), a parameter-efficient finetuning method that combines low-rank adaptation with dynamic sparsity patterns. While LoRA has become standard for efficient transformer adaptation, we observe that static low-rank structures may underutilize model capacity. Our approach alternatively learns sparsity masks and low-rank updates in an iterative EM-style procedure. During the E-step, we prune weights using magnitude-based thresholds with learned layer-wise budgets; during the M-step, we update the remaining low-rank components via gradient descent. Experiments on GLUE tasks show 1-2% average improvement over standard LoRA with 15-30% fewer parameters. However, gains are inconsistent across tasks (some datasets show <0.5% improvement), and the iterative procedure increases training time by 2.3x. Analysis reveals benefits primarily on tasks benefiting from selective attention reweighting. We open-source our framework, though reproducibility is limited by sensitivity to threshold hyperparameters and non-deterministic sparsity learning on GPUs.",
    "id": 713
  },
  {
    "title": "Instance-Adaptive Label Smoothing: A Simple Extension of Cross-Entropy with Marginal Calibration Benefits",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Gonzalez, A."
    ],
    "abstract": "Label smoothing has become a standard regularization technique in deep learning, uniformly adjusting the target distribution for all training examples. We propose instance-adaptive label smoothing (IALS), which dynamically adjusts the smoothing parameter based on per-sample confidence estimates from a teacher network or ensemble. Our method requires only two additional forward passes during training and no changes at inference. On CIFAR-10, CIFAR-100, and ImageNet, IALS achieves 0.1-0.3% accuracy improvements over standard label smoothing, with more substantial gains of 1-2% on noisy label benchmarks. While these improvements are modest, we show IALS provides better marginal calibration as measured by ECE, particularly for high-confidence predictions. Theoretically, we characterize IALS as minimizing an upper bound on the instance-weighted cross-entropy. However, ablation studies reveal that 50% of the gains can be achieved by simpler baselines like temperature scaling. Our results suggest that adaptive regularization based on instance uncertainty is beneficial primarily in low-data or noisy label regimes, limiting its broader applicability.",
    "id": 714
  },
  {
    "title": "Gradient Surgery Revisited: A Taylor Approximation Perspective on Multi-Task Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often struggles with conflicting gradients across tasks. While existing gradient surgery methods like PCGrad and GradDrop successfully mitigate interference, they lack theoretical justification and can be overly aggressive in dropping gradient information. We re-examine these heuristics through the lens of second-order Taylor approximations of the multi-task loss surface. Our key insight is that gradient conflicts can be viewed as local curvature misalignment, leading to a principled criterion for gradient modification. We propose TaylorGrad, which computes task-specific gradient projections based on approximate Hessian information estimated through gradient differences. On standard multi-task benchmarks including NYUv2 and CIFAR-100 with 5 auxiliary tasks, TaylorGrad achieves modest improvements (+0.8% average accuracy) over PCGrad while reducing computational overhead by 35%. However, we find that performance gains diminish on more complex architectures like ResNet-50, suggesting our second-order approximations may be insufficient for high-dimensional parameter spaces. Ablations reveal that our method is particularly sensitive to the gradient difference window size and fails when task gradients are nearly orthogonal. While TaylorGrad provides theoretical grounding for existing heuristics, we acknowledge that empirical improvements are incremental rather than transformative.",
    "id": 715
  },
  {
    "title": "Revisiting Adaptive Gradient Methods at the Edge of Divergence",
    "authors": [
      "Liu, K.",
      "Chen, J.",
      "Rodriguez, A."
    ],
    "abstract": "While adaptive gradient methods like Adam and AdaGrad achieve strong empirical performance in deep learning, their theoretical guarantees remain restrictive to specific regimes. This work re-examines these methods when standard assumptions fail, particularly where Lipschitz smoothness is violated. We propose a simple modification incorporating per-coordinate clipping with momentum, motivated by analyzing the gap between theoretical bounds and practical optimization trajectories. Our approach requires minimal hyperparameter tuning and introduces no additional memory overhead. We conduct experiments on variants of ResNet and Vision Transformers trained on ImageNet and CIFAR-10, showing modest improvements over baselines when training moves into unstable regimes. Our results indicate 1-3% relative improvement in final accuracy under certain learning rate schedules, though gains are inconsistent across tasks. Theoretical analysis shows our modified method converges under relaxed smoothness conditions, though the rate is slower than comparable SGD variants. These findings suggest adaptive methods behave differently in practice than theory predicts, highlighting gaps between worst-case bounds and empirical performance, while raising questions about the value of such modifications for general optimization.",
    "id": 716
  },
  {
    "title": "Gradient Surgery with Adaptive Dropout: A Lightweight Approach to Multi-Task Learning in Transformer Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning in transformer architectures often suffers from conflicting gradients between tasks, leading to suboptimal performance across all objectives. While recent gradient surgery methods show promise, they require computing Hessian information or maintaining task-specific parameters, introducing significant computational overhead. We propose AdaGradDrop, a lightweight alternative that dynamically identifies and drops conflicting gradient directions using an adaptive dropout mechanism. Our method applies Bernoulli masks to gradient components based on their alignment with task-specific objectives, requiring only O(n) additional memory where n is the parameter count. Experiments on GLUE and SuperGLUE benchmarks with a shared BERT-base model show 2.1% average improvement over vanilla multi-task training and 1.3% over gradient surgery baselines, while reducing training time by 18%. However, we observe performance degradation on tasks with highly correlated objectives and limited gains on smaller models. Our PyTorch implementation adds less than 50 lines of code. The method provides a practical trade-off between computational efficiency and multi-task performance, though theoretical convergence guarantees remain an open question.",
    "id": 717
  },
  {
    "title": "Gradient Surgery is Not Enough: Addressing Gradient Interference via Adaptive Branching in Multi-Task Networks",
    "authors": [
      "Chen, L.",
      "Srinivasan, V.",
      "Johnson, K."
    ],
    "abstract": "Multi-task learning often suffers from gradient interference, where conflicting gradients from different tasks impede optimization. While recent work has proposed various gradient surgery techniques to project gradients onto conflict-free subspaces, we argue these methods fail to address a fundamental limitation: the fixed computation graph. We propose Adaptive Branching Networks (ABN), which dynamically creates task-specific branches during training when gradient conflicts exceed a learned threshold. Our method computes gradient cosine similarities within mini-batches and spawns new network branches through selective parameter duplication. Experiments on three benchmark datasets (CIFAR-100, NYUv2, QM9) show modest improvements (+1.2-2.3%) over gradient surgery baselines, though with 1.4x increased parameters. While ABN reduces negative cosine similarities between task gradients by 47%, we observe diminishing returns beyond three tasks and increased training instability on small datasets. Our results suggest that architectural flexibility can partially mitigate gradient interference, but highlight the need for better regularization techniques to control model complexity. Code is available at [anonymous link].",
    "id": 718
  },
  {
    "title": "Scheduled Sampling with Learned Correction for Improved GAN Training Stability",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Park, J."
    ],
    "abstract": "We propose a simple modification to Generative Adversarial Network training that combines scheduled sampling with learned correction terms to address training instability. Our approach gradually transitions from synthetic to real data during discriminator updates, while simultaneously learning additive corrections to compensate for distribution mismatch. We derive the correction terms through a variational lower bound on the true data likelihood, yielding a training objective that does not require additional hyperparameter tuning beyond standard GAN settings. Experiments on CIFAR-10 and CelebA demonstrate modest improvements in Inception Score and FID over baseline GAN variants, with particularly stable training curves on the more challenging ImageNet subset. While our contributions are incremental, the technique is easy to implement and may be of practical interest for practitioners struggling with GAN training dynamics. We also present preliminary theoretical justification showing convergence to local Nash equilibrium under simplified assumptions.",
    "id": 719
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Continuous Relaxation with Learned Temperature",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Thompson, J."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. While continuous relaxations such as the Gumbel-Softmax trick provide differentiable alternatives, they introduce bias through fixed temperature parameters. We propose a simple extension that learns layer-specific temperatures during training, allowing the relaxation to adaptively trade off bias and variance. Our method requires minimal additional compute and can be implemented in fewer than 20 lines of PyTorch. Experiments on structured prediction tasks demonstrate 3-7% improvements over standard baselines on the Yelp and synthetic arithmetic datasets, though gains are inconsistent across benchmarks and vanish on larger architectures. Theoretical analysis reveals that our approach is equivalent to reparameterizing a tempered evidence lower bound, though we do not establish convergence guarantees. While our contribution is incremental, our results suggest that learned temperature schedules may be a practical improvement to existing relaxation techniques for practitioners working with small-scale discrete variable models.",
    "id": 720
  },
  {
    "title": "Self-Attention with Learned Sparse Patterns via Differentiable Thresholding",
    "authors": [
      "Liu, J.",
      "Chen, M.",
      "Thompson, K."
    ],
    "abstract": "We propose a method to learn sparse attention patterns in Transformers by introducing a differentiable thresholding mechanism that dynamically selects attention heads during training. Our approach uses a dual-network architecture where a lightweight gating network predicts threshold values for each head based on input statistics, while the main model learns enriched representations from the selected sparse patterns. Through experiments on three language modeling tasks and two vision benchmarks, we achieve 12-18% FLOP reduction compared to dense attention baselines while maintaining comparable accuracy. However, we observe performance degradation on tasks requiring long-range dependencies, and our method introduces additional hyperparameters that require careful tuning. Theoretically, we provide a simple bound showing that sparse patterns can approximate full attention within bounded error, though this bound is looser than existing theoretical results for fixed sparse patterns. Our results suggest learned sparsity offers computational benefits for short sequences but may not fully capture global context when it matters. Code and models are available at [anonymous-link].",
    "id": 721
  },
  {
    "title": "Block-Diagonal Adam: A Memory-Efficient Adaptive Optimizer for Language Model Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Fine-tuning large language models with Adam creates memory bottlenecks due to its dense gradient history storage. We propose Block-Diagonal Adam (BD-Adam), which approximates the complete gradient history by maintaining only block-diagonal covariance matrices, reducing memory usage by 30-50% while preserving convergence properties. Our key insight is that parameter interactions in transformer fine-tuning exhibit strong locality patterns that can be exploited via block structure. Extensive experiments on GLUE and SuperGLUE tasks show BD-Adam achieves 97-99% of Adam's performance while using significantly less memory. However, we observe degraded performance when applying BD-Adam to smaller models (<1B parameters) or domains with less structured parameter interactions. We provide theoretical analysis showing BD-Adam converges at an O(1/\u221aT) rate under standard assumptions, though with a slightly worse constant factor compared to Adam. While BD-Adam offers practical benefits for memory-constrained fine-tuning, its advantages diminish for sufficiently large batch sizes or when using gradient checkpointing. Our implementation is available at [anonymous link].",
    "id": 722
  },
  {
    "title": "When Less Data Hurts: Implicit Regularization Effects of Dataset Pruning on Neural Networks Beyond Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Dataset pruning\u2014selecting informative subsets for training\u2014has emerged as a practical technique for reducing computational costs in deep learning. While existing work focuses primarily on generalization trade-offs, we investigate how dataset pruning affects neural network optimization dynamics. Through empirical studies on ResNet and Vision Transformer architectures across CIFAR-10, CIFAR-100, and ImageNet subsets, we find that aggressive pruning (>50% data removal) introduces subtle but measurable changes in the loss landscape geometry, leading to shallower minima and altered feature learning trajectories. Our analysis reveals that pruned training exhibits distinct early training phase dynamics, with implications for transfer learning performance on downstream tasks. Specifically, we demonstrate an 8-12% degradation in few-shot transfer accuracy when networks pretrained on pruned datasets are adapted to new domains, despite comparable in-domain performance. While these findings provide new insights into the optimization effects of dataset pruning, our theoretical understanding remains limited\u2014our attempts to characterize the implicit regularization through Rademacher complexity and Hessian-based measures show only partial explanatory power. The work suggests dataset pruning affects more than just generalization, opening questions about unintended consequences for representation learning that merit further investigation.",
    "id": 723
  },
  {
    "title": "Improved Generalization Bounds for Low-Rank Matrix Completion via Packaged Rademacher Complexity",
    "authors": [
      "Liu, K.",
      "Zhao, H.",
      "Mukherjee, S."
    ],
    "abstract": "We present new generalization bounds for matrix completion under the assumption that the true matrix has approximately low rank. By introducing a 'packaged' version of Rademacher complexity that groups entries according to their row/column membership patterns, we obtain bounds that scale as O\u0303((d\u2081+d\u2082)r log\u00b2(n)/\u221an) for rank-r matrices with dimensions d\u2081\u00d7d\u2082 and n observed entries. While this improves existing bounds by at most logarithmic factors, our analysis introduces a novel decoupling technique that may be of independent interest. We validate our theoretical results on synthetic low-rank matrices, where empirical generalization error aligns with our predicted rates. However, on real collaborative filtering datasets, the practical improvement over existing bounds is modest, suggesting limitations in the distributional assumptions underlying our analysis. Our work provides a tighter theoretical characterization of low-rank matrix completion, though the practical impact appears limited to specific problem regimes.",
    "id": 724
  },
  {
    "title": "Gradient Surgery via Taylor Approximation: A Lightweight Approach to Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks. While recent approaches like PCGrad and GradNorm modify gradients through costly projected descent or additional loss terms, we propose Taylor Surgery (TS): a lightweight gradient modification technique based on first-order Taylor approximations of task losses. TS approximates the contribution of each parameter to the total loss using local curvature estimates from the diagonal of the Hessian, then rescales gradients proportionally. Our method requires only O(n) additional computation compared to standard backpropagation, unlike O(n\u00b2) full Hessian methods. We evaluate TS on three benchmarks: multi-task image classification (CIFAR-100 with 20 auxiliary tasks), semantic segmentation on Cityscapes++, and a 5-task reinforcement learning environment from MetaWorld. Results show 3-5% improvements over single-task baselines, comparable to PCGrad while being 2.3x faster in wall-clock time. However, TS struggles with heavily correlated tasks (performance degrades when task correlation > 0.8) and shows limited benefits on synthetic linear tasks where our Taylor approximation assumptions break down. While not revolutionary, TS provides a practical trade-off between computational efficiency and multi-task performance for moderately correlated task sets.",
    "id": 725
  },
  {
    "title": "Gradient Descent with Momentum Adapts to Data Heterogeneity in Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate whether simple gradient descent with momentum (GDM) can mitigate the performance degradation caused by data heterogeneity in federated learning without requiring explicit client-specific optimization. While prior work has focused on sophisticated optimization techniques to handle non-IID data distributions, we demonstrate that standard GDM with an appropriately chosen momentum parameter achieves surprisingly competitive convergence rates. Our theoretical analysis establishes that GDM converges at a rate of O(1/\u221aT) under standard assumptions, matching the rate observed in IID settings when momentum \u03b2 \u2265 0.9. We empirically validate our findings on CIFAR-10 and FEMNIST datasets, showing that GDM reduces the performance gap between heterogeneous and homogeneous settings by 23-31% compared to vanilla SGD. However, we observe that these improvements diminish as the number of clients increases beyond 100, suggesting fundamental limitations of momentum-based approaches. Our results provide practical guidance for practitioners but indicate that GDM alone may be insufficient for extreme heterogeneity regimes.",
    "id": 726
  },
  {
    "title": "Gradient Descent with Layer-wise Learning Rates for Transformer Fine-tuning",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Garcia, M."
    ],
    "abstract": "We propose Layer-Adaptive Gradient Descent (LAGD), a simple modification to standard gradient descent that assigns different learning rates to transformer layers based on their gradient norms. Building on observations that lower layers often have smaller gradients during fine-tuning, we introduce a lightweight heuristic that sets layer-wise learning rates proportional to the inverse of median gradient norms from the first 10 training steps. Experiments on GLUE tasks using RoBERTa-base show LAGD achieves average 1.2% improvement over AdamW with comparable hyperparameter tuning effort, while reducing memory usage by 25% by avoiding adaptive optimizer states. However, gains diminish with larger models (\u22640.5% for RoBERTa-large) and are inconsistent across tasks (-0.8% to +2.9% range). Analysis reveals improvements correlate with tasks where gradient norms vary significantly across layers (Pearson r=0.67). While our method is easy to implement and offers modest efficiency gains, limited theoretical justification and diminishing returns at scale suggest careful consideration is needed before adoption. Code is available at anonymous-link.",
    "id": 727
  },
  {
    "title": "Revisiting Weight Decay Through the Lens of Neural Tangent Kernel Evolution",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Thompson, J."
    ],
    "abstract": "Weight decay is a fundamental component of modern neural network training, yet its interaction with the evolving neural tangent kernel (NTK) during optimization remains poorly understood. We provide an empirical analysis of how \u21132 regularization alters the NTK trajectory in overparameterized networks trained with gradient descent. By tracking the kernel's spectral properties throughout training, we observe that weight decay induces a phase transition in the NTK dynamics, transitioning from rapid initial evolution to slower, quasi-static behavior. Our theoretical analysis characterizes this transition for two-layer ReLU networks, deriving bounds on the regularization strength that preserve kernel stability while maintaining generalization benefits. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate that our theoretical predictions correlate moderately with observed generalization gaps (R\u00b2=0.72), though we note deviations for larger architectures. While our results offer fresh perspective on weight decay's implicit bias, we acknowledge limitations in extending our analysis to deeper networks and more complex optimizers. We release our code and training checkpoints to facilitate reproducibility.",
    "id": 728
  },
  {
    "title": "Revisiting Weight Decay Through the Lens of Sharpeness-Aware Gradient Alignment",
    "authors": [
      "Chen, L.",
      "Vaswani, A.",
      "Rodriguez, M."
    ],
    "abstract": "Recent work has shown that weight decay implicitly regularizes neural networks by reducing sharpness of the loss landscape, but the theoretical understanding of this phenomenon remains limited. We propose Gradient Alignment Regularization (GAR), a simple modification to standard weight decay that explicitly aligns parameter updates with directions of minimal sharpness. By introducing a computationally efficient approximation based on per-layer gradient covariance, GAR achieves comparable regularization to SAM while requiring 40% fewer gradient computations. Experiments on CIFAR-10/100 show consistent improvements over weight decay baselines (1.2-2.3% accuracy gains), particularly for overparameterized ResNet architectures. However, benefits are less pronounced on larger datasets like ImageNet, where we observe diminishing returns (0.3% improvement over tuned baselines). Our theoretical analysis reveals GAR minimizes an upper bound on generalization error under uniform stability assumptions, though the bound depends on restrictive Lipschitz constants that may not hold in practice. While GAR demonstrates promising regularization effects for small to medium-scale vision tasks, questions remain about scalability to large language models where compute overhead becomes significant.",
    "id": 729
  },
  {
    "title": "Improved Gradient Descent via Momentum with Lookahead: A Theoretical and Empirical Analysis",
    "authors": [
      "Chen, M.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose Momentum Lookahead (MoLA), a simple modification to stochastic gradient descent that combines Polyak momentum with asynchronous weight updates. While momentum methods accelerate optimization along low-curvature directions, we observe that Lookahead's slow weights can amplify this benefit without additional hyperparameters. Our theoretical analysis shows MoLA achieves an O(1/\u221aT) convergence rate for smooth, convex objectives, matching standard SGD but with improved constants in the presence of gradient noise. On ImageNet training, MoLA achieves 76.5% top-1 accuracy with 1.3\u00d7 faster wall-clock time compared to baseline SGD+Momentum, though gains plateau with larger batch sizes. Ablation studies reveal the momentum coefficient as the primary driver of improvement, raising questions about synergistic effects between components. While MoLA shows consistent but modest improvements across standard benchmarks, our method fails to outperform tuned AdamW in transformer fine-tuning tasks. Code will be released upon acceptance.",
    "id": 730
  },
  {
    "title": "Improving Transformer Efficiency through Learnable Sparse Attention with Dynamic Block Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Dynamic Block Sparse (DBS) attention, a method for reducing transformer computational complexity by learning sparse attention patterns that adapt to input content. While existing sparse attention mechanisms rely on fixed sparsity patterns or simple heuristics, DBS learns to dynamically group tokens into variable-sized blocks based on learned similarity scores, achieving o(n\u00b2) complexity reduction without task-specific tuning. Our method introduces a lightweight gating network that predicts block boundaries for each attention head, trained jointly with the main model through a differentiable relaxation of the discrete grouping problem. Experiments on language modeling and machine translation show 2.1\u00d7 speedup during training and 2.7\u00d7 during inference compared to standard attention, with minimal performance degradation on WikiText-103 (17.3 vs 16.8 perplexity) and comparable BLEU scores on WMT'14 En-De (27.4 vs 27.9). However, we observe the approach shows diminishing returns on longer sequences (>2k tokens) and struggles with tasks requiring precise long-range dependencies. Our code and models are available at [url].",
    "id": 731
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Deep Neural Networks",
    "authors": [
      "Chen, J.",
      "Liu, S.",
      "Kim, H."
    ],
    "abstract": "We propose ANoise, a simple yet effective technique that adaptively injects Gaussian noise into model gradients during training to improve generalization. Unlike previous noise injection methods that use fixed variance schedules, ANoise dynamically adjusts noise intensity based on gradient statistics and training progress. Our method adds minimal computational overhead and can be integrated into any optimizer with three lines of code. Experiments on CIFAR-10, CIFAR-100, and ImageNet show modest improvements over baseline SGD with momentum (0.5-1.2% accuracy gains), though results vary significantly across architectures and datasets. In particular, ANoise performs well on ResNet architectures but shows negligible benefits on Vision Transformers. While we provide empirical evidence that the technique acts as implicit regularization, a thorough theoretical understanding remains elusive. Our analysis reveals that the optimal noise schedule is highly dependent on batch size and learning rate, making hyperparameter tuning crucial. Code will be made available upon acceptance.",
    "id": 732
  },
  {
    "title": "Improving Transformer Efficiency via Self-Attention Pruning with Learned Gating Mechanisms",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, T."
    ],
    "abstract": "Transformer models achieve state-of-the-art results across many tasks but suffer from quadratic complexity in sequence length due to their self-attention mechanism. While recent pruning methods reduce computational cost through fixed sparsity patterns, they often degrade performance on long-context tasks. We propose Dynamic Attention Gates (DAG), a novel approach that learns to selectively disable attention heads and positions during inference based on input-dependent gating. Our method uses lightweight MLP controllers that predict pruning decisions for each attention head and token pair using only local context, avoiding the need for full-attention computations. We evaluate DAG on language modeling and machine translation benchmarks, achieving 1.4x speedup at similar perplexity compared to dense baselines on sequences up to 4K tokens. Notably, DAG maintains within 0.5 BLEU score of full transformers on WMT'14 En-De while reducing FLOPs by 30%. However, we observe performance degradation on tasks requiring fine-grained cross-attention patterns. Our ablation study reveals the trade-off between pruning aggressiveness and task-specific accuracy, suggesting that learned gating may over-prune long-range dependencies. While DAG provides practical efficiency gains, its benefits are most pronounced for moderate sequence lengths where partial attention maintains sufficient context.",
    "id": 733
  },
  {
    "title": "Lipschitz-Regularized Kernel Networks for Improved Generalization in Low-Data Regimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Kumar, A."
    ],
    "abstract": "Deep kernel methods have gained attention as a principled approach to combining the flexibility of neural networks with the uncertainty quantification of Gaussian processes. However, their deployment in low-data regimes remains challenging due to overfitting and poor generalization bounds. We propose Lipschitz-Regularized Kernel Networks (LRKNs), which impose Lipschitz constraints on the feature mapping through spectral normalization of the kernel parameters. This leads to tighter PAC-Bayes generalization bounds and improved performance on few-shot classification tasks. Our experiments on Mini-ImageNet and CIFAR-FS demonstrate 3-5% improvement over standard deep kernel approaches, while requiring 30% less training time. However, these gains diminish as dataset size increases, and our method shows limitations on larger-scale benchmarks like ImageNet. Theoretical analysis reveals that the Lipschitz constraint may overly restrict the hypothesis space in high-dimensional settings. We provide open-source code and pre-trained models for reproducibility.",
    "id": 734
  },
  {
    "title": "LoRA-Drop: Dynamic Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Liu, K."
    ],
    "abstract": "We propose LoRA-Drop, a simple yet effective method for improving the efficiency of Low-Rank Adaptation (LoRA) in fine-tuning large language models. While LoRA has emerged as a popular parameter-efficient fine-tuning technique, we observe that not all layers benefit equally from low-rank adaptation, with some contributing minimal performance gains. Our key insight is to dynamically drop LoRA modules from layers where they provide diminishing returns during training. We introduce a lightweight importance scoring mechanism based on gradient norms that identifies which layers can safely have their LoRA modules removed without significant performance degradation. On the GLUE benchmark with RoBERTa-large, LoRA-Drop reduces the number of trainable parameters by 35-50% compared to standard LoRA while maintaining 97-99% of the original performance. We also demonstrate consistent improvements on parameter-constrained regimes, though gains diminish when computational budgets are relaxed. Our method adds minimal computational overhead and can be integrated into existing LoRA implementations with <20 lines of code. While our approach shows promise for practical deployment scenarios, we acknowledge limitations including task-specific hyperparameter sensitivity and reduced effectiveness on smaller models. Our implementation is available at [redacted for anonymity].",
    "id": 735
  },
  {
    "title": "LayerNorm Without the Norm: Training Deep Networks with Affine Transformations Only",
    "authors": [
      "Lee, S.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "Normalization layers like LayerNorm have become standard in transformer architectures, but their computational cost on modern hardware remains non-trivial, particularly for inference. We investigate whether the normalization step itself is necessary, or if the learned affine parameters alone provide sufficient conditioning. We propose Affine-Only LayerNorm (AOL), which retains the learned scale and shift parameters while removing the normalization computation. Through extensive experiments on language modeling and vision tasks, we show that AOL achieves competitive performance with standard LayerNorm on smaller models (\u22641B parameters), reducing inference latency by 8-12% on GPU and 15-20% on CPU. However, we observe performance degradation on larger models and tasks requiring precise calibration of attention scores. Analysis reveals that AOL works best in architectures with pre-activation residual connections and warmup schedules, suggesting the normalization statistics serve as a form of implicit learning rate scheduling. While this provides a practical optimization for resource-constrained deployments of smaller models, our results also highlight the continued importance of normalization for training stability at scale.",
    "id": 736
  },
  {
    "title": "Meta-Weight Regularization for Few-Shot Domain Adaptation with Limited Target Labels",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Few-shot domain adaptation (FDA) addresses the practical challenge of adapting models to new domains with minimal labeled data, yet current approaches struggle when target labels are extremely scarce (\u22645 samples per class). We propose Meta-Weight Regularization (MWR), a simple but effective method that leverages meta-learning to dynamically reweight source domain samples during training. MWR alternates between standard domain adaptation on the source data and meta-updates that adjust sample weights based on their gradient alignment with a small validation set of target labels. Unlike meta-learning approaches that require full model adaptation, MWR only optimizes scalar weight vectors, making it computationally efficient. On three benchmarks (Office-31, VisDA-2017, and DomainNet), MWR achieves 2.3-4.1% improvement over state-of-the-art FDA methods with 5 target labels per class, while maintaining performance with larger target sets. However, our method shows diminishing gains when source-target domain gaps are extreme, and theoretical analysis reveals sensitivity to the size of the meta-validation set. We release our code to facilitate reproducibility.",
    "id": 737
  },
  {
    "title": "Improving Transformer Efficiency through Attention Pattern Recycling",
    "authors": [
      "Liu, C.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "The quadratic complexity of self-attention in transformers remains a critical bottleneck for long-sequence applications. We propose Attention Pattern Recycling (APR), a method that caches and reuses attention patterns across similar inputs to reduce computational overhead. Our approach identifies structural similarities in attention distributions through lightweight clustering during training, then reuses cached patterns for subsequent forward passes when similarity thresholds are met. The method requires no architectural modifications and can be integrated as a wrapper around existing transformer implementations. Experiments on language modeling and machine translation tasks show 1.4-1.7x speedup with <1% performance degradation on sequences up to 4K tokens. However, performance drops significantly (3-5 BLEU score reduction) for longer sequences (>8K tokens) and tasks requiring precise positional attention patterns. While APR demonstrates clear efficiency gains for certain workloads, the reliance on attention pattern similarity limits its broader applicability. Our code will be made available upon acceptance.",
    "id": 738
  },
  {
    "title": "Regularized Gradient Descent with Memory-Efficient Second-Order Updates for Large-Scale Optimization",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S.",
      "Garcia, M."
    ],
    "abstract": "We propose SAMOSA, a practical variant of gradient descent that incorporates second-order information while maintaining memory efficiency comparable to first-order methods. Our approach approximates the Hessian via low-rank updates using historical gradient information, combined with an adaptive regularization scheme that limits curvature exploitation based on gradient noise estimates. The method requires only 50% additional memory compared to standard SGD and introduces minimal computational overhead through careful reuse of existing gradient computations. We evaluate SAMOSA on standard vision and language benchmarks, showing 2-7% improvements over tuned SGD baselines on CIFAR-10/100 and modest gains on GLUE tasks, particularly on out-of-distribution validation sets. While our theoretical analysis provides convergence guarantees for convex quadratic objectives, the gap between theory and practice remains significant for general non-convex settings. Our results suggest the method is most beneficial when training data exhibits moderate ill-conditioning, though benefits diminish on well-regularized architectures. Code and hyperparameter configurations are provided for reproduction.",
    "id": 739
  },
  {
    "title": "Curriculum Learning via Policy Distillation for Partially Observable Environments",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "Curriculum learning has shown promise for training agents in complex environments, yet most methods assume full observability and require handcrafted curricula. We propose a curriculum learning framework that automatically generates curricula for partially observable environments by distilling policies from fully-observed teacher agents. Our approach trains a curriculum generator using reinforcement learning to sequence environment modifications that progressively remove observability of state variables. Experiments on MiniGrid and Atari environments show our method achieves 12-18% faster convergence compared to standard RL training, while maintaining 94% of final performance in five environments. We demonstrate that the learned curricula transfer to harder variants of the same domains. However, we observe that the approach struggles with environments requiring long-term memory and shows diminishing returns when the initial observability is already severely limited. Our results suggest that policy distillation can provide an effective signal for curriculum generation in partially observable settings, though the method's applicability may be constrained by environment complexity and available teacher policies.",
    "id": 740
  },
  {
    "title": "Rethinking Batch Norm Placement: A Study on Learned Normalization Schedules",
    "authors": [
      "Chen, L.",
      "Johnson, M.",
      "Perez, S."
    ],
    "abstract": "Batch Normalization (BN) has become a staple in deep neural network training, yet its optimal placement within the network architecture remains largely unexplored beyond heuristic positions. We propose Learned Normalization Scheduling (LNS), a gradient-based approach that dynamically adjusts the placement and scale of BN layers during training. Our method uses an auxiliary controller network that predicts whether to apply, skip, or scale BN operations at each layer based on gradient statistics and activation patterns. On ImageNet and CIFAR-10, LNS achieves comparable accuracy to standard BN placement while reducing training time by 12-15% on ResNet-50. However, benefits diminish with smaller networks (\u226450M parameters), where gains are statistically insignificant. Our theoretical analysis reveals that LNS approximates a form of adaptive L2 regularization, explaining some performance improvements but also highlighting convergence challenges with certain initialization schemes. While our method introduces additional hyperparameters, extensive ablations suggest it is robust within reasonable ranges. Code is available at anonymous.url.",
    "id": 741
  },
  {
    "title": "LoRA-Tune: Memory-Efficient Fine-tuning via Low-Rank Adapter Collapse",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J.",
      "Thompson, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular method for parameter-efficient fine-tuning of large language models, but its memory footprint during training remains substantial for large adapters. We propose LoRA-Tune, a simple technique that progressively collapses LoRA adapters into the base model during training, reducing peak memory usage by 35-45% with minimal performance degradation. Our approach uses a novel scheduling mechanism that collapses adapters when their gradient norms fall below a dynamic threshold determined by the Fisher information of the base model parameters. We evaluate LoRA-Tune on the GLUE benchmark using Llama-2 (7B) and various downstream tasks. While our method achieves comparable results to standard LoRA (+0.3% average F1 score), we show it enables training on single RTX 3090 GPUs for tasks previously requiring A100s. However, we observe degradation on tasks requiring fine-grained reasoning (drop of 2-4% on MNLI and QNLI), suggesting the collapse mechanism may prematurely discard important adapter knowledge. Ablation studies indicate our gains are primarily from reduced activation memory rather than parameter reduction. LoRA-Tune provides a practical solution for resource-constrained fine-tuning, though careful threshold tuning is needed for reasoning-heavy tasks.",
    "id": 742
  },
  {
    "title": "Gradient Crafting: Efficient Fine-Tuning via Iterative Magnitude Pruning with Momentum Recovery",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Crafting, a simple fine-tuning method that combines iterative magnitude pruning with momentum-based weight recovery to achieve parameter-efficient model adaptation. Our approach alternates between pruning low-magnitude weights during gradient descent and selectively restoring previously pruned weights using momentum statistics, resulting in models with 30-50% fewer parameters while maintaining downstream performance. We evaluate on GLUE benchmarks and ImageNet transfer learning tasks, demonstrating competitive accuracy to standard fine-tuning with reduced memory footprint. Theoretical analysis reveals our method implicitly performs a form of regularized optimization, though we observe instabilities when pruning rates exceed 60%. While Gradient Crafting shows promise for resource-constrained deployment, its benefits diminish on larger models (>1B parameters) and sensitivity to hyperparameter choices limits practical adoption. Our code is available at anonymized-link.",
    "id": 743
  },
  {
    "title": "LoRA-RT: Low-Rank Adaptation with Runtime Thresholding for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA-RT, a simple extension to Low-Rank Adaptation (LoRA) that introduces dynamic thresholding of adapter weights during training. While LoRA has enabled parameter-efficient fine-tuning by learning low-rank updates to pre-trained models, we observe that many learned adapter weights remain close to zero across different tasks, suggesting redundancy. Our method adds a learnable threshold parameter that dynamically masks negligible weight updates during training, effectively pruning the adapter while maintaining performance. We evaluate LoRA-RT on GLUE and SuperGLUE benchmarks using RoBERTa-base and Llama-2-7B, achieving comparable task performance to standard LoRA while reducing the number of active adapter parameters by 15-35%. However, our approach introduces an additional hyperparameter and shows mixed results on generative tasks, with some degradation on longer sequence generation. While LoRA-RT demonstrates promise for reducing adapter storage costs, we acknowledge that our gains are incremental and primarily benefit deployment scenarios with strict memory constraints. Code will be made available upon acceptance.",
    "id": 744
  },
  {
    "title": "LoRA-FA: Low-Rank Adaptation with Feature Alignment for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular method for parameter-efficient fine-tuning of large language models, but suffers from suboptimal feature representations when applied to downstream tasks with significant domain shifts. We propose LoRA-FA, which augments standard LoRA with a lightweight feature alignment mechanism that aligns intermediate representations during fine-tuning. Our method introduces an additional orthogonal loss term that encourages the adapted features to remain close to the pre-trained manifold while allowing task-specific adaptation. We evaluate LoRA-FA on GLUE and SuperGLUE benchmarks using LLaMA-2-7B, achieving 1.2% average improvement over standard LoRA with only 0.05% additional parameters. While our results are statistically significant (p < 0.05 via paired t-test), the improvements are modest and concentrated on specific tasks (CoLA and RTE). Theoretical analysis shows our alignment term can be viewed as a form of trust region optimization, though we establish only loose bounds on approximation error. Code is available at anonymous-url.",
    "id": 745
  },
  {
    "title": "Data Augmentation via Neural Feature Space Mixup for Small-Scale Classification",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Data augmentation is crucial for training neural networks on small datasets, yet conventional image augmentation techniques fail to leverage the rich feature representations learned by modern architectures. We propose Neural Feature Space Mixup (NFSM), a simple augmentation strategy that performs linear interpolation between hidden representations of training samples rather than input space mixing. Our method applies mixup between intermediate features across different layers of a pre-trained backbone network, guided by a lightweight layer selection algorithm based on representational similarity. Experiments on CIFAR-10/100 with reduced training data show 2-4% accuracy improvements over baseline mixup variants when using only 20-40% of the training data, narrowing the gap to full-data performance by 15-30%. However, gains diminish with larger datasets, suggesting NFSM primarily benefits low-data regimes. While our approach introduces minimal computational overhead and requires no additional hyperparameter tuning beyond standard mixup, we find it provides limited benefits when combined with strong regularization techniques like CutMix or AutoAugment. Code and pretrained models will be released upon publication.",
    "id": 746
  },
  {
    "title": "ReLoRA: Low-Rank Adaptation via Recursive Matrix Factorization for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Krishnan, V.",
      "Santos, J."
    ],
    "abstract": "We present ReLoRA, a parameter-efficient fine-tuning method that recursively decomposes pre-trained weight matrices into products of low-rank factors. Unlike existing LoRA variants that add low-rank matrices or adapters, ReLoRA iteratively factorizes existing parameters while maintaining frozen components, achieving comparable performance with 15-40% fewer trainable parameters across language and vision tasks. Our approach exploits the observation that pre-trained representations often exhibit hierarchical low-rank structure, which can be successively approximated through rank-constrained SVD updates. On GLUE and VTAB benchmarks, ReLoRA matches full fine-tuning accuracy (\u00b11.1%) while using only 0.15% of pre-trained parameters. However, our method requires careful rank scheduling and shows degraded performance on generative tasks (CNN/DailyMail summarization drops 4.3 BLEU). Theoretical analysis reveals ReLoRA implicitly performs a form of block-coordinate descent on matrix manifolds, though convergence guarantees remain limited. While ReLoRA offers slight parameter savings over LoRA variants, the practical benefits may be modest given increased implementation complexity. Code and pre-trained checkpoints will be released upon acceptance.",
    "id": 747
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Head Pruning with Learnable Gumbel Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models waste computation through redundant attention heads, but existing pruning methods require costly retraining or suffer from unstable convergence. We propose Gumbel Threshold Pruning (GTP), a simple approach that learns per-head importance scores using Gumbel-Softmax reparameterization during standard fine-tuning. Our method adds just 0.1% parameters and automatically identifies heads for removal without modifying the training objective. Experiments on WMT'14 and GLUE show 15-20% FLOP reduction with <1% performance drop on translation and <2% on classification tasks. While less aggressive than magnitude-based methods, GTP maintains inter-head dependencies and achieves better zero-shot transfer than distillation baselines. However, gains diminish on larger models (>1B parameters), and our theoretical analysis reveals the method converges to local optima in certain attention patterns. Code will be released upon acceptance.",
    "id": 748
  },
  {
    "title": "Improving Neural ODE Stability Through Adaptive Checkpoint Intervals",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Continuous-depth neural networks have shown promise for modeling physical systems, but suffer from numerical instability during training when integration paths become stiff. We propose an adaptive checkpoint interval strategy that dynamically adjusts the ODE solver's step size based on local curvature estimates, particularly addressing the vanishing gradient problem inherent in naive implementations. Our method introduces a lightweight gating mechanism that predicts optimal integration intervals without significant computational overhead. On standard benchmarks including CIFAR-10 and sequential MNIST, we achieve marginally better accuracy (0.7% improvement) compared to fixed-interval Neural ODE baselines while reducing training wall-clock time by 12%. While our theoretical analysis provides stability guarantees only for Lipschitz-constrained architectures, empirical results suggest broader applicability. The primary contribution is a practical training stabilization technique that maintains the continuous-depth formulation while addressing some convergence issues, though questions remain about scalability to state-of-the-art vision architectures. Code and pretrained models will be made available upon acceptance.",
    "id": 749
  },
  {
    "title": "Memory-Efficient Training of Transformers via Structured Sketching of Attention Matrices",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose SketchAttention, a training method that reduces the memory footprint of self-attention in Transformers by projecting attention matrices into low-dimensional sketches. Our approach combines recent work on linear attention with CountSketch-based dimensionality reduction, achieving sub-quadratic memory complexity in sequence length. While previous linear attention mechanisms suffer from accuracy degradation on complex reasoning tasks, we introduce a learnable sketching operator that adapts to the data distribution during training. Experiments on standard NLP benchmarks show modest improvements over vanilla linear attention (2-3% absolute improvements on GLUE), with memory savings comparable to other efficient attention methods. However, we observe that our method underperforms full quadratic attention on tasks requiring fine-grained reasoning, particularly on long-context datasets like TriviaQA. Theoretical analysis reveals that our sketching approach preserves attention probabilities up to a multiplicative error bound, though this bound becomes loose for heavily skewed attention patterns. While SketchAttention provides practical memory benefits for training large models on consumer GPUs, its trade-offs between efficiency and accuracy may limit adoption for applications where small accuracy differences are critical.",
    "id": 750
  },
  {
    "title": "Task-Specific Gradient Sparsity: A Simple Dropout Alternative for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Liu, C.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Task-Specific Gradient Sparsity (TSGS), a method that identifies sparse subnetworks for downstream tasks without requiring additional training or hyperparameter tuning. Unlike existing pruning approaches that rely on magnitude-based criteria or extensive retraining, TSGS leverages gradient information computed during a single forward-backward pass to determine which weights should be frozen for downstream adaptation. Our method achieves 2-3x parameter reduction compared to standard fine-tuning while maintaining accuracy within 1-2% of the full model on GLUE tasks. However, we observe that TSGS performance degrades significantly on out-of-domain tasks by 8-15%, suggesting sparsity patterns are task-specific. While our approach provides computational benefits and simplifies deployment, our theoretical analysis reveals that the sparsity bounds depend on strong assumptions about gradient smoothness that may not hold in practice. Extensive experiments across BERT, ResNet, and ViT architectures demonstrate consistent but modest improvements over baseline methods. Code and pre-trained models are available at [URL withheld for anonymity].",
    "id": 751
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Regularized Approach to Minima Selection in Neural Networks",
    "authors": [
      "Liu, C.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "Recent work has shown that not all global minima are created equal\u2014sharp minima tend to generalize poorly compared to flat ones. We propose Sharpness-Aware Gradient Surgery (SAGS), a simple regularization technique that re-weights gradients based on local sharpness estimates during training. Our method combines ideas from gradient surgery and sharpness-aware minimization, but avoids the computational overhead of second-order methods by leveraging a first-order approximation of the Hessian. On CIFAR-10 and ImageNet subsets, SAGS achieves modest improvements (1-2% accuracy gain) over standard training, with particularly strong gains on noisy labels. However, our gains vanish on larger-scale experiments and fail to consistently outperform strong baselines like SAM on full ImageNet. Our theoretical analysis provides convergence guarantees under restrictive assumptions, but relies on approximate bounds that may not hold in practice. Code and pretrained models will be made available.",
    "id": 752
  },
  {
    "title": "Adaptive Gradient Clipping with Memory-Efficient Second-Order Estimation",
    "authors": [
      "Liu, J.",
      "Kumar, S.R.",
      "Zhao, Y."
    ],
    "abstract": "Gradient clipping is widely used in training deep neural networks, particularly transformers, but fixed clipping thresholds often struggle with varying gradient scales across layers and training phases. We propose AdaClip, a method that automatically adjusts clipping thresholds using a diagonal approximation of the Fisher information matrix computed via mini-batch gradients. Unlike previous approaches that require maintaining gradient histories, AdaClip estimates second-order information from a small buffer of recent gradients, yielding memory overhead of <0.1% compared to baseline training. Our method introduces a lightweight online estimation procedure that adapts clipping thresholds every few hundred steps rather than every iteration, reducing computational cost while maintaining adaptivity. Experiments on language modeling (WikiText-103) and vision tasks (ImageNet-1k) show modest improvements over standard clipping\u20140.8% better perplexity and 0.3% higher accuracy respectively\u2014across various transformer architectures. While AdaClip demonstrates consistent small improvements across settings, our analysis reveals these gains diminish when training budgets are large (300k+ steps), suggesting potential value primarily in resource-constrained scenarios. Code is available at anonymized.",
    "id": 753
  },
  {
    "title": "Repairing Gradient Descent with Second-Order Moments: A Memory-Efficient Quasi-Newton Method for Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We present Momentum-Adjusted Diagonal Quasi-Newton (MADQN), a memory-efficient second-order optimization method for deep neural networks that combines diagonal Hessian approximations with momentum-based updates. Motivated by the limitations of Adam's diagonal preconditioning and the computational burden of full-matrix optimizers, MADQN maintains only O(n) parameters while incorporating curvature information through moving averages of past gradients and parameter updates. Our key insight is that carefully reweighted second moments can approximate diagonal Hessian entries without explicit Hessian-vector products. On ResNet-50/ImageNet, MADQN achieves 0.8% higher top-1 accuracy compared to AdamW (+0.3% vs. SGD+Momentum) while using 47% less memory than Shampoo. We observe particularly strong gains on ViT training, where MADQN reduces training time by 12% relative to baselines. However, performance varies across architectures: CNNs show modest improvements, while Transformer training benefits more substantially. Our method introduces two hyperparameters requiring architecture-specific tuning, limiting its immediate practical adoption. Theoretical analysis proves O(1/T) convergence for quadratic objectives under standard assumptions, though this extends imperfectly to the non-convex case. Code is available at [anonymous link].",
    "id": 754
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Batch Sizes for Non-stationary Environments",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Johnson, M."
    ],
    "abstract": "Policy gradient methods face significant challenges when deployed in non-stationary environments where the reward distributions drift over time. We propose VR-PGAS, a variance-reduced policy gradient method that automatically adjusts batch sizes based on estimated non-stationarity. Our approach combines gradient estimates from multiple temporal scales using an exponential weighting scheme, theoretically reducing variance by a factor of O(\u221aT) compared to standard methods. We derive convergence guarantees for both Lipschitz and smooth policy classes under mild assumptions about the drift rate. Empirical evaluation on three simulated domains (MuJoCo with injected drift, dynamic inventory management, and non-stationary bandits) shows modest improvements over strong baselines, achieving 5-12% better average returns on stationary benchmarks but only 2-4% gains in non-stationary settings. While our theoretical contributions are incremental and the practical improvements are limited, our framework provides a principled way to handle mild non-stationarity that could be useful for practitioners working with slowly changing domains.",
    "id": 755
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Pruning for Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We present LoRA-Drop, a method to improve the efficiency of Low-Rank Adaptation (LoRA) by dynamically pruning adapter weights during fine-tuning. Building on recent work showing that LoRA modules exhibit varying importance across layers, we introduce a magnitude-based pruning criterion combined with a learnable dropout probability that adapts based on gradient signals. Our approach reduces parameter overhead by 30-50% while maintaining 95-98% of full LoRA performance on downstream tasks. Through extensive experiments on GLUE and three vision-language benchmarks, we demonstrate that LoRA-Drop achieves comparable or slightly better accuracy than standard LoRA with 2\u00d7 faster inference. However, we observe that pruning decisions appear somewhat unstable across random seeds in smaller models (\u22641B parameters), suggesting the need for better layer importance estimation. Additionally, while our method shows consistent benefits in resource-constrained settings, the computational overhead of dynamic pruning may offset gains in scenarios with abundant memory. Our code will be released upon acceptance.",
    "id": 756
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Does It Actually Help?",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder optimization. While gradient surgery methods like PCGrad have shown empirical success, their theoretical understanding remains limited. We provide a formal characterization of when gradient surgery yields improvements over standard multi-task training. Using tools from convex analysis, we prove that PCGrad can reduce optimization variance when task gradients are negatively correlated, but may increase bias when tasks are nearly aligned. Inspired by this analysis, we propose AdaptivePCGrad, which dynamically weights gradient projections based on task similarity. Experiments on three benchmark datasets (NYUv2, CelebA, and CityScapes) show modest but consistent improvements over PCGrad, with average performance gains of 2.1%. However, we find that benefits diminish when tasks have strong positive correlation or highly imbalanced losses. While our theoretical results offer new insights into gradient surgery mechanisms, the empirical improvements are small compared to simpler baselines like loss weighting. Our work highlights the importance of understanding when sophisticated optimization tricks are actually necessary versus just adding complexity.",
    "id": 757
  },
  {
    "title": "Low-Rank Adaptation with Gradient Accumulation for Memory-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We present a simple modification to Low-Rank Adaptation (LoRA) that reduces memory usage during fine-tuning of large language models by accumulating gradients over multiple forward passes before updating the low-rank matrices. While LoRA has become a standard method for parameter-efficient fine-tuning, it still requires storing activations in memory during the backward pass, creating a bottleneck for training large models on consumer hardware. Our approach, GRAD-LoRA, addresses this by decoupling the gradient computation from the parameter update, allowing gradients to be computed in smaller batches and accumulated before applying the low-rank update. We evaluate GRAD-LoRA on instruction tuning tasks across 7B and 13B parameter models, achieving comparable performance to standard LoRA while reducing peak memory usage by 35-40% with minimal computational overhead (3-5% increase in training time). However, our method introduces a new hyperparameter (accumulation steps) that can affect convergence, and we observe increased instability in certain configurations. While GRAD-LoRA provides practical benefits for resource-constrained fine-tuning, we acknowledge its contributions are primarily incremental and may not justify publication on novelty alone. Code will be released upon acceptance.",
    "id": 758
  },
  {
    "title": "Gradient Surgery for Mixed Precision Training: A Heuristic Approach to Dynamic Loss Scaling",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Singh, N."
    ],
    "abstract": "Training deep neural networks in mixed precision suffers from gradient underflow when loss scaling factors are poorly chosen. While existing methods use static or hand-tuned schedules, we propose GradSurgery, an adaptive loss scaling mechanism that performs \"gradient surgery\" by analyzing curvature information from the Hessian diagonal. Our method dynamically adjusts scaling factors based on local gradient statistics and a novel confidence measure derived from the signal-to-noise ratio of mini-batch gradients. In experiments on ResNet-50 and BERT-base, GradSurgery achieves competitive accuracy to manual tuning with 15-30% faster convergence on ImageNet and GLUE benchmarks. However, we observe that GradSurgery's performance degrades on extremely deep architectures (\u22651000 layers), where the Hessian approximation becomes unreliable. Our PyTorch implementation adds minimal computational overhead (<3% increase in training time) but requires full-batch gradient computation every 100 steps for curvature estimation. While not universally superior to existing automatic methods like AdaLoss, our approach offers a principled alternative that reduces manual tuning effort for practitioners. Code and models will be made available upon acceptance.",
    "id": 759
  },
  {
    "title": "LoRA-Plus: Incremental Low-Rank Adaptation with Gradient Amplification",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have emerged as practical alternatives to full model fine-tuning, but we observe that gradient magnitudes in LoRA modules tend to be disproportionately small compared to the original parameters. We propose LoRA-Plus, a simple modification that applies fixed gradient amplification factors to the low-rank matrices during training. Our method requires no additional hyperparameters beyond standard LoRA and adds negligible computational overhead. We evaluate LoRA-Plus on GLUE, SuperGLUE, and vision-language tasks using T5-base, RoBERTa-base, and CLIP-ViT-B/32. Results show consistent improvements over vanilla LoRA (average +1.2% GLUE score, +0.8% SuperGLUE, +0.9% zero-shot CIFAR-10 accuracy), approaching and occasionally exceeding full fine-tuning performance while maintaining parameter efficiency. However, we note that improvements diminish with larger models (T5-large shows only +0.3% average gain), and theoretical justification for the amplification factors remains heuristic. Code and pre-trained adapters are available at [anonymized].",
    "id": 760
  },
  {
    "title": "LoRA-Drop: Structured Pruning for Parameter-Efficient Fine-tuning via Learnable Rank Allocation",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Chaudhari, P."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular approach for parameter-efficient fine-tuning of large language models, but its fixed rank allocation across layers remains suboptimal. We propose LoRA-Drop, a simple method that learns to drop redundant LoRA modules during fine-tuning while maintaining task performance. Our approach introduces a learnable gating mechanism that automatically identifies and prunes low-importance adapter modules based on accumulated gradient information. Rather than requiring extensive hyperparameter tuning, LoRA-Drop uses a single threshold hyperparameter controlled by a validation-based early stopping strategy. We evaluate our method on GLUE and SuperGLUE benchmarks using LLaMA-7B and RoBERTa-large, showing 15-30% parameter reduction with minimal accuracy degradation (averaging 0.8% drop across tasks). Despite achieving reasonable compression rates, our analysis reveals that the learned pruning patterns are dataset-specific and may not generalize across tasks. While LoRA-Drop provides a practical approach to reducing adapter parameters, we acknowledge that performance gains are modest compared to more sophisticated compression techniques, and theoretical justification for the pruning criterion remains limited.",
    "id": 761
  },
  {
    "title": "LoRA-Detox: Parameter-Efficient Fine-Tuning for Bias Mitigation in Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We study whether Low-Rank Adaptation (LoRA), a popular parameter-efficient fine-tuning method for large language models (LLMs), can effectively reduce social biases without full model retraining. Motivated by the computational infeasibility of retraining LLMs at scale, we investigate if injecting low-rank bias correction matrices during fine-tuning suffices to mitigate harmful outputs. Our method adds rank-16 LoRA modules trained on a carefully curated dataset of biased prompts and safe completions, optimizing a combination of standard language modeling loss and bias reduction objectives. Experiments on Bias Benchmark for QA (BBQ) and RealToxicityPrompts show modest but statistically significant improvements: 8-12% reduction in biased responses across three model families (7B-13B parameters) with negligible impact on downstream task performance. However, we find the approach less effective on intersectional biases and observe performance degradation on specific knowledge-intensive benchmarks. While LoRA-Detox offers a lightweight alternative to expensive bias mitigation pipelines, our results suggest fundamental limitations in addressing complex biases through low-rank interventions alone.",
    "id": 762
  },
  {
    "title": "Adaptive Gradient Clipping with Moving Percentiles for Stable Transformer Training",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Gonzalez, J."
    ],
    "abstract": "Transformer training instability often arises from gradient explosions, particularly when using large batch sizes or low-precision arithmetic. While gradient clipping is the de facto solution, existing approaches rely on fixed thresholds that fail to adapt to varying loss landscapes. We propose Adaptive Percentile Clipping (APC), which dynamically sets clipping thresholds based on the p-th percentile of gradient norms over a rolling window. This simple modification requires no hyperparameter tuning beyond choosing p \u2208 [90, 99] and shows consistent improvements across downstream tasks. Experiments on Wikitext-103 and three vision transformer benchmarks demonstrate APC reduces training failures by 23% and achieves 0.3-0.7 BLEU improvements when training with mixed precision. However, our analysis reveals APC provides diminishing returns when combined with existing stabilizers like gradient noise injection or careful initialization. The method's primary value lies in its plug-and-play nature rather than advancing theoretical understanding of training dynamics.",
    "id": 763
  },
  {
    "title": "Progressive Knowledge Transfer via Layer-wise Similarity Matching for Continual Learning",
    "authors": [
      "Mishra, S.",
      "Zhao, L.",
      "Chen, J."
    ],
    "abstract": "Continual learning remains challenging due to catastrophic forgetting when new tasks are introduced sequentially. We propose PKT-LSM, a method that transfers knowledge between tasks by aligning layer-wise activation similarities. Our approach leverages the observation that adjacent layers often exhibit correlated responses across similar tasks. We introduce a similarity matching loss that encourages new task representations to maintain semantic relationships with previous task representations, implemented through an efficient distillation process that scales linearly with task sequence length. On standard benchmarks including Split-CIFAR100 and 5-dataset continual learning, PKT-LSM achieves 3.2-4.7% improvement in average accuracy over baseline methods while maintaining reasonable computational overhead (1.3\u00d7 training time). While our empirical results are consistent with existing rehearsal-free approaches, we find the method particularly effective for sequences with natural similarity between tasks. The theoretical analysis offers limited guarantees beyond specific similarity conditions, and performance degrades significantly when task distributions are highly dissimilar. This work provides a practical approach for scenarios where storage constraints prevent rehearsal methods, though it does not address fundamental limitations of current continual learning paradigms.",
    "id": 764
  },
  {
    "title": "Lightweight Mixture of Experts via Iterative Token Routing Pruning",
    "authors": [
      "Chen, L.",
      "Rodriguez, S.",
      "Kim, J.",
      "Anderson, K."
    ],
    "abstract": "We propose a simple yet effective method for scaling down large mixture of experts (MoE) models while preserving their downstream performance. Our key observation is that the learned token-to-expert routing patterns exhibit strong redundancy across layers, with many experts receiving similar token distributions. We introduce Iterative Token Routing Pruning (ITRP), a two-stage method that first identifies and removes expert modules with highly correlated routing patterns, followed by retraining with a knowledge distillation objective. Our experiments on standard language modeling benchmarks show that ITRP can reduce the number of active experts by 30-50% with minimal perplexity degradation. However, we observe that the effectiveness varies significantly across tasks, with reasoning-intensive benchmarks showing larger performance drops. While our approach provides computational benefits during inference, the required retraining phase introduces additional training overhead that may limit practical adoption. We acknowledge that concurrent work has explored similar directions, and our primary contribution lies in the systematic analysis of routing redundancy patterns and a computationally efficient pruning strategy. Our code and pruned models are available at [URL].",
    "id": 765
  },
  {
    "title": "Gradient Descent with Exponentially Increasing Learning Rates: A Bridge Between Convergence and Chaos",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Thompson, S."
    ],
    "abstract": "While traditional optimization theory recommends decaying learning rates for convergence, empirical observations suggest that certain neural networks benefit from increasing step sizes during training. We propose AgileGrad, a simple modification to standard SGD that exponentially increases the learning rate throughout training. Our theoretical analysis shows that under restricted conditions\u2014specifically for overparameterized linear regression with Gaussian features\u2014AgileGrad converges to a neighborhood of the global optimum with a rate that surprisingly improves for larger increase factors. We demonstrate that on small-scale vision tasks (CIFAR-10 with ResNet-18), AgileGrad achieves comparable performance to SGD+Momentum while requiring 20% fewer training steps. However, performance degrades significantly on larger architectures (ViT-base) and language tasks. Our ablation study reveals that the method is highly sensitive to the initialization scale and batch size. While our theoretical framework provides some justification for these empirical observations, the gap between our analysis assumptions and practical scenarios remains substantial. These results suggest that exponentially increasing learning rates may have niche applications but require careful tuning and are unlikely to replace standard decay schedules for general deep learning optimization.",
    "id": 766
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Towards Stable Multi-Task Learning in Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning with Transformer architectures often suffers from gradient interference, where conflicting gradients across tasks destabilize training. While gradient surgery methods like PCGrad address this by projecting conflicting gradients, we find they discard potentially useful optimization directions. We propose Adaptive Memory Gradient Surgery (AMGS), which stores historical gradient information to identify when seemingly conflicting gradients actually encode complementary structure. Our method maintains a small memory bank of recent gradients per task, using attention mechanisms to reweight gradient contributions based on their alignment with stored directions. Experiments on GLUE and SuperGLUE benchmarks show 2.3% average improvement over standard gradient surgery baselines, with particularly strong gains on low-resource tasks. However, we observe that gains shrink to 0.8% when scaling to larger models (>1B parameters), suggesting diminishing returns in high-capacity regimes. While AMGS introduces minimal computational overhead (<5% training time), the memory requirements scale quadratically with task count, limiting applicability to extremely multi-task settings. Our results indicate that historical gradient information can provide modest but consistent improvements in multi-task stability, though the benefits may saturate as model capacity increases.",
    "id": 767
  },
  {
    "title": "Gradient Surgery Makes Pre-trained Language Models More Robust to Domain Shift",
    "authors": [
      "Liu, M.",
      "Kumar, A.",
      "Chen, S."
    ],
    "abstract": "Large-scale pre-trained language models often struggle with distribution shifts between pre-training and downstream domains. While fine-tuning typically adapts these models effectively, we find that standard gradient descent can overfit to domain-specific features, hurting generalization. We propose Gradient Surgery for Domain Robustness (GSDR), a simple modification to fine-tuning that selectively removes gradient components likely to encode spurious correlations. Specifically, GSDR identifies gradient directions that maximize in-domain loss while minimally affecting held-out domain performance using a small validation set. Experiments on natural language inference and sentiment analysis across 6 domain pairs show GSDR improves average accuracy by 2.3% over standard fine-tuning, with gains concentrated on target domains dissimilar to the source. However, performance remains 5-7% below specialized domain adaptation methods. Ablations reveal the method works best with moderate domain shifts; in extreme shifts, the gradient projections become unreliable. While GSDR is computationally lightweight and implementation requires <20 lines of code, we acknowledge it provides incremental rather than transformative improvements. Our results suggest gradient-level interventions during fine-tuning may offer a practical middle ground between naive transfer and full domain adaptation.",
    "id": 768
  },
  {
    "title": "Improving Gradient Descent with Adaptive Learning Rate Scheduling Based on Gradient Noise Scale",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Noise Scale Scheduling (GNSS), a simple modification to stochastic gradient descent that adjusts the learning rate based on the ratio of gradient variance to norm at each step. While recent theoretical work has shown that gradient noise scale correlates with optimal learning rates, practical algorithms exploiting this relationship remain underexplored. GNSS uses a running estimate of the noise scale to modulate the learning rate while maintaining stochastic gradient descent's computational efficiency. We evaluate GNSS on CIFAR-10, ImageNet, and several reinforcement learning benchmarks. Results show modest improvements over cosine annealing and linear warmup-decay schedules, achieving 0.3-0.8% higher accuracy on vision tasks and 2-5% faster convergence for policy gradients. However, performance gains diminish with well-tuned baselines and careful hyperparameter selection. Our empirical analysis reveals GNSS works best when training under noisy data or small batch sizes, suggesting its primary benefit is increased robustness rather than strict performance gains. The algorithm requires no additional hyperparameters beyond standard SGD. While the theoretical motivation is sound, our method's limited scope of improvement raises questions about the practical significance of gradient noise scale in modern training pipelines.",
    "id": 769
  },
  {
    "title": "Lookahead Gradient Descent with Adaptive Restart for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T.",
      "Johnson, K."
    ],
    "abstract": "We propose Lookahead Gradient Descent with Adaptive Restart (LGAR), a simple modification to the popular Lookahead optimizer that incorporates periodic restart criteria based on gradient statistics. While Lookahead has shown empirical success in deep learning, its convergence properties remain poorly understood for non-convex objectives. LGAR introduces an adaptive restart mechanism that monitors the inner optimizer's gradient alignment, triggering a reset when the fast weights diverge from their slow weight anchor. We prove O(1/\u221aT) convergence for non-convex smooth objectives under standard assumptions, matching the rate of standard SGD despite the two-time-scale update. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet show LGAR achieves competitive performance to AdamW and Lookahead (+0.2-0.7% accuracy), while requiring 15-20% fewer iterations to reach comparable validation loss. However, gains are inconsistent across architectures and datasets, with minimal improvement on transformer models. Theoretically, our analysis relies on bounded gradient norms and does not explain the practical speedups observed. We release a PyTorch implementation with 30 lines of core code.",
    "id": 770
  },
  {
    "title": "LoRA-Prune: Iterative Low-Rank Adapter Pruning for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "While Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, its fixed-rank design often leads to over- or under-parameterization in downstream tasks. We propose LoRA-Prune, an iterative pruning framework that dynamically adjusts the rank of LoRA adapters during fine-tuning. Our method employs a simple magnitude-based pruning criterion coupled with a warm restart mechanism to recover potentially useful pruned dimensions. Through experiments on GLUE and SuperGLUE benchmarks using RoBERTa-base and T5-large, we show that LoRA-Prune achieves comparable performance to standard LoRA while reducing the number of trainable parameters by 15-30%. However, we find that these savings diminish as model size increases, and the computational overhead of iterative pruning sometimes outweighs the parameter reduction benefits. Our analysis reveals that the effectiveness of rank selection varies significantly across tasks and layers, suggesting that more sophisticated pruning criteria may be necessary. While LoRA-Prune provides modest improvements in parameter efficiency, our results indicate that static rank selection remains surprisingly competitive for most downstream applications.",
    "id": 771
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Learning via Historical Gradient Replay",
    "authors": [
      "Liu, K.",
      "Santos, M.",
      "Chen, J.",
      "Thompson, L."
    ],
    "abstract": "Multi-task learning faces a fundamental challenge: conflicting gradients between tasks can destabilize training and degrade performance. While recent gradient surgery methods like PCGrad and GradDrop have shown promise, they make local decisions based only on instantaneous gradients, potentially discarding useful information. We propose Gradient Surgery with Adaptive Memory (GSAM), a novel approach that maintains a small memory bank of historical gradients to inform surgery decisions. By using an adaptive weighting scheme based on cosine similarity between current and past gradients, GSAM identifies and preserves gradient directions that were beneficial in previous iterations but may appear conflicting locally. Our method introduces only a 3% memory overhead and minimal computational cost. We evaluate GSAM on four multi-task vision and NLP benchmarks, showing 4-7% improvements over PCGrad on average F1 scores. However, gains are inconsistent across tasks\u2014GSAM helps most when task conflicts are moderate, with diminishing returns in highly conflicting setups. Analysis reveals that memory size (k=5-10 is optimal) and the adaptive weighting parameter \u03bb (\u03bb\u2208[0.1,0.3]) are critical. While GSAM represents a step toward more principled gradient surgery, our theoretical analysis remains limited to toy settings, and the method's benefits over simpler ensemble approaches are modest.",
    "id": 772
  },
  {
    "title": "Momentum Residual Connections: Fast Convergence at the Cost of Generalization",
    "authors": [
      "Liu, K.",
      "Mukherjee, S.",
      "Kumar, V."
    ],
    "abstract": "Residual connections have become ubiquitous in deep learning, but their training dynamics remain poorly understood. We propose Momentum Residual Connections (MRC), a simple modification that adds a momentum term to skip connections, theoretically accelerating convergence by \u03a9(log(1/\u03b5)). Our analysis shows MRC performs gradient descent in an augmented space where momentum acts as a learnable regularizer. On CIFAR-10/100 and ImageNet, MRC achieves 15-20% faster convergence compared to standard ResNets while maintaining competitive accuracy (within 1-2%) on clean data. However, we observe that MRC exhibits significantly reduced robustness: adversarial accuracy drops by 5-8% and OOD detection degrades by 10-15% AUROC compared to baselines. Through extensive ablations and theoretical justification via neural tangent kernel analysis, we demonstrate this represents a fundamental trade-off between optimization speed and generalization. While our empirical results are intriguing, we acknowledge limitations including evaluation on relatively small datasets and lack of downstream task performance. Our work suggests momentum in residual connections may be a double-edged sword, accelerating training while potentially compromising robustness.",
    "id": 773
  },
  {
    "title": "LoRaMo: Low-Rank Momentum for Training Quantized Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, S.",
      "Kim, H."
    ],
    "abstract": "Quantized neural networks offer significant memory savings but suffer from unstable training dynamics when using standard optimizers. We propose LoRaMo, a simple modification to momentum-based optimizers that maintains low-rank approximations of gradient history. By projecting momentum buffers into a rank-10 subspace updated every K iterations, we reduce memory overhead by 35% while stabilizing training. Our method combines naturally with existing quantization techniques without changing the forward pass. Experiments on ImageNet show LoRaMo enables 4-bit training of ResNet50 to 75.8% accuracy (vs. 76.1% full precision), outperforming AdamW with gradient clipping by 1.2%. The approach is particularly effective for transformer architectures, improving convergence speed by 15% on BERT-base fine-tuning. While our theoretical analysis provides convergence guarantees only for convex objectives, empirical results across 5 vision and NLP benchmarks suggest practical benefits. Our PyTorch implementation requires <50 lines of code. However, benefits diminish for smaller models (<10M parameters) and we observe 2-3% accuracy drops on some tasks compared to full-precision momentum.",
    "id": 774
  },
  {
    "title": "Gradient Surgery with Memory: A Simple Extension to Adam for Improved Transformer Fine-tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Adam-M, a simple modification to the Adam optimizer that incorporates gradient history beyond the exponential moving average. Motivated by observations that transformer fine-tuning exhibits distinct gradient patterns across layers, Adam-M maintains an explicit memory buffer of recent gradients for each parameter group, using lightweight attention mechanisms to weight historical information. While maintaining the computational efficiency of vanilla Adam, our method achieves modest but consistent improvements across GLUE tasks (average +0.8% over baseline) and three vision transformer benchmarks. We provide theoretical analysis showing Adam-M converges under similar assumptions to Adam, though our proof requires bounded gradient assumptions that may not hold in practice. Experiments reveal the benefits are most pronounced in low-data regimes (\u22641k examples) and small models (\u2264100M parameters). However, we observe minimal gains on larger models and standard datasets, suggesting limited scalability. Code and pre-trained checkpoints are available for reproducibility.",
    "id": 775
  },
  {
    "title": "Improving Transformer Efficiency via Hierarchical Token Pruning with Learnable Retention Thresholds",
    "authors": [
      "Chen, L.",
      "Santos, J.",
      "Kumar, V."
    ],
    "abstract": "Self-attention mechanisms in Transformers exhibit quadratic complexity with respect to sequence length, creating computational bottlenecks for long sequences. While previous work has explored static token pruning approaches, we propose Hierarchical Adaptive Token Selection (HATS), which employs learnable retention thresholds that adaptively determine token importance across different model layers. Our method combines a lightweight gating mechanism with hierarchical pruning decisions, enabling dynamic computation allocation based on input complexity. We evaluate HATS on machine translation and language modeling tasks using WMT'14 and Wikitext-103 benchmarks. Experiments show 1.7x speedup during inference with less than 0.5 BLEU or perplexity degradation compared to full attention baselines. However, we observe performance drops on tasks requiring fine-grained reasoning over long contexts. Our analysis reveals that HATS excels at pruning redundant tokens in high-resource settings but struggles with low-resource domains. Code and trained models will be made available upon publication.",
    "id": 776
  },
  {
    "title": "Lipschitz-Constrained Transformers Improve Out-of-Distribution Robustness but Sacrifice In-Distribution Performance",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We investigate whether enforcing Lipschitz constraints on transformer architectures improves robustness to out-of-distribution (OOD) data while maintaining practical performance. Our approach introduces a computationally efficient method for constraining the Lipschitz constant of attention mechanisms through spectral normalization and gradient clipping. We evaluate on vision and language tasks using ImageNet-C, CIFAR-10-C, and GLUE diagnostic datasets. Results show modest OOD improvements (2-3% accuracy gains) but consistent in-distribution performance degradation (5-7% drops) compared to unconstrained baselines. Surprisingly, the benefit varies significantly across domains\u2014vision tasks show clearer robustness gains than NLP tasks. While our method provides theoretical guarantees, the computational overhead (20-30% training slowdown) and performance trade-offs raise questions about practical deployment. Our contributions include: (1) a novel method for Lipschitz-bounded attention, (2) extensive empirical evaluation across multiple domains, and (3) revealing insights about when Lipschitz constraints help versus harm. Code is available [redacted for review].",
    "id": 777
  },
  {
    "title": "Gradient Accumulation Without Momentum: A Second-Order Adaptive Method for Limited-Memory Training",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M.",
      "Andersson, T."
    ],
    "abstract": "Training large neural networks on memory-constrained hardware remains challenging due to the quadratic memory requirements of second-order optimizers. We propose Gradient Accumulation without Momentum (GAM), a novel optimization method that achieves second-order adaptivity while maintaining the memory footprint of standard SGD. GAM approximates the diagonal of the Hessian using accumulated gradient statistics from multiple mini-batches, without storing per-parameter momentum buffers. Our method introduces a lightweight feedback mechanism that dynamically adjusts learning rates based on local curvature estimates computed during backpropagation. Experiments on ResNet-50, Vision Transformer, and BERT demonstrate that GAM achieves comparable convergence to AdamW while using 35% less GPU memory during training. On ImageNet, GAM reaches 76.2% top-1 accuracy (vs 76.5% for AdamW) while enabling training with 20% larger batch sizes on memory-constrained GPUs. However, we find that GAM shows degraded performance on tasks with high gradient noise and requires careful tuning of window sizes. While GAM offers a practical memory-efficient alternative to adaptive optimizers, our theoretical analysis reveals convergence rates that are worse than full second-order methods by a logarithmic factor, suggesting room for improvement.",
    "id": 778
  },
  {
    "title": "Gradient Surgery in Neural Networks: A Systematic Study of Parameter-Wise Pruning During Training",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Dubois, Y."
    ],
    "abstract": "We investigate whether gradient-based pruning of individual parameters during training can achieve compression ratios comparable to post-training magnitude pruning. Our method surgically removes parameters whose gradients consistently fall below a learned threshold across mini-batches, theoretically justified through a connection between gradient flow and information content. Experiments on CIFAR-10/100 and ImageNet show modest compression (20-35%) with negligible accuracy loss, achieving similar performance to iterative magnitude pruning across ResNet and Vision Transformer architectures. However, computational overhead increases training time 2-3\u00d7, and effectiveness saturates for sparsity >40%. Compared to state-of-the-art pruning methods, our approach trades some compression efficiency for training-time flexibility. We release code and pre-trained models, though hyperparameter sensitivity makes exact replication challenging.",
    "id": 779
  },
  {
    "title": "Momentum-Based Gradient Descent with Memory-Efficient Lookahead for Deep Network Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, M."
    ],
    "abstract": "We propose Momentum LookAhead (MoLA), a simple modification to standard SGD with momentum that reduces memory consumption during neural network training by 30-50% while maintaining comparable performance to existing optimizers. Our method combines gradient lookahead with compressed momentum states using a novel quantization scheme for historical gradients. We theoretically show that MoLA achieves O(1/T) convergence for convex objectives and empirically validate performance across image classification tasks on CIFAR-10/100 and ImageNet subsets. Results demonstrate MoLA matches or slightly underperforms (0.2-0.5% accuracy gap) AdamW and SGD with momentum while using significantly less memory, particularly for large language model fine-tuning. The method is straightforward to implement and requires minimal hyperparameter tuning. While the improvement over existing memory-efficient baselines is modest and the theoretical analysis assumes convexity, MoLA provides a practical trade-off between memory constraints and model performance for resource-limited settings.",
    "id": 780
  },
  {
    "title": "Learning with Approximately Invariant Representations via Group-Theoretic Regularization",
    "authors": [
      "Johnson, K.",
      "Mukherjee, S.",
      "Zhao, L."
    ],
    "abstract": "We propose a lightweight regularization technique for enforcing approximate equivariance in neural networks without requiring explicit group structure preservation. Our approach encodes approximate invariance through a penalty term on feature covariance matrices, enabling learning with noisy or incomplete group annotations. The regularizer requires minimal architectural modifications and adds negligible computational overhead. We evaluate on rotation-invariant image classification and time-series forecasting tasks with synthetic symmetries. Experiments demonstrate consistent improvements over standard baselines (2-4% accuracy gains), though results are mixed compared to fully equivariant architectures. Theoretical analysis shows that our regularization provides a bounded approximation to true equivariance under Lipschitz continuity assumptions. While our method achieves reasonable empirical performance with minimal complexity, we observe degradation in settings with strong symmetry requirements. Our work suggests that approximate invariance may suffice for many practical applications, though we acknowledge limitations in extreme transformation regimes.",
    "id": 781
  },
  {
    "title": "Gradient Surgery with Cyclic Learning Rates: A Simple Recipe for Stabilizing Multi-Task Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that destabilize training. We propose a surprisingly simple approach that combines two well-studied techniques: gradient surgery and cyclic learning rates. Our key insight is that existing gradient surgery methods (PCGrad, GradDrop) can fail when tasks have vastly different convergence speeds. By modulating learning rates cyclically, we create natural synchronization points where gradient conflicts are more likely to be resolved. We validate our method on standard multi-task vision benchmarks (NYUv2, Cityscapes, CIFAR-100 splits) and show 2-4% improvements over strong baselines. While our theoretical analysis is limited\u2014we provide only a one-dimensional convergence proof under restrictive assumptions\u2014empirical results suggest the method is robust across hyperparameter settings. The approach is trivial to implement (10 lines of code) and adds negligible computational overhead. However, we acknowledge that improvements are most pronounced in settings with extreme task imbalance, and benefits may diminish for well-balanced tasks. Our code will be released upon acceptance.",
    "id": 783
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Learning Through Historical Gradient Recombination",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that destabilize training, particularly in high-capacity models. While recent gradient surgery methods show promise, they rely solely on instantaneous gradient information, potentially discarding useful historical context. We propose Adaptive Memory Gradient Surgery (AMGS), a simple modification that maintains a compressed history of task gradients using exponential moving averages. During each optimization step, AMGS solves a quadratic program to find the closest gradient update that satisfies both current and historical gradient alignment constraints. Our theoretical analysis shows AMGS provides tighter regret bounds under mild assumptions about gradient correlation. Empirically, AMGS achieves modest but consistent improvements over existing gradient surgery techniques on standard multi-task benchmarks, including 2.3% average improvement on the NYU Depth v2 dataset and 1.7% on PASCAL-Context. However, we observe these gains diminish in very large models (>100M parameters) and certain task combinations. While our method introduces negligible computational overhead, the storage requirements scale linearly with the number of tasks, limiting applicability beyond 5-6 tasks. Our work suggests historical gradient information can provide stability benefits, though the improvements may be smaller than anticipated for many practical applications.",
    "id": 784
  },
  {
    "title": "Adaptive Gradient Clipping with Learnable Thresholds via Meta-Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is essential for training deep networks with exploding gradients, but selecting appropriate clipping thresholds remains largely heuristic. We propose MetaClipped, a method that learns clipping thresholds through meta-optimization on a validation loss. Our approach formulates the clipping threshold as a learnable parameter updated via implicit differentiation through the training trajectory. While simple in concept, we demonstrate improvements over fixed clipping on language modeling and vision tasks, achieving 2-3% better perplexity on Wikitext-103 and 1.2% higher accuracy on ImageNet when training unstable architectures. However, the method introduces computational overhead (15-20% slower training) and hyperparameter sensitivity. Our theoretical analysis shows bounded regret under convexity assumptions, though these conditions rarely hold in practice. Experiments reveal inconsistent gains across tasks, with significant improvements only when training dynamics are severely unstable. Code and hyperparameters will be released.",
    "id": 785
  },
  {
    "title": "Gradient Surgery is All You Need: Improving Multi-Task Learning with Simple Magnitude-Based Reweighting",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to suboptimal performance. While recent approaches like PCGrad and GradNorm use sophisticated gradient manipulation techniques, we propose a surprisingly simple alternative: Magnitude-based Gradient Reweighting (MGR), which rescales each task's gradient based solely on its L2 norm. Our method requires no hyperparameter tuning beyond the standard learning rate, making it practically appealing. We provide theoretical justification showing MGR approximately achieves gradient alignment when task gradients are L-smooth. Extensive experiments on 3 standard multi-task benchmarks (CIFAR-100, NYUv2, and Cityscapes) demonstrate MGR performs competitively with more complex methods, achieving 1-2% improvements over standard multi-task training while maintaining the same computational overhead. However, we observe MGR underperforms on tasks with significantly different gradient scales and can exhibit unstable training dynamics when gradient norms are computed on small batches. Our results suggest that simple gradient reweighting can be surprisingly effective, though careful normalization strategies may be needed for broader applicability.",
    "id": 786
  },
  {
    "title": "Don't Throw Away Your L2 Regularizer: A Lightweight Hybrid Approach for Improving Out-of-Distribution Robustness",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Recent work suggests that traditional L2 weight regularization can harm out-of-distribution (OOD) robustness when training modern deep networks. We re-examine this claim and propose a simple hybrid approach that preserves L2's benefits while improving OOD performance. Our method, HybridNorm, combines L2 regularization with a lightweight spectral normalization layer applied only to the final block of the network. We theoretically show that this preserves the implicit bias of L2 while adding controlled Lipschitz constraints. On CIFAR-10-C and ImageNet-C benchmarks, HybridNorm achieves 2-3% improvements in corruption robustness over standard training with negligible compute overhead (0.2% parameter increase). While these gains are smaller than state-of-the-art adversarial training methods, our approach requires no additional hyperparameter tuning beyond standard L2 weight decay. We also find HybridNorm provides complementary benefits when combined with existing OOD techniques, suggesting practical applications in resource-constrained settings. However, analysis reveals the method's effectiveness is highly dependent on architecture choice and dataset characteristics, with minimal benefits on some benchmarks like CIFAR-100. Code will be made available upon acceptance.",
    "id": 787
  },
  {
    "title": "Adaptive Gradient Clipping with Layer-wise Curvature Estimation for Stable Transformer Training",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Kumar, V.",
      "Johnson, A."
    ],
    "abstract": "We propose Adaptive Gradient Clipping with Curvature (AGC), a method for stabilizing transformer training by dynamically adjusting gradient norms based on layer-wise curvature estimates. While gradient clipping is widely used to prevent exploding gradients, existing approaches use fixed thresholds that fail to account for the heterogeneous curvature across transformer layers. AGC computes per-layer curvature estimates using the Hessian trace approximation via Hutchinson's method, then sets clipping thresholds proportional to the inverse square root of curvature. We evaluate AGC on standard language modeling tasks (WikiText-103, C4) and machine translation (WMT14 En-De), showing 0.3-0.7 BLEU improvements over baseline clipping. Ablations reveal that uniform clipping thresholds can over-restrict gradients in low-curvature layers while insufficiently clipping high-curvature ones. However, our curvature estimation incurs 15-20% computational overhead and shows diminishing returns on smaller models (<100M parameters). The method provides marginal but consistent improvements, suggesting that while curvature-aware clipping is useful, its benefits are most pronounced for training larger transformers (>1B parameters) where existing clipping strategies struggle with stability.",
    "id": 788
  },
  {
    "title": "Gradient Norm Regularization Improves Generalization in GAN Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, A."
    ],
    "abstract": "We propose an explicit gradient norm regularization technique for stabilizing GAN training and improving sample quality. While previous work focuses on implicit regularization through spectral normalization or gradient penalties on the discriminator alone, we demonstrate that applying \u21132 gradient norm constraints to both generator and discriminator leads to more stable optimization dynamics. Our method adds a simple regularization term \u03bb(\u2016\u2207_\u03b8D\u2016\u2082\u00b2 + \u2016\u2207_\u03b8G\u2016\u2082\u00b2) to the standard adversarial loss, which we show enforces local Lipschitz continuity and encourages smoother loss landscapes. We evaluate on CIFAR-10 and CelebA using standard GAN architectures, where our approach achieves competitive FID scores (CIFAR-10: 18.4 \u00b1 0.3 vs. 19.7 \u00b1 0.4 baseline) while reducing training instability as measured by gradient variance and mode collapse frequency. Surprisingly, we find that our regularization scheme also improves performance for WGAN-GP variants, suggesting broader applicability beyond vanilla GANs. Though our theoretical analysis is limited to the case of overparameterized networks, empirical results indicate that gradient norm regularization offers a simple alternative to existing stabilization techniques.",
    "id": 789
  },
  {
    "title": "Gradient Amplification with Quasi-Newton Curvature Estimates for Large Language Model Fine-tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Pre-trained language models require extensive fine-tuning on downstream tasks, but traditional gradient descent methods can be inefficient due to ill-conditioned loss landscapes. We propose GRAQ (Gradient Rescaling Approximate Quasi-Newton), a novel method that approximates curvature information using low-rank updates from the Karakida-Fukumizu eigenvalue spectrum. Our approach rescales stochastic gradients using second-order curvature estimates while maintaining memory efficiency through a sliding window approximation of the Hessian diagonal. We evaluate GRAQ on GLUE tasks using RoBERTa-base and GPT-2 medium, achieving 1.8% average improvement over AdamW baselines on classification tasks, with 12% faster convergence in training steps. However, benefits vary significantly across tasks (-2.1% to +4.3% range), and computational overhead becomes prohibitive for larger models. Our method shows promise for medium-scale fine-tuning scenarios but exhibits instability on tasks with limited training data. Code is available at anonymized-url.github.io/GRAQ.",
    "id": 790
  },
  {
    "title": "Attention is Not All You Need: Adapting Transformer Architectures for Resource-Constrained Settings via Structured Matrices",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Anderson, K."
    ],
    "abstract": "Large transformer models have achieved remarkable success across many domains, but their quadratic complexity in sequence length hinders deployment on resource-constrained devices. We propose SMFormer, a modification to the standard transformer architecture that replaces self-attention calculations with low-rank structured matrices. Our approach leverages Kronecker products and circulant decompositions to reduce memory complexity from O(n\u00b2) to O(n log n) while maintaining competitive performance. We evaluate SMFormer on standard NLP benchmarks (GLUE, SQuAD) and show that it achieves 85-92% of BERT-base performance while requiring 70% less memory on sequences of length 512. However, we observe significant performance degradation on tasks requiring long-range dependencies (e.g., RACE reading comprehension drops from 65.8% to 43.2%). Our method provides a practical middle ground between full attention and linear attention variants, though it lacks the theoretical guarantees of recent sparse attention methods. We release our implementation and pretrained models at [link anonymized].",
    "id": 791
  },
  {
    "title": "ReLU Networks Can Learn Low-Degree Polynomials in Sub-Exponential Time... Sometimes",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Zhou, M."
    ],
    "abstract": "We study the learnability of low-degree polynomials by two-layer ReLU networks trained via gradient descent. While recent work has established polynomial-time convergence for specific polynomial families, our work explores whether these results extend to general polynomials. We prove that gradient descent with random initialization can learn any d-degree polynomial in n variables in time n^{O(d^{1/2} log d)}, provided the initialization scale and learning rate satisfy certain technical conditions. The key insight is a decomposition of the target polynomial into a hierarchy of interactions, where we show the network first learns low-order terms before fitting higher-order ones. Our analysis leverages the neural tangent kernel at initialization but requires a novel non-asymptotic treatment to handle the polynomial approximation error. Experiments on synthetic data demonstrate our bounds are nearly tight for certain polynomial classes, though we observe empirically that convergence depends non-trivially on the polynomial's coefficient structure. While our theoretical framework unifies several previous results, the polynomial dependence on degree implies exponential scaling in worst cases, leaving open whether truly sub-exponential learnability is possible for general polynomials.",
    "id": 792
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Riemannian Perspective with Adaptive Weighting",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Multi-task learning faces optimization challenges when task gradients conflict during training. We propose Riemannian Gradient Surgery (RGS), which projects conflicting gradients onto the tangent space of a learned task manifold, combined with an adaptive weighting scheme based on gradient similarity. Our method incorporates second-order information through an approximation of the Riemannian metric tensor, allowing more principled gradient modification than existing heuristic approaches. Experiments on standard multi-task benchmarks show 2-5% improvements over PCGrad and GradNorm, with particular gains when task conflict is severe. However, we find the computational overhead scales quadratically with the number of tasks, limiting applicability to scenarios with many tasks. While RGS demonstrates consistent improvements, we acknowledge that gains are modest and computational cost may not justify deployment in resource-constrained settings. Theoretical analysis is limited to simplified cases. Code will be released upon acceptance.",
    "id": 793
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Trust Region Clipping for Sample-Efficient Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose a variance-reduced policy gradient method that combines adaptive trust region clipping with control variates for improved sample efficiency in continuous control tasks. Our approach extends proximal policy optimization by incorporating a learned baseline that dynamically adjusts based on local curvature estimates of the policy landscape. The key innovation is an adaptive clipping mechanism that modulates the effective step size based on gradient variance estimates, theoretically grounded in a variance-aware analysis that extends the standard policy gradient theorem. We evaluate our method on a suite of MuJoCO benchmarks and demonstrate 15-25% sample efficiency improvements over PPO on half of the environments, while maintaining comparable performance on the remainder. However, we observe degradation on tasks with sparse rewards, suggesting limitations in our variance reduction strategy when signal-to-noise ratios are low. Theoretical convergence guarantees are provided for the convex case, but extending these results to non-convex policy classes remains an open challenge. Our empirical results, combined with ablation studies showing marginal benefits from individual components, suggest the approach provides incremental rather than transformative improvements for policy optimization.",
    "id": 794
  },
  {
    "title": "Improved Gradient Estimation for Training Diffusion Models with Limited Compute",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training diffusion models remains computationally intensive, particularly when accessing high-end GPUs is limited. We propose a simple modification to the standard denoising objective that reduces gradient variance through a biased low-rank approximation of the Jacobian. Our approach requires only a single additional matrix multiplication per step and can be implemented in 20 lines of PyTorch code. While the theoretical bias introduced by our approximation does not vanish asymptotically, we demonstrate consistent improvements over standard training on CIFAR-10 and ImageNet-32 across 3 different architectures. Specifically, we achieve 5-7% improvement in FID scores when using 50% less training time compared to baseline methods. However, we find these improvements diminish as model size increases beyond 400M parameters. Our method is most effective for practical deployment scenarios where training budgets are moderately constrained. The code and pre-trained models are available at [anonymous link].",
    "id": 795
  },
  {
    "title": "Momentum Residual Networks: A Slight Tweak with Modest Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We revisit the integration of momentum into residual networks by introducing Momentum Residual Networks (MoReNets), a simple architectural modification that adds a momentum term to the skip connections in ResNets. While intuition suggests this could improve gradient flow and training stability, we find the benefits are limited to specific regimes. Our approach adds minimal computational overhead, requiring only a scalar momentum hyperparameter and standard backpropagation. Through extensive experiments on CIFAR-10/100 and ImageNet, we demonstrate consistent but modest improvements over baseline ResNets (0.5-1.2% accuracy gains) when proper hyperparameter tuning is performed. However, these gains diminish or disappear when compared against stronger baselines like ResNet-D or ResNeSt variants. Our theoretical analysis reveals that the momentum term primarily affects optimization in early training phases, providing limited benefits in well-regularized setups. While MoReNets are simple to implement and never harm performance, we acknowledge their contribution is incremental and may not justify adoption beyond standard ResNets. Code is available at github.com/fakeuser/morenets.",
    "id": 796
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning with Task-Specific Noise Adaptation",
    "authors": [
      "Liu, C.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder performance on individual tasks. While existing gradient surgery methods address gradient conflicts through projection schemes, we observe that these approaches neglect the inherent noise characteristics unique to each task. We propose Noise-Adaptive Gradient Surgery (NAGS), which incorporates task-specific noise estimates into the gradient conflict resolution process. Our method computes per-task gradient noise variance using a simple moving average of recent gradients, then weights the projection operation to preserve more reliable gradient directions. Experiments on CIFAR-100/SVHN supervised multi-task and NYUv2 scene understanding benchmarks show 2-3% improvements over standard gradient surgery baselines, with particular gains on harder tasks. However, we find that NAGS provides diminishing returns when task gradients are already well-aligned and adds non-trivial computational overhead (15-20% training slowdown). Our theoretical analysis suggests the noise-based weighting scheme converges to a stationary point under standard assumptions, though the convergence rate depends on unknown noise parameters. While our approach shows promise for scenarios with significant gradient conflicts, the practical benefits appear limited to specific task combinations, raising questions about the broader applicability of noise-aware gradient manipulation.",
    "id": 797
  },
  {
    "title": "Improving Transformer Generalization Through Layer-Wise Learning Rate Scheduling",
    "authors": [
      "Chen, L.",
      "Johnson, M.K.",
      "Rodriguez, A."
    ],
    "abstract": "We propose Layer-wise Adaptive Rate Scaling (LARS), a simple modification to the Adam optimizer that applies different learning rates to different transformer layers based on parameter norm statistics. Our method scales the learning rate for each layer inversely proportional to the ratio of its gradient norm to parameter norm, using exponential moving averages computed during training. We evaluate LARS on small-to-medium scale language modeling tasks (GPT-2 125M-1.5B parameters) and image classification with Vision Transformers on ImageNet. Experiments show modest improvements in validation perplexity (0.05-0.1 reduction) and downstream accuracy (+0.3-0.8% on GLUE tasks), with particularly consistent gains when training data is limited to 10-20% of full datasets. While these improvements are statistically significant across 5 seeds, the effect size remains small. Theoretical analysis suggests LARS reduces the effective Lipschitz constant of the loss landscape, though this connection is heuristic rather than rigorous. Our implementation requires minimal code changes (20 lines) and negligible computational overhead. However, we find LARS does not consistently improve performance at larger scales (6B+ parameters) or with more sophisticated optimizers like AdamW. The approach shows promise for data-constrained regimes but may have limited impact in modern large-scale training.",
    "id": 798
  },
  {
    "title": "Incremental Domain Adaptation through Progressive Feature Augmentation",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Rodriguez, S."
    ],
    "abstract": "Domain adaptation becomes challenging when target distributions evolve gradually over time, yet most existing methods assume abrupt shifts or stationary environments. We propose Progressive Feature Augmentation (PFA), a simple yet effective method for incremental domain adaptation where a pre-trained source model is adapted to gradually shifting target distributions. PFA operates by selectively augmenting features at each time step based on their estimated domain relevance, computed through a lightweight meta-network that learns to predict feature importance from minimal labeled target samples. Our method requires no retraining of the original model and adds only 2.5K additional parameters. Experiments on three standard benchmarks\u2014Rotating MNIST, Portraits, and Office-Home with synthetic drift\u2014show that PFA achieves 4-7% accuracy improvements over naive fine-tuning while using 60% less target data. However, we observe that gains diminish when domain shifts exceed 30\u00b0 rotations or when source and target domains have significantly disjoint label spaces. While our theoretical analysis guarantees monotonic improvement under strict assumptions that rarely hold in practice, we believe PFA offers a practical middle ground between expensive full retraining and fragile zero-shot adaptation.",
    "id": 799
  },
  {
    "title": "Gradient Norm Regularization Improves Generalization in Overparameterized Networks: A Kernel Perspective",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "While gradient regularization is known to improve generalization in neural networks, theoretical understanding remains limited beyond toy settings. We propose Gradient Norm Regularization (GNR), a simple modification to standard training that penalizes the L2 norm of gradients with respect to inputs. Through NTK analysis, we show that GNR encourages solutions with smaller effective capacity in an infinite-width limit, providing theoretical justification for its regularization effects. We validate GNR on CIFAR-10/100 and ImageNet subsets, demonstrating modest but consistent improvements over baselines (1-2% accuracy gains). Our empirical study reveals GNR particularly benefits networks with poor initialization or aggressive data augmentation. However, we observe diminishing returns for well-tuned models and find optimization challenges in deeper architectures. Our results suggest gradient-based regularization operates through kernel approximation effects, offering new theoretical insights while highlighting practical limitations in modern settings.",
    "id": 800
  },
  {
    "title": "Feature Mixup: A Simple Data Augmentation Technique for Improving Generalization in Low-Data Regimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Feature Mixup, a lightweight variant of Mixup that operates in the feature space of pre-trained networks rather than input space. Unlike traditional Mixup which interpolates between raw inputs, our method computes convex combinations of intermediate representations while preserving label interpolation ratios. We demonstrate that this approach yields consistent improvements over standard Mixup across several low-data settings on CIFAR-10, CIFAR-100, and ImageNet subsets, achieving 2-5% absolute gains when training with 5-20% of the full dataset. Feature Mixup retains the simplicity of the original method with no additional hyperparameters beyond the interpolation parameter \u03b1. Experiments across ResNet, DenseNet, and Vision Transformer architectures show robustness to the choice of mixing layer. While the technique shows diminishing returns with abundant data and underperforms other augmentation strategies when combined with strong regularization, we offer theoretical justification through Rademacher complexity analysis and provide empirical evidence suggesting the method acts as a form of implicit ensembling. Our results suggest that feature-space interpolation may be a useful tool for practitioners working with limited data, though we acknowledge the approach lacks clear mechanisms for adapting to full-data regimes.",
    "id": 801
  },
  {
    "title": "LoRA-D: Low-Rank Adaptation with Dynamic Rank Selection via Gating Mechanisms",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method, but selecting the optimal rank remains a tedious hyperparameter search problem. We propose LoRA-D, which augments standard LoRA with learnable gating mechanisms that dynamically determine the effective rank during training. Our method maintains LoRA's computational efficiency while automatically adjusting the rank based on layer-specific importance. Experiments on GLUE and SuperGLUE with RoBERTa-large show LoRA-D achieves comparable performance to the best fixed-rank LoRA in 60% of cases (average improvement of 0.3% on GLUE over standard LoRA), with rank selection converging within the first 1K steps. However, we observe diminishing returns on larger models (\u22657B parameters), where simple rank heuristics nearly match LoRA-D's performance. While our method reduces hyperparameter tuning by eliminating rank search, the computational overhead increases training time by 15%. Our results suggest dynamic rank selection provides modest benefits primarily for smaller fine-tuning tasks, offering practical value in resource-constrained settings but limited impact for state-of-the-art model adaptation.",
    "id": 802
  },
  {
    "title": "Gradient Norm Regularization for Improved Transformer Training Stability",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "While transformer architectures have achieved remarkable success across domains, training instability remains a persistent issue, particularly for larger models. We propose a simple regularization technique that penalizes large gradient norms during training to improve optimization stability. Our method adds a penalty term \u03bb||\u2207\u03b8L||\u2082\u00b2 to the standard training loss, encouraging the optimizer to find flatter local minima. We theoretically analyze this approach using a quadratic approximation of the loss landscape, showing that gradient norm regularization improves the lower bound on the Hessian's minimum eigenvalue. Experiments on Wikitext-103 language modeling and CIFAR-10 image classification with ViT models show consistent improvements in training stability: we observe 15-25% reduction in gradient variance and 8-12% improvement in final perplexity over standard baselines, while requiring only 2% additional compute. However, the benefits diminish for well-tuned learning rates and smaller model sizes. Our results suggest that gradient norm regularization provides a practical but modest improvement to transformer training, particularly in regimes where hyperparameter tuning is expensive or limited.",
    "id": 803
  },
  {
    "title": "Improving Transformer Efficiency via Hybrid Attention with Learnable Sparsity Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "The quadratic complexity of self-attention in Transformers remains a critical bottleneck for processing long sequences. While sparse attention mechanisms have emerged as a promising solution, existing approaches either rely on fixed patterns that may not be optimal for all tasks, or require complex learned routing mechanisms that introduce significant overhead. We propose Hybrid Sparse Attention (HSA), a simple yet effective method that combines fixed local attention with learnable global sparsity patterns. Our key innovation is a differentiable gating mechanism that dynamically activates a small subset of global attention connections based on input content, while maintaining a computational budget of O(n log n). Experiments across language modeling, document classification, and protein sequence prediction show 1.2-1.8\u00d7 speedup over dense attention while preserving 95-98% of the original accuracy. However, our method underperforms domain-specific attention patterns on certain datasets (e.g., 2.3% drop on Long Range Arena), suggesting that our generic sparsity prior may not capture all task-specific structure. Code is available at [URL].",
    "id": 804
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Liu, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that can impede optimization. While existing gradient surgery methods modify all gradients to minimize conflict, we investigate whether selectively modifying only the most conflicting gradients improves performance. We propose Conflict-Aware Gradient Selection (CAGS), which applies gradient surgery only to task pairs whose gradient cosine similarity falls below a learned threshold. Our key insight is that minor conflicts may preserve beneficial task interactions. Through experiments on three standard benchmarks and two new datasets, we show CAGS achieves comparable or slightly better performance (average +0.3% accuracy over PCGrad) while reducing computational overhead by approximately 15%. However, our results reveal threshold sensitivity across datasets, and theoretical analysis suggests the improvement is bounded by the orthogonality of task gradients. Our method provides a middle ground between full gradient surgery and naive multi-task optimization, though we acknowledge its benefits are most pronounced in scenarios with high task similarity. Code and datasets are available at anonymized-link.com.",
    "id": 805
  },
  {
    "title": "Self-Guided Data Augmentation via Learned Transformation Embeddings",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Data augmentation is crucial for improving generalization in machine learning models, but traditional augmentation strategies rely on manually designed transformations that may not align with the intrinsic structure of the data. We propose a novel approach that learns to generate meaningful augmentations by encoding transformations into a latent space guided by the model's own uncertainty estimates. Our method uses a lightweight auxiliary network trained jointly with the main classifier to predict transformation parameters that maximize the model's learning signal. Unlike recent adversarial augmentation techniques, our approach avoids expensive optimization loops by conditioning transformations on learned embeddings that capture task-relevant variations. Experiments on CIFAR-10 and ImageNet show modest improvements (0.8-1.2% accuracy gains) over strong baselines while reducing training time by 30% compared to state-of-the-art augmentation methods. However, we observe diminishing returns on larger datasets and limited effectiveness on domains where the learned transformation prior does not match the true data manifold. While our framework offers a computationally efficient alternative augmentation strategy, extensive hyperparameter tuning is required for each new domain, suggesting the need for more robust priors. Code is available at anonymous-url.github.io.",
    "id": 806
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Autoencoders via Continuous Relaxation Tricks",
    "authors": [
      "Liu, Q.",
      "Johnson, K.",
      "Rodriguez, A."
    ],
    "abstract": "Training discrete variational autoencoders (VAEs) remains challenging due to high-variance gradient estimators of the evidence lower bound (ELBO). While continuous relaxations such as Gumbel-Softmax and straight-through estimators provide biased but low-variance gradients, their combination has not been systematically studied. We propose Reparameterized Straight-Through (RST), a simple hybrid approach that reparameterizes the relaxed distribution during the forward pass but applies a modified straight-through estimator during backpropagation. Our method combines the low variance of continuous relaxations with the controlled bias of straight-through estimators. Experiments on binarized MNIST and language modeling with Penn Treebank show 2-5% improvement in ELBO over Gumbel-Softmax baselines, with particular gains at low temperatures. However, we observe minimal improvement on larger-scale tasks like ImageNet generation. Theoretical analysis suggests our bias term scales with the relaxation temperature, providing a trade-off between bias and variance. While RST offers modest improvements and is easy to implement, its benefits appear limited to scenarios with high-dimensional discrete latents at moderate computational budgets.",
    "id": 807
  },
  {
    "title": "Gradient Descent with Dynamic Learning Rate Scaling: A Simple Heuristic for Faster Convergence",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "We propose Dynamic Learning Rate Scaling (DLRS), a lightweight modification to standard gradient descent that adaptively scales the learning rate based on the relative magnitude of past gradients. Unlike complex adaptive optimizers such as Adam or LAMB, DLRS introduces minimal computational overhead by using a simple exponentially weighted ratio of gradient norms. Our theoretical analysis shows DLRS achieves comparable convergence rates to vanilla SGD on strongly convex functions while requiring no hyperparameter tuning beyond the base learning rate. Empirically, we demonstrate 10-25% speedup in wall-clock time over SGD with momentum on ResNet-50 training on ImageNet and modest improvements on transformer fine-tuning tasks. However, our gains diminish on extremely large batch training and are inconsistent across different network architectures. While DLRS provides a practical alternative to manual learning rate tuning for practitioners, our results suggest the improvements are incremental rather than transformative. Code is available at [anonymized for review].",
    "id": 808
  },
  {
    "title": "SkipGrad: Learning to Skip Tasks for Efficient Multi-Task Optimization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning faces an inherent tension between sharing representations across tasks and avoiding negative transfer. While dynamic architectures have shown promise, current methods either require expensive architecture search or introduce complex routing mechanisms that complicate training. We propose SkipGrad, a simple gradient-based criterion that learns to skip backward passes for tasks that would negatively impact shared representations. Our key insight is that the gradient inner product between task-specific losses reveals when updates would conflict with other tasks. By selectively skipping these updates, SkipGrad achieves comparable or improved performance while reducing training time by 15-40% across standard benchmarks. We evaluate on three multi-task vision datasets (NYUv2, CityScapes, COCO) and a language modeling setup with auxiliary tasks. SkipGrad improves overall performance by 2.3% over traditional multi-task learning baselines while requiring minimal hyperparameter tuning. However, we find the approach offers diminishing returns as task similarity decreases, and theoretical guarantees remain limited. Our method provides a practical middle ground between static sharing and full dynamic architectures, though its applicability to extreme task heterogeneity requires further investigation.",
    "id": 809
  },
  {
    "title": "Improving Transformer Training Stability Through Attention Rollback",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Garcia, M.",
      "Johnson, T."
    ],
    "abstract": "Transformer architectures often exhibit unstable training dynamics, particularly when scaling to larger models or longer sequences. We propose Attention Rollback, a simple regularization technique that intermittently resets attention weights to their initialization values during training. Our method requires no architectural modifications and adds minimal computational overhead. We conduct experiments on language modeling and machine translation tasks across various model sizes (125M to 1.3B parameters). While Attention Rollback reduces gradient norm variance by 15-30% compared to standard training, model perplexity improvements are modest (2-4%) and inconsistent across tasks. Ablation studies reveal that the effectiveness depends heavily on the rollback schedule and initialization scheme, suggesting the benefits may be largely due to modified optimization dynamics rather than fundamental architectural improvements. Theoretical analysis provides limited insight into when and why this approach works. We release our code to facilitate reproduction of these mixed but potentially useful results for practitioners struggling with training stability.",
    "id": 810
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Data-Dependent Priors",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a novel approach for deriving tighter generalization bounds for neural networks by incorporating data-dependent priors into PAC-Bayesian analysis. Motivated by the observation that traditional PAC-Bayesian bounds often fail to explain the generalization behavior of overparameterized networks, we modify the prior to depend on the data's first-order statistics and Hessian information. Our method computes these priors efficiently using a small validation split, avoiding the computational overhead of full Hessian computation through diagonal approximations. We derive new bounds for both fully-connected and convolutional networks that demonstrate a 15-30% improvement over existing PAC-Bayesian bounds on CIFAR-10/100 and ImageNet subsets. While our theoretical results apply to general Lipschitz losses, we show experimental improvements primarily for binary classification tasks with cross-entropy loss. Empirical validation reveals that our bounds remain non-vacuous for networks up to 10M parameters, though they become loose for larger architectures. Though the improvement is incremental rather than transformative, our method provides a practical tool for understanding generalization in moderately-sized networks and opens promising directions for tighter data-dependent analyses.",
    "id": 811
  },
  {
    "title": "LoNGA: Localized Non-Linear Gradient Adjustment for Improved Transformer Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kumar, S."
    ],
    "abstract": "Gradient-based fine-tuning of large pre-trained transformers often struggles with catastrophic forgetting and unstable training dynamics. We propose LoNGA, a simple modification to standard gradient descent that applies localized non-linear transformations to gradients based on their historical statistics. Specifically, LoNGA maintains per-parameter momentum estimates and applies a learned element-wise transformation to suppress large gradient updates in regions where the loss landscape exhibits high curvature. Our method introduces only 0.01% additional parameters compared to standard fine-tuning and can be implemented in <30 lines of PyTorch code. We evaluate LoNGA on GLUE and SuperGLUE benchmarks across five different pre-trained transformer architectures. Results show modest but consistent improvements over AdamW baseline (0.8 average GLUE score increase, 1.2 on SuperGLUE), particularly on tasks with limited training data. Ablations reveal that the non-linear transformation is more effective than simple clipping or normalization, while the localized aspect prevents the method from degrading to standard behavior. Though LoNGA does not outperform recent more complex approaches like Adapters or LoRA, it provides a lightweight alternative requiring no architectural changes. We release code and pre-trained checkpoints at anonymous-url.github.io/LoNGA.",
    "id": 812
  },
  {
    "title": "LayerNorm in Disguise: Revisiting Normalization in Attention Bottlenecks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We investigate whether LayerNorm in transformer attention mechanisms can be replaced with simpler normalization schemes without significant performance degradation. Motivated by the observation that LayerNorm constitutes up to 15% of computational cost in modern transformers, we propose a family of normalization variants that use fixed statistics computed offline from training data. Our approach, termed Statistical Layer Normalization (SLN), replaces learnable parameters with pre-computed running statistics from the final training epoch. Through extensive experiments on language modeling (WikiText-103, C4) and image classification (ImageNet-1K), we find that SLN achieves 97.8% of baseline performance while reducing training time by 8-12%. However, performance drops are more pronounced on smaller datasets (up to 5% on PTB) and certain architectures like GPT-medium. We provide theoretical justification showing SLN approximates LayerNorm under assumptions of stable training dynamics, though these assumptions may not hold in practice. Our results suggest that while SLN can be effective for resource-constrained scenarios, LayerNorm's adaptive nature remains beneficial when computational budgets allow. Code and trained models will be released upon acceptance.",
    "id": 813
  },
  {
    "title": "Gradient Surgery for Cross-Task Interference in Multi-Task Transformer Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Gonzalez, M.",
      "Thompson, K."
    ],
    "abstract": "Multi-task learning with transformer models often suffers from destructive gradient interference, where optimization for one task harms performance on others. We propose Gradient Surgery for Multi-Task Transformers (GSMT), a simple yet effective approach that uses second-order gradient information to identify and selectively remove harmful gradient components during fine-tuning. Unlike prior work that requires complex architectural modifications or task-specific heads, GSMT operates as a lightweight wrapper around standard transformer training. Our method computes the Hessian-vector product for each task's loss and identifies conflicting gradient directions using cosine similarity thresholds. We evaluate GSMT on three diverse benchmarks: GLUE textual entailment, visual question answering, and code summarization. Results show 2-4% average improvements over standard multi-task training, competitive with more sophisticated approaches while adding minimal computational overhead. However, performance gains diminish when tasks are highly correlated or when using larger models (\u22653B parameters). While GSMT provides a practical solution for moderate-scale multi-task problems, limitations include sensitivity to threshold selection and unclear benefits in resource-rich settings.",
    "id": 814
  },
  {
    "title": "Gradient Descent with Momentum Works Even When the Momentum Isn't Optimal: A Non-Asymptotic Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We provide a non-asymptotic convergence analysis for gradient descent with Polyak momentum for strongly convex and smooth objectives. While classical analysis requires the momentum parameter to be set optimally, we show that convergence holds for a broader range of sub-optimal momentum values. Specifically, we prove that any momentum parameter in [1/4, 3/4] achieves \u0398(\u03ba log(1/\u03b5)) convergence rate, matching the optimal rate up to constant factors. Our analysis relies on a novel Lyapunov function that captures the dynamics when momentum is not tuned according to the condition number \u03ba. Empirically, we demonstrate that commonly used heuristic momentum schedules in practice fall within our theoretically justified range. While our contributions are technically correct, they serve primarily as a refinement of known results rather than addressing fundamentally new questions in optimization theory.",
    "id": 815
  },
  {
    "title": "Improving Transformer Training Stability Through Curvature-Aware Weight Initialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Curvature-Aware Initialization (CAI), a simple modification to standard weight initialization schemes for transformers that incorporates second-order information from the loss landscape. Our method computes a local curvature estimate using a small-batch approximation of the Hessian trace, then scales initial weights inversely proportional to layer-wise curvature magnitudes. Experimental results on language modeling tasks with GPT-2 medium (350M parameters) and BERT-base (110M parameters) show CAI reduces gradient norm variance by 23-31% during early training stages compared to Xavier/Glorot initialization. However, while CAI achieves marginally better perplexity on Wikitext-103 (18.7 vs 19.1) and converges 1.2x faster in wall-clock time, these improvements diminish as training progresses. Additional experiments on vision transformers show similar early-training benefits but no consistent downstream task improvements. Theoretical analysis proves CAI converges under standard assumptions, though the curvature estimates introduce a small asymptotic bias. While promising for reducing training instability, our results suggest the benefits of curvature-aware initialization may be limited to specific training regimes and could be achieved through simpler learning rate scheduling.",
    "id": 816
  },
  {
    "title": "BatchNorm in Disguise: An Empirical Study of Normalization's Role in Transformer Training Stability",
    "authors": [
      "Chen, S.",
      "Vasquez, J.",
      "Rodriguez, M."
    ],
    "abstract": "Recent work has shown that removing normalization layers from Transformers can improve performance on certain tasks, contradicting conventional wisdom about their necessity for training stability. We conduct a comprehensive empirical investigation of when and why normalization layers matter in Transformer training. Through systematic ablation studies on 8 NLP and vision benchmarks, we find that normalization-free Transformers can achieve competitive performance only when specific initialization conditions and learning rate schedules are met. We propose a simple modification to the attention mechanism\u2014adding learnable temperature scaling\u2014that recovers 87% of the training stability provided by LayerNorm while adding only 0.03% parameter overhead. However, our analysis reveals that this gain primarily emerges from implicit optimization landscape smoothing rather than controlling internal covariate shift. Experiments on out-of-distribution robustness show mixed results: removing normalization layers hurts robustness to distribution shift by 12.4% on average, but our temperature scaling recovers only 3.1% of this gap. While our findings offer practical guidance for designing low-overhead Transformers, the theoretical underpinnings of why certain architectural choices compensate for normalization removal remain unclear.",
    "id": 817
  },
  {
    "title": "Improving Transformer Efficiency through Layer-Dropping with Learnable Routing",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose SkipRoute, a dynamic layer-skipping mechanism for Transformer architectures that learns to route inputs through a subset of encoder layers at inference time. Our method trains auxiliary routing networks that predict the optimal layer composition for each input, enabling computational savings without modifying the original pre-trained weights. We evaluate SkipRoute on GLUE and SuperGLUE benchmarks using BERT-base and RoBERTa models. Results show 25-40% reduction in FLOPs with minimal accuracy degradation (<1.5% on average), outperforming static layer-dropping baselines. However, we observe significant performance drops on tasks requiring complex reasoning (CoLA drops by 4.7%), and routing networks add 5-8% parameter overhead. Analysis reveals the method primarily skips later layers, suggesting redundancy in deeper representations. While SkipRoute provides practical speedups for deployment scenarios, benefits are inconsistent across tasks and the routing decisions remain difficult to interpret. Code and models are available at [URL].",
    "id": 818
  },
  {
    "title": "Gradient Surgery with Memory: Improving Stability of Multi-Task Learning via Selective Gradient Dampening",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to destructive interference between tasks. While existing gradient surgery methods like PCGrad and GradDrop mitigate this issue, they can over-aggressively drop gradient information, harming learning dynamics. We propose MemGS (Memory-aware Gradient Surgery), which introduces a learnable memory buffer to selectively dampen gradient updates based on their historical alignment patterns. Our approach maintains a running estimate of per-task gradient directions over the past k iterations, then applies dampening factors that preserve aligned components while selectively reducing conflicting ones. We theoretically prove MemGS converges under mild assumptions and show empirically that it achieves better trade-offs between task interference and gradient information preservation. On three standard multi-task benchmarks (NYUv2, CityScapes, and QM9), MemGS outperforms PCGrad by 1.2-2.8% on average while using 15% fewer training iterations. However, we observe these gains diminish in scenarios with high task imbalance, suggesting the method's effectiveness depends on task distribution. Our results indicate MemGS provides marginal improvements over carefully tuned baselines, highlighting the challenge of general gradient surgery methods.",
    "id": 819
  },
  {
    "title": "LoRA-Bench: A Modular Framework for Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning technique, but empirical comparisons across different implementations remain fragmented and task-specific. We present LoRA-Bench, a standardized evaluation framework that systematically benchmarks LoRA variants across 15 diverse NLP and vision tasks. Our framework unifies existing LoRA improvements\u2014rank scheduling, initialization strategies, and gradient normalization\u2014in a modular design that enables controlled ablation studies. Through extensive experiments on models ranging from 125M to 7B parameters, we identify three key findings: (1) optimal rank allocation varies significantly across layers, with later layers requiring 2-4\u00d7 higher ranks, (2) initialization with pretrained weight residuals provides consistent gains over vanilla LoRA across tasks, and (3) combining multiple orthogonal improvements yields diminishing returns beyond a 12% average performance boost. While our results validate individual LoRA enhancements, the overall contribution remains incremental, raising questions about the saturation of rank-based adaptation methods. Code and evaluation scripts are publicly available.",
    "id": 820
  },
  {
    "title": "LoRA-VA: Low-Rank Adaptation with Variance Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, V.",
      "Zhou, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank selection limits performance across varying downstream tasks. We propose LoRA-VA, which introduces an adaptive rank mechanism based on the gradient variance observed during training. By monitoring per-layer gradient statistics, LoRA-VA dynamically increases the rank for layers with high variance while compressing those with stable gradients. Our approach adds only 0.5% additional parameters compared to standard LoRA and requires no hyperparameter tuning for rank selection. Experiments on GLUE benchmarks show LoRA-VA achieves 1.3% average improvement over LoRA baselines, with particularly strong gains on low-resource tasks. However, we observe diminishing returns on larger models (>7B parameters) and some instability when training with high learning rates. While the computational overhead remains modest, future work should address these scalability concerns and explore more sophisticated rank selection strategies. Our implementation is available at [anonymous link].",
    "id": 821
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Unified Framework for Improving Out-of-Distribution Robustness by Modifying Loss Landscape Geometry",
    "authors": [
      "Li, C.",
      "Wang, J.",
      "Rahimi, A.",
      "Kuang, K."
    ],
    "abstract": "We propose a unified framework that combines gradient surgery techniques with sharpness-aware minimization to improve out-of-distribution (OOD) robustness in deep neural networks. Motivated by recent observations that conflicting gradients and flat minima both correlate with OOD performance, our method, SharpMix, simultaneously addresses gradient interference while encouraging flatter loss landscapes. Specifically, we perform per-example gradient projection to resolve conflicting updates, followed by sharpness-aware regularization that penalizes both local curvature and gradient diversity. We evaluate SharpMix on distribution shift benchmarks including ImageNet-C, CIFAR-10.1, and WILDS-FMoW. Experiments show consistent improvements over baseline methods: 2.3% gains on corruption robustness and 1.1% on natural distribution shifts compared to naive fine-tuning. Ablation studies reveal that the gradient surgery component contributes marginally when used alone, while sharpness regularization provides the primary benefit. While our results are encouraging, we observe diminishing returns on larger models and limited effectiveness on certain synthetic corruptions. The method incurs a 1.5\u00d7 computational overhead during training. Code and pretrained models are available at [URL omitted for review].",
    "id": 822
  },
  {
    "title": "LoRA-C: Compressing Low-Rank Adaptation via Structured Pruning for Efficient Fine-Tuning",
    "authors": [
      "Chen, X.",
      "Kumar, R.S.",
      "Zhou, L."
    ],
    "abstract": "Fine-tuning large language models remains prohibitively expensive for many applications. While parameter-efficient fine-tuning methods like LoRA reduce memory requirements by learning low-rank adapter modules, these adapters still incur significant storage costs when deploying multiple task-specific models. We propose LoRA-C, a structured pruning approach for LoRA adapters that achieves 3-5x compression ratios with minimal performance degradation. Our method identifies and removes redundant low-rank matrices using a magnitude-based criterion combined with layer-wise importance scores. We also introduce a dynamic re-ranking mechanism to recover from pruning-induced capacity loss. Experiments on GLUE and SuperGLUE benchmarks using 7B and 13B language models show that LoRA-C achieves 92-96% of full LoRA performance while requiring 70-80% fewer parameters. However, we observe that compression effectiveness varies significantly across tasks, with more complex reasoning tasks experiencing larger drops (up to 8% accuracy loss). While our approach provides practical efficiency improvements, theoretical analysis reveals fundamental limitations in compressing adapters for tasks requiring high-rank adaptation. Code and compressed adapters will be made available.",
    "id": 823
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: A Frequency-Domain Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient interference remains a key challenge in multi-task learning, despite extensive work on gradient projection methods. We propose a frequency-domain approach to gradient surgery that decomposes gradients into low and high frequency components before applying conflict resolution. Our method, called FreqGrad, operates by analyzing the Fourier spectrum of gradient matrices and applying selective projection based on frequency content. While existing gradient surgery techniques focus purely on the spatial domain, we demonstrate that low-frequency gradient components often dominate task conflicts. Experiments on standard multi-task benchmarks (CityScapes, NYUv2) show modest improvements over PCGrad and GradDrop, achieving 0.5-1.2% average relative gains. Theoretically, we prove that under certain assumptions about the loss landscape, frequency-based projection preserves more useful gradient information compared to naive orthogonal projection. However, our analysis reveals that benefits diminish significantly when tasks have highly correlated features or when networks use normalization techniques like LayerNorm. Computational overhead is non-trivial (15-20% increase), which may limit deployment in resource-constrained settings. We believe this work opens interesting questions about the spectral properties of neural gradients, though significant additional work is needed to improve practical utility.",
    "id": 824
  },
  {
    "title": "Improving Transformer Efficiency through Learnable Sparse Attention Patterns with Fixed Memory Budgets",
    "authors": [
      "Chen, K.",
      "Rodriguez, S.",
      "Kim, J.",
      "Thompson, L."
    ],
    "abstract": "We propose FixedBudget Transformer (FBT), a method for reducing transformer memory usage by learning sparse attention patterns within a strict computational budget. Unlike prior sparse attention mechanisms that either use predefined patterns or require dynamic computation, FBT employs a lightweight routing network to predict attention sparsity patterns using only a fixed number of pre-allocated memory slots. Our approach introduces a differentiable top-k selection mechanism that enables end-to-end training while maintaining theoretical memory guarantees. We evaluate FBT on language modeling and machine translation tasks, demonstrating 2.1\u00d7 memory reduction compared to standard transformers at the cost of 0.8-1.3 BLEU score degradation on WMT'14 English-German translation. While our method achieves consistent memory savings, we find that performance drops are more pronounced on tasks requiring long-range dependencies. Experiments suggest FBT is most effective when the memory budget is moderately restrictive (50-70% of full attention), but provides diminishing returns under severe constraints. Code and pre-trained models are available at [url].",
    "id": 825
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Component Pruning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.M.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become standard for adapting large language models, but their fixed-rank design may allocate capacity inefficiently across layers and tasks. We propose LoRA-Drop, a simple extension that dynamically prunes low-rank components during training based on gradient magnitudes. Our method requires no additional hyperparameters beyond standard LoRA and adds minimal computational overhead. Experiments on GLUE and SuperGLUE show LoRA-Drop achieves comparable performance to full LoRA while reducing trainable parameters by 15-30%. While the parameter savings are consistent, we find minimal impact on inference efficiency and the approach shows variable benefits across model scales. We provide theoretical motivation based on low-rank approximation bounds, though our analysis relies on several simplifying assumptions that may not hold in practice. Code is available at [anonymous repository].",
    "id": 826
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its fixed rank selection across all layers remains suboptimal. We propose LoRA-Drop, a simple yet effective approach that dynamically prunes low-rank adapters during training based on gradient magnitude analysis. Our method initializes adapters with a conservative rank budget and iteratively drops the least significant rank components, reducing trainable parameters by 40-60% while maintaining downstream performance. We evaluate LoRA-Drop on GLUE, SuperGLUE, and domain-specific benchmarks using LLaMA-7B and RoBERTa-base. Results show comparable accuracy to standard LoRA (within 1.2% on average) with 25% fewer GPU hours during training. While our method achieves consistent parameter reduction, we observe performance degradation on complex reasoning tasks (e.g., 2.8% drop on BIG-Bench) and find the rank selection mechanism lacks theoretical guarantees. LoRA-Drop provides practical speedups for common fine-tuning scenarios but may require task-specific tuning of drop thresholds.",
    "id": 827
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Autoencoders via Continuous Relaxation with Learnable Temperature",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, J."
    ],
    "abstract": "Training discrete latent variable models remains challenging due to the high variance of gradient estimators. We propose a novel continuous relaxation approach for discrete variational autoencoders that adaptively learns the temperature parameter during training. Our method combines the Concrete distribution with a learnable temperature network that outputs instance-specific temperatures based on the input data. This allows the model to smoothly transition from continuous relaxations to discrete samples without falling into local optima. Experiments on binary MNIST, Omniglot, and Yelp rating prediction tasks demonstrate 5-15% improvement in ELBO compared to standard relaxed training with fixed temperatures. Notably, our approach requires minimal hyperparameter tuning and adds only modest computational overhead. While our theoretical analysis is limited to Gaussian encoder assumptions, empirical results suggest broader applicability. The method shows promise for improving training stability in discrete VAEs, though gains diminish on larger datasets and more complex architectures.",
    "id": 828
  },
  {
    "title": "Revisiting Gradient Accumulation for Memory-Efficient Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "Transformer models have become increasingly memory-intensive, limiting training on consumer GPUs. While gradient accumulation is widely used to simulate larger batch sizes, we provide theoretical and empirical evidence that the standard implementation introduces subtle optimization artifacts that degrade convergence quality, particularly when accumulation steps exceed 16. We propose Fractal Accumulation, a simple modification that maintains the memory efficiency of gradient accumulation while reducing these artifacts through recursive micro-batch normalization. Our experiments on ViT models show 2-4% absolute improvement in final validation accuracy compared to standard accumulation across ImageNet classification and COCO detection tasks. However, the benefits appear limited to models with >50M parameters, and we observe minimal improvements on smaller architectures. Additionally, our method incurs 10-15% computational overhead due to repeated forward passes. While Fractal Accumulation offers modest but consistent improvements for large-scale transformer training, the narrow applicability and computational cost may limit practical adoption.",
    "id": 829
  },
  {
    "title": "Gradient Descent with Momentum Works Better Than We Thought: A Second-Order Analysis of Sharp Minima",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Thompson, M."
    ],
    "abstract": "We revisit the relationship between momentum and generalization in deep neural networks through the lens of sharpness minimization. While recent work suggests SGD with momentum reaches flatter minima than plain SGD, we argue this observation is largely an artifact of step size scaling. Our theoretical analysis shows that under appropriate step size choices, momentum and plain SGD find minima with similar sharpness values, differing only by at most a factor of \u221a(1-\u03b2) where \u03b2 is the momentum coefficient. We verify this empirically on CIFAR-10 and ImageNet, finding surprising agreement between the methods when step sizes are carefully tuned. Our key insight reveals that the apparent flatness bias of momentum stems from implicit parameter re-scaling rather than fundamental algorithmic properties. While our results challenge conventional wisdom about momentum's role in optimization, they also suggest simple heuristics for step size tuning that could improve baseline performance. Experiments show our findings hold across ResNet architectures, though we observe more variability on Vision Transformers, possibly due to their different optimization landscape properties. Our work highlights the importance of proper baseline comparisons when studying implicit regularization.",
    "id": 830
  },
  {
    "title": "LayerNorm Tuning: A Simple Scaling Law for Pre-trained Transformer Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We investigate whether the success of parameter-efficient fine-tuning (PEFT) methods can be attributed to their specific architectural choices or to simpler mechanisms. Through extensive experiments on 12 NLP benchmarks and 5 vision tasks, we find that merely scaling the LayerNorm parameters in frozen pre-trained transformers achieves 92-98% of the performance of full fine-tuning, while using only 0.1% of learnable parameters. Our key insight is that the optimal scaling factor follows a predictable relationship with dataset size and model width, suggesting a simple scaling law for adaptation. While our method matches or exceeds the performance of popular PEFT techniques like adapters and LoRA on most tasks, we observe notable degradation on long-context reasoning benchmarks. Our theoretical analysis reveals that LayerNorm scaling primarily affects the attention patterns, providing a partial explanation for the observed limitations. Though the empirical gains are substantial, the contribution is incremental rather than transformative. All experiments are fully reproducible with our code and pre-trained checkpoints available at [URL].",
    "id": 831
  },
  {
    "title": "Gradient Surgery: A Simple Heuristic for Improving Multi-Task Optimization in Low-Resource Settings",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Multi-task learning often struggles with conflicting gradients that can lead to suboptimal performance across tasks. While recent approaches like PCGrad and GradNorm show promise in high-resource settings, they require significant computational overhead and hyperparameter tuning that limits their applicability. We propose Gradient Surgery (GradSurg), a surprisingly simple heuristic that detects gradient conflicts through cosine similarity and applies selective gradient scaling. Our method requires only one additional hyperparameter and adds negligible computational cost. We evaluate GradSurg on four multi-task benchmarks spanning NLP and vision domains. Results show modest improvements over standard multi-task training (2-4% average task improvement), though gains are inconsistent across datasets. Interestingly, GradSurg performs comparably to more complex baselines while requiring 10x less compute. However, we acknowledge that the method fails to improve performance when task gradients are naturally aligned. Ablation studies reveal that the heuristic's effectiveness is highly sensitive to the similarity threshold. While GradSurg won't revolutionize multi-task learning, it provides a practical baseline for resource-constrained researchers. Code is available at anonymous.url.",
    "id": 832
  },
  {
    "title": "Adversarial Feature Mixup: A Simple Data Augmentation Approach for Improving Robust Accuracy",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Adversarial training remains the dominant approach for achieving robustness against adversarial attacks, but at significant computational cost and with limited transferability across architectures. We propose Adversarial Feature Mixup (AFM), a lightweight data augmentation technique that interpolates latent representations between adversarial and clean examples during training. Rather than computing adversarial examples for each gradient step, AFM generates a small pool of adversarial examples per batch and mixes their features with clean examples using parameter-free interpolation in the penultimate layer. Our experiments on CIFAR-10 and CIFAR-100 show that AFM achieves 43.2% robust accuracy against PGD attacks, comparable to standard adversarial training (43.8%) while requiring 3.2x less training time. However, AFM shows limited effectiveness against stronger attacks like AutoAttack (31.1% vs. 39.4%). Analysis reveals AFM primarily improves robustness for easy-to-learn features, raising questions about the mechanism of robustness. While AFM offers a computationally efficient alternative to full adversarial training, its limitations highlight the need for better understanding of adversarial robustness beyond standard benchmarks.",
    "id": 833
  },
  {
    "title": "LoRA-GD: Low-Rank Gradient Descent for Memory-Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Fine-tuning large language models remains computationally expensive despite recent parameter-efficient methods. We propose LoRA-GD, a simple yet effective approach that applies low-rank approximations directly to gradient updates during fine-tuning. Our method decomposes gradients into low-rank components using an online singular value decomposition, achieving 2-3x memory reduction compared to standard fine-tuning while introducing minimal computational overhead. We evaluate LoRA-GD on instruction tuning and domain adaptation tasks across 7B and 13B parameter models. Results show competitive performance to LoRA on GLUE (average 88.7 vs 89.1) and Super-NaturalInstructions (ROUGE-L 42.3 vs 42.8), with mixed results on domain-specific tasks. Notably, our method sometimes underperforms full fine-tuning by 1-3 points on specialized benchmarks. While the computational savings are clear, the approach adds hyperparameter complexity and shows sensitivity to rank selection. We provide theoretical analysis showing convergence under certain smoothness assumptions, though these may not hold for all architectures. Code and models will be released.",
    "id": 834
  },
  {
    "title": "Spectral Normalization Without Suffering: A Simplified Approach to Lipschitz-Constrained Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a lightweight alternative to spectral normalization that enforces Lipschitz constraints in neural networks through gradient clipping and selective weight decay. While spectral normalization has become standard for stabilizing GAN training and controlling Lipschitz constants, its computational overhead remains significant, particularly for wide networks. Our method replaces the expensive singular value decomposition with a simple per-layer scaling factor derived from historical gradient norms. Experiments on CIFAR-10 and ImageNet show our approach achieves competitive FID scores (9.2 vs 8.8 for baseline spectral norm on CIFAR-10) while reducing training time by 25%. Additionally, we demonstrate improved stability in adversarial training for robustness, with a 2% increase in accuracy under PGD attacks compared to standard training. However, we observe that our method underperforms spectral normalization on more complex datasets like LSUN Bedrooms, and theoretical guarantees are weaker than those provided by exact spectral normalization. The approach is most effective for medium-scale applications where the full benefits of Lipschitz constraints are desired but computational resources are limited.",
    "id": 835
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning via Subspace Projections",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that degrade performance on individual tasks. While existing gradient surgery methods address this by modifying gradients at each optimization step, we show these approaches inadvertently destroy useful information encoded in earlier gradients, leading to catastrophic forgetting. We propose Gradient Surgery with Memory (GSM), a simple method that maintains a low-rank subspace of historical gradients and projects conflicting updates onto this subspace. GSM requires minimal additional computation (O(dr) per step where d is parameter dimension and r is the subspace rank) and adds only 2-4% training overhead. On standard multi-task benchmarks including CIFAR-100 and NYUv2, GSM achieves comparable performance to state-of-the-art gradient surgery methods while showing 15-30% better retention of previously learned tasks when evaluated with our proposed temporal generalization metric. However, our method's benefits diminish when tasks are highly correlated or when subspace rank parameter is not carefully tuned. Code and pretrained models will be released upon acceptance.",
    "id": 836
  },
  {
    "title": "Gradient Descent with Adaptive Step-Size Based on Local Gradient Variance",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose GRADVAR, an optimization method that adapts step-sizes based on local estimates of gradient variance for stochastic gradient descent. Motivated by the observation that gradient variance often correlates with optimization progress, our method computes a running estimate of variance using a small window of recent gradients and scales the step-size inversely proportional to this estimate. We provide theoretical analysis showing convergence under assumptions of Lipschitz continuity and bounded variance, achieving a convergence rate of O(1/\u221aT) for non-convex objectives. Experiments on CIFAR-10 and ImageNet with ResNet architectures demonstrate 5-15% faster convergence compared to Adam and SGD with momentum in early training stages, while performing comparably in final accuracy. However, we observe sensitivity to hyperparameter choices and minor degradation on language modeling tasks. GRADVAR offers a lightweight alternative to adaptive methods without requiring momentum buffers or second-moment estimates, though benefits appear limited to vision tasks with specific architectures. Code is available at anonymous-url.github.io/gradvar.",
    "id": 837
  },
  {
    "title": "LoRA-MoE: Parameter-Efficient Experts via Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA-MoE, a method to reduce the parameter overhead of Mixture of Experts (MoE) models by applying Low-Rank Adaptation (LoRA) to individual experts. While MoE architectures enable scaling language models efficiently, their memory footprint grows linearly with the number of experts. Our approach decomposes each expert's weight updates into low-rank matrices, reducing storage by up to 95% while maintaining expert diversity. We evaluate LoRA-MoE on language modeling and GLUE tasks using models with 8-64 experts. Results show modest improvements over standard LoRA (2-3% better perplexity) while using significantly fewer parameters than full-rank experts. However, we observe that performance saturates beyond 16 experts, suggesting that low-rank constraints limit capacity. Our theoretical analysis reveals a trade-off between expert efficiency and expressiveness, with rank-r updates capturing only O(r) modes per expert. Though LoRA-MoE provides memory savings for deployment, the computational overhead during training due to router complexity remains problematic. Code is available at anonymous-url.",
    "id": 838
  },
  {
    "title": "Gradient Descent with Memory: A Simple Heuristic for Accelerating Neural Network Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a lightweight modification to standard stochastic gradient descent that incorporates a 'memory' mechanism to accelerate convergence in neural network training. Inspired by momentum but seeking to reduce its computational overhead, our method maintains a running exponential average of past updates, but only applies this correction every k steps based on a threshold derived from gradient variance. This simple heuristic aims to capture the benefits of momentum while minimizing additional memory and computation. We evaluate our approach on ResNet-18/50 training on CIFAR-10/100 and ImageNet, as well as transformer fine-tuning on GLUE tasks. Across these benchmarks, our method achieves convergence 10-20% faster than standard SGD and 5-10% faster than momentum SGD in terms of wall-clock time, with comparable final accuracy. Theoretical analysis under convexity assumptions suggests convergence rates similar to vanilla SGD, with improved constants in certain scenarios. While the contribution is incremental and the theoretical justification remains incomplete, practitioners may find the method useful as a drop-in replacement that requires no hyperparameter tuning beyond standard SGD.",
    "id": 839
  },
  {
    "title": "Efficient Gradient Compression via Truncated Polynomial Approximation in Distributed Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed machine learning, but existing methods often sacrifice convergence guarantees for communication efficiency. We propose TOPAZ (Truncated Orthogonal Polynomial Approximation for gradient compressZion), which applies Chebyshev polynomial approximations to gradient vectors before quantization. Our key insight is that gradients follow approximately polynomial distributions in high dimensions, allowing aggressive truncation with controlled error. TOPAZ adaptively selects polynomial degree based on gradient norms, achieving up to 32\u00d7 compression while maintaining theoretical convergence rates. On CIFAR-10 and WikiText-103, we match baseline accuracy with 4-6\u00d7 less communication compared to quantization-only baselines. However, TOPAZ requires additional local computation (15-20% overhead) and shows sensitivity to hyperparameter tuning in some settings. Our analysis provides convergence guarantees for non-convex objectives, though the bound depends on potentially loose assumptions about gradient polynomial structure. While TOPAZ offers practical communication savings on typical workloads, its benefits diminish for extremely sparse gradients or small batch sizes. Code and experiments are available at [anonymized].",
    "id": 840
  },
  {
    "title": "Improved Gradient Clipping for Noisy Optimization Landscapes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient clipping is a widely-used heuristic for stabilizing training of neural networks, particularly in language modeling and reinforcement learning applications. While commonly viewed as a practical trick, we provide theoretical and empirical analysis showing that adaptive gradient clipping can provably improve convergence in optimization landscapes with heavy-tailed gradient noise. We introduce ContourClip, a simple variant that scales the clipping threshold based on gradient history rather than using a fixed value. Our method achieves marginally better wall-clock time than standard clipping without introducing significant computational overhead. Experiments on language modeling (IWSLT14 De-En and Wikitext-103) demonstrate 2-3% BLEU/perplexity improvements over baselines, though gains diminish on larger-scale pretraining tasks. Theoretical results establish convergence rates under the \u03bc-Lojasiewicz condition, extending previous work that assumed bounded gradients. Code is available at [redacted], but reproduction requires careful hyperparameter tuning. While our contribution is incremental and specific to gradient noise regimes, these findings may inform future adaptive optimizers for noisy training settings.",
    "id": 841
  },
  {
    "title": "Gradient Alignment Regularization: A Simple Trick to Improve Transformer Training Stability",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, J.",
      "Kumar, V."
    ],
    "abstract": "Transformer architectures often exhibit unstable training dynamics, particularly when scaling to larger models. We observe that gradient conflicts between different layers correlate strongly with training instability. Based on this insight, we propose Gradient Alignment Regularization (GAR), a lightweight training technique that encourages gradient agreement across layers through a modified loss function. GAR adds a regularization term that penalizes the cosine distance between gradients from adjacent layers, implemented via a simple modification to the backward pass requiring less than 10 lines of code. Our experiments on language modeling tasks show that GAR reduces loss spikes by 23% and improves final perplexity by 2.1 points over baselines on Wikitext-103, while only adding 3% computational overhead. However, benefits diminish on smaller models and certain domains. While GAR is not a silver bullet for training instabilities, it provides a practical tool for researchers and practitioners struggling with unstable transformer training.",
    "id": 842
  },
  {
    "title": "Gradient Surgery for Partially Observed Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We study gradient-based training in neural networks where only a subset of parameters is observable or trainable during optimization, a common scenario in transfer learning and model editing. We propose Gradient Surgery, a simple plug-and-play method that modifies backpropagated gradients by projecting them onto the subspace orthogonal to the frozen parameter directions. While this projection theoretically preserves the gradient flow through the remaining weights, we show this can lead to suboptimal convergence when the frozen and trainable subspaces are strongly coupled. We introduce a soft projection variant using learned interpolation coefficients that adaptively trades off between strict gradient orthogonality and training dynamics. Empirical results on CIFAR-10 and ImageNet transfer tasks demonstrate modest improvements (1-2% accuracy) over naive fine-tuning in low-data regimes, though gains diminish with larger datasets. Our method requires minimal computational overhead and applies to any architecture, but exhibits sensitivity to hyperparameter choices and dataset characteristics. While our theoretical analysis provides some convergence guarantees under strong assumptions, significant gaps remain between theory and practice. This work provides a practical tool for constrained optimization settings but leaves open questions about when gradient surgery is most beneficial.",
    "id": 843
  },
  {
    "title": "On the Effectiveness of Curriculum Learning in Transformer-based Neural Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Curriculum learning has shown promise in various domains, yet its impact on large-scale neural language models remains unclear. We investigate whether principled curriculum strategies improve convergence and final performance of Transformer-based language models trained on standard corpora. Our approach introduces a difficulty metric based on token-level surprisal, constructing curricula that progress from low to high surprisal sequences. Across three Transformer variants (350M, 770M, and 1.3B parameters) trained on OpenWebText, we find that curriculum training provides modest but consistent improvements: 0.8-1.2% perplexity reduction on in-domain text and 2-3% faster convergence during early training phases. However, these gains diminish with larger models and longer training, becoming statistically insignificant for the 1.3B parameter variant. Interestingly, curriculum-trained models show improved robustness to domain shift, maintaining 15-20% better perplexity when evaluated on out-of-domain corpora. Despite these benefits, our results suggest that standard pre-training procedures with sufficient data and compute may render curriculum strategies less critical than previously hypothesized. Our findings provide clarity on when curriculum learning is worth the additional complexity in modern language model training.",
    "id": 844
  },
  {
    "title": "Gradient Norm Regularization Improves Transformer Fine-tuning Stability Under Limited Data",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Abbasi, M.",
      "Rodriguez, P."
    ],
    "abstract": "Fine-tuning large pre-trained transformers on small datasets often leads to unstable optimization and poor generalization. While various regularization techniques exist, their interactions with gradient-based optimization remain poorly understood. We propose gradient norm regularization (GNR), a simple technique that penalizes large gradient norms during fine-tuning. By constraining the L2 norm of gradients with respect to intermediate activations, GNR implicitly limits the effective learning rate without requiring extensive hyperparameter tuning. We evaluate GNR on 8 GLUE tasks using BERT-base and RoBERTa-base models with training sets ranging from 1K to 10K examples. Results show modest improvements over standard fine-tuning (average +0.8% GLUE score, p < 0.05) and stronger baselines like dropout and weight decay. However, benefits diminish as dataset size increases (>10K examples). Ablation studies reveal GNR primarily affects the early training phase, preventing initial gradient spikes that lead to poor local minima. While the method is computationally lightweight and easy to implement, limitations include task-specific hyperparameter sensitivity and unclear theoretical guarantees. Our findings suggest gradient norm regularization provides a practical but incremental improvement for low-resource transformer fine-tuning scenarios.",
    "id": 845
  },
  {
    "title": "Gradient Descent with Lookahead is Secretly Coordinate Descent",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Obermayer, K."
    ],
    "abstract": "We establish a connection between the popular Lookahead optimizer and classical coordinate descent methods. Specifically, we prove that under quadratic loss with appropriate step sizes, Lookahead's inner loop dynamics can be reformulated as an asynchronous coordinate descent update on the outer parameters. This insight leads to a simplified convergence analysis for quadratic objectives, recovering the standard O(1/T) rate while revealing explicit dependence on the inner loop length. Motivated by this analysis, we propose Lookahead-C, a variant that adaptively selects coordinates based on gradient variance estimates. On standard ResNet-50 and LSTM training tasks, Lookahead-C achieves comparable validation accuracy to standard Lookahead (76.8% vs 77.1% on ImageNet) with 15-20% fewer inner loop iterations, though we observe minimal benefit over standard SGD on shallow networks. Our theoretical results hold only for quadratic losses, and we acknowledge the gap between this setting and practical neural networks. While our simplification is largely theoretical, it provides an interpretable lens on Lookahead's behavior and may inform future optimizer design.",
    "id": 846
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Dynamic Module Pruning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M.",
      "Ando, Y."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its fixed architectural choices remain suboptimal across diverse downstream tasks. We propose LoRA-Drop, a simple yet effective approach that dynamically prunes LoRA modules during fine-tuning based on gradient sensitivity scores. Our method begins with an over-parameterized LoRA configuration (rank=128) and progressively drops modules whose gradient norms fall below a learned threshold. Through experiments on GLUE and E2E benchmarks, LoRA-Drop achieves comparable performance to full LoRA (rank=16) while using 34% fewer parameters on average. We further demonstrate that the pruning patterns reveal task-specific architectural preferences, suggesting potential for automated neural architecture search. While our empirical results show consistent improvements over static LoRA baselines, we acknowledge that the computational overhead from dynamic pruning may limit benefits for smaller models. Nevertheless, LoRA-Drop provides a practical pathway for balancing parameter efficiency and task performance without extensive hyperparameter tuning.",
    "id": 848
  },
  {
    "title": "Decoupled Momentum: A Simpler Approach to Adaptive Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We present Decoupled Momentum (DeMo), a lightweight modification to standard momentum-based optimizers that separates the momentum accumulation from the parameter update step. Unlike adaptive methods such as Adam or RMSprop, DeMo maintains the computational simplicity of SGD with momentum while providing some benefits of adaptive learning rates. Our key observation is that by decoupling these operations, we can apply the momentum update to a transformed parameter space without the complexity of maintaining per-parameter second moment estimates. We evaluate DeMo on image classification tasks using ResNet architectures, showing modest but consistent improvements over SGD with momentum on CIFAR-10 (0.8% accuracy gain) and ImageNet (0.3% top-1 improvement). While these improvements are smaller than those achieved by full adaptive methods, DeMo requires minimal implementation changes and no additional hyperparameters beyond standard momentum. Theoretical analysis shows DeMo converges at the same rate as SGD for convex problems. Our experiments suggest DeMo may be most beneficial in resource-constrained settings where adaptive methods are prohibitive, though larger-scale evaluation is needed to quantify its practical utility.",
    "id": 849
  },
  {
    "title": "LoRA\u00b2: Recursive Low-Rank Adaptation for Parameter-Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular method for efficiently fine-tuning large language models by learning low-rank updates to pre-trained weights. However, we observe that LoRA's fixed-rank constraint often underutilizes the intrinsic rank of downstream tasks, while requiring manual rank selection for each layer. We propose LoRA\u00b2, which recursively decomposes the adaptation matrix using nested low-rank approximations, automatically determining effective ranks through a gating mechanism. Our method adds only 0.3% additional parameters compared to standard LoRA while achieving 2-5% improvement on GLUE and SuperGLUE benchmarks over baseline LoRA across various model sizes (350M-7B). Analysis reveals that learned ranks correlate with layer sensitivity, though we find diminishing returns on tasks requiring complex reasoning. While LoRA\u00b2 improves upon LoRA's limitations in rank selection, we acknowledge potential instability when applied to vision transformers and lack theoretical guarantees for the recursive decomposition. Our code is available at [URL].",
    "id": 850
  },
  {
    "title": "Lookahead Gradient Boosting: Trading Computation for Generalization in Gradient-Based Ensembles",
    "authors": [
      "Chen, L.",
      "Srinivasan, K.",
      "Gomez, M."
    ],
    "abstract": "We propose Lookahead Gradient Boosting (LGB), a simple modification to standard gradient boosting that reduces overfitting by taking multiple gradient steps in the direction of future residuals before fitting each weak learner. While vanilla boosting fits learners greedily against the current residuals, LGB computes a short optimization trajectory in residual space and fits against the endpoint, effectively implementing a lookahead mechanism. Our approach requires k\u00d7 more computation per boosting iteration but achieves regularization without shrinking or subsampling. On 12 tabular datasets, LGB improves test AUC by 2-4% over XGBoost with matching hyperparameters, though gains diminish with strong regularization. Theoretical analysis reveals LGB can be viewed as approximate mirror descent with adaptive regularization, providing partial justification for the empirical benefits. However, our regret bounds are loose and the proposed view doesn't fully explain performance gains on non-convex learners. Code is available but experiments were run on a single machine with fixed hyperparameters. While LGB offers a conceptually novel regularization approach, the computational overhead and limited theoretical understanding may restrict practical adoption.",
    "id": 851
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "As distributed training scales to larger models and clusters, communication overhead from gradient synchronization becomes a critical bottleneck. We propose Learned Gradient Quantization (LGQ), a framework that dynamically adjusts the precision of gradient compression based on local curvature estimates. Unlike static quantization schemes, LGQ uses a lightweight meta-network to predict optimal bit-widths for each gradient tensor given loss surface characteristics. Our method achieves up to 4.7\u00d7 communication reduction with minimal accuracy loss on ResNet-50 and BERT-Large training. However, we observe diminishing returns beyond 64 GPUs and significant hyperparameter sensitivity across tasks. While LGQ matches TopK and PowerSGD baselines on standard benchmarks, convergence properties remain theoretically fragile, particularly for non-convex objectives. Our results suggest learned compression schedules can provide practical benefits for medium-scale training, though the approach may require additional stabilization techniques for extreme-scale deployment.",
    "id": 852
  },
  {
    "title": "Mixture of Low-Rank Experts: A Memory-Efficient Approach to Sparse Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Mixture of Low-Rank Experts (MoLoRE), a parameter-efficient fine-tuning method that combines low-rank adaptation with sparse expert routing. While low-rank adaptation (LoRA) reduces memory overhead during fine-tuning, we observe that different downstream tasks benefit from distinct low-rank subspaces. MoLoRE learns a small set of low-rank expert matrices and a routing network that dynamically selects combinations of these experts for each input. Our method maintains the memory efficiency of LoRA while providing more expressive capacity through expert specialization. On GLUE and SuperGLUE benchmarks, MoLoRE achieves comparable performance to full fine-tuning using only 0.1% of trainable parameters, improving over LoRA by 1-2% on average. However, we find the gains are inconsistent across tasks, with minimal improvements on larger models. Analysis reveals significant redundancy among learned experts, suggesting current routing mechanisms may be suboptimal. While MoLoRE offers a practical trade-off between efficiency and performance, our results indicate the need for better expert regularization techniques. Code and pre-trained adapters will be released.",
    "id": 853
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simple but Fragile Solution",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. We propose Gradient Surgery with Adaptive Weighting (GSAW), a method that dynamically reweights task gradients based on their cosine similarity during training. Unlike existing approaches that project gradients onto conflict-free subspaces, GSAW selectively downweights gradients with high conflict while preserving gradient magnitude for aligned tasks. We evaluate GSAW on three standard benchmarks: NYUv2, CityScapes, and a newly collected multi-task dataset from industrial robotics. Our method achieves 2-3% improvement in average task performance compared to naive multi-task baselines, though gains diminish when tasks are naturally aligned. However, we find the method is sensitive to hyperparameters (particularly the similarity threshold) and exhibits a 15% performance drop on out-of-distribution data. While GSAW provides a practical improvement over basic multi-task training and offers interpretable gradient visualizations, the limited scope of evaluation and fragility to distribution shift suggest this may be an incremental rather than transformative contribution. Our implementation is available at [anonymous link].",
    "id": 854
  },
  {
    "title": "Gradient Surgery for Noisy Networks: A Lightweight Approach to Improving Robustness",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Thompson, J."
    ],
    "abstract": "We propose a simple gradient modification technique that reduces the sensitivity of deep neural networks to label noise and adversarial perturbations without requiring architectural changes or expensive adversarial training. Our method identifies and masks gradient components that exhibit high variance across mini-batches during training, effectively performing gradient surgery by removing noisy update directions. The approach adds minimal computational overhead (<3% increase in wall-clock time) and can be implemented in under 50 lines of PyTorch code. Experiments on CIFAR-10/100 and ImageNet subsets show 2-4% improvements in accuracy under label noise rates of 20-40%, and comparable robustness to FGSM attacks as adversarial training while requiring 5\u00d7 less training time. While our method achieves consistent improvements over baselines, we observe diminishing returns on tasks with clean labels and limited effectiveness against stronger attacks like PGD. Theoretical analysis provides convergence guarantees under restricted assumptions that may not hold in practice. Our code and pretrained models are available at [anonymous-link].",
    "id": 855
  },
  {
    "title": "Gradient-Starved transformers: A simple initialization scheme for training with limited data",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S."
    ],
    "abstract": "We propose an initialization scheme for transformer architectures targeted at small-data regimes where standard pre-training is impractical. Motivated by empirical observations that gradient norms in transformer lower layers become vanishingly small during early training on limited datasets, we design an initialization that re-scales attention and feed-forward weights using layer-wise gradient flow estimates. Our approach combines insights from signal propagation theory with a cheap approximation of Fisher information to set layer-wise learning rates. On three benchmark tasks with \u226410K training examples (CIFAR-100 few-shot, E2E NLG, and GLUE diagnostic), our method achieves 3-8% improvements over standard initialization and recent initialization heuristics, while requiring no hyperparameter tuning. Extensive ablations suggest the benefits primarily stem from stabilizing learning dynamics in early layers rather than improving representational capacity. While our initialization provides consistent gains on small datasets, we find diminishing returns as data scales increase. Our approach has limitations: performance gains are smaller for larger transformers (>300M parameters), and the theoretical analysis assumes isotropic data distributions. Code is available at [redacted].",
    "id": 856
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Structured Sparsity in Low-Rank Adaptors",
    "authors": [
      "Chen, Y.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its memory footprint during training remains substantial for billion-scale models. We propose LoRA-Lite, which combines structured sparsity patterns with rank-reduction techniques to further reduce memory usage. Our method applies block-wise sparsity masks to the low-rank matrices while maintaining trainability through a learnable gating mechanism. Experiments on GLUE and SuperGLUE benchmarks using GPT-3 scale models show 40% reduction in peak memory usage compared to standard LoRA, with minimal performance degradation (\u2264 1.5% relative drop). While our approach achieves consistent memory savings across tasks, we observe increased sensitivity to hyperparameter choices, particularly the sparsity ratio and initialization scale. Our analysis suggests the method works best for downstream tasks with sufficient training data (>10k examples), where the sparse structure can be effectively learned. Limitations include reduced efficacy on few-shot scenarios and potential interference with gradient flow in very deep networks.",
    "id": 857
  },
  {
    "title": "LoRA-Plus: An Improved Low-Rank Adaptation Method with Gradient-Based Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, J.H."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular approach for efficiently fine-tuning large language models, but the choice of rank remains largely heuristic. We propose LoRA-Plus, a simple extension that automatically selects rank values by monitoring gradient norms during the initial training phase. Our method maintains LoRA's parameter efficiency while adapts the rank for different layers based on observed gradient statistics. Experiments on the GLUE benchmark show 2-3% improvements over LoRA with fixed ranks across BERT-base, RoBERTa-large, and GPT-2 models. However, these gains diminish on larger models (\u22651B parameters), and we observe instability when ranks are adjusted too aggressively. While LoRA-Plus offers practical benefits for moderate-sized models, our theoretical analysis reveals that the gradient-based selection criterion lacks formal guarantees, and the method introduces additional hyperparameters that may limit its applicability. Code and pre-trained adapters are available at [anonymous URL].",
    "id": 858
  },
  {
    "title": "Revisiting Weight Averaging for Improving Adversarial Robustness",
    "authors": [
      "Liu, M.",
      "Kumar, S.",
      "Rodriguez, A."
    ],
    "abstract": "Adversarial training remains the primary defense against adversarial attacks, but suffers from robust overfitting and computational expense. Inspired by recent work on weight averaging in standard settings, we investigate whether simple weight averaging strategies can improve adversarial robustness without the computational overhead of ensembles. We propose Robust Weight Averaging (RWA), which averages model weights during adversarial training using an exponential moving average. While previous work has explored similar ideas, our key insight is that averaging weights too aggressively can actually hurt robustness by smoothing decision boundaries. We conduct extensive experiments on CIFAR-10 and CIFAR-100 using ResNet-18 and WideResNet-34 architectures. Under \u2113\u221e attacks with \u03b5=8/255, RWA improves robust accuracy by 2-3% over standard adversarial training baselines, but performs comparably to existing regularization techniques like early stopping. However, we observe that the benefits diminish on larger datasets like ImageNet, and our method shows inconsistent improvements across different attack strengths. Our results suggest that while weight averaging can provide modest improvements, it does not address the fundamental limitations of adversarial training. Code is available at [URL omitted for review].",
    "id": 859
  },
  {
    "title": "Locally Regularized Training Improves Small-Batch Convergence in Transformer Language Models",
    "authors": [
      "Liu, J.",
      "Kumar, S."
    ],
    "abstract": "Modern language model training increasingly relies on large-batch optimization for computational efficiency, but this often degrades model quality compared to small-batch training. We propose a simple technique called Localized Gradient Smoothing (LGS) that adds a regularization term during training to retain the optimization benefits of small batches while maintaining large-batch throughput. Our method computes an additional loss term based on the gradient variance within local mini-batches, encouraging stable optimization dynamics. Experiments on GPT-2 small/medium and BERT-base show consistent improvements in perplexity when scaling from batch sizes 512 to 4096, with average gains of 0.7-1.2 perplexity points across datasets. While these improvements are statistically significant (p < 0.01), the absolute gains remain modest compared to the cost overhead of LGS (15-20% training slowdown). Our theoretical analysis reveals that LGS can be viewed as an instance of gradient regularization, though the exact mechanism for its effectiveness remains unclear. Code and models are available at [link omitted for review].",
    "id": 860
  },
  {
    "title": "Gradient Norm Regularization Improves Transformer Fine-Tuning but Only on Smaller Datasets",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We investigate whether controlling gradient norms during transformer fine-tuning can improve downstream task performance. While gradient clipping is widely used for training stability, we propose explicitly regularizing the expected gradient norm as an additional training objective. Our method adds a simple term \u03bb||\u2207L(\u03b8)||\u00b2 to the loss function, encouraging smoother optimization trajectories. We conduct extensive experiments on GLUE, SuperGLUE, and three medical domain datasets using BERT, RoBERTa, and T5 models. Results show consistent improvements on smaller datasets (\u226450k examples), with average gains of 1.2-2.3% accuracy/F1 across tasks, but negligible or negative effects on larger benchmarks. Analysis reveals the regularization primarily helps in low-data regimes by reducing overfitting to spurious correlations. Surprisingly, we find that adaptive optimizers like AdamW already provide implicit gradient control, partially explaining the limited benefits on larger datasets. While our approach is computationally lightweight and implementation requires only 5 lines of code, the strong dependence on dataset size and modest overall improvements suggest gradient norm regularization has narrow applicability. Code and pre-trained checkpoints are available at anonymous-url.github.io.",
    "id": 861
  },
  {
    "title": "Sketch-to-Image Synthesis via Frequency-Aware Progressive Distillation",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a simple yet effective approach for sketch-to-image synthesis that leverages frequency-aware progressive distillation. Our method decomposes the generation process into sequential stages, where high-frequency sketch edges are first translated into low-frequency image structure before progressively adding fine-grained details. We introduce a lightweight frequency-based attention mechanism that operates at multiple scales, enabling more faithful rendering of sketch geometry compared to standard diffusion models. While our approach achieves competitive FID scores on the SketchyCOCO benchmark (FID 28.4 vs. 26.7 for the previous best), we observe that our model particularly excels on human-drawn sketches with simple line styles. Our primary contribution lies in demonstrating that frequency-space conditioning can provide a computationally efficient alternative to full diffusion models for sketch-guided generation. However, we acknowledge that our method struggles with complex scene compositions and unusual texturing. Experiments on three sketch datasets demonstrate 15-20% improvement in user preference for sketch fidelity, though generalization to out-of-domain sketches remains limited. Code and models will be released upon acceptance.",
    "id": 862
  },
  {
    "title": "Fast Proximal Policy Optimization via Curvature-aware Second-order Updates",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Proximal Policy Optimization (PPO) remains a dominant algorithm for reinforcement learning, but its sample complexity often limits practical applications. We propose Curvature-aware PPO (C-PPO), which incorporates second-order information without full Hessian computation by using diagonal approximations of the policy Hessian structure. Our method modifies the clipped surrogate objective with curvature-based trust regions, yielding faster convergence in low-data regimes. Experiments on continuous control benchmarks show 15-30% sample efficiency improvements over PPO on half of the MuJoCo environments, with comparable wall-clock time. However, performance gains diminish in high-dimensional action spaces, and we observe instabilities when applied to discrete action domains. While C-PPO demonstrates clear benefits in specific settings, the approach does not address fundamental PPO limitations in sparse reward scenarios. Our work suggests that lightweight second-order information can enhance policy gradient methods, though the technique faces practical challenges in scaling beyond medium-sized problems.",
    "id": 863
  },
  {
    "title": "Gradual Gradient Compression: A Curriculum-Based Approach to Communication-Efficient Distributed Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient compression is essential for scaling distributed deep learning, but aggressive compression often degrades model quality. We propose a curriculum-based approach that gradually reduces the precision of gradient communication throughout training. Our method starts with full-precision communication and progressively increases compression ratios using a learnable schedule adapted from online learning techniques. On CIFAR-10 and ImageNet, this yields 2-4\u00d7 communication reduction compared to static compression baselines while maintaining within 0.3% accuracy of uncompressed training. However, we observe diminishing returns on larger models, suggesting our schedule may not adapt well to different architectures. Theoretical analysis shows our approach converges at O(T^-1/2) rate under standard assumptions, matching uncompressed SGD. While the gains are modest, our work demonstrates that gradual compression can serve as a practical alternative to carefully tuned static schemes.",
    "id": 864
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: A Second-Order Perspective on Interference Mitigation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient interference remains a fundamental challenge when training multi-task neural networks, with existing first-order mitigation techniques showing limited effectiveness in highly overparameterized regimes. We present a second-order analysis revealing that gradient projection methods primarily address cross-task interference at initialization, while ignoring curvature-induced interference that emerges during training. Our approach, Hessian-Aware Gradient Surgery (HAGS), incorporates local curvature information through an efficient block-diagonal approximation of the Hessian to dynamically adjust gradient projections. Experiments on CIFAR-100 and MultiMNIST demonstrate 5-7% improvements over PCGrad and GradNorm baselines, though gains shrink to 1-2% when controlling for hyperparameter tuning budget. While our theoretical analysis provides convergence guarantees for quadratic objectives, extending results to general non-convex cases remains open. Code and pretrained models are available at [redacted link].",
    "id": 865
  },
  {
    "title": "Gradient Surgery in Stochastic Training: When Less Intervention Improves Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Singh, K."
    ],
    "abstract": "Gradient surgery techniques that modify per-sample gradients during training have gained popularity for mitigating memorization and improving generalization in deep learning. We revisit these methods through the lens of implicit regularization and propose a minimalist variant that selectively intervenes only on gradients with largest per-sample norm ratio. Unlike existing approaches that perform surgery on every sample, our method preserves statistical properties of stochastic gradients while still achieving regularization. We provide theoretical analysis showing our approach minimizes a modified loss that includes an adaptive regularizer, and empirically demonstrate improvements over baseline training on CIFAR-10 and Tiny-ImageNet. However, gains are inconsistent across architectures (ResNet vs Vision Transformer) and can be negative on some datasets (CIFAR-100). Our ablation studies reveal that performance improvements correlate strongly with the proportion of high-norm gradients in the dataset, suggesting the technique's benefits may be problem-dependent. Code and experiments are reproducible with less than 5 GPU days on a single A100.",
    "id": 866
  },
  {
    "title": "Low-Rank Adaptation is Secretly a Kernel Trick: A Unified View of Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) and its variants have emerged as popular methods for parameter-efficient fine-tuning, but their theoretical underpinnings remain poorly understood. We show that LoRA can be characterized as a form of kernel learning where the adapted weights implicitly define a reproducing kernel Hilbert space (RKHS) whose properties depend on the initial pre-trained weights and rank constraint. Our key insight is that the low-rank structure restricts the hypothesis space to a particular class of linear operators, yielding a representer theorem for gradient updates. This perspective unifies several PEFT methods including adapters and prefix tuning under a common theoretical framework. We derive generalization bounds using Rademacher complexity arguments and show how the rank parameter controls bias-variance tradeoffs, analogous to RKHS norm regularization. Empirically, we demonstrate that our theory can predict several observed phenomena, such as why LoRA performs well with smaller training sets. While our bounds are subject to simplifying assumptions and our experimental validation is limited to standard benchmarks, we believe this kernel interpretation provides valuable insight into why low-rank adaptation succeeds and suggests principled ways to select hyperparameters based on data characteristics.",
    "id": 867
  },
  {
    "title": "Variance-Reduced Policy Gradient Methods with Adaptive Second-Order Momentum",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Zhao, K."
    ],
    "abstract": "Policy gradient methods suffer from high variance in gradient estimates, particularly in continuous control tasks. While variance reduction techniques and adaptive optimizers exist, their combination remains underexplored. We propose VAPO (Variance-reduced Adaptive Policy Optimization), which integrates a novel second-order momentum term with recursive variance reduction. Our method computes importance-weighted gradient corrections using a sliding-window estimator, adapting the learning rate based on the evolution of per-parameter gradient curvature. We prove that VAPO achieves O(1/\u221aT) convergence in non-convex settings, matching the optimal rate for stochastic policy gradient methods. Experiments on MuJoCo continuous control benchmarks show 8-15% improvement over PPO and 5-12% over TRPO on 4 of 9 tested environments, though gains diminish in environments with sparse rewards. The variance reduction component provides 2-3\u00d7 faster convergence in early training phases but becomes negligible after ~1M steps. While our theoretical results hold for Lipschitz smooth policies, we observe instabilities when using neural networks with ReLU activations, suggesting our assumptions may be restrictive. Our method adds only 15% computational overhead compared to PPO, making it practical for standard RL workflows. Code is available in supplementary materials.",
    "id": 868
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Stochastic Optimization",
    "authors": [
      "Kim, S.",
      "Okafor, C.",
      "Li, J."
    ],
    "abstract": "We propose Adaptive Gradient Noise Injection (AGNI), a simple modification to stochastic gradient descent that adds scaled Gaussian noise to gradients during training. Unlike previous noise injection methods that use fixed schedules, AGNI adaptively adjusts noise variance based on the magnitude of recent gradients. Theoretically, we show that AGNI converges to first-order stationary points for smooth non-convex functions, though our convergence bounds are similar to standard SGD. Empirically, we evaluate AGNI on CIFAR-10/100 and ImageNet classification tasks using ResNet-18/34 architectures. Results show modest improvements in test accuracy (0.3-0.7%) over SGD with momentum, particularly in low-data regimes and for smaller models. Ablation studies indicate that the adaptive noise schedule is crucial, with fixed noise injection sometimes hurting performance. While the method is computationally efficient and easy to implement, the improvements are incremental and not consistent across all experimental settings. We provide PyTorch code and hyperparameter sensitivity analysis. Our work suggests that adaptive noise injection can be a useful regularization technique, though more investigation is needed to understand when and why it helps.",
    "id": 869
  },
  {
    "title": "Sharpness-Aware Minimization Revisited: A Second-Order Perspective with Improved Generalization Bounds",
    "authors": [
      "Liu, H.",
      "Vasquez, J.",
      "Chen, T."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a popular optimizer for improving generalization in deep learning, yet its theoretical understanding remains limited. We provide a second-order analysis of SAM by connecting sharpness to the trace of the Hessian matrix. Our key insight is that the SAM update can be interpreted as approximately minimizing an upper bound on the generalization error that explicitly incorporates both the Hessian trace and gradient norms. Building on this, we propose Trace-SAM, a variant that directly estimates and minimizes the Hessian trace using Hutchinson's method with automatic differentiation. We derive PAC-Bayesian generalization bounds that depend on the trace of the Hessian rather than the maximum eigenvalue, yielding tighter theoretical guarantees for overparameterized networks. Experiments on CIFAR-10/100 and ImageNet show that Trace-SAM achieves comparable performance to SAM while providing more principled regularization, with gains of 0.3-0.5% accuracy. However, our method incurs a 2-3x computational overhead due to Hessian trace estimation, and we observe limited benefits on smaller architectures. Our results suggest that while second-order information can provide theoretical insights, practical improvements over standard SAM are modest when computational costs are considered.",
    "id": 870
  },
  {
    "title": "Gradient Descent with Momentum: A New Perspective Through Continuous-Time Limits",
    "authors": [
      "Chen, Z.",
      "Liu, Q.",
      "Rodriguez, A."
    ],
    "abstract": "We revisit the classical momentum method from a continuous-time perspective, deriving a modified differential equation that captures the subtle effects of discrete-step updates often neglected in standard analyses. Our approach yields an implicit regularization term proportional to the learning rate and momentum parameter, providing novel insights into why momentum accelerates convergence in certain regimes but may hurt performance in others. We establish finite-time convergence guarantees for our continuous-time limit that match known rates up to constant factors, and validate our theoretical findings on a suite of benchmark optimization problems including synthetic quadratics, MNIST, and CIFAR-10. While our analysis provides a fresh perspective on momentum, we acknowledge that the practical implications remain incremental\u2014our theoretical bounds do not significantly improve upon existing results, and the empirical gains over standard baselines are modest (typically 2-5% improvement in wall-clock time). Nevertheless, our framework offers a principled way to understand momentum's behavior and suggests potential extensions to adaptive methods, which we leave for future work.",
    "id": 871
  },
  {
    "title": "Revisiting Gradient Clipping in Federated Learning: A Second-Order Perspective",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Lopez, M.",
      "Anderson, K."
    ],
    "abstract": "Gradient clipping is widely adopted in federated learning to mitigate the impact of heterogeneous client updates, but its theoretical understanding remains limited. We propose a second-order analysis framework that characterizes clipping's effect on convergence through the lens of Hessian approximation errors. Our approach introduces a clipped Newton-type update that adaptively adjusts clipping thresholds based on local curvature estimates, theoretically reducing convergence time by up to O(\u221a\u03ba) compared to standard clipping in quadratic-like regions. Experiments on standard federated benchmarks (CIFAR-10, EMNIST) demonstrate 8-15% improvement in convergence speed over baseline methods when client heterogeneity is moderate. However, we observe diminishing returns in highly non-convex regions where second-order information becomes unreliable. While our theoretical results hold for quadratic objectives, extending them to general non-convex settings requires additional assumptions that may not hold in practice. Our work provides new insights into when and why gradient clipping helps in federated learning, though the practical impact varies significantly across problem settings.",
    "id": 872
  },
  {
    "title": "Self-Supervised Learning with Noisy Student-Teacher Distillation Requires Very Careful Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "Self-supervised learning has achieved remarkable success through student-teacher architectures, but the sensitivity of these methods to optimization choices remains poorly understood. We investigate how noisy student-teacher distillation behaves under various scheduling strategies and discover that performance is highly sensitive to the rate at which noise is injected into the student network. Specifically, we find that introducing noise too early leads to representation collapse, while introducing it too late results in minimal gains over the baseline. Through comprehensive ablation studies across three vision benchmarks and two language tasks, we demonstrate that a carefully designed curriculum can improve top-1 accuracy by 2-3%, but requires tuning 5-7 hyperparameters that vary significantly across datasets. Theoretical analysis reveals that our scheduling approach helps maintain feature space separability during training, though the assumptions underlying our bounds do not hold for deeper architectures. While our empirical findings provide practical insights for practitioners, the lack of theoretical grounding and computational overhead limit immediate impact. Our results suggest that the promise of robust self-supervised learning through noise injection may be more fragile than commonly believed, highlighting the need for more principled approaches to curriculum design in student-teacher frameworks.",
    "id": 873
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simple Regularization Perspective",
    "authors": [
      "Chen, L.",
      "Ramos, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization. While recent work has proposed sophisticated gradient surgery techniques to address this, we show that most of these approaches can be unified under a simple regularization framework. Specifically, we demonstrate that existing gradient conflict resolution methods are equivalent to adding a quadratic penalty term on gradient disagreement. This perspective allows us to develop a lightweight approach that achieves comparable performance to state-of-the-art gradient surgery techniques with significantly reduced computational overhead. Our method, which we call Regularized Gradient Agreement (RGA), uses a single hyperparameter to control the trade-off between task-specific objectives and gradient alignment. Experiments on standard multi-task benchmarks (CIFAR-100 and NYUv2) show that RGA matches the performance of PCGrad and GradNorm while requiring 2-3x less memory. However, we observe that the benefits are most pronounced when task gradients exhibit high cosine similarity variance (>0.5) and diminish on tasks with naturally aligned objectives. While our theoretical analysis provides a unifying view, it relies on strong assumptions about quadratic approximations that may not hold in practice. Our results suggest that the reported improvements from gradient surgery might be partially attributed to implicit regularization effects rather than sophisticated gradient manipulation.",
    "id": 874
  },
  {
    "title": "Gradient Forking: A Lightweight Alternative to Ensemble Training via Parameter Trajectory Splitting",
    "authors": [
      "Liu, T.",
      "Chen, K.",
      "Rodriguez, J."
    ],
    "abstract": "Deep ensembles achieve state-of-the-art uncertainty calibration but require training multiple independent models, incurring significant computational overhead. We propose gradient forking, a simple technique that creates multiple model variants by splitting parameter trajectories during a single training run. After training for $T/2$ iterations on the standard objective, we create $K$ forked replicas with small random perturbations of the current parameters, each continuing training on a slightly perturbed loss landscape for the remaining $T/2$ iterations. Despite being trained for only half the compute budget of $K$ independent models, we show that gradient-forked ensembles achieve comparable accuracy to standard ensembles on CIFAR-10/100 and ImageNet, while improving uncertainty calibration on out-of-distribution data by 8-15%. We provide theoretical analysis showing that gradient forking increases functional diversity by exploiting the locally ergodic nature of SGD in the later stages of training. However, we find the approach is sensitive to the timing of the fork and performs poorly when forks occur too early. While gradient forking offers a practical trade-off between ensemble quality and training cost, its benefits diminish on larger models and datasets, limiting its applicability to state-of-the-art systems.",
    "id": 875
  },
  {
    "title": "Rethinking Batch Normalization at Initialization via Second-Order Optimization Landscapes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "Batch normalization (BN) has been widely adopted as a default component in deep neural network architectures, yet its theoretical understanding at initialization remains incomplete. We investigate how BN affects the optimization landscape in the early training phase through the lens of the neural tangent kernel's second-order structure. Our theoretical analysis shows that BN induces a data-dependent scaling of the Hessian spectrum, leading to improved conditioning compared to unnormalized networks only when the input covariance satisfies certain spectral decay properties. We validate these findings experimentally on CIFAR-10 and ImageNet subsets, demonstrating 5-12% faster initial convergence under our predicted conditions. However, we observe these benefits diminish for deeper architectures (>50 layers) and highly imbalanced datasets. While our analysis provides new theoretical insights into BN's initialization behavior, the practical implications for architecture design are limited to specific regimes. We release our code and experimental configurations for reproducibility.",
    "id": 876
  },
  {
    "title": "Memory-Efficient Training of Transformers via Structured Attention Pruning with Theoretical Guarantees",
    "authors": [
      "Chen, L.",
      "Mukherjee, S.",
      "Johnson, K."
    ],
    "abstract": "Training large transformer models faces memory bottlenecks due to quadratic attention complexity. While various pruning methods exist, they lack principled guarantees and often hurt convergence. We propose Structured Sparse Attention (SSA), a memory-efficient training framework that dynamically prunes attention heads using importance scores derived from gradient information. SSA maintains training stability by ensuring layer-wise Lipschitz constants remain bounded via partial attention matrix recovery. We prove that SSA preserves gradient flow and maintains convergence rates within a constant factor of dense training under mild assumptions about sparsity patterns. Experiments on BERT-base and GPT-2 medium show 1.8-2.4\u00d7 memory reduction with <3% perplexity increase. However, we observe that memorization-intensive tasks (like fact retrieval) suffer disproportionately, with 8-12% performance degradation. Our analysis reveals this stems from systematic removal of heads capturing rare token patterns. While SSA enables training larger models on single GPUs, the trade-offs between efficiency gains and performance loss may limit broader applicability. Code and pre-trained checkpoints are available at anonymous.url.",
    "id": 877
  },
  {
    "title": "Gradient Descent with Momentum is Provably Better than Adam at Minimizing Sharpness in Two-Layer ReLU Networks",
    "authors": [
      "Chen, L.",
      "Ivanov, D.",
      "Vaswani, A."
    ],
    "abstract": "We provide theoretical and empirical evidence that momentum-based gradient descent achieves better sharpness minimization compared to adaptive methods in overparameterized two-layer ReLU networks. Our theoretical contribution establishes a non-asymptotic convergence bound for momentum SGD on sharpness-related objectives, showing O(1/\u221aT) convergence for a modified sharpness measure we term effective local sharpness. While our bound depends on a strong assumption about initialization scale, our experiments on CIFAR-10 and ImageNet subsets demonstrate consistent sharpness reduction with momentum compared to Adam and RMSprop across various architectures. Experimental results show momentum achieves 15-20% lower sharpness scores after 100 epochs with comparable test accuracy. Our work suggests that the benefits of momentum in deep learning may extend beyond convergence speed to generalization through sharpness minimization. However, extending our theoretical results beyond two-layer networks remains challenging, and our empirical evaluation is limited to vision tasks. We provide code and a minimal reproduction package focusing on the sharpness evaluation protocols.",
    "id": 878
  },
  {
    "title": "Iterative Gradient Compression with Adaptive Residual Learning for Distributed Training",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Communication bottlenecks remain a key challenge in distributed deep learning, particularly when training large models across commodity networks. We propose Iterative Gradient Compression with Adaptive Residual Learning (IGCARL), a simple yet effective approach that combines block-wise gradient sparsification with adaptive residual accumulation. Unlike existing top-k or quantization methods that operate on full gradients, IGCARL iteratively compresses gradient blocks using learned importance scores derived from historical gradient norms, while maintaining residual information across training rounds. Our method requires minimal hyperparameter tuning and introduces less than 0.1% additional computational overhead. We evaluate IGCARL on standard benchmarks including ImageNet and WMT'14, demonstrating 3-5\u00d7 communication reduction compared to uncompressed baselines with less than 1% accuracy degradation in most settings. Theoretical analysis shows convergence under standard assumptions, though with looser bounds than comparable methods. While IGCARL achieves competitive performance to recent compression techniques, we observe that gains diminish in high-bandwidth environments or with smaller models. Our code will be made available upon publication.",
    "id": 879
  },
  {
    "title": "Momentum Residual Networks: A Slight Modification with Marginal Gains",
    "authors": [
      "Liu, K.",
      "Abdullah, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Momentum Residual Networks (MoResNets), a simple modification to standard residual networks that incorporates momentum-like connections across residual blocks. While residual networks have achieved remarkable success across vision tasks, we observe that the vanishing gradient problem persists in very deep networks (1000+ layers) with certain initialization schemes. Our approach uses a learnable momentum coefficient to create weighted skip connections that blend inputs from previous blocks, theoretically guaranteeing better gradient flow for arbitrarily deep networks. We derive conditions under which MoResNets maintain Lipschitz continuity and show that our modification adds minimal computational overhead (2-3% increase in parameters). Experiments on CIFAR-10/100 and ImageNet demonstrate improvements of 0.3-0.7% over baseline ResNets of comparable depth, with the largest gains observed in the 200-500 layer regime. However, we find that these benefits diminish with modern initialization techniques and residual block designs. While MoResNets provide theoretical guarantees and modest empirical gains, we acknowledge that the practical impact is limited given the current state of network architectures. Code is available at [redacted for review period].",
    "id": 880
  },
  {
    "title": "Improving Transformer Efficiency via Iterative Token Dropping with Learned Importance Thresholds",
    "authors": [
      "Chen, L.",
      "Vaswani, A.K.",
      "Kumar, S."
    ],
    "abstract": "Transformer models suffer from quadratic complexity in sequence length, limiting their applicability to long documents. While recent work has explored token dropping to reduce computational cost, existing methods rely on heuristics or static thresholds that may discard important information. We propose Adaptive Token Pruning (ATP), a lightweight mechanism that learns dynamic importance thresholds for token dropping at each layer. ATP introduces a parallel scoring network that computes per-token saliency scores, combined with a learned gating mechanism that determines drop decisions based on both local context and global sequence statistics. Our method requires only 0.3% additional parameters and integrates seamlessly with existing pretrained models. Across language modeling tasks on C4, OpenWebText, and arXiv datasets, ATP achieves 1.4\u00d7 speedup during inference with <2% perplexity degradation compared to full transformers. However, we observe that performance drops significantly (>8% accuracy loss) on tasks requiring fine-grained reasoning over long contexts. We demonstrate ATP's limitations through careful ablation studies and provide theoretical analysis showing that certain attention patterns cannot be preserved under aggressive pruning. Our code and models are available at anonymized-link.",
    "id": 881
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Frequency Domain Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning with conflicting gradients remains a fundamental challenge in training shared representations. While existing gradient surgery methods focus on gradient magnitude and direction in the spatial domain, we explore an unexplored frequency perspective. We propose FreqGradSurgery, which decomposes per-task gradients into frequency components using discrete Fourier transforms and performs conflict resolution at different frequency bands. Our key insight is that gradient conflicts manifest non-uniformly across frequencies, allowing more selective interference removal. Experiments on standard multi-task benchmarks (CIFAR-100, NYUv2, Cityscapes) show 2-4% improvements over PCGrad and GradNorm baselines, with particularly strong gains on tasks where high-frequency features dominate. However, we observe these gains diminish on larger-scale datasets and tasks with low-frequency dominance. Theoretically, we establish convergence bounds showing FreqGradSurgery maintains O(1/T) rates under standard assumptions, though with worse constants than vanilla gradient descent. Our method introduces minimal computational overhead (<5% increase in wall-clock time) but requires careful hyperparameter tuning for the frequency band partitioning. While not achieving state-of-the-art performance, our work provides a new lens for analyzing gradient conflicts and suggests frequency-aware optimization merits further investigation.",
    "id": 882
  },
  {
    "title": "LoRA-TTA: Test-Time Adaptation for Parameter-Efficient Fine-Tuning via Low-Rank Updates",
    "authors": [
      "Chen, L.",
      "Mukherjee, A.",
      "Rodriguez, J."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become standard for adapting large language models, but their fixed nature limits performance when test distributions shift. We propose LoRA-TTA, a lightweight test-time adaptation framework that continues to update only low-rank matrices during inference while maintaining memory constraints. Our method introduces a Taylor-approximated optimization objective and momentum-based gradient accumulation to stabilize adaptation across diverse downstream tasks. Theoretical analysis shows our approach converges under mild assumptions when distribution shifts are bounded by O(\u03b5). Experiments on four benchmarks (GLUE, DomainNet, CIFAR-10C, and WMT'14) demonstrate 2-5% improvement over frozen LoRA baselines with minimal computational overhead (\u226415% increase in FLOPs). While effective for moderate distribution shifts, we observe performance degradation under severe covariate shift (dTV > 0.3) and find limited benefit when LoRA rank exceeds 64. Our code is available at anonymous-link.",
    "id": 883
  },
  {
    "title": "Gradient Surgery with Memory: Alleviating Catastrophic Forgetting in Multi-Task Transformers via Parameter-Specific Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "Multi-task learning in transformers often suffers from destructive gradient interference, leading to catastrophic forgetting and suboptimal performance across tasks. While recent gradient surgery methods address this by projecting conflicting gradients, they treat all parameters uniformly and lack mechanisms to retain previously learned knowledge. We propose a memory-aware gradient surgery approach that assigns parameter-specific learning rates based on their historical gradient coherence patterns. Our method maintains an exponential moving average of task-specific gradients to identify parameters that have consistently served multiple objectives versus those causing interference. During optimization, we apply standard gradient surgery only to conflicting parameters while freezing or using reduced learning rates for stable ones. Experiments on GLUE and three proprietary e-commerce datasets show 3.2% average improvement over vanilla multi-task training and 1.8% over PCGrad, though gains diminish with larger models (\u22653B parameters). Analysis reveals improvements primarily stem from reduced forgetting on low-resource tasks rather than better feature sharing. While our method adds minimal computational overhead (5%), it requires tuning a sensitivity threshold and performs inconsistently across task combinations. Code will be released upon acceptance.",
    "id": 884
  },
  {
    "title": "Adaptive Gradient Clipping with Feedback: A Simple but Effective Approach for Stable Transformer Training",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Anderson, J."
    ],
    "abstract": "Training stability remains a persistent challenge for large transformer models, particularly when scaling to longer sequences and deeper architectures. While gradient clipping is widely employed, existing methods use fixed thresholds that fail to adapt to varying loss landscapes during training. We propose Adaptive Gradient Clipping with Feedback (AGCF), a lightweight modification that dynamically adjusts clipping thresholds based on gradient statistics from previous iterations. Our method incorporates a momentum-based estimate of gradient variance and employs a conservative update rule to prevent catastrophic updates. We evaluate AGCF across various transformer architectures on language modeling tasks (WikiText-103, C4) and vision transformers on ImageNet. While AGCF demonstrates improved training stability compared to standard clipping (53% reduction in gradient spikes), we observe only modest improvements in final perplexity (3.2% relative gain) and no significant benefits on downstream tasks. Our ablations reveal that the effectiveness of AGCF diminishes with proper hyperparameter tuning of existing methods. These results suggest that while AGCF provides practical benefits for practitioners facing training instability, the fundamental limitations of gradient clipping approaches may require more sophisticated optimization landscapes for substantial gains. Code is available at anonymous.github.io/agcf.",
    "id": 885
  },
  {
    "title": "Gradient Descent with Adaptive Restart via Local Curvature Estimation",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Garcia, M."
    ],
    "abstract": "While adaptive optimization methods like Adam and RMSprop achieve excellent practical performance, their theoretical understanding remains limited. We propose Curvature-based Adaptive Restart (CAR), a simple modification to gradient descent that uses cheap Hessian-vector products to estimate local curvature and trigger restarts when the optimization landscape becomes ill-conditioned. Our key insight is that the Rayleigh quotient of stochastic gradients provides an unbiased estimator of sharpness that can be computed in O(d) time. We prove that CAR achieves a convergence rate of O(\u221a(\u03ba)/T) for \u03ba-quadratic problems, matching standard gradient descent while avoiding the need for explicit learning rate tuning. Experiments on CIFAR-10 and ImageNet with ResNet architectures show 5-15% faster convergence compared to vanilla SGD+momentum, though gains diminish with sophisticated learning rate schedules. Our method introduces a single hyperparameter (restart threshold) that transfers well across tasks. However, we observe that CAR provides limited benefits on problems where the Hessian spectrum changes slowly. Our implementation requires only 15 lines of PyTorch code and minimal memory overhead.",
    "id": 886
  },
  {
    "title": "Gradient Descent with Learned Step Sizes: A Meta-Optimization Approach for Quadratic Objectives",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Learned Step Size Gradient Descent (LSSGD), a meta-optimization framework that learns optimal step sizes for gradient descent on quadratic objectives. While existing adaptive methods like Adam and RMSprop use heuristics based on gradient statistics, LSSGD trains a small neural network to predict step sizes given gradient directions and local curvature information. Our key insight is that for quadratic functions, optimal step sizes can be characterized by the ratio of gradient norm to curvature along the descent direction, which can be approximated efficiently using Hessian-vector products. We evaluate LSSGD on synthetic quadratic problems and two real-world applications: logistic regression and matrix completion. On 100-dimensional quadratic objectives, LSSGD achieves convergence in 35% fewer iterations than tuned SGD, while maintaining generalization to problems with different eigenvalue distributions. However, we find that LSSGD offers diminishing returns for high-dimensional problems (\n000 dimensions) and struggles with ill-conditioned objectives. Our results suggest that learning step sizes is most beneficial for medium-scale problems with moderately varying curvature. While our approach shows promise in specialized settings with repeated similar objective structures, the overhead of meta-learning may not justify the modest gains over carefully tuned baselines for general optimization tasks.",
    "id": 887
  },
  {
    "title": "Improving Transformer Training Stability through Adaptive Learning Rate Warmup in Low-Resource Settings",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Anderson, K."
    ],
    "abstract": "Recent work has shown that transformer architectures struggle with training instability when applied to low-resource datasets (\u22641M examples), particularly in natural language understanding tasks. We propose Adaptive Warmup Length (AWL), a simple modification to standard transformer training that automatically adjusts warmup duration based on gradient statistics during the first few hundred steps. Our method requires only 2 additional hyperparameters and negligible computational overhead. We evaluate AWL on 8 low-resource text classification datasets from diverse languages and domains. Results show modest but consistent improvements: 1.2-2.3% average accuracy gains over strong baselines, with particular benefits on datasets containing noisy labels. However, the improvements diminish on more complex generative tasks like machine translation, where we observe negligible gains over tuned baselines. While AWL provides a practical improvement for practitioners working with limited data, our theoretical analysis reveals the method's benefits are fundamentally limited by the signal-to-noise ratio in small datasets. Code and models will be made available.",
    "id": 888
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: A Kernel Regime Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "The Training Dynamics of Gradient Descent (GD) in overparameterized neural networks exhibit rich phenomena that challenge classical optimization theory. We propose Gradient Surgery (GS), a simple modification to GD that selectively masks gradient components based on the alignment with the Neural Tangent Kernel (NTK) features. Our approach connects to recent work on the kernel regime, but deviates from the strict NTK limit by allowing controlled feature learning through selective gradient updates. We prove that GS converges linearly for two-layer networks under standard assumptions, though our theoretical bounds are tighter only in specific parameter regimes. Empirically, GS achieves comparable performance to standard training on CIFAR-10 and ImageNet subsets while reducing the effective rank of the feature covariance matrix by 15-30%. However, we observe diminishing benefits as network width increases, with GS performing comparably to standard GD for very large networks. Our work suggests that targeted gradient manipulation can provide modest improvements in sample efficiency when networks operate near the kernel-to-rich transition, though the practical impact remains limited compared to architectural innovations. Code and pre-trained models are available at anonymous.org/gradient-surgery.",
    "id": 889
  },
  {
    "title": "Adaptive Momentum via Gradient Variation Clipping for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Variation Clipping (GVC), a simple modification to standard momentum-based optimizers that adapts the momentum coefficient based on local gradient variation. Intuitively, when gradients exhibit high variance, GVC reduces momentum to prevent overshooting; when gradients are stable, it allows faster convergence. Formally, we derive a clipping threshold that ensures convergence for smooth non-convex objectives, matching the O(1/\u221aT) rate of stochastic gradient descent. Experiments on CIFAR-10/100 and ImageNet show modest improvements over SGD+Momentum and AdamW: 0.3-0.7% absolute accuracy gains on ResNet-18/50 and 1.2% on Vision Transformer, though benefits diminish with careful hyperparameter tuning. Ablations reveal the variation-based adaptation contributes more than the clipping mechanism itself. While GVC provides no theoretical breakthrough, its simplicity and plug-and-play nature could benefit practitioners seeking robust optimization without extensive tuning. Code and pretrained models are available.",
    "id": 890
  },
  {
    "title": "Gradient Surgery with Memory: Improving Multi-Task Learning Through Selective Gradient Replay",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to suboptimal performance across tasks. While recent approaches like PCGrad and GradDrop provide sophisticated gradient projection techniques, they fundamentally discard potentially useful gradient information. We propose Gradient Surgery with Memory (GSM), a method that stores discarded gradient components in a memory buffer and selectively replays them when they become less conflicting with other tasks. Our approach introduces a lightweight conflict detection mechanism based on cosine similarity thresholding, combined with a reservoir sampling strategy for memory management. We evaluate GSM on standard multi-task vision benchmarks including NYUv2 and Cityscapes, achieving modest improvements over PCGrad (average delta +0.8% on primary metrics) while adding minimal computational overhead (5% increase in training time). However, we observe that gains are inconsistent across task combinations and largely disappear when tasks are well-aligned. We provide empirical evidence that GSM's benefits correlate strongly with intrinsic task conflict levels, suggesting the method may have limited applicability beyond explicitly conflicting scenarios. Code and experiments are available at [anonymous link].",
    "id": 891
  },
  {
    "title": "Gradient Compression via Adaptive Structured Pruning for Communication-Efficient Distributed Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Liu, J."
    ],
    "abstract": "Communication bottlenecks severely limit the scalability of distributed machine learning, particularly in bandwidth-constrained environments. While gradient compression techniques like quantization and sparsification reduce communication overhead, they often struggle to maintain model performance across diverse architectures and datasets. We propose Adaptive Structured Pruning for Gradients (ASPG), a method that dynamically compresses gradients by learning to prune structured blocks (e.g., channels, attention heads) based on their estimated impact on convergence. ASPG uses a lightweight controller network trained alongside the main model to predict optimal pruning ratios for different architectural components. On ImageNet with ResNet-50, ASPG achieves 25\u00d7 gradient compression with only 0.8% top-1 accuracy degradation, outperforming existing methods by 2-3\u00d7 in compression-to-accuracy tradeoffs. However, our experiments reveal the controller adds 15% training overhead and unstable convergence on certain architectures like Vision Transformers. Theoretical analysis suggests our pruning strategy satisfies a relaxed version of gradient unbiasedness under assumptions that may not hold for all optimizers. While ASPG demonstrates promising empirical results, we acknowledge limitations in theoretical guarantees and generalization to emerging architectures.",
    "id": 892
  },
  {
    "title": "A Closer Look at Minimum Description Length Regularization in Modern Neural Networks",
    "authors": [
      "Liu, S.",
      "Kumar, V.",
      "Johnson, A."
    ],
    "abstract": "While Minimum Description Length (MDL) has long been proposed as a principled regularization technique for neural networks, its practical implementation faces significant computational challenges in modern deep learning architectures. We introduce a computationally tractable approximation of MDL regularization based on stochastic gradient Langevin dynamics, which we call SGLD-MDL. Our method estimates the description length of neural network parameters using gradient-based sampling, achieving O(d) complexity per iteration compared to O(d^2) for naive implementations. We evaluate SGLD-MDL on image classification and language modeling tasks, demonstrating modest improvements over weight decay (0.3-1.2% accuracy improvements on ImageNet), with particularly notable gains on small dataset regimes. However, we observe that the benefits diminish as model size increases, and our experimental results show higher variance compared to standard baselines. Theoretically, we establish PAC-Bayesian generalization bounds under simplified assumptions, though these bounds remain loose for practical architectures. Our work suggests that while MDL-inspired regularization can provide marginal benefits in data-limited settings, computational constraints significantly limit its utility for state-of-the-art models.",
    "id": 893
  },
  {
    "title": "Adaptive Gradient Norm Clipping Can Improve Transformer Training (Sometimes: A Large-Scale Empirical Study",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Chen, J.",
      "Thompson, L."
    ],
    "abstract": "Gradient clipping is widely used in training large language models, but its impact on final performance remains poorly understood. We conduct a large-scale empirical study of adaptive gradient norm clipping across 50 Transformer variants (350M-7B parameters) on diverse NLP tasks including language modeling, summarization, and code generation. Our adaptive method adjusts clipping thresholds based on gradient statistics, achieving 2-3% relative improvements in perplexity on 40% of tasks. However, we observe that benefits are concentrated in high-curvature regimes: when validation loss exhibits high variance during training (\u03c3 > 0.05), adaptive clipping outperforms fixed thresholds 68% of the time; otherwise, results are comparable or worse. Surprisingly, traditional fixed clipping outperforms adaptive methods for 7B+ models with curated data. Despite extensive ablations, we lack theoretical justification for these empirical observations. While our method provides practical guidance for clipping threshold selection, the modest and inconsistent gains suggest fundamental limitations of gradient norm-based clipping for Transformer optimization. We release comprehensive logs and configurations to facilitate future reproduction.",
    "id": 894
  },
  {
    "title": "LoRA-Drop: Adaptive Rank Reduction for Efficient Fine-tuning via Gradient Sparsity",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a popular method for efficiently fine-tuning large language models, but its fixed-rank structure often leads to suboptimal parameter allocation across layers. We propose LoRA-Drop, a simple yet effective approach that dynamically reduces the rank of LoRA adapters during training based on gradient norms. Our method begins with a generous rank allocation and progressively drops the least significant components, reducing both memory footprint and computational overhead. We introduce a thresholding scheme that considers both gradient magnitudes and layer-wise sensitivity, maintaining performance while pruning up to 60% of parameters. Experiments on instruction tuning and domain adaptation tasks across 7B and 13B parameter models show modest improvements in perplexity (1-2%) while achieving 1.5-2\u00d7 speedup during training, though benefits diminish on smaller downstream tasks. Our approach offers a practical compromise between full fine-tuning and static low-rank methods, though theoretical justification for the adaptive scheme remains limited. Code is provided for reproducibility.",
    "id": 895
  },
  {
    "title": "Improving Transformer Efficiency Through Learnable Token Routing",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, V."
    ],
    "abstract": "Transformers suffer from quadratic complexity in sequence length, limiting applicability to long contexts. We propose Token Routing Networks (TRNs), a method that dynamically routes tokens through a sparse subset of model layers based on learned importance scores. Our approach uses a lightweight gating mechanism to identify tokens requiring full computation versus those that can skip layers, reducing FLOPs by 30-50% during inference. We evaluate TRNs across language modeling, summarization, and question answering tasks using T5 and BART architectures. Experiments show modest perplexity improvements (0.2-0.4 points) on standard benchmarks, with computational savings saturating at 512-token sequences. While our method provides measurable efficiency gains for certain sequence lengths, we observe degradation on tasks requiring fine-grained token interactions. The routing parameters exhibit task-specific behavior that complicates transfer learning. Our code and pre-trained models are available at [URL omitted for review].",
    "id": 896
  },
  {
    "title": "Gradient Descent with Adaptive Restart via Spectral Residual Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, S."
    ],
    "abstract": "We propose SAGRA (Spectral Adaptive Gradient Restart Algorithm), a simple modification to gradient descent that uses the spectral residual of the Hessian to determine automatic restart points. While momentum-based methods like Nesterov acceleration achieve optimal convergence rates for convex problems, their performance is sensitive to hyperparameter tuning. SAGRA estimates the local condition number through an efficient spectral analysis of gradient residuals, triggering restarts when the estimated condition number exceeds a threshold. Our theoretical analysis establishes O(\u221a\u03ba log(1/\u03b5)) convergence for quadratic objectives, matching Nesterov's bound but with adaptive parameter selection. On CIFAR-10 and ImageNet training with ResNet-18, SAGRA achieves competitive accuracy to SGD+momentum while reducing the variance in final performance across hyperparameter settings by 23-34%. However, we find that the spectral estimation adds 15-25% computational overhead compared to standard methods, and benefits are less pronounced for problems with rapidly changing curvature. Our ablation studies suggest the restart criterion may be overly conservative in certain regimes. The method performs best on convex and mildly non-convex problems where the Hessian spectrum is relatively stable.",
    "id": 897
  },
  {
    "title": "Gradient Descent with Lookahead Momentum: A Simple Modification for Improved Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Lookahead Momentum (LaM), a lightweight modification to standard stochastic gradient descent that maintains a slow-moving copy of the parameters while performing gradient updates on fast weights. The fast weights are reset to the slow weights every k steps, while the slow weights follow an exponential moving average of the fast weights. Unlike traditional momentum methods that smooth the update direction, LaM performs \"coarse corrections\" that we show encourages the optimizer to find flatter minima. We prove convergence under standard convex assumptions and demonstrate empirically that LaM improves test accuracy by 1-2% on CIFAR-10/100 and ImageNet compared to SGD with momentum, while requiring minimal hyperparameter tuning. However, experimental gains vary significantly across architectures and datasets, with notable improvements only observed for moderate-sized ResNets. While our theoretical analysis provides some justification, we acknowledge that the connection between lookahead updates and flat minima remains largely phenomenological. We provide PyTorch code and pre-trained models.",
    "id": 898
  },
  {
    "title": "Residual Connections Enable Gradient Descent for Deep Spline Networks with Near-Optimal Sample Complexity",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We study the convergence properties of gradient descent for deep ReLU networks with residual connections, establishing polynomial-time optimization guarantees under weaker assumptions than prior work. Our key insight is that residual connections allow us to treat the network as a shallow ensemble where each residual block acts as a near-identity mapping. We prove that for target functions representable by L-layer residual networks, gradient descent with step size \u03b7 = O(1/L\u00b2) converges to an \u03b5-approximate solution in O\u0303(poly(L)log(1/\u03b5)) iterations using \u00d5(L\u00b2/\u03b5\u00b2) samples. While our sample complexity matches known lower bounds up to logarithmic factors, our analysis requires the technical restriction that the residual blocks remain close to identity initialization throughout training. Empirically, we demonstrate improved optimization on synthetic datasets, though our theoretical assumptions (bounded initialization, sufficiently wide networks) remain restrictive compared to practical settings. Our results suggest residual connections provide provable benefits beyond training stability, though the gap between theory and practice limits immediate applicability.",
    "id": 899
  },
  {
    "title": "Gradient Surgery in Transformer Models: A Surgical Approach to Attention Head Pruning via Second-Order Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Liu, S.",
      "Kumar, V."
    ],
    "abstract": "Pruning attention heads in transformers remains a critical challenge for deployment efficiency, yet existing methods either rely on expensive retraining or ad-hoc importance metrics. We propose Surgical Gradient Pruning (SGP), a second-order optimization approach that leverages Hessian information to identify and remove attention heads with minimal performance degradation. Our method computes layer-wise influence scores using an efficient block-diagonal approximation of the Hessian, enabling targeted pruning decisions without full retraining. Experiments on BERT-base and ViT-base demonstrate 15-25% FLOP reduction with <2% accuracy loss on GLUE tasks and ImageNet, outperforming magnitude-based baselines. However, we find SGP's benefits diminish significantly on larger models (BERT-large, ViT-large), where second-order approximations become unstable. While our theoretical analysis provides convergence guarantees for quadratic objectives, extending these results to the non-convex transformer landscape remains an open challenge. Code and pre-trained models will be released upon publication.",
    "id": 900
  },
  {
    "title": "Gradient Stabilization Through Adaptive Noise Injection for Training Deep Residual Networks",
    "authors": [
      "Nguyen, T.K.",
      "Johnson, L.M.",
      "Zhao, H."
    ],
    "abstract": "We propose an adaptive noise injection technique to stabilize gradient flow in very deep residual networks. Motivated by observations that gradient norms become increasingly unstable during training of networks beyond 100 layers, our method introduces carefully calibrated Gaussian noise into residual connections based on layer-wise gradient statistics. The noise magnitude is adjusted dynamically using a running estimate of gradient variance, theoretically grounded in stochastic differential equation analysis of gradient descent. Experiments on CIFAR-10/100 and ImageNet show modest improvements over standard ResNet training, with the technique providing more stable training particularly for depths exceeding 200 layers. However, performance gains saturate for moderately deep networks and the computational overhead may not justify deployment in all settings. While our theoretical analysis provides insights into the gradient noise trade-off, we acknowledge limitations in extending beyond residual architectures and the need for additional hyperparameter tuning across different datasets. Code will be made available upon acceptance.",
    "id": 901
  },
  {
    "title": "LoRA-Drop: Dynamic Low-Rank Adaptation with Gradient-Based Pruning for Efficient Fine-Tuning",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers remains suboptimal. We propose LoRA-Drop, a simple yet effective extension that dynamically adjusts LoRA ranks during training using gradient-based pruning. Our method computes a saliency score for each LoRA module based on the Frobenius norm of its gradient matrix, removing modules that fall below a learnable threshold. Experiments on GLUE and SuperGLUE benchmarks with RoBERTa-base and T5-base show LoRA-Drop achieves 92-98% of LoRA's performance while using 30-45% fewer parameters. Additionally, LoRA-Drop reduces training time by 25% on average. While our approach provides clear practical benefits, we acknowledge limitations: the method introduces two sensitive hyperparameters (pruning frequency and aggressiveness), and performance drops sharply when over-pruning occurs. We provide theoretical justification through a PAC-Bayesian analysis showing LoRA-Drop maintains generalization under mild assumptions. Code and checkpoints will be made available upon acceptance.",
    "id": 902
  },
  {
    "title": "Momentum-Scheduled Dropout: A Simple Fix for Overfitting in Transformer Fine-tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Abebe, S."
    ],
    "abstract": "Transformer models consistently struggle with overfitting when fine-tuned on small datasets, particularly in low-resource NLP tasks. While dropout remains the dominant regularization technique, its static application during training fails to adapt to the model's evolving capacity. We propose Momentum-Scheduled Dropout (MS-Drop), which dynamically adjusts dropout rates based on gradient momentum norms during fine-tuning. Our method applies a simple scheduling function that reduces dropout in layers with stable gradient magnitudes while maintaining or increasing regularization where gradients fluctuate significantly. Across eight GLUE tasks with limited training data (1K-4K examples), MS-Drop achieves modest but consistent improvements over standard dropout (average +1.2% absolute F1), particularly on single-sentence tasks. Ablation studies reveal that our momentum-based scheduling captures the same regularization effect as early stopping but prevents the 4-6% accuracy drops typically seen when fine-tuning continues beyond optimal stopping points. While MS-Drop is straightforward to implement with two additional lines of code, our theoretical analysis remains limited to linear approximations of the transformer layers, leaving open questions about its interaction with attention mechanisms.",
    "id": 903
  },
  {
    "title": "Looped Transformers Can Learn to Plan but Fail to Generalize Compositionally",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, V."
    ],
    "abstract": "We investigate whether looped transformers can learn algorithmic planning tasks from demonstrations. Unlike standard transformers, looped variants repeatedly apply the same parameters for a variable number of steps, potentially enabling computation proportional to task complexity. We train these models on shortest path and Sokoban planning problems, achieving 95% success on training distributions. However, we find that these models fail to generalize compositionally: when test instances combine familiar sub-problems in novel ways, performance drops to near-random levels despite the sub-problems being solved successfully during training. Our analysis reveals that the models learn to exploit dataset-specific statistical regularities rather than developing general planning algorithms. While looped transformers show promise for length generalization within the training distribution, their inability to compose learned sub-policies poses fundamental limitations for out-of-distribution planning. Interestingly, we observe that increasing the number of loops during inference can partially recover performance, suggesting potential avenues for test-time adaptation. Our results highlight the brittleness of neural planning systems and the challenges of achieving systematic generalization in neural network planners.",
    "id": 904
  },
  {
    "title": "LoRA-Inf: Memory-Efficient Inference for Fine-Tuned Language Models via Structured Sparsity",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Thompson, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its impact on inference efficiency remains underexplored. We present LoRA-Inf, a systematic approach to accelerate inference for LoRA-adapted models without full parameter integration. Our method exploits the low-rank structure of LoRA updates by jointly pruning both the base model and adaptation weights using a magnitude-based structured sparsity criterion. We develop a block-wise pruning strategy that preserves the mathematical equivalence of LoRA updates while reducing memory footprint by 35-50% and achieving 1.2-1.7\u00d7 speedup on standard benchmarks. Experimental evaluation on GPT-2 medium and LLaMA-7B across GLUE, SuperGLUE, and downstream tasks shows modest but consistent improvements over standard LoRA inference, with accuracy degradation within 1.2% of full-parameter baselines. While our approach provides practical benefits for deployment scenarios with memory constraints, we acknowledge that more aggressive sparsity ratios lead to significant performance drops. We open-source our implementation and pre-trained models to support reproducibility and future extensions.",
    "id": 905
  },
  {
    "title": "Label Propagation with Adaptive Confidence Thresholds for Noisy Data",
    "authors": [
      "Chen, L.",
      "Mukherjee, S.",
      "Anderson, K."
    ],
    "abstract": "We present a graph-based semi-supervised learning method that addresses noisy labels through adaptive confidence thresholds during label propagation. While existing approaches typically use fixed thresholds for pseudo-label selection, we propose dynamically adjusting these thresholds based on local neighborhood consistency in the graph. Our method first constructs a k-NN graph using deep features, then iteratively propagates labels while maintaining separate confidence scores for each node. During propagation, we reject pseudo-labels whose predicted confidence falls below an adaptively computed threshold that depends on the average label agreement within the node's neighborhood. On CIFAR-10 with 20% synthetic noise, our approach achieves 87.3% accuracy compared to 85.1% for standard label propagation. However, experiments on larger datasets (ImageNet-subset) show diminishing gains, with only 1.2% improvement over a strong supervised baseline. The method performs particularly well at low label rates (1-2 labels per class) but shows limited benefit when clean labels are abundant. Our approach requires minimal hyperparameter tuning and adds only 15% computational overhead compared to standard graph methods. While the adaptive thresholding shows promise for small-scale noisy datasets, extending these ideas to larger-scale settings requires addressing scalability challenges in the graph construction step.",
    "id": 906
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, D.",
      "Srinivasan, K.",
      "Lopez, A."
    ],
    "abstract": "Multi-task learning in deep networks often suffers from conflicting gradients that can hinder performance across tasks. While existing gradient surgery methods like PCGrad explicitly modify gradients to resolve conflicts, we investigate whether this intervention can sometimes hurt performance. We show that aggressive gradient manipulation can discard useful gradient information, leading to worse convergence rates in certain scenarios. We propose Conservative Gradient Surgery (CGS), a simple threshold-based approach that only modifies gradients when conflicts exceed a learned threshold. Our method achieves comparable performance to PCGrad on standard benchmarks (CIFAR-100/SuperCLEVR split), with 2.3% average improvement over PCGrad in high-conflict regimes but only 0.8% in low-conflict settings. Surprisingly, we also demonstrate that in some cases, removing gradient surgery entirely and using standard weighted losses can outperform both PCGrad and CGS. Our analysis suggests that the benefits of gradient surgery are highly dataset-dependent and that current benchmarks may not fully capture when these techniques are necessary. While our method shows promise in controlled settings, we acknowledge limitations in scaling to larger models and more complex task relationships.",
    "id": 907
  },
  {
    "title": "Gradient Entropy Regularization for Improved Generalization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a simple regularization technique that penalizes the entropy of gradients with respect to inputs during training. Motivated by the observation that overfitting often coincides with unstable gradients on out-of-distribution samples, we add a term that discourages large variations in gradient direction across similar inputs. Unlike Jacobian regularization, our method only requires first-order gradients and incurs minimal computational overhead. Experiments on ResNet-18 and VGG-16 architectures show 1.2-2.3% accuracy improvements on CIFAR-10 and ImageNet-C relative to standard training, with particularly notable gains under label noise (up to 8% improvement with 40% noisy labels). While theoretically grounded only for linear models, we demonstrate empirically that the regularizer improves stability even in deep nonlinear networks. However, we find diminishing returns on larger architectures (ResNet-50+) and limited benefits on naturally robust training procedures like mixup or CutMix. Our method provides a lightweight alternative to adversarial training for certain scenarios, though it does not achieve state-of-the-art robustness against strong adversarial attacks.",
    "id": 908
  },
  {
    "title": "LoRA-FS: Few-Shot Fine-Tuning with LoRA-Based Feature Selection for Efficient Transfer Learning",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Gonzalez, M."
    ],
    "abstract": "Parameter-efficient fine-tuning techniques like LoRA have shown promise for few-shot learning, but their effectiveness varies significantly across tasks and domains. We propose LoRA-FS, a method that combines LoRA adaptation with automated feature selection to improve transfer learning in data-scarce scenarios. Our approach introduces a lightweight gating mechanism that selectively activates LoRA modules based on task-specific feature importance scores derived from minimal validation data. We evaluate LoRA-FS on 12 diverse few-shot benchmarks spanning NLP, vision, and tabular domains, achieving average improvements of 5-8% over standard LoRA fine-tuning while using 20-30% fewer parameters. However, our results reveal that gains are primarily concentrated in tasks with clear domain similarity to pre-training data, and performance degrades substantially for cross-domain transfer. We provide a theoretical analysis showing that our gating mechanism achieves near-optimal feature selection under certain distributional assumptions, though these assumptions may not hold in practice. Our code and experimental protocols are available for reproducibility.",
    "id": 909
  },
  {
    "title": "LoRA-\u0394: Low-Rank Adaptation with Dynamic Rank Selection via Bayesian Evidence Maximization",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chen, L."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods like LoRA have become standard for adapting large language models, but selecting optimal rank hyperparameters remains an art. We propose LoRA-\u0394, an extension that automatically determines per-layer adaptation ranks using a Bayesian evidence maximization framework. Our method treats each LoRA layer's rank as a latent variable and optimizes an evidence lower bound that trades off adaptation capacity with model complexity. Experiments on GLUE and SuperGLUE show 0.7-1.2% average improvements over vanilla LoRA with tuned ranks, while using 15-30% fewer trainable parameters on BERT-Large and GPT-2 models. The approach adds minimal computational overhead (<5% training time) and provides principled confidence estimates for rank selection. However, improvements diminish on larger models (\u22657B parameters), and our evidence approximation relies on strong independence assumptions. We release our code and pre-trained models for reproducibility.",
    "id": 910
  },
  {
    "title": "An Empirical Study of Gradient Noise Scale Regularization for Improving Transformer Training Stability",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, J."
    ],
    "abstract": "We propose gradient noise scale regularization (GNSR) as a simple technique to stabilize training of large transformers. Motivated by theoretical work linking gradient noise scales to training dynamics, we add a lightweight loss term that penalizes large noise-to-signal ratios during optimization. Our method requires no architectural changes and introduces minimal computational overhead. We evaluate GNSR on standard language modeling and translation benchmarks using base-size models (340M parameters) trained on standard datasets. Results show consistent but modest improvements: 0.3-0.7 BLEU on translation tasks and 2-3 perplexity points on WikiText-103, while reducing training variance across 3 random seeds. Ablation studies reveal most benefits come from early training regularization rather than asymptotic performance gains. While our approach shows promise for practitioners facing training instability, we acknowledge limitations including unclear theoretical guarantees and diminishing returns on well-tuned baselines. Code and hyperparameters will be released upon acceptance.",
    "id": 911
  },
  {
    "title": "Block-Distributed Adam: Mitigating Communication Bottlenecks in Federated Fine-tuning",
    "authors": [
      "Huang, J.",
      "Subramanian, V.",
      "Zhao, K."
    ],
    "abstract": "Federated fine-tuning of large language models faces severe communication bottlenecks, with Adam-based optimizers requiring 4\u00d7 the bandwidth of SGD due to momentum buffers. We propose Block-Distributed Adam (BAdam), which partitions both the model and optimizer states into disjoint blocks, allowing worker nodes to update only a subset of parameters each round. While this sacrifices theoretical convergence guarantees, our empirical analysis shows BAdam achieves comparable perplexity to standard federated Adam on GPT-2 fine-tuning tasks while reducing communication volume by 62-68%. The method is particularly effective when the data distribution across clients has moderate non-iidness (\u03b1 > 0.3). However, we observe instability in the final training stages for smaller models (\u2264 125M parameters), an issue we partially mitigate through a simple momentum warm-up scheme. Our implementation requires only ~50 lines of additional code over standard federated averaging. Experiments span 8 NLP datasets across 50-500 clients with realistic network conditions (100-200ms latency, 10-50 Mbps bandwidth).",
    "id": 912
  },
  {
    "title": "Gradient Scheduling for Transformer Fine-tuning via Loss Curvature Estimation",
    "authors": [
      "Chen, L.",
      "Johnson, M.",
      "Garcia, A.",
      "Liu, Y."
    ],
    "abstract": "While adaptive optimizers like Adam dominate transformer fine-tuning, their uniform learning rates may miss opportunities for faster convergence in different parameter blocks. We propose Curvature-Adaptive Gradient Scheduling (CAGS), which dynamically adjusts learning rates for attention heads and feed-forward layers based on local loss curvature estimates. Our method computes inexpensive Hessian trace approximations using only gradient statistics already available during training, avoiding the computational overhead of full second-order methods. Experiments on GLUE and SuperGLUE benchmarks show 5-12% faster convergence compared to standard AdamW fine-tuning, with comparable final accuracy. However, benefits diminish with larger models (>1B parameters), and we observe sensitivity to batch size and warmup schedules. Our analysis reveals that CAGS primarily accelerates convergence in the middle training phases, with limited impact on early optimization or final generalization. While promising for resource-constrained scenarios, these limitations suggest careful consideration of computational trade-offs versus gains.",
    "id": 913
  },
  {
    "title": "Gradient Surgery with Adaptive Dropout for Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder convergence and performance across tasks. We propose AdaGradDrop, a simple yet effective method that combines gradient surgery with learned dropout rates to automatically resolve gradient conflicts while maintaining task-specific representations. Our approach applies standard gradient projection techniques to identify conflicting directions, then adaptively drops neurons contributing to these conflicts based on their historical gradient statistics. We evaluate AdaGradDrop on three multi-task benchmarks spanning computer vision and NLP, achieving an average 3.2% improvement over vanilla multi-task approaches. While our method shows consistent gains over strong baselines including PCGrad and GradNorm, gains are most pronounced when task gradients are moderately aligned (cosine similarity < 0.3). We provide theoretical analysis showing AdaGradDrop approximately minimizes an upper bound on task interference, though our bound relies on strong assumptions about task similarity. Experimental results suggest our method's effectiveness is sensitive to the dropout initialization scale and may degrade performance when tasks are highly dissimilar. Our PyTorch implementation requires minimal code changes to standard multi-task architectures.",
    "id": 914
  },
  {
    "title": "LoRA-Fi: Efficient Fine-tuning through Low-Rank Fisher Approximation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, V."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods like LoRA have achieved remarkable success in adapting large language models, yet lack theoretical justification beyond empirical observations. We present LoRA-Fi, a principled enhancement that approximates the Fisher information matrix using low-rank updates. By leveraging the natural gradient computed from a rank-r decomposition of Fisher blocks, LoRA-Fi achieves 12-15% faster convergence compared to standard LoRA on downstream tasks including GLUE and Super-NaturalInstructions. Our key insight is that low-rank adapters naturally align with the dominant eigenvectors of the Fisher information, suggesting LoRA implicitly performs natural gradient descent. We provide convergence guarantees under the assumption of restricted strong convexity, though our bounds depend on problem-dependent constants that may scale poorly with model size. Experiments across 7B to 65B parameter models demonstrate consistent improvements, particularly on low-resource datasets (<1k examples). However, we observe minimal gains for sufficiently large fine-tuning datasets (>50k examples), and computational overhead increases non-trivially due to gradient covariance estimation. While LoRA-Fi offers modest but reliable improvements over LoRA, our theoretical framework may provide deeper understanding of PEFT dynamics.",
    "id": 915
  },
  {
    "title": "Revisiting Softmax Temperature Scaling with Learnable Mixtures of Temperaments",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Temperature scaling is widely adopted for confidence calibration in neural networks, yet its effectiveness diminishes on datasets exhibiting distribution shift. We propose a simple extension: modeling the softmax temperature as a learned mixture of multiple temperature values, each weighted by the input instance. Our method introduces only 0.02% additional parameters but demonstrates consistent improvements in expected calibration error across 5 benchmark datasets, achieving 2-8% better calibration compared to standard temperature scaling. While the approach is theoretically motivated by considering the posterior as a mixture of tempered exponentials, we acknowledge our analysis assumes conditional independence that may not hold in practice. Experiments reveal the gains are most pronounced on corrupted versions of CIFAR-10/100 and ImageNet-C, with diminishing returns on in-distribution data. The simplicity of our method may benefit practitioners, though we recognize the contribution is incremental and the theoretical justification remains incomplete. Code is available at [redacted-for-reviews].",
    "id": 916
  },
  {
    "title": "Improving Transformer Efficiency through Iterative Attention Pruning During Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J.",
      "Anderson, K."
    ],
    "abstract": "Fine-tuning large pre-trained transformers for downstream tasks often introduces computational burdens that limit deployment on resource-constrained devices. While numerous pruning methods exist, most require expensive retraining or yield significant accuracy degradation. We propose Iterative Attention Pruning (IAP), a lightweight fine-tuning method that gradually removes attention heads based on their gradient-based importance scores computed during standard task-specific fine-tuning. IAP requires no additional pre-training or hyperparameter tuning beyond standard fine-tuning procedures. On GLUE and SuperGLUE benchmarks, IAP reduces 25-40% of attention heads while maintaining 92-97% of original accuracy across BERT-base and RoBERTa-base models. Analysis reveals IAP preferentially removes heads with high redundancy scores and low diversity indices, suggesting our method captures meaningful patterns in attention structure. However, we observe diminishing returns on larger models (BERT-large) and tasks requiring complex reasoning, where our pruning criterion correlates poorly with downstream performance. Though IAP offers practical efficiency gains for certain scenarios, its applicability to emerging larger models and multi-modal transformers remains unexplored.",
    "id": 917
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Module Pruning for Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become standard for adapting large language models, but their training cost remains high for multi-task scenarios. We present LoRA-Drop, a simple technique that dynamically identifies and prunes redundant LoRA modules during training. Our method uses a lightweight importance scoring mechanism based on gradient accumulation to selectively drop up to 40% of LoRA modules without task-specific knowledge. Experiments on GLUE and SuperGLUE benchmarks show LoRA-Drop achieves 1.2-1.8x training speedup while maintaining 95-98% of original accuracy across BERT-base and T5-base models. However, we observe performance degradation (>5% drop) on certain tasks requiring complex reasoning, suggesting the scoring heuristic may be overly aggressive. Theoretical analysis provides limited insight beyond empirical observations. While LoRA-Drop offers practical benefits for resource-constrained settings, its limitations on challenging benchmarks highlight the need for more sophisticated pruning strategies. Code is available at [URL omitted for blind review].",
    "id": 918
  },
  {
    "title": "Revisiting BatchNorm through the Lens of Implicit Gradient Regularization",
    "authors": [
      "Chen, L.",
      "Johnson, D.",
      "Krishnan, S."
    ],
    "abstract": "Batch Normalization (BatchNorm) remains a crucial component in modern deep learning architectures, yet its theoretical understanding remains incomplete. We investigate BatchNorm through the lens of implicit gradient regularization and uncover a previously overlooked connection to stationary-point-aware optimization. Our method, GradNorm++, introduces a lightweight modification that adaptively reweights gradients based on learned batch statistics during training. Through extensive experiments on CIFAR-10/100 and ImageNet, we demonstrate 0.7-1.2% improvements in top-1 accuracy over standard BatchNorm baselines across ResNet18/50 architectures. While our empirical results are consistent across multiple datasets, we acknowledge that improvements diminish with larger models and vanish entirely on simpler architectures like LeNet. Our theoretical analysis provides convergence guarantees under restrictive assumptions about Lipschitz continuity and bounded weight norms. The method requires minimal hyperparameter tuning and introduces only 0.3% additional parameters. Though GradNorm++ shows promise for medium-scale vision tasks, we limit our claims to acknowledgment that the improvements, while consistent, may not justify the increased implementation complexity for all practitioners.",
    "id": 919
  },
  {
    "title": "Gradient Descent with Memory: An Empirical Study of Past-Query Adaptation in Neural Network Training",
    "authors": [
      "Chen, L.",
      "Srinivasan, K.",
      "Johnson, E."
    ],
    "abstract": "We investigate a simple modification to standard gradient descent that incorporates information from previous optimization steps without introducing additional hyperparameters. Our method, Past-Query Gradient Descent (PQ-GD), maintains a small memory buffer of recent gradients and adaptively reuses them to potentially accelerate convergence. Unlike momentum or adaptive methods, PQ-GD does not require learning rate scheduling or individual parameter adaptation, making it straightforward to implement. We evaluate PQ-GD on standard image classification benchmarks (CIFAR-10, CIFAR-100) and medium-scale language modeling tasks, comparing against SGD, Adam, and Adagrad. While PQ-GD shows modest improvements over baselines in 30% of experimental configurations (particularly for smaller networks), performance is inconsistent across architectures and datasets, with some runs achieving 2-3% better test accuracy but others underperforming by similar margins. Theoretical analysis reveals that PQ-GD can be viewed as a restricted form of momentum with decaying weights, potentially limiting its expressiveness. Despite mixed empirical results, our work highlights the underexplored potential of reusing past gradient information in simple ways. Code is available at anonymized.github.io/pqgd.",
    "id": 920
  },
  {
    "title": "Gradient Surgery for Federated Learning is Not Always Optimal: A Spectral Perspective on Convergence Trade-offs",
    "authors": [
      "Liu, S.",
      "Chen, J.",
      "Brown, D."
    ],
    "abstract": "Gradient compression techniques in federated learning typically treat client updates as independent vectors and apply uniform quantization or sparsification. We propose SpectralFed, a method that decomposes client gradients using eigendecomposition and selectively compresses components based on their alignment with the global gradient spectrum. Our approach applies different compression rates to eigenvectors based on their associated eigenvalues, theoretically leading to better convergence for non-convex objectives. Experiments on CIFAR-10/CIFAR-100 with ResNet-18 demonstrate 1.2-1.4\u00d7 communication reduction compared to FedAvg and FedProx baselines, while maintaining comparable accuracy (\u00b10.3%). However, we observe decreasing benefits as data heterogeneity increases, with smaller federated datasets showing minimal improvement. Theoretical analysis provides convergence guarantees under bounded gradient dissimilarity assumptions, but requires strong smoothness conditions that may not hold in practice. While SpectralFed offers a principled alternative to existing compression schemes, its computational overhead and sensitivity to eigengap values limit practical deployment. Our code is available anonymously at [URL].",
    "id": 921
  },
  {
    "title": "Gradient Descent with Lookahead via Moving Average Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Lookahead Moving Average (LMA), a simple modification to gradient descent that combines two popular optimization ideas: lookahead mechanisms and moving average gradients. LMA maintains two sets of parameters: a fast-moving 'inner' optimizer that performs gradient descent steps, and a slow-moving 'outer' update that computes an exponential moving average of the inner parameters. Unlike existing lookahead methods that require two optimizers, LMA uses a single optimizer with modified update rules, reducing memory overhead while maintaining theoretical convergence guarantees under standard smoothness assumptions. Our experiments on ResNet and Vision Transformer architectures across CIFAR-10, ImageNet, and WMT'14 translation tasks show modest but consistent improvements over vanilla SGD and AdamW, achieving 0.3-0.8% higher accuracy and 1.2-2.1% better BLEU scores. However, these gains are typically smaller than those achieved by more sophisticated adaptive methods like AdamW or LAMB. We provide theoretical analysis showing LMA converges at a rate of O(1/T) for convex functions, though this matches the rate of standard lookahead rather than improving it. While LMA offers a practical lightweight alternative to existing methods, its benefits appear most pronounced in specific regimes of learning rates and batch sizes, suggesting limited general impact.",
    "id": 922
  },
  {
    "title": "Gradient Pruning with Adaptive Block-wise Sparsity for Memory-Efficient Training",
    "authors": [
      "Chen, L.",
      "Gonzalez, M.",
      "Kim, D."
    ],
    "abstract": "We propose Gradient Block Pruning (GBP), a simple method for reducing memory usage during neural network training by selectively storing gradient updates for only a subset of parameter blocks. Unlike existing gradient compression techniques that require modification to the optimization algorithm, GBP maintains exact gradients within each selected block while entirely dropping gradients for others. Our approach uses a computationally lightweight importance score based on gradient norms from previous steps to adaptively determine which blocks to prune at each iteration. We evaluate GBP on standard image classification and language modeling benchmarks, achieving 30-60% reduction in gradient memory at the cost of 1-6% accuracy loss compared to full gradient training. While theoretical analysis shows GBP converges for convex objectives under standard assumptions, empirical performance degrades significantly when pruning rates exceed 70%. Our results suggest that gradient block pruning provides a practical but limited improvement over existing memory-reduction techniques, with most benefits realized in memory-constrained scenarios where accuracy loss is acceptable.",
    "id": 923
  },
  {
    "title": "Re-weighted Gradient Descent: A Simple Heuristic for Better Generalization in Overparameterized Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose re-weighted gradient descent (RGD), a simple modification to standard gradient descent that re-weights updates based on parameter norms during training. Our method applies an element-wise rescaling factor \u03b3 \u2208 (0,1] to gradients, where \u03b3 decreases monotonically with parameter magnitude. Intuitively, this encourages the optimizer to focus on smaller parameters, potentially biasing solutions toward lower effective rank representations. We provide theoretical analysis showing RGD minimizes an upper bound on the Rademacher complexity for two-layer ReLU networks, though our bound requires restrictive assumptions on the data distribution. Empirically, RGD demonstrates modest but consistent improvements over SGD with momentum on CIFAR-10/100 (0.4-0.8% accuracy gains) and ImageNet (0.3% top-1 improvement) when training ResNet-18/50 architectures. Ablations reveal most benefits occur in sparse training regimes with learning rate decay schedules. While our method is computationally cheap and simple to implement, gains remain small compared to recent regularization techniques, and we observe performance degradation on some NLP tasks (BERT fine-tuning on GLUE). Our code will be made available upon acceptance.",
    "id": 924
  },
  {
    "title": "Efficient Few-Shot Learning via Learned Gradient Compression in Meta-Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "We propose MetaGradComp, a meta-learning approach that learns to compress gradients during meta-training for improved few-shot learning efficiency. While gradient-based meta-learning methods like MAML achieve strong performance, their computational cost scales poorly with model size due to second-order optimization. Our key insight is that most gradient information in meta-training is redundant for task adaptation. MetaGradComp learns a lightweight compression network that identifies task-relevant gradient directions, reducing meta-gradient computation by 60-85% across benchmarks. The compression network is trained jointly with the base learner using a novel meta-objective that balances gradient fidelity and computational efficiency. On standard few-shot classification benchmarks (mini-ImageNet, CIFAR-FS), our method achieves comparable accuracy to MAML while reducing training time by 2-3x on ResNet-18 architectures. However, we observe performance degradation on cross-domain tasks and complex architectures, suggesting limited generalizability of our learned compression. Our work represents a practical trade-off between efficiency and accuracy in meta-learning, providing insights into gradient redundancy in meta-training.",
    "id": 925
  },
  {
    "title": "Gradient Surgery Meets Momentum: An Empirical Analysis of Adaptive Gradient Manipulation in Federated Learning",
    "authors": [
      "Liu, S.",
      "Roberts, J.",
      "Kumar, V."
    ],
    "abstract": "Federated learning faces critical challenges when client gradients conflict due to data heterogeneity. While gradient surgery techniques like FedAvg and FedProx attempt to address this through careful aggregation, we observe these methods can produce erratic training dynamics when combined with local momentum updates. We propose FedGradMix, a simple modification that reweights conflicting gradients based on their alignment with the global momentum buffer, requiring minimal hyperparameter tuning and no additional communication. Our extensive experiments across CIFAR-10, CIFAR-100, and FEMNIST under various non-IID partitions show FedGradMix improves convergence speed by 15-25% over baselines in moderately heterogeneous settings, but performance degrades sharply under extreme skew (>90% label imbalance). Surprisingly, we find that traditional momentum actually harms performance in highly heterogeneous cases, suggesting the need for adaptive momentum scheduling. While we demonstrate effectiveness on standard benchmarks, theoretical analysis of convergence remains limited, and computational overhead increases linearly with client count. Our code is available but requires significant engineering effort for reproduction.",
    "id": 926
  },
  {
    "title": "FixMatch with Temperature Scaling: A Simple Baseline for Semi-Supervised Learning under Distribution Shift",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Semi-supervised learning (SSL) methods like FixMatch achieve strong performance on benchmark datasets, but their robustness to distribution shift remains unclear. We propose FixMatch+T, which applies temperature scaling to FixMatch's pseudo-label confidence threshold, adapting the model's confidence requirements based on estimated distribution shift. Our method requires minimal hyperparameter tuning and adds only 3 lines of code to existing FixMatch implementations. We evaluate on CIFAR-10-C and ImageNet-C with label rates of 5%, 10%, and 40%. FixMatch+T improves FixMatch accuracy by 2.3% on average under moderate corruptions (severity 3-5) while maintaining comparable performance on clean data. However, under severe distribution shift (severity 5), both methods degrade significantly, with FixMatch+T showing only marginal improvement (0.7%). Our theoretical analysis provides loose generalization bounds that fail to capture the empirical benefits. While temperature scaling offers measurable gains in specific scenarios, our results suggest the need for more sophisticated approaches to truly address SSL under distribution shift.",
    "id": 927
  },
  {
    "title": "Memory-Efficient Training of Transformers via Adaptive Block-Diagonal Attention Approximation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.M.",
      "Kim, S."
    ],
    "abstract": "While transformer models achieve state-of-the-art results across domains, their quadratic memory complexity in sequence length remains a critical bottleneck. We propose ABDAttn, a training-time approximation that reduces memory usage by constructing adaptive block-diagonal attention patterns based on learned importance scores. Unlike prior sparsity approaches that use fixed patterns or require costly re-training, ABDAttn dynamically determines which query-key pairs to compute during the forward pass using a lightweight auxiliary network trained jointly with the main model. Our method achieves 2.5-3.1x peak memory reduction compared to standard attention on GPT-style models trained on C4 and OpenWebText, while maintaining within 2% of the original perplexity. On downstream tasks, we observe slight degradations (0.5-1.2 F1 drop) on GLUE benchmark tasks, with larger gaps on tasks requiring long-range dependencies. Analysis reveals our method struggles with certain attention patterns crucial for syntactic parsing tasks. While our approach offers practical memory savings, the computational overhead of the auxiliary network (15-20% increase in training time) and task-specific trade-offs suggest careful consideration of the deployment context.",
    "id": 928
  },
  {
    "title": "Gradient Boosting with Adaptive Tree Restructuring for Partial Monotonicity Constraints",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose TreeMonotonic Boosting, a gradient boosting framework that enforces partial monotonicity constraints through adaptive tree restructuring during training. While existing methods typically modify loss functions or apply post-processing corrections, our approach dynamically adjusts tree splits to maintain monotonic relationships between specified features and predictions. The method employs a modified split-finding algorithm that penalizes violations of monotonicity constraints during tree construction, combined with a gradient approximation scheme to maintain computational efficiency. Experimental results on nine tabular datasets demonstrate consistent improvements over standard gradient boosting trees on a monotonicity violation metric (average reduction of 23-45%), with moderate trade-offs in predictive accuracy (1-3% degradation on RMSE). The framework supports any differentiable tree-based boosting variant and introduces minimal computational overhead (12% increase in training time). However, we note performance degradation when constraints conflict with strong feature interactions or when monotonic features exhibit complex non-linear relationships with the target. Our implementation extends XGBoost with approximately 300 lines of C++ code.",
    "id": 929
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning via Parameter-Specific Importance Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning in neural networks often suffers from gradient conflicts that lead to catastrophic forgetting when tasks are learned sequentially. While recent gradient surgery methods mitigate these conflicts through gradient projection or scaling, they remain memory-intensive and can overly constrain gradient directions, potentially limiting performance on complex tasks. We propose MEM-GS (Memory-aware Gradient Surgery), a simple yet effective approach that selectively applies gradient surgery based on parameter-specific importance measures computed via Fisher information. Our method maintains a small reservoir of important parameters for each task, applying surgical constraints only when the gradient would significantly impair these parameters. On CIFAR-100 and CelebA benchmarks with 5 sequential tasks, MEM-GS achieves 3.2% average improvement over standard gradient surgery while reducing memory overhead by 47%. However, we find limited benefits on tasks with high semantic similarity, where gradient conflicts are naturally smaller. Our results suggest that selective gradient surgery is most valuable for dissimilar tasks, though gains diminish as model capacity increases. Code is available at [anonymous] but lacks comprehensive hyperparameter ablation studies.",
    "id": 930
  },
  {
    "title": "Gradient Surgery Without Surgery: Controlling Gradient Interference via Learned Importance Weighting",
    "authors": [
      "Kumar, P.",
      "Chen, S.",
      "Jackson, L."
    ],
    "abstract": "Multi-task learning often suffers from gradient interference, where conflicting gradient directions hurt overall performance. While existing gradient surgery methods explicitly modify gradients using hand-crafted rules, we propose Gradient Importance Weighting (GIW) \u2014 a simpler approach that learns task-specific weights to attenuate gradient interference during optimization. Our method uses a lightweight meta-network that predicts importance scores for each task's loss based on the current parameter values, trained to maximize validation performance across tasks. Despite its simplicity, GIW achieves competitive results on standard benchmarks, improving average accuracy by 2.3% on CIFAR-100 multi-task splits and 1.8% on NYUv2 compared to naive multi-task baselines. However, we find that GIW struggles on highly imbalanced tasks, where gradient surgery methods still prevail. Our analysis reveals that the learned weights correlate poorly with principled notions of task similarity, suggesting the improvements may arise from implicit regularization rather than true interference reduction. While GIW offers practical benefits through its ease of implementation, its theoretical grounding remains weak, and empirical gains are modest compared to more sophisticated alternatives.",
    "id": 931
  },
  {
    "title": "Adaptive Gradient Clipping with Layer-wise Curvature Estimates for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training large transformers remains unstable despite extensive tuning of gradient clipping thresholds. We propose Adaptive Curvature Clipping (ACC), which adjusts clipping thresholds per layer based on local curvature estimates computed via low-rank approximations of the Hessian. ACC uses gradient history to maintain running estimates of layer-wise curvature without additional forward passes. We evaluate on GPT-2 (124M-1.5B) and ViT-B/16 across 5 datasets, demonstrating 2-3\u00d7 faster initial convergence compared to standard adaptive optimizers. However, performance gains diminish in later training stages (epochs 40+), where ACC reduces to near-identical behavior as baseline clipping. Theoretical analysis shows ACC implicitly regularizes optimization paths by bounding local Lipschitz constants, though this effect becomes negligible as gradients vanish. While ACC provides practical benefits for early training instability at moderate computational overhead (~8% per step for 125M models), its long-term impact on final model quality is limited. Code and pretrained checkpoints will be released.",
    "id": 932
  },
  {
    "title": "Momentum Accelerates Gradient Descent Even When You Don't Expect It: An Empirical Study of Non-Monotonic Step Sizes",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Rodriguez, M."
    ],
    "abstract": "We investigate the behavior of momentum-based optimizers when combined with non-monotonic step size schedules, revealing counterintuitive improvements in convergence for certain classes of neural network training. Starting from the observation that momentum methods exhibit periodic overshooting behavior, we systematically analyze the interaction between momentum coefficients and cyclical learning rate schedules. Our theoretical analysis establishes that momentum can provide acceleration even when immediate progress appears negative, extending classical results from convex optimization to non-convex settings under specific smoothness assumptions. We propose a simple modification to existing momentum algorithms that leverages this insight, requiring only 2-3 lines of code changes to popular frameworks. Experiments on CIFAR-10 and ImageNet with ResNet architectures demonstrate 8-15% faster convergence compared to standard momentum SGD, though gains diminish for very large batch training. While our theoretical guarantees hold only under restrictive conditions rarely met in practice, the empirical benefits persist across diverse architectures and datasets. Code is available at anonymous.url.",
    "id": 933
  },
  {
    "title": "LoRA-FT: Low-Rank Adaptation for Faster Fine-Tuning in Resource-Constrained Federated Learning",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Singh, A."
    ],
    "abstract": "Fine-tuning large pre-trained models in federated settings remains prohibitively expensive for edge devices with limited memory and compute. We present LoRA-FT, a method that combines low-rank adaptation (LoRA) with selective layer freezing to reduce communication and computational costs during federated fine-tuning. By analyzing the Hessian spectrum of transformer layers, we identify which components can be safely frozen while maintaining performance. Our approach reduces gradient communication by 73% and local compute by 54% compared to standard federated fine-tuning on the GLUE benchmark, while achieving within 1.2% of the baseline accuracy. However, we observe that performance degradation becomes significant (>3%) for smaller models (<100M parameters) and tasks requiring substantial adaptation. We provide theoretical analysis showing convergence under convexity assumptions, but acknowledge that these assumptions do not hold for our primary transformer experiments. Our PyTorch implementation requires fewer than 50 lines of code change to existing federated learning frameworks.",
    "id": 934
  },
  {
    "title": "Gradient Norm Clustering: A Simple Regularization Technique for Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Garcia, M."
    ],
    "abstract": "Large neural networks exhibit complex training dynamics where gradient directions across different batches can vary significantly, potentially leading to unstable optimization and poor generalization. We propose Gradient Norm Clustering (GNC), a lightweight regularization technique that encourages gradient vectors to cluster in norm-space during training. GNC adds an auxiliary loss term that penalizes the variance of gradient norms across mini-batches, implemented efficiently through a momentum-based running estimate. Our experiments on CIFAR-10, CIFAR-100, and ImageNet-subset show consistent improvements over standard SGD across various architectures, with particularly strong gains (0.5-1.2% accuracy) for deeper networks like ResNet-50 and ViT-Tiny. While GNC demonstrates practical benefits and theoretical connections to Lipschitz regularization, our analysis reveals the improvements diminish when combined with strong data augmentation techniques like MixUp. The method adds minimal computational overhead (\u22483% training time) and requires no architectural modifications, making it suitable for practitioners. However, we acknowledge the improvement magnitude is modest and may not justify the added complexity in all settings.",
    "id": 935
  },
  {
    "title": "Augmenting Language Models with External Memory via Differentiable Cache Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "While large language models demonstrate impressive capabilities, they struggle with maintaining long-term consistency when processing extended dialogues or documents. We propose Memformer, a lightweight memory augmentation framework that enables transformer-based models to maintain an external memory cache updated through differentiable operations. Our approach uses a learned retrieval mechanism to select relevant memory slots and a low-rank update rule to incorporate new information without catastrophic forgetting. We evaluate Memformer on three tasks: multi-session dialogue, long document question answering, and sequential instruction following. Experiments show 12-18% improvements in consistency metrics compared to standard fine-tuning baselines, while adding only 3M parameters to existing models. However, we observe that performance gains diminish when dialogue contexts exceed 8K tokens, suggesting fundamental limitations in the memory architecture. Our results indicate that external memory can provide modest benefits for targeted applications, though the gains may not justify the added complexity for general-purpose use.",
    "id": 936
  },
  {
    "title": "Lookahead Gradient Descent with Adaptive Polyak Momentum for Non-Convex Optimization",
    "authors": [
      "Chen, J.",
      "Rodriguez, A.",
      "Liu, W.",
      "Kim, H."
    ],
    "abstract": "We propose a simple modification to gradient descent that combines Lookahead's slow weights update rule with an adaptive variant of Polyak's classical momentum. Our method, LAGD-APM, maintains two sets of weights: fast weights updated by standard gradient descent, and slow weights that periodically synchronize with the fast weights using a momentum-based interpolation. Key to our approach is an adaptive momentum coefficient that depends on the relative alignment between consecutive gradients, eliminating the need for manual tuning. On non-convex objectives including ResNet training on CIFAR-10 and Transformer training on WikiText-103, LAGD-APM achieves comparable performance to Lookahead+SGD and AdamW while using 15-30% fewer iterations. However, we observe diminishing returns on simpler architectures like VGG and MLP-Mixer. Our theoretical analysis shows convergence to stationary points for non-convex smooth objectives, though the convergence rate does not improve over standard Lookahead. While the adaptive momentum scheme introduces minimal overhead, it adds complexity without clear theoretical justification beyond empirical benefits. Ablation studies suggest that most gains come from the Lookahead wrapper rather than our momentum adaptation, raising questions about the significance of our contribution. Code will be released upon acceptance.",
    "id": 937
  },
  {
    "title": "Adaptive Gradient Clipping with Moving Percentiles for Improved Transformer Training Stability",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Du, S."
    ],
    "abstract": "Training large transformer models often suffers from gradient instability, leading to loss spikes and poor convergence. While gradient clipping is widely used, the choice of clipping threshold remains heuristic and dataset-dependent. We propose Adaptive Percentile Clipping (APC), a simple modification to standard gradient clipping that automatically adjusts the clipping threshold based on moving percentiles of gradient norms. Unlike fixed-threshold clipping or adaptive methods like AdamW that normalize per-parameter, APC operates globally while adapting to local geometry. Our method introduces two hyperparameters: a percentile value (default 95th) and a momentum coefficient for tracking the percentile. On C4 language modeling with GPT-2 medium, APC reduces the frequency of loss spikes by 34% compared to fixed clipping while maintaining wall-clock training time. On ImageNet with ViT-B, we observe modest improvements in final accuracy (0.3% top-1) but more stable training curves. While our experiments are limited to standard benchmarks and do not demonstrate clear improvements over strong baselines with careful hyperparameter tuning, APC offers a practical approach to reducing the sensitivity of large model training to clipping thresholds. Code will be released upon acceptance.",
    "id": 938
  },
  {
    "title": "Improving Transformer Efficiency through Selective Attention Pruning in Early Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Anderson, J."
    ],
    "abstract": "While transformers have become the dominant architecture across NLP and vision tasks, their quadratic complexity in sequence length remains a fundamental limitation. We propose Gradient-Aware Attention Pruning (GAAP), a method that identifies and removes attention heads during the early stages of training based on their gradient sensitivity scores. Unlike prior pruning approaches that require full training before compression, GAAP dynamically reduces model size after only 20-30% of training epochs, achieving comparable performance with 15-30% fewer parameters on standard benchmarks. Our method uses second-order gradient analysis to predict which attention heads contribute minimally to the overall loss landscape, combined with a gradual pruning schedule that maintains training stability. Experiments on BERT-base and ViT-tiny architectures show 1.3-1.8x speedup in training time and 18% reduction in memory usage on GLUE and ImageNet downstream tasks, though we observe performance degradation on longer sequences (>512 tokens) and more complex reasoning tasks requiring cross-attention mechanisms. While GAAP offers practical training efficiency gains, our theoretical analysis reveals fundamental trade-offs between pruning aggressiveness and representation capacity that may limit broader applicability without architectural modifications.",
    "id": 939
  },
  {
    "title": "Adaptive Gradient Clipping with Moving Percentiles for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Transformer models often suffer from gradient explosion during training, leading to unstable optimization. While gradient clipping is widely used, the choice of clipping threshold remains heuristic and dataset-dependent. We propose Adaptive Gradient Percentile Clipping (AGPC), which dynamically sets clipping thresholds using moving percentiles of gradient norms observed during training. Unlike traditional fixed-threshold approaches, AGPC adapts to the evolving gradient distribution without hyperparameter tuning. Our method computes thresholds based on the 95th percentile of gradient norms over a sliding window, with exponential decay to maintain stability. Experimental results on language modeling benchmarks (Wikitext-103, C4) and vision transformers on ImageNet show 2-3% improvements in final perplexity/accuracy over baseline clipping, particularly in low-resource settings. However, the benefits diminish with careful hyperparameter tuning of standard clipping methods. Ablation studies reveal that percentile choices around 90-95% work well across tasks, but the optimal window size varies. While our approach provides a principled alternative to manual threshold selection, the computational overhead of online percentile estimation (5-10% increase in training time) may limit its practical impact. Code will be made available upon acceptance.",
    "id": 940
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Learned Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to high-variance gradient estimators. While control variates can reduce variance, their effectiveness is limited by the quality of baseline functions. We propose a meta-learning approach that learns to predict optimal control variate baselines for REINFORCE-style estimators. Our method trains a small neural network that takes local context (layer activations, parameter norms) as input and outputs baseline values that minimize gradient variance. On MNIST-VAE experiments, our approach achieves 15-30% lower gradient variance compared to standard baselines, translating to modest improvements in likelihood (0.5-1.2 nats improvement) and perceptual quality scores. Theoretical analysis shows our learned baselines reduce variance by implicitly capturing correlations between gradients and model parameters. While our improvements are consistent, they remain incremental compared to recent work on continuous relaxations. Our computational overhead is roughly 5-10% during training. We release PyTorch code for reproducibility.",
    "id": 941
  },
  {
    "title": "Improved Gradient Bounds for Overparameterized Networks via Local Jacobian Conditioning",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, M."
    ],
    "abstract": "We study the gradient dynamics of overparameterized neural networks through the lens of local Jacobian conditioning, proposing a novel measure that captures layer-wise gradient stability more precisely than existing Jacobian-based bounds. Our key insight is that the conditioning of feature matrices at each layer provides a tighter characterization of gradient norm bounds during training, particularly for networks with batch normalization. We derive new theoretical bounds showing that gradient norms scale with the product of local condition numbers rather than global spectral norms, leading to improved convergence guarantees for SGD. Empirically, we validate our theoretical findings on CIFAR-10 and ImageNet, demonstrating 5-12% faster convergence compared to standard bounds when using our refined learning rate schedules. While our bounds are tighter than previous work for networks with width \u2265 512, we find diminishing improvements for narrower architectures. Our results provide practical guidance for setting layer-wise learning rates, though we acknowledge our analysis currently assumes Gaussian initialization and does not extend to residual connections, which we leave as important future work.",
    "id": 942
  },
  {
    "title": "Improved Convergence of Adam with Exponential Moving Average Restart Schedules",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "Despite being the de facto optimizer for training deep networks, Adam often exhibits worse generalization than SGD with momentum. We investigate whether this gap can be closed through carefully designed restart schedules for Adam's exponential moving averages (EMAs). By periodically resetting the first and second moment estimates, we prevent the optimizer from getting trapped in poor local geometry induced by the accumulated gradient statistics. Our schedule depends solely on two interpretable hyperparameters: restart frequency and momentum decay rate. Theoretical analysis shows that EMA restarts improve the regret bound by a factor of O(log T) in convex settings. In practice, across 6 vision and 3 language tasks, our method achieves 0.5-1.2% improvement over Adam baselines, matching SGD+Momentum performance while maintaining adaptive benefits. However, gains diminish on larger models (>1B parameters), suggesting fundamental limitations. Our implementation requires only 3 lines of code, making it practical for researchers already using Adam. While preliminary experiments are promising, we acknowledge that our theoretical analysis covers only convex cases and our empirical evaluation is limited to standard benchmarks.",
    "id": 943
  },
  {
    "title": "LoRA-Prune: Structured Pruning of Low-Rank Adaptation Modules for Efficient Fine-Tuning",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chen, L.",
      "Kumar, V."
    ],
    "abstract": "We propose LoRA-Prune, a method for reducing the computational overhead of Low-Rank Adaptation (LoRA) during fine-tuning by selectively removing rank components based on their contribution to the downstream task. While LoRA has emerged as a parameter-efficient fine-tuning technique, we observe that many learned rank components exhibit redundancy, particularly in vision and language models. Our approach introduces a simple iterative pruning criterion based on gradient-based saliency scores, progressively removing low-contribution rank components during training. We evaluate LoRA-Prune on GLUE benchmarks and ImageNet classification, achieving 25-40% reduction in inference time with <2% performance degradation compared to standard LoRA. However, we find that aggressive pruning (>60% rank reduction) leads to significant accuracy drops, suggesting our method works best as a complementary optimization rather than a replacement for careful rank selection. Our code is available at [anonymous repo].",
    "id": 944
  },
  {
    "title": "Revisiting Attention Dropout: A Simple Baseline for Transformer Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Self-attention mechanisms in Transformers are prone to overfitting on small to medium-sized datasets. While numerous architectural modifications and regularization schemes have been proposed, we show that a carefully tuned attention dropout strategy\u2014simply dropping entire attention heads during training\u2014achieves competitive performance across a range of tasks. We conduct extensive experiments on GLUE, WMT'14, and ImageNet-1k, demonstrating that our approach matches or exceeds recent complex regularization techniques while requiring minimal hyperparameter tuning. Our theoretical analysis reveals that head dropout induces an implicit form of low-rank regularization in the attention matrices. However, we observe that performance gains diminish on larger datasets and modern pretrained models, suggesting our method may be most beneficial for resource-constrained settings. Our findings call into question the necessity of sophisticated regularization schemes when simpler alternatives exist, though we acknowledge our approach lacks the architectural novelty of concurrent work. Code and pretrained models will be released upon acceptance.",
    "id": 945
  },
  {
    "title": "Revisiting Momentum in Federated Learning: A Simple Modification with Modest Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "Federated momentum methods often diverge on non-IID data, leading practitioners to disable momentum entirely. We propose FedMom++, a straightforward modification to standard federated momentum that introduces client-specific momentum buffers weighted by participation frequency. Our approach requires only two additional scalars per client and negligible communication overhead. Theoretical analysis shows convergence rates comparable to vanilla FedAvg under standard assumptions, with slightly improved constants on client drift. Experiments on CIFAR-10, FEMNIST, and StackOverflow achieve 2-4% accuracy improvements over strong baselines, though these gains vanish with careful hyperparameter tuning. Notably, FedMom++ shows consistent but small improvements across different participation patterns. While our method does not address all challenges of federated momentum, its simplicity and minimal computational cost make it a practical drop-in replacement. Code is available at [anonymous link].",
    "id": 946
  },
  {
    "title": "Improved Convergence Bounds for Stochastic Gradient Descent with Cyclical Step Sizes",
    "authors": [
      "Liu, Q.",
      "Kohlmayer, P.",
      "Chen, S."
    ],
    "abstract": "We analyze the convergence properties of stochastic gradient descent (SGD) with cyclical step size schedules, a technique commonly used in practice but lacking theoretical justification. While previous work established convergence rates of O(1/\u221aT) for SGD with decreasing step sizes under standard assumptions, we show that cyclical schedules that periodically reset the step size can achieve improved convergence rates of O(log T/T) under additional smoothness conditions. Our analysis relies on a novel potential function that captures the periodic nature of the learning rate schedule. We validate our theoretical results on several benchmark datasets, demonstrating modest improvements in convergence speed compared to standard decay schedules. However, the practical gains are limited to specific problem instances where the Hessian has favorable spectral properties. Our findings provide partial theoretical grounding for a widely-used heuristic, though the improvement over baseline SGD is not as dramatic as initially hypothesized. Code is available at anonymous-url.",
    "id": 947
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder optimization. While recent approaches like PCGrad and GradNorm attempt to address this through sophisticated gradient modification schemes, we show that a surprisingly simple alternative can be equally effective. We propose Truncated Gradient Projection (TGP), which discards the bottom 10% of gradient magnitudes across tasks before combining them. Through analysis on 5 benchmarks spanning computer vision and NLP domains, TGP achieves comparable performance to state-of-the-art methods while reducing computational overhead by 25-40%. Our theoretical analysis reveals that this truncation implicitly regularizes the optimization landscape in a task-dependent manner. However, we observe that TGP's benefits diminish with larger models (>100M parameters) and highly imbalanced task distributions. Our code will be made available upon publication.",
    "id": 948
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning through Selective Parameter Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts and catastrophic forgetting when tasks compete for shared parameters. We propose Gradient Surgery with Memory (GSM), a simple modification to existing multi-task optimization that maintains an episodic memory of past gradients to inform selective parameter updates. Our method identifies conflicting gradients through cosine similarity analysis and selectively freezes or adapts learning rates for specific parameters based on their contribution to both current and previous tasks. On CIFAR-100 split into 5 tasks and a new benchmark of 8 NLP tasks from GLUE and SuperGLUE, GSM achieves comparable performance to state-of-the-art methods while requiring 30% less memory and 2x fewer hyperparameters. However, we find that GSM's benefits diminish when task similarity is low or when the number of tasks exceeds 15. While the approach shows promise, our theoretical analysis reveals that the selective update rule can create undesirable local minima in specific parameter regimes. Our results suggest that gradient-based solutions to catastrophic forgetting may be fundamentally limited when task distributions differ significantly.",
    "id": 949
  },
  {
    "title": "Improved Generalization via Iterative Model Compression with Adaptive Dropout",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, V."
    ],
    "abstract": "We propose an iterative compression framework that alternates between pruning neural network weights and dynamically adjusting dropout rates based on layer-wise sensitivity scores. While existing compression methods typically apply static dropout or remove weights independently, our approach uses a second-order approximation to identify which connections contribute most to the loss curvature, then adjusts dropout probabilities inversely proportional to pruning sensitivity. Experiments on CIFAR-10/100 and ImageNet demonstrate 2-4% accuracy improvements over standard magnitude-based pruning when targeting similar compression ratios. However, gains diminish with larger architectures, and our method introduces computational overhead from repeated sensitivity calculations. Theoretical analysis shows our approach reduces the capacity-compression bound by O(log d) compared to naive pruning, though this improvement is modest relative to the practical complexity. Code and pretrained models are available at [URL withheld for review].",
    "id": 950
  },
  {
    "title": "Repulsive Gibbs Sampling: Variance Reduction through Negative Correlation",
    "authors": [
      "Liu, K.",
      "Bennett, J.C.",
      "Ramanathan, A."
    ],
    "abstract": "Markov Chain Monte Carlo (MCMC) methods often exhibit high variance when sampling from complex, multimodal distributions. We propose Repulsive Gibbs Sampling (RGS), a simple modification to standard Gibbs sampling that introduces negative correlation between consecutive samples by discouraging return to previously visited states. Our method adds a repulsive potential to the conditional distributions, controlled by a kernel on the sample history. Unlike deterministic alternatives, RGS maintains the theoretical guarantees of MCMC while improving empirical performance. We prove that RGS preserves the target stationary distribution and establish mixing time bounds under mild assumptions. On synthetic benchmarks with multiple modes, RGS reduces variance by 10-40% compared to standard Gibbs and Metropolis-Hastings. In applications to Bayesian neural networks and topic models, we observe consistent variance reduction in posterior sampling, though improvements in downstream predictive performance are modest. While limited to discrete spaces and partially exchangeable models, RGS offers a lightweight approach to variance reduction without hyperparameter tuning. We provide PyTorch code for reproducibility.",
    "id": 951
  },
  {
    "title": "Adaptive Learning Rates via Online Thinning of Recurrent States",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a novel approach to learning rate adaptation in recurrent neural networks by maintaining and selectively pruning intermediate hidden states during training. Our method, called Recurrent State Thinning (RST), maintains a fixed-size reservoir of previously computed states and uses a lightweight online procedure to identify which states provide the most predictive signal for future steps. States deemed less informative are excluded from gradient computations, effectively creating an adaptive sparse computation graph. Experiments on language modeling and time series prediction tasks show modest improvements over carefully tuned baselines: RST achieves 2-4% lower perplexity on Penn Treebank and 5-7% better MSE on synthetic sinusoidal datasets. While the computational overhead is negligible (under 3% additional runtime), the approach requires careful hyperparameter tuning and provides limited benefits on shorter sequences. Our analysis reveals that the primary advantage stems from preventing gradient accumulation in redundant state dimensions rather than discovering fundamentally new learning dynamics. Code and experiments are available at anonymous-link.github.io.",
    "id": 952
  },
  {
    "title": "LoRA-ME: Low-Rank Adaptation with Meta-Initialized Bases for Few-Shot Learning",
    "authors": [
      "Chen, Y.",
      "Kumar, S.",
      "Vasquez, J."
    ],
    "abstract": "We propose LoRA-ME, an extension of Low-Rank Adaptation (LoRA) that uses meta-learned initialization of the low-rank matrices for improved few-shot adaptation. While LoRA has become a standard parameter-efficient fine-tuning method, we observe that randomly initialized low-rank matrices often require substantial data to converge to meaningful directions. Our key insight is that we can meta-learn a better initialization for these low-rank matrices by optimizing across tasks such that the adaptation matrices align with task-relevant subspaces. We achieve this by training a small meta-network that outputs the initial matrices given task-specific features extracted from the pre-trained model's activations. On the Meta-Dataset benchmark, LoRA-ME achieves 2.3% absolute improvement over standard LoRA (56.1% vs 53.8%) and 1.1% over full fine-tuning (55.0%). However, results are inconsistent across datasets, with improvements mainly on fine-grained classification tasks. While our method introduces minimal overhead and preserves LoRA's parameter efficiency, the meta-learning procedure requires careful tuning and can be unstable for larger model sizes. These limitations suggest LoRA-ME provides modest but not transformative improvements, making it most useful for practitioners with limited computational budgets who prioritize parameter efficiency.",
    "id": 953
  },
  {
    "title": "Gradient Surgery Revisited: Do We Really Need Structured Sparsity in Neural Network Pruning?",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Neural network pruning methods increasingly rely on sophisticated gradient-based techniques to determine which weights to remove, often incorporating complex regularization schemes and structured sparsity patterns. We revisit the foundations of gradient-based pruning through an extensive empirical study of 500+ trained networks across vision and NLP tasks, revealing that simple magnitude pruning with appropriate initialization and learning rate scheduling achieves competitive compression ratios (within 2% of state-of-the-art) while eliminating the need for intricate sparsity structures. Our theoretical analysis shows that the effectiveness of many contemporary pruning methods can be attributed to implicit regularization effects rather than explicit gradient-based criteria, suggesting that the computational overhead of these approaches may be unnecessary. We propose a simplified pruning framework that achieves comparable compression to SOTA methods with 3-5x faster training time. While our results challenge the necessity of complex pruning algorithms, they also highlight fundamental limitations in current pruning benchmarks and raise questions about the true source of compression benefits in structured approaches.",
    "id": 954
  },
  {
    "title": "Lookahead Gradient Descent with Adaptive Memory: Balancing Speed and Stability in Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "We propose Lookahead Gradient Descent with Adaptive Memory (LGAM), a simple modification to gradient-based optimization that adaptively blends current gradients with an exponentially decaying history of past directions. While look-ahead methods have shown empirical promise in deep learning, theoretical understanding remains limited. We provide convergence guarantees for LGAM in non-convex settings under standard smoothness assumptions, showing O(1/T) rates with slightly improved constants compared to vanilla SGD in certain cases. Our key contribution is an adaptive memory mechanism that automatically adjusts the effective window size based on gradient variance estimates. Experiments on CIFAR-10/100 and ImageNet show 1-3% improvements over tuned baselines on ResNet architectures, though gains vary significantly across tasks and hyperparameter settings. We conjecture LGAM's effectiveness stems from implicit variance reduction during optimization. While the method is straightforward to implement and requires minimal memory overhead, experiments reveal sensitivity to the variance estimation hyperparameter, and benefits diminish with increasingly aggressive data augmentation. Code is available at [URL].",
    "id": 955
  },
  {
    "title": "LoRA++: A Slight Tweak to Low-Rank Adaptation That Sometimes Helps",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed low-rank structure may not optimally adapt to downstream tasks. We present LoRA++, a simple extension that adds a learned scaling parameter to the low-rank matrices. Our method introduces minimal computational overhead (0.01% additional parameters) while achieving modest improvements on standard benchmarks. We conduct extensive experiments on GLUE and SuperGLUE with various base models (BERT-base, RoBERTa-large, Llama-2-7B), showing average improvements of 0.8-1.2% over standard LoRA across tasks. While some tasks show gains up to 3%, we find performance can slightly degrade in others. Theoretical analysis reveals our scaling factor adapts the effective rank of adaptations, though the benefits diminish with larger models. Ablation studies indicate that initializing the scaling parameter at 1.0 is crucial. Our code and checkpoints are available. These results suggest that while LoRA++ is easy to implement and occasionally helpful, its impact is not consistent enough to recommend universal adoption.",
    "id": 956
  },
  {
    "title": "Residual Policy Gradient Methods with Adaptive Trust Region Scaling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Policy gradient methods often struggle with sample efficiency and stable convergence, particularly in high-dimensional action spaces. We propose Residual Policy Gradient (RPG), a simple modification to standard policy gradient algorithms that learns a residual correction to a base policy using adaptive trust region constraints. Our key insight is that the residual formulation allows for more aggressive updates while maintaining monotonic improvement guarantees. We derive theoretical bounds showing that RPG achieves comparable sample complexity to TRPO while requiring fewer hyperparameter tunings. Empirical evaluation on continuous control benchmarks demonstrates 15-30% improvement over PPO and SAC baselines on half of the MuJoCo environments, though results are inconsistent across domains. Our ablation studies reveal that the adaptive trust region is crucial: removing it degrades performance below vanilla policy gradient levels. While our theoretical analysis relies on strong assumptions about Lipschitz continuity that may not hold in practice, we provide empirical evidence suggesting the method remains effective when these assumptions are violated. Code and pre-trained models are available at anonymous-github-link.",
    "id": 957
  },
  {
    "title": "Adaptive Block-Size Scheduling for Gradient Descent: A Middle-Ground Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose Adaptive Block-Size Scheduling (ABSS), a simple modification to mini-batch gradient descent that adjusts batch size based on gradient variance estimates during training. While existing adaptive-batch methods rely on complex second-order approximations or expensive hyperparameter tuning, ABSS uses only running averages of gradient norms to dynamically scale batch sizes between 1/4 and 4x of the initial value. Our theoretical analysis shows ABSS achieves convergence rates within a logarithmic factor of optimally-tuned fixed-batch methods for convex and limited non-convex settings. Empirically, ABSS improves wall-clock training time by 5-15% on standard vision and language benchmarks compared to tuned baselines, but the gains diminish with sophisticated optimizers like AdamW. Our method requires minimal code changes and introduces negligible computational overhead. However, we observe that benefits are most pronounced in data-limited regimes, and results are inconsistent across highly-tuned architectures like Vision Transformers. ABSS provides a practical middle-ground between fixed-batch training and more complex adaptive methods, though its impact may be limited in well-resourced training scenarios.",
    "id": 958
  },
  {
    "title": "Memory-Efficient Training of Transformers via Blockwise Token Dropping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose TokenDrop, a simple method for reducing memory usage during transformer training by dynamically dropping non-critical tokens in attention blocks. Unlike previous approaches that drop tokens based on fixed heuristics, TokenDrop learns a lightweight gating mechanism that identifies and removes tokens with minimal impact on downstream performance. Specifically, we train a small per-layer network that predicts token importance scores using only local context, allowing us to discard up to 40% of tokens in encoder layers while maintaining gradients for the remaining tokens. Our method requires no changes to model architecture and can be applied to existing pretrained checkpoints with minimal fine-tuning. Experiments on GLUE and Wikitext-103 show 25-35% memory reduction with <2% performance degradation on most tasks, though we observe larger drops on tasks requiring long-range dependencies. While our approach primarily targets memory-constrained training scenarios, we also demonstrate benefits for inference latency when combined with batch processing. However, we acknowledge that TokenDrop may not be suitable for tasks where arbitrary token dropping is harmful, and our method currently lacks theoretical guarantees on the impact of dropping policies.",
    "id": 959
  },
  {
    "title": "Improving Transformer Training Stability via Layer-wise Perturbation Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Transformer training instability remains a persistent challenge, particularly when scaling to deeper architectures. We propose a lightweight layer-wise perturbation analysis framework that identifies unstable layers during training and applies targeted regularization. Our method computes gradient covariance statistics at each layer and introduces a novel regularization term that penalizes directions with high gradient variance. This approach requires minimal computational overhead (less than 2% increase in training time) and can be integrated into existing training pipelines without architectural modifications. Experiments on language modeling and machine translation tasks with 12-24 layer Transformers show modest improvements: 0.3-0.7 BLEU score gains on WMT14 English-German translation and 1.2 perplexity reduction on Wikitext-103. While our regularization improves training stability metrics including gradient norm consistency and loss curve smoothness, ablation studies reveal that benefits diminish with careful hyperparameter tuning of baseline models. Code and pre-trained models will be released upon acceptance.",
    "id": 960
  },
  {
    "title": "Improving Transformer Efficiency through Layer-wise Gradient Magnitude Pruning at Initialization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Gradient-Aware Initialization Pruning (GAIP), a method for pruning transformer networks at initialization using layer-wise gradient magnitude analysis. Our approach computes gradient magnitudes for each layer using a small batch of training data, then removes the lowest magnitude 30% of weights before training begins. Unlike existing pruning methods that require extensive retraining or complex algorithmic overhead, GAIP operates in a single forward-backward pass at initialization. Experiments on GLUE and WMT translation tasks show GAIP reduces parameter counts by 25-35% while maintaining 92-96% of original performance across tasks, compared to 87-94% for magnitude pruning baselines. The method achieves 1.1x training speedup and modest memory savings without architectural modifications. Our theoretical analysis connects gradient magnitudes at initialization to final layer importance through a simplified linear approximation, though we acknowledge this provides only loose bounds for deeper networks. Limitations include slight convergence instability on smaller datasets and reduced effectiveness on downstream tasks requiring substantial fine-tuning. While GAIP offers practical benefits for practitioner workflows, particularly the ability to prune models without hyperparameter search, we recognize the theoretical foundations require strengthening for more principled guarantees.",
    "id": 961
  },
  {
    "title": "Improving Gradient Descent with Adaptive Step-Size Recycling",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Recycling Descent (GRD), a simple modification to standard gradient descent that reuses past gradients to inform step-size selection. By maintaining an exponentially decaying average of previous gradients weighted by their alignment with the current gradient direction, GRD adaptively adjusts the learning rate without requiring additional hyperparameters beyond the base learning rate. We provide theoretical analysis showing convergence guarantees for convex objectives that match standard gradient descent rates up to constant factors, and demonstrate empirical improvements over vanilla SGD on CIFAR-10 and ImageNet classification tasks, achieving 0.8-1.2% accuracy improvements over baselines when combined with standard data augmentation. While our method shows promise, we acknowledge limitations including computational overhead from gradient storage and theoretical gaps for non-convex settings. Implementation requires only 5 lines of code and can be integrated into existing training pipelines. Code is available at anonymous.url.",
    "id": 962
  },
  {
    "title": "Rethinking Curriculum Learning for Neural Architecture Search: A Curriculum-Based Sampling Strategy",
    "authors": [
      "Chen, L.",
      "Joshi, K.",
      "Thompson, M."
    ],
    "abstract": "Neural Architecture Search (NAS) remains computationally expensive despite recent advances in weight sharing and gradient-based methods. We propose Curriculum Neural Architecture Search (CNAS), which applies curriculum learning principles to progressively guide the search towards more complex architectures. Our method begins by training simpler architectures with shared weights, then gradually introduces more sophisticated operations based on their historical validation performance. Specifically, we maintain a curriculum scheduler that dynamically adjusts operation probabilities based on their observed learning difficulty, analogous to curriculum learning in supervised training. We evaluate CNAS on CIFAR-10 and ImageNet, achieving competitive results (2.7% and 25.4% test error respectively) while reducing search cost by 35% compared to standard DARTS. However, our approach introduces sensitivity to curriculum hyperparameters and shows diminishing gains on larger datasets. While CNAS provides a novel perspective on combining curriculum learning with NAS, its improvements appear incremental and the computational savings may not justify the additional complexity. Our code is available at anonymous.url.",
    "id": 963
  },
  {
    "title": "Gradient Amplification Makes Transformers More Efficient: A Surprising Discovery in Language Model Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We present a simple yet counterintuitive technique that significantly accelerates transformer training by selectively amplifying gradient magnitudes during backpropagation. Our method, termed Gradient Amplification for Transformers (GAT), multiplies gradients by a learned factor \u03b2 at each layer, where \u03b2 is dynamically computed using a lightweight parameter prediction network. While gradient clipping is standard practice to prevent exploding gradients, we show that controlled amplification in early training stages leads to faster convergence and improved final perplexity across 7 language modeling datasets. Experiments on GPT-2 (124M-774M parameters) demonstrate 1.15-1.3\u00d7 speedup in wall-clock time to match baseline performance, though benefits diminish for larger models. Theoretical analysis reveals GAT implicitly performs a form of trust-region optimization, explaining its efficacy. However, we observe the technique can destabilize training for certain hyperparameter configurations, particularly with large learning rates. Our method requires minimal code changes and adds <0.5% parameter overhead. While the speed improvements are consistent, they are modest; we acknowledge GAT may be most useful as a practical training trick rather than a fundamental advance.",
    "id": 964
  },
  {
    "title": "Lookahead-Guided Gradient Clipping for Training Transformers on Hardware-Accelerated Gradients",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, J.",
      "Wang, H."
    ],
    "abstract": "Training large transformer models remains computationally expensive, with gradient clipping serving as a crucial but often heuristic component. We revisit gradient clipping through the lens of hardware-accelerated gradient statistics, arguing that existing clipping strategies underutilize information available during backpropagation. Our approach, Lookahead-Guided Gradient Clipping (LGGC), computes gradient norms over local temporal windows using vectorized operations already present in modern hardware kernels. By maintaining exponentially-decayed gradient statistics directly on accelerator memory, LGGC achieves adaptive clipping without additional synchronization overhead. On decoder-only architectures (125M-1B parameters), LGGC achieves a 12-15% reduction in training iterations compared to standard clipping baselines, with particularly strong gains on long-sequence tasks (>2k tokens). However, benefits diminish for larger models (7B+), and our method requires careful hyperparameter tuning across datasets. Theoretically, we establish convergence guarantees under standard smoothness assumptions but show our clipping threshold lower bounds are looser than prior work. While not universally superior, LGGC provides a practical approach for medium-scale deployments where compute efficiency matters.",
    "id": 965
  },
  {
    "title": "LoRA-Quant: Efficient Fine-tuning Through Low-Rank Adapter Quantization",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M.",
      "Singh, P."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its memory footprint remains prohibitive for deployment on edge devices. We propose LoRA-Quant, a simple approach that quantizes LoRA adapters to 4-bit precision while maintaining a full-precision base model. Our method combines a novel quantization-aware training strategy with an importance-aware bit allocation scheme that assigns higher precision to adapters in critical layers. On GLUE and SuperGLUE benchmarks, LoRA-Quant achieves within 2% accuracy of full-precision LoRA with 4.3\u00d7 memory reduction during inference. However, we observe significant performance degradation on tasks requiring complex reasoning (e.g., 8.2% drop on DROP), suggesting limitations in our quantization strategy. While our results demonstrate clear memory benefits, the task-specific degradation raises questions about the universality of our approach. Code and models are available at [URL].",
    "id": 966
  },
  {
    "title": "Single-Timeline Transformer: Efficient Attention for Long Streaming Inputs",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "While transformers have demonstrated remarkable capabilities across domains, their quadratic complexity in sequence length remains a fundamental limitation for streaming applications. We propose Single-Timeline Transformer (STT), a simple modification to standard attention that processes sequences along a single temporal timeline rather than in discrete chunks. STT maintains a fixed-size memory bank that dynamically ages out older hidden states based on learnable temporal decay, eliminating the need for complex hierarchical attention or sparse patterns. Our method requires only 8 lines of code to implement and can be dropped into existing transformer architectures with minimal hyperparameter tuning. On long-range language modeling benchmarks (PG-19, arXiv), STT achieves perplexity within 3% of Longformer while using 4x fewer parameters and training 1.5x faster on 8xA100 GPUs. However, we observe instabilities on sequences longer than 64k tokens, and our fixed decay rate may be suboptimal for certain data distributions. While STT provides a practical alternative to existing long-form methods, its reliance on monotonic forgetting might limit performance on tasks requiring fine-grained long-range dependencies.",
    "id": 967
  },
  {
    "title": "Towards More Robust Non-Contrastive Self-Supervised Learning via Adaptive Eigenvalue Regularization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Garcia, J."
    ],
    "abstract": "Non-contrastive self-supervised learning methods like BYOL and SimSiam have shown impressive performance on vision tasks, but their objective functions exhibit notable instability during training, particularly in the final layers. We identify that this stems from spurious eigenvalue concentrations in the learned representation covariance matrices. Motivated by this observation, we propose Adaptive Eigenvalue Regularization (AER), a simple yet effective technique that applies spectral decay constraints weighted by the batch-wise eigengap distribution. Our method requires only a single hyperparameter and adds minimal computational overhead. We validate AER across CIFAR-10, CIFAR-100, and ImageNet-100, where it improves linear probing accuracy by 1.2-2.3% over baseline methods with 15% reduced training variance. While our empirical results are encouraging, we note that the theoretical justification for why eigenvalue regularization improves non-contrastive learning remains incomplete. Furthermore, performance gains diminish on larger-scale datasets. Our extensive ablations show AER is most beneficial when the pretraining dataset is small-to-medium scale and the encoder architecture is relatively shallow. Code and pretrained models are available at [redacted URL].",
    "id": 968
  },
  {
    "title": "LoRA-G: Low-Rank Adaptation with Learnable Growth Rates for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular approach for efficiently fine-tuning large language models, but its effectiveness relies heavily on the choice of rank hyperparameters that are typically fixed across layers. We propose LoRA-G, a simple extension that learns layer-specific growth rates to dynamically adjust rank allocations during training. Our method introduces a lightweight gating mechanism that modulates the contribution of each low-rank decomposition based on gradient statistics, requiring only 0.3% additional parameters. Experiments on GLUE and SuperGLUE benchmarks show improvements of 1.2-2.1% over standard LoRA when using comparable parameter budgets, though gains vary significantly across tasks and model scales. While LoRA-G consistently outperforms static baselines on smaller models (\u22643B parameters), benefits diminish on larger architectures. We provide theoretical analysis suggesting the approach is most beneficial when layer-wise gradient patterns exhibit high variance. Our code and pre-trained adapters will be released upon acceptance.",
    "id": 969
  },
  {
    "title": "Self-Supervised Contrastive Learning with Adaptive Negative Sampling and Curriculum",
    "authors": [
      "Chen, L.",
      "Johnson, M.K.",
      "Rodriguez, A."
    ],
    "abstract": "We propose Adaptive Curriculum Contrastive Learning (ACCL), a self-supervised framework that dynamically adjusts negative sample selection during training. While recent contrastive methods rely on fixed negative mining strategies, we observe that the importance of negatives varies significantly across training stages. Our approach uses an auxiliary network to predict the informativeness of negative samples based on gradient signals, prioritizing hard negatives early in training and shifting to more diverse negatives later. We introduce a curriculum schedule that gradually increases the number of negatives from 32 to 4,096, guided by an entropy-based measure of feature collapse. On ImageNet-1K, ACCL achieves 69.1% linear evaluation accuracy with ResNet-50, improving over SimCLR by 1.8% while using 23% less training time. The method also shows consistent gains on transfer tasks including CIFAR-10 (+1.2%) and Places205 (+0.9%). However, our ablations reveal that the adaptive mechanism provides only marginal benefits over strong baselines when proper hyperparameters are selected. Code and pretrained models will be made available upon acceptance.",
    "id": 970
  },
  {
    "title": "Frequency-Aware Gradient Clipping for Noisy Dataset Training",
    "authors": [
      "Liu, S.",
      "Chen, H.",
      "Dubey, A."
    ],
    "abstract": "We study the problem of training deep neural networks on datasets with label noise, focusing on the underexplored interaction between noisy gradients and frequency components of learned representations. While gradient clipping is widely used to stabilize training, standard approaches treat all gradient directions uniformly. We propose Frequency-Aware Gradient Clipping (FAGC), which modulates clipping thresholds based on the spectral energy of gradients in the Fourier domain. Our method computes running averages of gradient spectra during training and applies adaptive clipping that is more aggressive for high-frequency gradient components, which we empirically show correlate with label noise. Across three benchmark datasets (CIFAR-10/100 with synthetic noise, WebVision, and Clothing1M), FAGC achieves modest improvements (0.5-2.1% accuracy) over strong baselines like GJS and DivideMix, while requiring minimal hyperparameter tuning. However, we find ablations revealing that half these gains can be achieved by tuning standard clipping parameters on validation data. Theoretical analysis proves convergence under restricted noise models, but extending to realistic settings remains open. Our code is available at [anonymized].",
    "id": 971
  },
  {
    "title": "Gradient Descent with Adaptive Momentum via Curvature-Weighted Polyak Steps",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a modification to gradient descent that adaptively sets the momentum parameter based on local curvature estimates weighted by Polyak step sizes. Our method combines inexpensive curvature estimates from the gradient history with the theoretically-motivated Polyak step size, yielding an optimizer that automatically adjusts momentum without hyperparameter tuning. The key insight is that sharper curvatures warrant reduced momentum to prevent overshooting, while flat regions benefit from increased momentum for faster convergence. Through extensive experiments on ResNet-18 trained on CIFAR-10 and several language modeling tasks with Transformers, we demonstrate 5-12% faster convergence in wall-clock time compared to standard momentum SGD and Adam, while maintaining comparable final accuracy. Our method requires minimal computational overhead (2-3% increase in training time) and works with existing automatic differentiation frameworks. We provide theoretical analysis showing convergence for convex quadratic objectives and establish non-convex convergence rates matching standard momentum methods. While our empirical gains are consistent across vision and language tasks, we observe negligible improvements on extremely noisy datasets when using heavy regularization.",
    "id": 972
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Continual Learning through Selective Forgetting",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rodriguez, A.",
      "M\u00fcller, K."
    ],
    "abstract": "We propose a simple modification to existing gradient-based continual learning methods by introducing an adaptive memory mechanism that selectively forgets gradient directions based on their relevance to current tasks. Our method, GAM (Gradient surgery with Adaptive Memory), computes task-specific gradient projections in a learned memory space where less important directions are periodically pruned based on accumulated Fisher information. Unlike prior work requiring task labels at inference, GAM operates solely on gradient statistics, making it more practical for real-world scenarios. We evaluate GAM on standard benchmarks including Split-CIFAR-100 and Sequential Mini-ImageNet, achieving moderate gains over naive gradient projection methods (2-3% improvement in average accuracy) while reducing memory overhead by 40%. However, our approach underperforms compared to more sophisticated methods that explicitly model task relationships. Theoretical analysis shows GAM optimizes a relaxed upper bound on catastrophic forgetting, though this bound is looser than those of competing methods. Our results suggest that while selective forgetting shows promise for mitigating catastrophic forgetting, the practical benefits remain modest and may not justify the additional complexity in many settings. Code is available at [URL].",
    "id": 973
  },
  {
    "title": "Gradient Amplification Reduces Mode Collapse in GAN Training",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Thompson, A."
    ],
    "abstract": "We present a simple modification to the generator update in GAN training that significantly reduces mode collapse without architectural changes. Our method, Gradient Amplification for Generator Training (GAG), scales the generator gradients by the ratio of discriminator loss to generator loss during training. This amplifies updates when the generator is underperforming relative to the discriminator, encouraging exploration of underrepresented modes. We provide theoretical motivation via analysis in a simplified bilinear setting, establishing local convergence guarantees under standard assumptions. Empirically, GAG achieves modest but consistent improvements across image generation benchmarks: on CIFAR-10, we obtain FID scores of 28.3\u00b10.4 (vs 30.1\u00b10.5 for vanilla DCGAN) and demonstrate improved mode coverage on synthetic 2D datasets. While our empirical gains are limited to simpler architectures and the theoretical analysis requires restrictive assumptions, GAG offers a lightweight alternative to complex regularization schemes for practitioners. Code will be released upon publication.",
    "id": 974
  },
  {
    "title": "Revisiting Label Smoothing with Temperature Scaling: A Unified Framework for Confidence Calibration",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Label smoothing and temperature scaling are widely used techniques for improving confidence calibration in deep neural networks, yet their relationship remains poorly understood. We propose a unified framework that interprets both methods as instances of a general regularization principle based on Renyi entropy maximization. Our theoretical analysis establishes that label smoothing corresponds to a fixed-point regularization, while temperature scaling adapts this regularization based on the logit distribution. Building on this insight, we introduce Adaptive Label Temperature (ALT), which combines dynamic smoothing with learned temperature parameters. While ALT achieves modest improvements in Expected Calibration Error (2.1% reduction on ImageNet compared to standard temperature scaling), we find its benefits are most pronounced on small datasets (CIFAR-100) where overconfident predictions are common. However, the computational overhead of learning additional parameters and the limited out-of-domain robustness raise questions about practical adoption. Our extensive ablations reveal that simpler temperature scaling combined with moderate label smoothing often matches ALT's performance with fewer hyperparameters. We open-source our implementation to facilitate reproducibility and future work on calibration techniques.",
    "id": 975
  },
  {
    "title": "Temporal-Consistent Data Augmentation for Self-Supervised Video Representation Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Self-supervised learning has shown remarkable success for image representation learning, but extending these methods to video remains challenging due to the complexity of temporal dynamics. We propose TC-Aug, a temporal consistency regularization framework for self-supervised video representation learning that enforces smooth transitions between augmented clips. Our approach introduces differentiable augmentation operations that preserve temporal coherence while maximizing mutual information between temporally overlapping clips. We achieve this through a novel consistency loss that penalizes abrupt feature transitions in the learned representation space. Experiments on standard action recognition benchmarks (UCF-101 and HMDB-51) demonstrate 2.3-3.7% improvements over baseline methods when fine-tuning with limited labeled data. However, our approach shows diminishing returns with larger labeled datasets and performs comparably to simpler pre-training strategies on downstream tasks like video retrieval. While our method provides consistent gains in the low-data regime, the computational overhead of our temporal consistency constraints may limit practical adoption. Code and pre-trained models will be released upon publication.",
    "id": 976
  },
  {
    "title": "Improving Transformer Sample Efficiency via Adaptive Token Dropping with Learnable Budgets",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Rodriguez, J."
    ],
    "abstract": "Transformer models exhibit impressive performance but suffer from quadratic computational complexity with respect to sequence length. While recent token dropping methods reduce computation by pruning non-essential tokens, they rely on fixed dropping ratios or heuristics that fail to adapt across tasks and input distributions. We propose Adaptive Token Dropping (ATD), which learns task-specific dropping budgets through a lightweight controller network conditioned on input statistics. Our method dynamically adjusts dropping ratios across layers, preserving informative tokens while removing others via attention-weighted scoring. Experiments on GLUE, SQuAD, and ImageNet show ATD reduces FLOPs by 22-35% with <1% accuracy degradation compared to full attention. However, we observe performance drops on tasks requiring long-range dependencies, and our approach requires additional hyperparameter tuning for optimal results. While ATD provides modest efficiency gains, the learned budgets often converge to near-static values, suggesting limited adaptability. Our code and checkpoints are available at: [url to be released after review]",
    "id": 977
  },
  {
    "title": "Revisiting Momentum Schedulers: A Simple Baseline for Faster Convergence in Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "While adaptive optimizers like Adam and RMSprop dominate neural network training, momentum remains widely used due to its simplicity and robust performance. We explore whether carefully-designed momentum schedules can bridge the gap between momentum and adaptive methods. We propose Cyclical Momentum (CyMo), a simple cosine-annealed momentum schedule that adjusts \u03b21 based on the relative gradient noise magnitude during training. Our empirical analysis on CIFAR-10/100 and ImageNet demonstrates that CyMo achieves 8-12% faster convergence than standard momentum SGD on ResNet architectures, with comparable final accuracy. However, gains diminish on larger batch settings (\u22654096) and transformer architectures, where adaptive methods retain advantages. We provide theoretical insights through a quadratic approximation showing CyMo's noise-adaptive property reduces variance while maintaining bias. While not universally outperforming baselines, CyMo offers a parameter-free alternative to grid search, and our extensive ablations reveal sensitivity to learning rate warmup schedules. Our code is available at github.com/cymo-team/cyclical-momentum.",
    "id": 979
  },
  {
    "title": "Revisiting Curriculum Learning with Adaptive Difficulty Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Curriculum learning has shown promise in accelerating convergence and improving final performance across various domains, but existing approaches rely heavily on hand-designed curricula or domain-specific heuristics. We propose Adaptive Difficulty Sampling (ADS), a simple yet effective method that automatically adjusts the sampling distribution of training examples based on model confidence scores. Unlike prior work that requires explicit curriculum design, ADS uses the model's own uncertainty estimates to dynamically favor examples of appropriate difficulty. Our method requires minimal computational overhead and can be integrated with existing training pipelines. We evaluate ADS on image classification benchmarks (CIFAR-10/100) and a neural machine translation task, achieving 2-3% improvements over baseline training in the low-data regime. However, these gains diminish with larger datasets and more complex architectures. While our approach provides a lightweight alternative to manual curriculum design, we acknowledge its limitations in domains where confidence estimates are unreliable. Our results suggest that adaptive sampling based on model uncertainty offers modest but consistent benefits in specific scenarios, though further investigation is needed to understand when and why such methods succeed.",
    "id": 980
  },
  {
    "title": "Revisiting Weight Averaging with Learned Interpolation Coefficients",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Weight averaging has emerged as a simple yet effective technique for improving model robustness in deep learning. However, the standard approach of using uniform weights across checkpoints may be suboptimal. We propose Adaptive Checkpoint Interpolation (ACI), a method that learns optimal interpolation coefficients for weight averaging based on the curvature characteristics of the loss landscape. Our approach uses a small validation-based objective to find non-uniform weights that minimize expected sharpness while preserving training loss. Experiments on CIFAR-10/100 and ImageNet show modest improvements over standard weight averaging, with ACI achieving 0.8-1.2% accuracy gains on ResNet architectures at no additional inference cost. While our theoretical analysis shows these benefits are bounded by the linear approximation quality, empirical results suggest the method is particularly effective when checkpoints exhibit high gradient diversity. Our implementation requires minimal code changes and adds only 2% training overhead. Though the improvements are incremental, ACI provides a practical alternative to uniform averaging and highlights the potential for geometry-aware model combination techniques.",
    "id": 981
  },
  {
    "title": "Spectral Normalization with Learnable Frequency Constraints for Improved GAN Training",
    "authors": [
      "Chen, L.",
      "Zhang, K.",
      "Rodriguez, J."
    ],
    "abstract": "We propose Learnable Spectral Normalization (LSN), a variant of spectral normalization that introduces frequency-dependent constraint parameters learned during GAN training. While standard spectral normalization uniformly constrains all frequency components of weight matrices to enforce Lipschitz continuity, LSN allows the model to dynamically adjust these constraints based on the discriminator's frequency sensitivity. Our method adds only 0.3% additional parameters and minimal computational overhead. We demonstrate improved Fr\u00e9chet Inception Distance (FID) on CIFAR-10 (from 11.2 to 10.1) and STL-10 (from 19.4 to 17.8) compared to standard SN, though gains on ImageNet remain marginal (36.7 vs 37.2). Analysis reveals LSN primarily benefits mid-frequency ranges, suggesting spectral normalization may be overly restrictive at certain frequencies. However, we observe similar improvements can be achieved with tuned regularization coefficients, raising questions about the method's novelty versus careful hyperparameter optimization.",
    "id": 982
  },
  {
    "title": "Improving Neural Network Generalization Through Layer-Wise Entropy Regularization",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Layer-wise Entropy Regularization (LER), a simple yet effective technique for improving generalization in deep neural networks. LER adds entropy penalties at individual layers to encourage disentangled representations, drawing inspiration from information bottleneck principles. Unlike end-to-end mutual information regularization, our method computes entropy locally based on mini-batch statistics, making it computationally lightweight and tuning-free. Through experiments on CIFAR-10/100 and ImageNet-subset, we demonstrate 2-3% accuracy improvements over vanilla baselines with minimal hyperparameter tuning. While our theoretical analysis only establishes loose generalization bounds, ablations show that LER provides complementary benefits to standard regularization techniques like dropout and weight decay. However, we observe diminishing returns on larger models and our method adds non-trivial memory overhead during training. Our empirical results suggest LER may be most beneficial for moderate-scale vision tasks, though we leave theoretical justification and broader applicability as open questions.",
    "id": 983
  },
  {
    "title": "Efficient Gradient Compression through Learned Sparse Patterns in Distributed Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient compression is essential for scaling distributed training to large clusters, but existing methods either require significant hyperparameter tuning or suffer from accuracy degradation in federated settings. We propose Learned Sparse Patterns (LSP), a compression scheme that adaptively identifies low-importance gradient coordinates using a lightweight auxiliary network trained alongside the primary model. Our method achieves compression ratios of 50-100x while maintaining model accuracy within 2% of uncompressed baselines on ImageNet and language modeling tasks. The auxiliary network learns to predict compression masks based on gradient statistics, eliminating manual threshold selection required by Top-K sparsification. Experiments on up to 64 GPUs demonstrate 2-3x communication speedup over existing methods like PowerSGD and SignSGD, though we observe diminishing returns beyond 90% sparsity and sensitivity to auxiliary network initialization. While our approach is limited to synchronous training and smaller models due to auxiliary network overhead, LSP provides a practical trade-off between compression, accuracy, and implementation complexity for distributed settings with bandwidth constraints.",
    "id": 984
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Does It Actually Help?",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks. Recent work proposes gradient surgery methods like PCGrad that modify gradients to reduce interference. While these methods show empirical gains, our theoretical analysis reveals their effectiveness depends crucially on the geometric alignment of task gradients in weight space. We prove that gradient surgery provides minimal improvement when task gradients are either highly aligned or severely conflicting, only helping in a narrow intermediate regime. To address this limitation, we propose Adaptive Gradient Surgery (AGS), which dynamically adjusts the intervention threshold based on gradient statistics. Experiments on 8 multi-task benchmarks show AGS improves over PCGrad by 3-7% when task conflicts are moderate, but achieves no gains with extreme alignment or divergence. Our results suggest gradient surgery is not universally beneficial and highlight the need for task-aware optimization strategies.",
    "id": 985
  },
  {
    "title": "Progressive Gradient Pruning with Adaptive Sparsity Budgets for Resource-Constrained Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Neural network pruning methods often rely on expensive retraining procedures or require careful tuning of pruning ratios across layers. We propose Progressive Gradient Pruning (PGP), a simple method that uses gradient information to dynamically adjust pruning decisions during training. PGP maintains an adaptive sparsity budget that increases based on layer-wise gradient statistics, removing weights with low magnitude gradients rather than low magnitude weights. We theoretically analyze PGP for convex quadratic objectives, showing convergence to a stationary point under moderate assumptions. Empirically, PGP achieves 75-90% sparsity on ResNet-50 and BERT with <1% accuracy loss on ImageNet and GLUE tasks, matching or slightly improving upon magnitude pruning baselines. However, we observe that PGP's benefits diminish with modern optimizers like AdamW and adaptive schedules. Our method provides a practical alternative to existing pruning techniques, though gains over simple baselines are modest and implementation introduces additional hyperparameters requiring careful tuning.",
    "id": 986
  },
  {
    "title": "Improving Transformer Training Efficiency with Progressive Depth Scaling",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a training strategy for transformers that gradually increases model depth during training, aiming to reduce computational costs while maintaining final performance. Our method starts training with a shallow network and progressively adds new transformer layers as convergence slows. The approach is motivated by the observation that early training phases primarily focus on learning lower-level features, which may not require the full model capacity. We introduce a layer addition schedule based on gradient norm statistics and a knowledge distillation mechanism to maintain training stability. Experiments on language modeling (WikiText-103) and machine translation (WMT'14 EN-DE) show 18-25% reduction in training FLOPs compared to standard training, with less than 0.5 BLEU drop in translation quality. However, we find the method is sensitive to the addition schedule and shows diminishing returns on larger models (>1B parameters). The technique is complementary to existing efficiency methods but requires careful hyperparameter adjustment for different tasks.",
    "id": 987
  },
  {
    "title": "LazySAM: Efficient Segment Anything Model Training via Gradient Dropout",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kumar, S."
    ],
    "abstract": "The Segment Anything Model (SAM) has demonstrated impressive zero-shot segmentation capabilities but requires massive training resources. We propose LazySAM, a simple yet effective training technique that reduces SAM's training time by 37% with minimal performance degradation. Our key insight is that only certain image regions contribute meaningfully to gradient updates during contrastive learning. By introducing a stochastic gradient dropout mechanism that selectively freezes backbone weights during low-information training batches, we achieve training efficiency without architectural changes. Experiments on COCO and LVIS datasets show that LazySAM maintains 94.3% of SAM's zero-shot mIoU while reducing GPU hours from 192 to 121. While our method shows promise for democratizing SAM-style model development, we acknowledge limitations: performance drops exceed 5% on rare object categories, and gradient dropout requires careful tuning across datasets. Nevertheless, LazySAM offers a practical approach for researchers with limited computational budgets to adapt SAM to new domains, as demonstrated through successful fine-tuning on medical imaging datasets with 8\u00d7 reduced training resources.",
    "id": 988
  },
  {
    "title": "Gradient Surgery in the Wild: Practical Considerations for Multi-Task Optimization in Deep Neural Networks",
    "authors": [
      "Liu, C.",
      "Johnson, M.",
      "Chen, S."
    ],
    "abstract": "Multi-task learning promises improved sample efficiency and generalization, yet gradient conflicts between tasks often lead to suboptimal performance. While recent work has proposed various gradient surgery techniques (e.g., PCGrad, GradDrop), their practical effectiveness remains poorly understood. We conduct an extensive empirical study of 8 gradient manipulation methods across 6 multi-task benchmarks spanning computer vision and NLP. We find that simple baselines often match or exceed the performance of sophisticated surgery methods, with diminishing returns beyond 4 well-selected tasks. Surprisingly, naive gradient averaging with carefully tuned loss weights achieves 95% of the performance of the best surgery method across experiments. While our results clarify the practical limitations of existing approaches, they also suggest that the community may be over-engineering solutions to a problem that requires better task selection rather than more complex optimization. We release our codebase and comprehensive hyperparameter sweeps to facilitate reproducibility.",
    "id": 989
  },
  {
    "title": "Towards Stability in Vision Transformers: Batch Normalization at Attention Interfaces",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "While Vision Transformers (ViTs) have achieved impressive results across vision tasks, their training remains notoriously unstable, particularly when scaling beyond standard configurations. We identify that attention layer activations exhibit high variance during training, leading to gradient instabilities that worsen with model depth. Instead of modifying core attention mechanisms, we propose BNAI (Batch Normalization at Attention Interfaces) - a simple, modular approach that inserts BatchNorm directly after attention computations but before residual connections. This placement allows us to leverage batch statistics to regularize attention outputs without disrupting the self-attention mechanism itself. We conduct experiments on ImageNet-1k with ViT-B/16 and ViT-L/16 architectures, showing up to 5% faster convergence and 0.7% top-1 accuracy improvements over baselines. Ablation studies reveal the importance of placement - moving BatchNorm elsewhere (post-MLP or pre-attention) degrades performance. While our method improves stability, we observe marginal gains on smaller datasets (CIFAR-10/100), suggesting the approach primarily benefits larger-scale training regimes. We release code to reproduce our results.",
    "id": 990
  },
  {
    "title": "Gradient Surgery Revisited: An Analysis of Multi-Task Optimization Trade-offs in Overparameterized Networks",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "Multi-task learning often employs gradient surgery techniques to address conflicting gradients between tasks, but theoretical understanding remains limited. We revisit gradient surgery through the lens of implicit gradient regularization in overparameterized networks. Our key insight is that gradient conflict resolution acts as an implicit regularizer on the optimization trajectory, affecting generalization in non-obvious ways. We prove that under certain conditions, gradient surgery can actually hurt generalization on individual tasks, even when the multi-task objective is improved. Using a combination of linear network analysis and empirical studies on CIFAR-scale vision tasks, we show that the effectiveness of gradient surgery depends critically on task similarity and network width. Our theoretical results characterize regimes where gradient surgery provides benefits versus where it introduces excessive regularization. Experiments on task groups from Meta-dataset demonstrate 2-3% improvements over standard methods in some settings, but also reveal consistent 1-2% degradation in others. These findings suggest that the choice of gradient surgery techniques should be data-dependent rather than universally applied, and provide practical guidelines for when to avoid these methods.",
    "id": 991
  },
  {
    "title": "Improving Neural Network Robustness via Progressive Input Gradient Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We propose a novel regularization technique for improving adversarial robustness in neural networks by penalizing the input gradients during training. Unlike previous approaches that use fixed regularization weights, our method progressively increases the penalty based on the local Lipschitz constant estimated during training. We demonstrate improvements on CIFAR-10 and ImageNet against PGD attacks, achieving 2-3% better robust accuracy compared to standard adversarial training baselines. While our approach is computationally lightweight (adding <5% training time) and works with existing architectures, we observe that the benefits diminish on stronger attacks not seen during training. Theoretical analysis shows our regularizer bounds the local Lipschitz constant but relies on strong assumptions about data manifold structure. Experiments on additional datasets and attack scenarios yield inconsistent results, highlighting the need for careful hyperparameter tuning. Our method provides a practical trade-off between clean accuracy and robustness but may not directly advance the fundamental understanding of adversarial examples. Code and pre-trained models will be made available.",
    "id": 992
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning with Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, particularly in overparameterized neural networks. We propose Gradient Surgery (GS), a simple modification to standard multi-task optimization that detects and resolves gradient conflicts through selective projection. Unlike previous approaches that require task-specific architectures or expensive Hessian computations, GS operates purely on the gradients computed during backpropagation, making it compatible with existing training pipelines. Our method projects each task's gradient onto the null space of conflicting directions, prioritizing tasks with higher loss reductions. Experiments on standard benchmarks including NYUv2, CityScapes, and CIFAR-100 show 2-5% improvement over baselines. However, we find gains diminish on larger architectures (>100M parameters) and when tasks are weakly correlated. While GS reduces gradient interference, our theoretical analysis reveals limited improvement in the final minima quality due to the presence of multiple adequate solutions. The simplicity of our approach makes it practical for moderate-scale multi-task scenarios, though it does not address fundamental questions about task interference in the overparameterized regime.",
    "id": 993
  },
  {
    "title": "Improving Transformer Training via Layer-wise Curriculum Learning",
    "authors": [
      "Liu, S.",
      "Morrison, J.",
      "Chen, B."
    ],
    "abstract": "We investigate whether gradually increasing transformer depth during training can improve final performance and training efficiency. Our approach, Progressive Depth Curriculum (PDC), begins training with a shallow 4-layer transformer and incrementally adds layers every 3 epochs until reaching the full architecture. We present a simple addition technique that preserves learned representations when adding new layers, based on duplicating and perturbing existing weights. Experiments on English-French translation and WikiText-103 language modeling show modest but consistent improvements: 0.3 BLEU score gains on WMT'14 and 0.8 perplexity reduction compared to baseline training. While these improvements are statistically significant (p<0.05), the practical impact is marginal. Our analysis reveals that PDC primarily helps by providing better initialization for deeper layers, reducing early training loss, but fails to address fundamental optimization challenges in transformers. The method introduces minimal computational overhead (97.5% of baseline training time) and can be implemented in under 20 lines of PyTorch code. However, ablations show benefits disappear when using stronger optimizers like AdamW, suggesting curriculum effects may be optimizer-dependent. We conclude that while layer-wise curriculum learning offers theoretical appeal, its practical utility appears limited for standard transformer architectures.",
    "id": 994
  },
  {
    "title": "Re-Thinking Feature Attribution in Vision Transformers: A Topological Perspective",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "We propose TopoVI, a method for explaining Vision Transformer (ViT) predictions using persistent homology on attention maps. While existing attribution methods for ViTs focus on individual attention heads or token interactions, we show that considering the topological structure of attention patterns provides complementary insights. Our approach computes persistence diagrams from multi-head attention matrices, capturing the scale and stability of feature relationships across layers. We introduce a novel topological score that aligns these diagrams with model outputs, enabling attribution at both patch and object levels. Experiments on ImageNet and CIFAR-10 show TopoVI achieves comparable faithfulness to gradient-based methods while offering qualitatively different explanations, particularly for misclassified examples. However, we find that the computational overhead (3-5x slower than integrated gradients) and the need for task-specific hyperparameter tuning limit practical deployment. Our results suggest topological analysis reveals useful inductive biases in ViTs, though the benefits over simpler baselines may be modest for standard architectures. Code and precomputed diagrams are provided at anonymous-url.github.io/topo-vi.",
    "id": 995
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Importance Weighted Sampling in Stochastic Gradient Descent",
    "authors": [
      "Chen, L.",
      "Vaswani, A.",
      "Jain, P.",
      "Zhao, K."
    ],
    "abstract": "We investigate the theoretical properties of importance weighted sampling (IWS) in neural network optimization, motivated by its empirical success in other domains. While SGD is widely used, its uniform sampling of training examples may lead to suboptimal generalization when training data exhibits class imbalance. We propose IW-SGD, which reweights samples based on their gradient norm history during training. Our theoretical analysis provides PAC-Bayesian generalization bounds that depend on the effective sample size under the learned sampling distribution. Experiments on CIFAR-10/-100 and ImageNet subsets show 1-3% accuracy improvements over standard SGD, particularly on long-tailed datasets. However, we observe diminishing returns as network capacity increases. While our bounds are theoretically valid, they are an order of magnitude looser than existing bounds. Importantly, our experiments rely on careful hyperparameter tuning and the gains appear dataset-specific. The method adds minimal computational overhead (\u22485% increase in training time) and can be implemented in few lines of code. Our results suggest IWS is a promising but incremental contribution to the generalization puzzle, with clear limitations in settings with balanced data or overparameterized networks.",
    "id": 996
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Towards Stable Multi-Task Learning in Transformers",
    "authors": [
      "Liu, K.",
      "Anderson, J.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning in transformers often suffers from conflicting gradients that destabilize training and limit performance gains. While recent gradient surgery methods like PCGrad reduce interference between task gradients, they rely on static projection mechanisms that can negatively impact learning dynamics. We propose GAM-Surgery, a memory-augmented approach that selectively retains beneficial gradient directions using an adaptive queue mechanism. Our method maintains a small reservoir of historical gradients (\u2248 1% of model parameters) and computes similarity scores to determine when surgical intervention is necessary. Experiments on GLUE and SuperGLUE benchmarks show 2-4% improvements over standard multi-task baselines, particularly on low-resource tasks. However, we observe that gains diminish with larger models (\u2265 1B parameters) and are sensitive to hyperparameter choices in the memory update frequency. Theoretical analysis reveals our method provides a local Lipschitz guarantee under restricted assumptions that may not hold in practice. While GAM-Surgery offers practical improvements for mid-scale transformer applications, we acknowledge its limitations in extreme multi-task regimes and provide extensive ablations to guide future work.",
    "id": 997
  },
  {
    "title": "Gradient Noise Improves Sharpness-Aware Minimization in Low-Resource Settings",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "We investigate the empirical observation that adding carefully calibrated Gaussian noise to gradients during training can sometimes improve generalization in Sharpness-Aware Minimization (SAM). Through experiments on CIFAR-10/100 with ResNet-18 and Vision Transformers, we find that gradient noise with standard deviation \u03c3 \u2248 0.01 \u00d7 ||g||/\u221ad (where g is the gradient and d is parameter count) can improve test accuracy by 1-2% in low-data regimes (5-20% of training data). Our theoretical analysis for quadratic loss functions suggests this benefit emerges from noise-induced smoothing of SAM's implicit regularizer, though the effect diminishes with larger datasets. While experiments on ImageNet show minimal improvement, our results suggest gradient noise could be a simple enhancement for SAM in data-limited scenarios. Code and checkpoints are available at [link].",
    "id": 998
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simple Baseline That (Sometimes) Works",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts between tasks, leading to suboptimal performance and unstable training. We propose GSAM-S (Gradient Surgery with Adaptive Mixing), a lightweight extension to existing gradient surgery methods that dynamically adjusts the mixing coefficient based on gradient similarity. Our method adds minimal computational overhead (<1% increase in training time) and can be integrated into existing training pipelines with less than 10 lines of code. On a suite of 8 standard multi-task benchmarks, GSAM-S achieves modest improvements over PCGrad (average +1.3% F1) and gains are most pronounced when task gradients have low cosine similarity (<0.3). However, we observe that performance gains diminish or even become negative on tasks with aligned gradients, suggesting the method's benefits are context-dependent. Through ablation studies, we find that the adaptive component contributes minimally to final performance (+0.2% on average), raising questions about whether simple heuristics might suffice. While our method provides a practical baseline for handling gradient conflicts, we acknowledge that gains are incremental and may not justify the added complexity in many applications. Our code and pretrained models are available at anonymized-url.github.io.",
    "id": 999
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning via Low-Rank Gradient Perturbations",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to catastrophic forgetting when optimizing shared representations. While recent gradient surgery methods provide promising solutions, they require computing and storing full gradients for all tasks, creating computational bottlenecks that scale poorly with task count. We propose Memory-GS, a lightweight gradient surgery approach that maintains task-specific gradient directions using low-rank perturbations of a shared gradient basis. Our method approximates task gradients using rank-r matrices (r<<d) updated via efficient rank-one updates, reducing memory from O(dT) to O(drT) where d is parameter dimension and T is task count. Experiments on 10-layer MLPs across 5 multi-task benchmarks show our method achieves 91% of the performance of full-gradient surgery while using 4.7\u00d7 less memory. However, we observe significant performance degradation when tasks exhibit high gradient conflict (>60\u00b0 alignment), suggesting limitations in highly adversarial multi-task settings. While not a universal solution to multi-task learning, Memory-GS provides a practical trade-off between efficiency and effectiveness for moderately-sized task suites.",
    "id": 1000
  },
  {
    "title": "Learning to Forget: Selective Memory Damping for Continual Learning in Small Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Continual learning in language models remains challenging due to catastrophic forgetting when adapting to new tasks. We propose Selective Memory Damping (SMD), a simple regularization technique that selectively suppresses gradient updates to parameters based on their historical importance scores. Our method maintains a running estimate of parameter sensitivity using Fisher information and applies soft weight decay proportional to accumulated gradients. Unlike rehearsal-based methods, SMD operates without storing previous examples or maintaining explicit task boundaries. We evaluate SMD on a diverse set of NLP tasks including sentiment analysis, named entity recognition, and question-answering across 5 sequential settings. Results show modest improvements over standard fine-tuning (average F1 improvement of 3.2%) and competitive performance against stronger baselines like EWC, while adding minimal computational overhead (15% increase in training time). While our method achieves promising results on small models (GPT-2 scale), scaling challenges emerge when applied to larger architectures. Our findings suggest that selective regularization provides a lightweight alternative for mitigating forgetting, though it may not fully resolve the stability-plasticity dilemma in large-scale continual learning settings.",
    "id": 1001
  },
  {
    "title": "LoRA-CLIP: Parameter-Efficient Fine-Tuning for Cross-Modal Retrieval at Scale",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Cross-modal retrieval models require expensive fine-tuning on target domains, yet suffer from catastrophic forgetting when adapting to new tasks. We propose LoRA-CLIP, a parameter-efficient adaptation method that injects low-rank adapters into CLIP's vision and text encoders at multiple transformer layers. Our key insight is that cross-modal alignment benefits from learning task-specific interactions while preserving universal representations. We evaluate on 12 downstream retrieval benchmarks spanning e-commerce, medical imaging, and scientific literature. LoRA-CLIP achieves 94.2% of full fine-tuning performance with only 2.3% trainable parameters, requiring 4.7\u00d7 less GPU memory. However, we observe significant performance drops (up to 12%) on out-of-distribution data compared to full fine-tuning. Through ablation studies, we identify that adapter placement in deeper layers substantially impacts retrieval precision, suggesting learned features may overfit to dataset-specific correlations. While our method enables practical deployment of large vision-language models, theoretical analysis remains limited, and hyperparameter sensitivity across domains poses deployment challenges for practitioners.",
    "id": 1002
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Historical Gradient Recombination",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impair optimization. While existing gradient surgery methods like PCGrad and GradDrop modify gradients at each step, we propose Adaptive Memory Gradient Surgery (AMGS) which leverages historical gradient information to better resolve conflicts. Our key insight is that past gradient directions contain useful information about task relationships that can inform current gradient modification. AMGS maintains an exponentially decaying memory of per-task gradients and uses attention mechanisms to compute task-specific gradient projections that minimize interference. On three standard multi-task vision datasets (CityScapes, NYUv2, and CIFAR-100), AMGS shows consistent improvements over gradient surgery baselines, achieving +1.2% mIoU and +0.8% accuracy on average. However, we find these gains diminish in low-data regimes and when task count exceeds 5. While AMGS provides a practical improvement over existing methods with minimal computational overhead (5% training time increase), our theoretical analysis suggests the approach may not guarantee convergence in pathological cases. Code and pretrained models are available at [repository].",
    "id": 1003
  },
  {
    "title": "Gradient Descent with Periodic Restarts Improves Robustness to Dataset Corruption",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Restarted Gradient Descent (RGD), a simple modification to standard gradient descent that periodically resets the optimizer state while maintaining a decaying learning rate schedule. Our key observation is that common vision datasets contain memorizable corrupted examples that can dominate the learning dynamics, and periodic restarts help escape these spurious minima. We provide theoretical analysis showing RGD achieves similar convergence rates to standard GD on convex objectives while offering improved robustness to label noise. Empirically, we demonstrate 2-5% accuracy improvements over vanilla SGD on CIFAR-10 and ImageNet when 20-40% of labels are corrupted. While our method shows promise for noisy training scenarios, we acknowledge the improvements are modest and task-specific. Ablation studies reveal the benefits diminish as training duration increases, suggesting RGD may primarily act as a form of implicit regularization. Our code is available, though we note hyperparameter sensitivity in the restart schedule requires careful tuning. These results suggest periodic restarts as a lightweight addition to existing training pipelines when robustness to data quality is a concern.",
    "id": 1004
  },
  {
    "title": "Gradient Descent with Memory-Augmented Step Sizes: A Lightweight Approach to Adaptive Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose MANGO (Memory-Augmented Neurally Guided Optimization), a lightweight optimizer that augments standard gradient descent with a small recurrent network trained to predict step sizes based on historical gradient patterns. Unlike expensive learned optimizers, MANGO uses only 32 hidden units and operates online without meta-training on task distributions. Our key insight is that recent gradient history contains sufficient signal to predict reasonable step sizes for many practical problems, suggesting that complex learned optimizers may be overparameterized. We evaluate MANGO on image classification benchmarks and language modeling tasks, where it achieves competitive performance to Adam/AdamW while using 10-50x fewer parameters than previous learned optimizers. However, MANGO shows inconsistent gains on transformer architectures and fails to outperform SGD+momentum on some ResNet experiments. Analysis reveals the learned step size policy primarily exploits second-order structure that could be captured more efficiently by quasi-Newton methods. While our approach provides a middle ground between hand-designed and fully learned optimizers, its benefits appear constrained to moderate-scale vision tasks. Code and experiments are available at [URL].",
    "id": 1005
  },
  {
    "title": "Gradient Descent with Adaptive Step Sizes via Online Learning of Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose OLLR, a new optimizer that uses online learning to adaptively set per-parameter learning rates during gradient descent. While existing adaptive methods like Adam and Adagrad rely on historical gradient norms, OLLR frames the learning rate adaptation problem as a multi-armed bandit problem, treating each parameter as an independent agent choosing between a small set of candidate learning rates. Our approach builds on EXP3, using gradient magnitudes as bandit rewards with a novel variance-reduction technique. Experiments on ResNet-18 for CIFAR-10 and a transformer for Wikitext-103 show OLLR achieves comparable final accuracy to AdamW (within 0.5%) while requiring 20-40% fewer updates to reach 95% of final performance. However, OLLR introduces two additional hyperparameters and increases memory usage by ~30%. Theoretical analysis provides a regret bound of O(\u221aT log K) where K is the number of candidate learning rates, matching standard online learning rates. We discuss limitations including increased computational overhead and sensitivity to the candidate learning rate set. Code will be made available.",
    "id": 1006
  },
  {
    "title": "Gradient Descent with Memory-Efficient Second-Order Momentum: A Practical Compromise",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Anderson, J."
    ],
    "abstract": "While second-order optimization methods can accelerate neural network training, their memory requirements typically scale quadratically with parameter count, limiting practical adoption for large models. We propose Memory-Efficient Second-Order Momentum (MESOM), a hybrid approach that maintains the computational simplicity of first-order methods while incorporating selective curvature information. MESOM approximates diagonal blocks of the Hessian using a compressed representation based on random projections, reducing memory overhead from O(d\u00b2) to O(d log d) where d is the parameter dimension. Our method requires only minor modifications to standard SGD implementations and introduces a single hyperparameter controlling the trade-off between curvature information and memory usage. Experiments on ResNet and Transformer architectures show 5-12% faster convergence compared to AdamW with tuned hyperparameters, though gains plateau for very large models (>1B parameters). While MESOM does not achieve the asymptotic benefits of full second-order methods, it provides consistent improvements over first-order baselines for mid-scale models at minimal implementation cost. Code will be released upon acceptance.",
    "id": 1007
  },
  {
    "title": "Efficient Gradient Compression via Learned Basis Decomposition",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, T."
    ],
    "abstract": "We propose a novel gradient compression technique for distributed deep learning that learns basis vectors for gradient decomposition on-the-fly. Unlike existing quantization or sparsification methods that rely on fixed heuristics, our approach dynamically optimizes a low-dimensional basis tailored to each layer's gradient statistics. The method uses a small auxiliary neural network trained concurrently with the main model to predict optimal compression parameters given the current gradient distribution. We demonstrate compression ratios of 40-60x on ResNet-50 and Transformer architectures with minimal accuracy degradation (<1% on ImageNet). While our approach shows promising results compared to static methods like top-k and QSGD, particularly for large-batch training, the computational overhead of basis learning reduces throughput gains when bandwidth is not severely constrained. Our implementation achieves 1.2-1.4x speedup in high-latency scenarios but shows marginal benefits (>0.9x slowdown) on well-connected clusters. The method is most effective when gradients exhibit low-rank structure, which we observe more consistently in later training phases, suggesting potential integration with existing compression schedules.",
    "id": 1008
  },
  {
    "title": "Gradient Surgery Made Simple: A Lightweight Framework for Multi-Task Learning via Adaptive Projection",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance. While recent gradient surgery methods have shown promise, they typically require expensive eigenvalue computations or introduce significant memory overhead. We propose a lightweight alternative that uses low-rank projections to resolve gradient conflicts without heavy machinery. Our key insight is that gradient conflicts can be effectively mitigated by projecting updates onto a shared subspace that aligns task-specific gradients, computed using a fast randomized SVD. On standard benchmarks, our method achieves comparable performance to state-of-the-art gradient surgery techniques while reducing computational cost by 2-3x. However, we observe that benefits diminish for tasks with highly dissimilar loss landscapes, where more sophisticated approaches may be warranted. Our experiments on CIFAR-100 and NYUv2 demonstrate modest improvements over single-task baselines (1-3% average accuracy gains), suggesting that gradient conflict is not always the primary bottleneck in multi-task learning. While our approach offers practical advantages for resource-constrained settings, we acknowledge limitations in handling extreme task conflicts and leave theoretical guarantees for future work.",
    "id": 1009
  }
]