[
  {
    "title": "Frequency-Adaptive Gradient Clipping for Noisy Optimization in Low-Precision Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is essential for stable training with noisy gradients, but standard global clipping fails to account for the frequency characteristics of gradient noise. We propose Frequency-Adaptive Gradient Clipping (FAGC), which adjusts clipping thresholds based on spectral analysis of gradient components. Our method computes the power spectral density of gradients via FFT over a sliding window and sets clipping thresholds inversely proportional to high-frequency energy. Experiments on CIFAR-10 and ImageNet show 2-5% improvements in final accuracy compared to standard clipping when training with 8-bit precision and label noise. The approach is most effective when training with aggressive learning rates or corrupted labels. While FAGC adds computational overhead (15-20% training time), we demonstrate its benefits persist across vision transformer and ResNet architectures. Limitations include sensitivity to the spectral window parameter and marginal benefits in clean, full-precision settings. Code will be made available post-publication.",
    "id": 1010
  },
  {
    "title": "AdaCurvature: Adaptive Step-Sizes via Second-Order Finite Differences in SGD",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose AdaCurvature, a simple modification to stochastic gradient descent that adjusts step-sizes using an estimate of the local curvature computed via finite differences of gradients. Unlike quasi-Newton methods that maintain full Hessian approximations, AdaCurvature uses only coordinate-wise curvature estimates with O(d) complexity per iteration, making it practical for large-scale training. Our method combines insights from trust-region methods with a novel forgetting mechanism that down-weights outdated curvature information. Experiments on CIFAR-10/100 and ImageNet with ResNet-18/50 show 5-12% faster convergence compared to AdamW on computer vision tasks, while matching or slightly exceeding SGD with manual tuning. However, we observe diminishing benefits on language modeling tasks, where gradient noise dominates curvature estimates. Theoretical analysis establishes convergence for convex functions under standard assumptions, though the rates do not improve upon SGD. While the computational overhead of 2\u00d7 gradient evaluations may not always justify the gains, AdaCurvature provides a practical alternative to manual learning rate tuning when some overhead is acceptable. Code is available at [URL].",
    "id": 1011
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique for Stochastic Optimization via Iterative Gradient Masking",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Gradient Dropout, a novel regularization technique that randomly zeros out gradient components during optimization. Unlike standard dropout which operates on activations, our method stochastically masks a fraction of gradient entries in each iteration, theoretically reducing the effective Lipschitz constant of the loss landscape. While similar in spirit to gradient clipping and noise injection, Gradient Dropout achieves regularization by selectively ignoring gradient directions rather than scaling them. We provide convergence guarantees for convex objectives under standard assumptions, extending standard SGD analysis to our biased gradient estimator. Empirically, we observe modest improvements over vanilla SGD on CIFAR-10 and CIFAR-100 with ResNet architectures, achieving 0.3-0.5% absolute accuracy gains on average. However, performance degrades on smaller networks and more challenging datasets like ImageNet. Our results suggest the technique's benefits are most pronounced when the gradient signal contains significant noise or redundancy. While the theoretical analysis is complete, the practical impact appears limited to specific regimes. Code is available to reproduce all experiments.",
    "id": 1012
  },
  {
    "title": "Gradient Descent with Layer-wise Learning Rates for Improved Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Layer-Adaptive Gradient Descent (LAGD), a simple modification to standard SGD that uses different learning rates for different transformer layers based on gradient norm statistics. Unlike existing adaptive optimization methods that modify parameter update directions, LAGD maintains the same update direction but scales step sizes using a lightweight running average of squared gradient norms. We show that this approach leads to faster convergence in early training phases for both language modeling and vision transformers, achieving 5-12% speedup in reaching target validation loss on CIFAR-10 and Wikitext-103. While LAGD appears to help optimization dynamics, we find the benefits diminish with careful hyperparameter tuning of baselines and do not consistently translate to downstream task improvements. Theoretical analysis reveals the method can be viewed as approximate diagonal preconditioning, though with weaker guarantees than full adaptive methods. Our experiments demonstrate the approach is particularly effective when base learning rates are mis-specified, suggesting LAGD primarily provides robustness benefits rather than fundamentally better optimization paths.",
    "id": 1013
  },
  {
    "title": "Rethinking Momentum Schedules for Low-Resource Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Vision Transformers (ViTs) have demonstrated remarkable performance across various computer vision tasks, but their training typically requires large datasets and extensive computational resources. While several techniques have been proposed to improve ViT training efficiency, the role of optimization hyperparameters, particularly momentum schedules, remains under-explored. We propose Adaptive Polyak Momentum (APM), a simple momentum scheduling method that dynamically adjusts the momentum parameter based on gradient coherence across mini-batches. Unlike fixed momentum schedules, APM increases momentum when gradients exhibit high directional consistency and decreases it during noisy updates. We evaluate APM when training ViT-Tiny and ViT-Small models on ImageNet-1K using only 10% of the training data. Our experiments show consistent 2-3% accuracy improvements over SGD with momentum and 1-2% gains over AdamW on average, while requiring minimal computational overhead. However, the benefits diminish with larger models and full dataset training, suggesting APM is most effective in low-resource regimes. Additionally, we observe high sensitivity to batch size choices and warming-up schedule length. While our theoretical analysis provides partial justification for the approach under simplified assumptions, we acknowledge the empirical gains may not generalize beyond the specific setups studied. Code and pre-trained models will be made available.",
    "id": 1014
  },
  {
    "title": "LoRA-Prune: Adaptive Low-Rank Adaptation for Efficient Parameter-Efficient Fine-Tuning",
    "authors": [
      "Kumar, S.",
      "Li, J.",
      "Garcia, M."
    ],
    "abstract": "We present LoRA-Prune, a method that combines low-rank adaptation with structured pruning to enable more parameter-efficient fine-tuning of large language models. While LoRA has emerged as a popular approach for reducing memory requirements during fine-tuning, we observe that many adapted low-rank matrices remain highly sparse, suggesting room for further compression. Our key insight is that the rank of LoRA adaptations can be dynamically adjusted per layer based on gradient information, while simultaneously removing unimportant rows/columns through magnitude-based pruning. We introduce a simple thresholding scheme that identifies low-contribution ranks without requiring additional validation data or expensive retraining. Experiments on GLUE and SQuAD benchmarks using RoBERTa-Large show LoRA-Prune achieves 35-50% parameter reduction over standard LoRA with <2% performance degradation in most tasks. However, we find the method underperforms on tasks requiring complex reasoning (e.g., DROP), suggesting the pruning heuristic may be overly aggressive. While additional gains are modest compared to existing compression techniques, LoRA-Prune offers a lightweight drop-in replacement for standard LoRA that reduces memory footprint without architectural changes. Code and pre-trained adapters will be made available.",
    "id": 1015
  },
  {
    "title": "Gradient Descent with Momentum is Provably Near-Optimal for Some Neural Network Optimization Problems",
    "authors": [
      "Kim, S.",
      "Garcia, L.",
      "Zhang, M."
    ],
    "abstract": "We study the convergence properties of stochastic gradient descent with momentum (SGDM) for training two-layer neural networks with smooth activations. Building on recent work in neural tangent kernel (NTK) analysis, we establish that SGDM achieves O(1/\u221aT) convergence for a specific class of radial basis function networks under minimal assumptions on initialization. While this rate matches the best known bounds for standard SGD, we show that momentum provides a small but non-negligible acceleration factor for certain problem instances when the target function has specific spectral properties. Experiments on synthetic regression tasks demonstrate 8-12% improvement over vanilla SGD in selected settings, though we observe diminishing returns on standard benchmarks like CIFAR-10. Our analysis relies on a modified NTK regime where the network width scales polylogarithmically with sample size rather than polynomially. While our theoretical results are more restrictive than concurrent work on overparameterized networks, we identify a small but well-defined problem class where momentum provably helps. Code and proofs are provided for reproducibility.",
    "id": 1016
  },
  {
    "title": "Gradient Routing with Learned Curvature Estimates for Neural Network Training",
    "authors": [
      "Liu, K.",
      "Chen, Y.",
      "Rodriguez, M."
    ],
    "abstract": "Recent work has shown that second-order optimization methods struggle to outperform carefully tuned SGD variants in deep learning, despite their theoretical advantages. We propose Gradient Routing Networks (GRNs) that adaptively combine first and second-order gradient directions using learned curvature estimates. Our approach trains a small auxiliary network to predict when second-order updates are beneficial, routing gradients through either standard backpropagation or a damped Newton step. We evaluate GRNs on image classification and language modeling tasks, finding consistent but modest improvements over AdamW and Shampoo baselines (0.5-1.2% top-1 accuracy on ImageNet, 1-3 perplexity points on WikiText-103). While our method achieves competitive results with lower computational cost than full second-order approaches, our gains are task-dependent and require careful hyperparameter tuning. Code and pretrained models are available at anonymous-url.github.io.",
    "id": 1017
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Merging with Differentiable Gating",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose TokenMerge, a lightweight method to reduce computational costs in Transformers by dynamically merging similar tokens during inference. Building on recent work in token pruning, our approach uses a learnable gating mechanism to softly merge tokens based on their attention patterns, preserving information while reducing sequence length by up to 50%. Our method introduces only 0.1% additional parameters and can be applied to pre-trained models without fine-tuning. Experiments on GLUE and ImageNet classification show 1.3-2.1x speedup with <2% accuracy loss on BERT-Base and ViT-Base. While our approach achieves consistent improvements over static baselines, gains are less pronounced on tasks requiring fine-grained token interactions. Analysis reveals the gating mechanism learns reasonable merging patterns but occasionally struggles with boundary tokens. Though promising for efficient deployment, our method currently underperforms on smaller models and shows diminishing returns on well-optimized implementations.",
    "id": 1018
  },
  {
    "title": "Frozen Pre-trained Transformers are Already Good Tabular Feature Extractors",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "While deep learning has dominated vision and NLP, tabular data remains the domain of tree-based methods like XGBoost and Random Forests. We investigate whether frozen pre-trained transformers can serve as strong tabular feature extractors without fine-tuning. By encoding numerical features as text tokens and categorical features as special tokens, we feed tabular rows into frozen BERT and RoBERTa models. We demonstrate that the resulting representations, when combined with simple linear models or shallow networks, achieve competitive performance on 20 benchmark datasets (average AUROC 0.834 vs. 0.839 for XGBoost). Surprisingly, our frozen transformer approach outperforms gradient-boosted trees on 8 datasets, particularly those with high-cardinality categorical variables. However, the effectiveness varies dramatically across domains \u2014 transformers excel on e-commerce and survey data but struggle on low-dimensional datasets. Our results suggest transformer-based tabular methods may not require expensive fine-tuning, but questions remain about scalability to large tables and interpretability of learned representations.",
    "id": 1019
  },
  {
    "title": "Gradient Perturbation Scheduling for Improved Federated Learning Convergence",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "Federated learning faces challenges from device heterogeneity and communication constraints, often requiring methods like gradient compression or selective participation that inject noise into the optimization process. While theoretical work has analyzed convergence properties under various noise models, practical implementations typically use fixed noise schedules that may not adapt to local training dynamics. We propose a simple heuristic scheduling approach that adjusts the magnitude of gradient perturbations based on local gradient statistics observed during training. Our method, inspired by adaptive optimizers like Adam, scales perturbation variance inversely with the exponential moving average of gradient norms. Unlike prior theoretical analyses requiring knowledge of problem parameters, our approach requires only tuning a single hyperparameter. On standard federated benchmarks with realistic client heterogeneity (MNIST with Dirichlet(0.1) partitioning and CIFAR-10 with device-specific augmentations), our scheduling improves convergence speed by 15-25% over fixed noise baselines while maintaining final accuracy. However, we observe diminishing returns in settings with small client datasets or low communication rounds. Our analysis reveals that improvements primarily occur when local data distributions exhibit moderate similarity between clients. While the method provides practical benefits for federated deployment, theoretical convergence guarantees remain limited to simplified convex settings, and understanding of when adaptive scheduling outperforms fixed approaches in non-convex optimization remains incomplete.",
    "id": 1020
  },
  {
    "title": "Revisiting Batch Normalization Through the Lens of Low-Rank Weight Matrices",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "We investigate the relationship between batch normalization and the low-rank structure of neural network weight matrices. While batch normalization is widely used for training deep networks, its interaction with parameter redundancy remains poorly understood. We propose a simple method that exploits low-rank approximations of weight matrices to reduce the computational cost of batch normalization during training. Our approach combines truncated SVD with a modified normalization scheme that operates on the compressed representation. We demonstrate 20-30% reduction in training time on standard image classification benchmarks (CIFAR-10/100, ImageNet) with minimal accuracy loss (<1%). However, we find that our method struggles with very deep networks (>100 layers) and certain architectures like transformers. Theoretical analysis suggests our approximation error grows with the effective rank of activations, though we lack tight bounds. While our results show promise for efficient training, the gains over existing pruning methods are incremental, and the general applicability beyond vision tasks remains unclear. Our implementation and trained models will be made publicly available.",
    "id": 1021
  },
  {
    "title": "Gradient Surgery on Transformer Attention Patterns Improves Few-Shot Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We propose Attention Gradient Surgery (AGS), a lightweight technique for improving few-shot learning in pre-trained transformers by strategically modifying attention patterns. Unlike existing meta-learning approaches that require extensive fine-tuning, AGS selectively amplifies or attenuates gradient flows through attention heads based on their consistency with support set labels. Our method introduces a saliency-corrected attention score that identifies which heads contribute to both task-relevant features and spurious correlations. Experiments on 8 few-shot benchmarks (mini-ImageNet, tiered-ImageNet, 4 NLP tasks, 3 medical datasets) show 2-4% improvements over standard fine-tuning baselines using only 5 gradient steps, with minimal computational overhead (additional 0.2% parameters). However, gains diminish as shot count increases, and we observe instability on tasks with distribution shift between support and query sets. Our analysis reveals AGS primarily benefits under-represented classes through improved attention entropy, but provides diminishing returns when base pre-training includes sufficient task diversity. Code and checkpoints will be released.",
    "id": 1022
  },
  {
    "title": "Gradient Surgery in Practice: When Multi-Task Optimization Falls Short",
    "authors": [
      "Liu, K.",
      "Chen, H.",
      "Rodriguez, P."
    ],
    "abstract": "Multi-task learning promises improved sample efficiency by leveraging shared representations across related tasks, but optimization conflicts often lead to suboptimal solutions. Recent gradient surgery methods address this by selectively combining gradients to minimize interference. We conduct a systematic empirical study of these methods across 12 tasks from SuperGLUE, Meta-World, and Cityscapes. Our results reveal that gradient surgery provides modest improvements in 7/12 tasks (average +1.3% over baselines) but introduces unexpected brittleness in low-resource regimes. Through careful ablation, we identify three failure modes: over-regularization of minority classes, gradient magnitudes leading to unstable training dynamics, and hyperparameter sensitivity. We propose a simple heuristic that uses task uncertainty to weight gradients adaptively, achieving 80% of the gains from more complex surgery approaches with no additional compute. While our method is theoretically grounded in second-order optimization theory, we acknowledge several limitations including restricted to shared-bottom architectures and lack of theoretical convergence guarantees. Our findings suggest that while gradient surgery can help, the fundamental challenge of task interference in multi-task learning remains inadequately addressed. Code and experiments available at anonymous.url.",
    "id": 1023
  },
  {
    "title": "Gradient Surgery for Training Transformers with Partial Parameter Sharing",
    "authors": [
      "Liu, J.",
      "Kumar, S.R.",
      "Chen, W."
    ],
    "abstract": "We present a simple but effective method for training transformer models where partial parameter sharing is enforced through gradient-based surgical procedures. Motivated by the observation that different layers in transformers often learn redundant representations, we propose to periodically merge similar parameters during training while preserving model capacity through careful gradient masking. Our approach selects parameters for merging based on cosine similarity of their gradients, and uses a first-order Taylor approximation to adjust the remaining parameters' gradients to account for the sharing operation. On standard NLP benchmarks including GLUE and SuperGLUE, our method achieves 97% of full parameter performance while reducing parameter count by 15-25%. While the gains are modest, we believe this represents a practical approach for resource-constrained deployment. However, we find the method is sensitive to hyperparameter choices and performs inconsistently across tasks. Theoretical analysis shows our gradient adjustment maintains first-order optimality but may lose higher-order guarantees. Code and pre-trained models will be released upon acceptance.",
    "id": 1024
  },
  {
    "title": "Gradient Surgery for Neural Network Pruning: When Less Structured Sparsity Might Be More",
    "authors": [
      "Chen, L.",
      "Joshi, K.",
      "Rodriguez, M."
    ],
    "abstract": "Neural network pruning has proven effective for reducing model size, but most methods rely on magnitude-based criteria that may not preserve gradient flow during fine-tuning. We propose Gradient Surgery for Neural Network Pruning (GSNP), a simple approach that removes weights based on their expected impact on gradient norms during subsequent optimization steps. Unlike iterative magnitude pruning, GSNP uses a first-order approximation to estimate how removing each weight affects the gradient norms of remaining parameters. We evaluate GSNP on ResNet-18 and ResNet-50 trained on ImageNet and CIFAR-10, achieving 70-75% sparsity with minimal accuracy degradation compared to baselines. Surprisingly, our method yields less structured sparsity patterns than prior work, yet converges faster during fine-tuning. While our approach provides competitive compression ratios, theoretical analysis reveals that gradient-based criteria can be suboptimal when weight magnitudes are highly correlated with gradient directions. Ablation studies show the method is sensitive to learning rate schedules and batch size choices. Code and pre-trained models will be released upon acceptance.",
    "id": 1025
  },
  {
    "title": "Gradient Surgery for Transformer Fine-tuning: A Combinatorial Approach to Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rossi, M.",
      "Kumar, S."
    ],
    "abstract": "We propose GradSurg, a novel approach to parameter-efficient fine-tuning that combines low-rank adaptation with gradient-based pruning decisions. Unlike existing methods that use fixed rank allocation, GradSurg dynamically selects which weight matrices to adapt based on a combinatorial optimization objective that balances downstream performance with computational budget. Our method prunes low-impact adapters using a surrogate gradient approximation, then surgically reconnects previously pruned adapters when evidence suggests their utility. We evaluate GradSurg on GLUE, SuperGLUE, and vision-language benchmarks, demonstrating 5-12% parameter reduction compared to LoRA while maintaining 97-99% of full fine-tuning accuracy. However, we observe high variance across tasks, with performance gaps of up to 8% on out-of-distribution data. Theoretical analysis shows our pruning decisions satisfy a local optimality guarantee under mild assumptions, though global optimality remains elusive. While GradSurg offers practical memory savings, our ablations suggest simpler rank scheduling heuristics achieve comparable results, raising questions about the necessity of our combinatorial framework.",
    "id": 1026
  },
  {
    "title": "Gradient Surgery Revisited: Do Adaptive Methods Really Need Gradient Surgery in Multi-Task Learning?",
    "authors": [
      "Liu, K.",
      "Thompson, E.",
      "Zhao, J."
    ],
    "abstract": "Multi-task learning (MTL) methods often struggle with conflicting gradients during optimization. While recent approaches like PCGrad and GradNorm have shown promise, their reliance on gradient surgery raises questions about generalization and computational overhead. We propose Adaptive Gradient Scaling (AGS), a simple modification that adjusts gradient magnitudes based on task uncertainty estimates rather than explicit gradient manipulation. Our method requires only 0.3% additional compute compared to standard multi-task training while achieving comparable or better performance on popular MTL benchmarks. On NYUv2, Cityscapes, and Taskonomy, AGS matches PCGrad's performance (\u00b10.8% absolute) while reducing training time by 15-20%. However, we find that AGS benefits are highly sensitive to task weight initialization and show diminishing returns as network capacity increases. Our theoretical analysis reveals AGS approximates gradient surgery only under limited conditions, suggesting more sophisticated approaches may be needed when tasks are strongly conflicting. While AGS provides a computationally efficient alternative to existing methods, our results indicate the multi-objective optimization problem in MTL remains fundamentally challenging, particularly for highly imbalanced or adversarial task combinations.",
    "id": 1027
  },
  {
    "title": "Gradient Alignment for Improved Transfer Learning: A Lightweight Alternative to Fine-tuning in Limited Data Regimes",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Transfer learning typically relies on computationally expensive fine-tuning procedures, which face challenges when target datasets are small. We propose Gradient Alignment for Transfer (GAT), a simple approach that aligns pretrained feature extractors with target tasks by optimizing only a small set of alignment parameters while keeping the backbone frozen. Our method computes gradient correlations between source and target domains to identify which layers require minimal adjustments, reducing trainable parameters by 95-99% compared to full fine-tuning. Experiments on five vision benchmarks and two NLP tasks show GAT achieves 87-94% of full fine-tuning performance while requiring 3-5x less training time. However, we observe consistent 2-4% performance gaps on larger target datasets, suggesting fundamental limitations in the alignment approach compared to standard fine-tuning. Theoretical analysis reveals that gradient alignment approximates a first-order Taylor expansion of full fine-tuning, explaining both its computational benefits and performance limitations. Our code is available at [anonymous link].",
    "id": 1028
  },
  {
    "title": "Learning with Partial Ambiguity Sets: A Distributionally Robust Approach to Soft Labels",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We consider supervised learning scenarios where soft labels (e.g., from noisy annotators or weak supervision) are provided alongside hard labels, but the reliability of these soft labels is unknown. While previous work has treated soft labels as ground truth or completely discarded them, we propose a distributionally robust optimization (DRO) framework that models partial ambiguity sets using Wasserstein balls centered at empirical distributions. Our key contribution is a theoretically grounded method for adaptively determining the radius of these ambiguity sets based on both the soft label reliability and the hardness of individual examples. We provide generalization bounds that depend on a novel notion of effective sample complexity that accounts for the quality of soft labels. Experiments on CIFAR-10 and ImageNet subsets with synthetic label noise show improvements of 1-3% over baselines when soft labels are moderately accurate, but performance degrades when noise dominates. While our approach provides principled handling of label uncertainty, the computational overhead (1.5-2\u00d7 training time) and the heuristic nature of the radius selection may limit practical impact.",
    "id": 1029
  },
  {
    "title": "Gradient Surgery for Partially Misaligned Objectives in Multi-Task Learning",
    "authors": [
      "Kumar, S.",
      "Chen, J.",
      "Zhao, L."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients when optimizing multiple objectives simultaneously. We propose a simple gradient modification scheme that selectively modifies conflicting components while preserving aligned directions. Our method computes the cosine similarity between task-specific gradients and intervenes only when similarity falls below a learned threshold \u03b1. We demonstrate modest improvements on three standard benchmarks: CIFAR-100 with 5 auxiliary tasks (+2.1% average accuracy), NYUv2 semantic segmentation (+1.8% mIoU), and Meta-World reinforcement learning (+3.4% success rate). While our approach is computationally lightweight and requires only gradient-level access, we find performance gains are inconsistent across task combinations and sensitive to threshold selection. Theoretical analysis shows our method recovers the Pareto optimal solution under certain alignment assumptions, though these conditions rarely hold in practice. Code is available at anonymous-url.github.io.",
    "id": 1030
  },
  {
    "title": "Efficient Second-Order Optimization for Neural Networks via Block-Diagonal Kronecker Approximation",
    "authors": [
      "Chen, L.",
      "Vaswani, S.",
      "Dubois, Y."
    ],
    "abstract": "Second-order optimization methods offer faster convergence than Adam for neural networks, but their computational cost and memory requirements remain prohibitive. We propose BOKA (Block-diagonal Optimizer with Kronecker Approximation), a practical second-order method that approximates the Hessian using block-diagonal Kronecker products of smaller matrices. Our key insight is that activation patterns in deep networks create natural block structures amenable to Kronecker factorization. By restricting updates to these blocks and using a damping schedule based on gradient norms, BOKA achieves comparable per-step cost to Adam while incorporating curvature information. Experiments on ResNet-50 training on ImageNet and GPT-2 fine-tuning show 1.3x and 1.1x speedups in convergence over Adam respectively, without hyperparameter tuning. While these gains are smaller than reported by previous methods like K-FAC or Shampoo, our approach requires significantly less memory (2x Adam vs. 4x for competitors) and can be implemented in ~200 lines of PyTorch. These results suggest BOKA as a middle-ground between first-order and more sophisticated second-order methods.",
    "id": 1031
  },
  {
    "title": "LoRA-Dropout: Structured Low-Rank Adaptation with Stochastic Regularization for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a dominant approach for parameter-efficient fine-tuning of large language models, but its regularization properties remain poorly understood. We propose LoRA-Dropout, a simple modification that applies structured dropout to the low-rank matrices during training. Our method requires only one additional hyperparameter and negligible computational overhead. Through experiments on GLUE, SuperGLUE, and three domain-specific datasets, we find that LoRA-Dropout improves robustness to rank selection in 65% of tasks, with an average 1.3 F1 improvement over standard LoRA when ranks are misspecified. However, results show high variance across tasks, with performance degrading on 2 out of 11 datasets. Theoretical analysis reveals that LoRA-Dropout acts as implicit nuclear norm regularization, but only under restrictive assumptions about the data distribution that may not hold in practice. While our method shows promise for rank-agnostic fine-tuning, the empirical benefits are modest and task-dependent. Code will be released upon acceptance.",
    "id": 1032
  },
  {
    "title": "LoRA-DC: Partial Weight Updates with Dynamic Compensation for Efficient Fine-Tuning",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Okafor, C."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become the de facto standard for parameter-efficient fine-tuning, but its fixed-rank limitation often leads to suboptimal trade-offs between performance and efficiency. We propose LoRA-DC, a method that dynamically adjusts the intrinsic rank during training while compensating for approximation errors through a lightweight correction mechanism. Our approach decomposes weight updates into a low-rank component and an error-correction term, both trained jointly via an alternating optimization procedure. The correction term is constrained to be ultra-sparse, adding minimal overhead (0.5-3% of original parameters). Experiments on BERT, RoBERTa, and Llama-2 show improvements of 0.5-2.3 F1 points over vanilla LoRA across GLUE and SuperGLUE tasks, while remaining within 2% of full fine-tuning. However, these gains are inconsistent across datasets, with some tasks showing no benefit or slight degradation. Runtime overhead is 15-30% compared to standard LoRA during training but negligible at inference. The method's effectiveness appears correlated with downstream task complexity, suggesting limitations in our rank selection heuristic. Code and models are available at [redacted].",
    "id": 1033
  },
  {
    "title": "Gradient Compression via Learned Binary Masks: A Simple Baseline for Distributed Training",
    "authors": [
      "Chen, L.",
      "Srinivasan, K.",
      "Johnson, M."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed deep learning, but most existing methods rely on complex sparsification schemes or quantization techniques that require careful hyperparameter tuning. We propose a surprisingly simple approach: learning binary masks to zero out gradient entries via a lightweight auxiliary network trained alongside the main model. Our method, LBM-SGD, uses a small 2-layer MLP that takes gradient magnitudes as input and outputs dropout probabilities for each parameter block. Compared to Top-K sparsification and 8-bit quantization, LBM-SGD achieves comparable compression ratios (70-90%) on ResNet-50 and Transformer training while maintaining model accuracy within 0.5% of baseline. However, we find our gains are most pronounced in specific regimes: when bandwidth is severely limited (<1 Gbps) and for models with >100M parameters. The learned masks exhibit block-level structure correlated with layer types, suggesting they identify less critical gradient components. While LBM-SGD does not consistently outperform carefully tuned traditional methods, its simplicity and lack of dataset-specific tuning make it a practical baseline for resource-constrained training. We release code to reproduce experiments on up to 16 GPUs across 2 nodes.",
    "id": 1034
  },
  {
    "title": "Residual Adapter Layers Improve Few-Shot Learning When Fine-Tuning Already Works",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like adapters have shown promise for few-shot learning, but their benefits over simple full fine-tuning remain unclear. We investigate when adapter layers provide advantages by systematically analyzing their performance across 16 diverse few-shot tasks. While previous work suggests adapters consistently outperform standard fine-tuning, we find their effectiveness crucially depends on the baseline method's performance: adapters only improve accuracy when full fine-tuning achieves >70% of supervised performance. On tasks where full fine-tuning struggles (<50% baseline), adding adapters provides no benefit and sometimes hurts performance. To address this limitation, we propose Residual Adapter Layers (RAL) that selectively update only the top-k adapter parameters based on gradient norms. Across 8 benchmarks spanning vision and NLP tasks, RAL achieves modest improvements over full fine-tuning (2.3\u00b11.1% accuracy gain on high-performing baselines), while matching standard fine-tuning on harder tasks. Our results suggest adapter methods are most useful for refining already-effective models rather than enabling few-shot learning in challenging scenarios.",
    "id": 1035
  },
  {
    "title": "Revisiting MAML with Adaptive Inner-Loop Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Singh, K."
    ],
    "abstract": "Model-Agnostic Meta-Learning (MAML) has become a popular approach for few-shot learning, yet its inner loop optimization uses fixed learning rates across all tasks and parameters. We propose Adaptive-MAML (A-MAML), which learns task-adaptive step sizes for the inner loop by adding a small meta-network that outputs per-parameter learning rates. Our method extends MAML with minimal computational overhead, requiring only 15% additional parameters. We evaluate A-MAML on standard few-shot image classification benchmarks (mini-ImageNet, CUB-200) and achieve 1-2% absolute improvements over MAML baselines. Furthermore, we demonstrate that adaptively adjusting learning rates reduces sensitivity to hyperparameter selection, particularly for the inner-loop learning rate and number of adaptation steps. While the improvements are consistent across benchmarks, they are modest and primarily achieved through careful tuning of the meta-network architecture. Code is available at [redacted].",
    "id": 1036
  },
  {
    "title": "Attention with a Twist: Improving Transformer Robustness via Learnable Positional Permutations",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "While adversarial robustness in vision transformers has improved significantly through architectural modifications and training techniques, the role of positional encodings remains underexplored. We propose Permuted Position Encoding (PPE), a simple plug-and-play modification that learns task-dependent permutations of absolute positional embeddings during training, then freezes optimal permutations for inference. Our method requires no changes to model architecture beyond adding a small permutation matrix (d\u00d7d parameters) that is optimized via straight-through gradient estimation. On ImageNet-C and common adversarial attacks, PPE improves robust accuracy by 2-4% over standard and relative position encodings across multiple ViT variants (B/16, B/32, L/16) with negligible computational overhead (<0.1% parameter increase). However, gains saturate quickly and do not transfer to temporally permuted inputs. While our approach provides consistent improvements for vision transformers, the benefits disappear for CNNs and do not address fundamental vulnerabilities to patch-wise attacks. Code and pretrained models will be released upon acceptance.",
    "id": 1037
  },
  {
    "title": "Gradient Surgery with Adaptive Blending for Multi-Task Learning",
    "authors": [
      "Liu, K.",
      "Garcia, J.",
      "Thompson, A."
    ],
    "abstract": "Multi-task learning often faces gradient conflicts when optimizing shared representations. While recent gradient surgery methods like PCGrad and GradDrop modify conflicting gradients to improve training dynamics, we argue these approaches suffer from over-correction that can discard useful information. We propose Adaptive Gradient Blending (AGB), a simple technique that softly blends gradients based on per-dimension alignment scores rather than hard thresholding. AGB computes cosine similarity at the coordinate level and blends gradients with learned blending coefficients that adjust automatically based on task uncertainty. On standard MT-Bench tasks including NYUv2 and Cityscapes, AGB achieves modest improvements over PCGrad (0.8% average improvement on mIoU) while using 15% fewer parameters than existing approaches. However, we find these gains diminish with larger models, raising questions about the practical significance of our improvements. Theoretical analysis shows AGB converges under similar assumptions to standard gradient descent, but our bounds are not tighter than prior work. While AGB provides a lightweight alternative to existing surgery methods and demonstrates consistent if modest improvements on established benchmarks, the limited scope of improvement and lack of novel theoretical insights may restrict broader applicability.",
    "id": 1038
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Correlated Perturbations",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. We propose Correlated Perturbation Gradient Estimation (CPGE), a simple modification to REINFORCE-based methods that reduces variance by introducing systematic correlations between perturbations across mini-batches. Our key insight is that carefully controlled positive correlations between samples can cancel out variance terms while maintaining unbiasedness of the gradient estimate. We derive theoretical bounds showing CPGE achieves lower variance than standard REINFORCE for Bernoulli and categorical variables under mild conditions. On benchmark tasks including VAE training with binary latent codes and reinforcement learning with discrete action spaces, CPGE demonstrates 5-15% improvements in training stability and 2-4\u00d7 variance reduction compared to baseline estimators, while requiring only an additional O(d) memory where d is the latent dimension. However, we observe diminishing returns when the number of latent variables exceeds 1000, and our approach shows limited benefits for continuous relaxations such as Gumbel-softmax. Our results suggest CPGE provides a practical trade-off between implementation simplicity and performance gains for medium-scale discrete latent variable models.",
    "id": 1039
  },
  {
    "title": "Towards Stable Semi-Supervised Learning via Adaptive Data Augmentation Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Semi-supervised learning algorithms often exhibit unstable performance when the unlabeled data distribution differs from the labeled data. We propose AdaAug-SSL, a simple adaptive scheduling mechanism that dynamically adjusts data augmentation strength based on confidence measures from the model's predictions. Our approach monitors the entropy of predictions on unlabeled batches and applies stronger augmentations when prediction confidence is high, while reverting to weaker augmentations during uncertain phases. We demonstrate AdaAug-SSL on CIFAR-10 with 4,000 labels and SVHN with 1,000 labels, achieving modest improvements of 1.2-1.8% over strong baselines like FixMatch and FlexMatch. While our results are consistent across these benchmarks, we observe that the gains diminish when the label/unlabeled data ratio exceeds 15%. Our extensive ablation reveals that the scheduling strategy is more critical than the specific augmentation types used. Although AdaAug-SSL provides a lightweight alternative to extensive hyperparameter tuning, we acknowledge that the mechanism adds computational overhead and may not generalize well to extremely imbalanced SSL settings. Code and hyperparameters are available at anonymized-url.",
    "id": 1040
  },
  {
    "title": "MagNet: Margin-based Gradient Normalization for Improved Training Stability in Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Training instability remains a pervasive challenge in transformer architectures, particularly when scaling to deeper networks or smaller datasets. While gradient clipping and adaptive optimizers offer partial solutions, they often require careful hyperparameter tuning. We propose MagNet, a simple modification to existing optimizers that renormalizes gradients based on learned margin boundaries. Our approach maintains directional information while adaptively controlling gradient magnitudes through a lightweight auxiliary network. We demonstrate empirically that MagNet improves training stability across 6 transformer variants on standard benchmarks, reducing gradient norm variance by 42% on average. However, we find that performance gains saturate beyond modest model sizes, and computational overhead scales poorly for very large models (\u2265 10B parameters). Additionally, our theoretical analysis reveals that MagNet's stability guarantees break down under certain adversarial perturbation settings. While promising for medium-scale applications, these limitations suggest the need for more sophisticated approaches at extreme scales. Our code is available at [anonymized for review].",
    "id": 1041
  },
  {
    "title": "Looped Transformers with Learned Positions Can Perform Gradient Descent, Sometimes",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Gonzalez, M."
    ],
    "abstract": "Recent work has shown that transformers can implement gradient-based optimization algorithms when provided with appropriate positional encodings. We investigate whether these mechanisms emerge naturally when training looped transformers to solve quadratic optimization problems without explicit positional information. We propose a novel architecture that combines learned positional embeddings with recurrent transformer blocks, allowing the model to iterate indefinitely while adapting step sizes based on local curvature. Our theoretical analysis shows that such models can approximate gradient descent with diminishing step sizes under strong assumptions about the data distribution, though the approximation degrades rapidly for ill-conditioned problems. Empirically, we demonstrate that our model achieves comparable performance to standard transformers on synthetic optimization tasks while using 30% fewer parameters. However, we observe significant performance drops on even mildly non-quadratic objectives, suggesting the learned mechanisms may be more brittle than previously thought. While our work provides theoretical insights into how transformers might implement iterative algorithms, the practical impact remains limited to narrow problem settings. Code will be made available upon acceptance.",
    "id": 1042
  },
  {
    "title": "Joint Optimization of Gradient Compression and Momentum Parameters for Distributed Stochastic Optimization",
    "authors": [
      "Kumar, S.",
      "Zhou, L.",
      "M\u00fcller, J."
    ],
    "abstract": "We present a framework for simultaneously tuning gradient compression ratios and momentum parameters in distributed stochastic optimization. While prior work typically treats these design choices independently, we show that their interaction significantly impacts convergence behavior. Our approach formulates the problem as a bilevel optimization where the inner loop performs standard distributed SGD with fixed compression and momentum parameters, while the outer loop adaptively adjusts these hyperparameters based on validation set performance. Theoretically, we derive convergence bounds that quantify the joint effect of compression error and momentum on convergence rates, extending existing analyses that consider each factor in isolation. Empirically, we evaluate our method on CIFAR-10/100 and ImageNet training with ResNet-18/50 architectures across 8-64 workers. Results show 1.1-1.4\u00d7 speedup over state-of-the-art gradient compression methods while maintaining target accuracy. However, our gains diminish at larger worker counts (>32), suggesting fundamental limitations in our current formulation. Our framework requires minimal code changes and integrates with existing distributed training libraries.",
    "id": 1043
  },
  {
    "title": "Gradient Flow in Residual Networks: An Empirical Study of Scale-Dependent Dynamics",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Anderson, K."
    ],
    "abstract": "We investigate how initialization scale affects training dynamics in residual networks through empirical analysis of gradient flow trajectories. While theoretical work suggests residual connections facilitate optimization, we find the relationship between initialization scale and convergence is more nuanced than previously reported. Using tools from dynamical systems theory, we track how gradient norms evolve during training and identify a critical scaling regime (\u03c3 \u223c 1/\u221ad) where networks exhibit smooth optimization but potentially suboptimal final performance. Our experiments on CIFAR-10 and ImageNet show that slightly larger initializations can improve test accuracy by 1-2% but introduce optimization instabilities that vary unpredictably with depth. We propose a simple scaling rule based on our empirical observations and demonstrate modest improvements on standard benchmarks. While our findings provide practical guidance for initialization in deep residual networks, we acknowledge the analysis remains largely empirical and the theoretical mechanisms underlying the observed phenomena require further investigation. Code is available at [URL].",
    "id": 1044
  },
  {
    "title": "Improved Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rosenfeld, N."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed training, but existing methods rely on hand-crafted schedules that may not adapt to changing data distributions. We propose AdaptiveQuant, a simple approach that learns compression schedules during training using a lightweight meta-controller. Our method augments standard quantization schemes with a small recurrent network that predicts optimal bit-widths at each layer based on gradient statistics and training progress. Unlike prior work, AdaptiveQuant requires no hyperparameter tuning and adds minimal computational overhead (2-3% training time increase). We evaluate on CIFAR-10 and ImageNet with ResNet-50, showing modest improvements over fixed schedules: 3-5% better accuracy at the same compression rate, or 10-15% additional compression at comparable accuracy. While our theoretical analysis is limited to smooth convex objectives, empirical results suggest the approach extends to non-convex settings. Code will be released upon acceptance.",
    "id": 1045
  },
  {
    "title": "Improved Gradient Estimation for Stochastic Neural Networks via Learned Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a simple extension to the reparameterization gradient estimator that reduces variance through learned control variates. Our method trains an auxiliary neural network to predict the gradient magnitude given the input parameters, using this prediction to scale a zero-mean control variate. Unlike recent techniques requiring model-specific derivations, our approach applies directly to any reparameterizable distribution. Empirical evaluation on variational autoencoders shows 10-30% variance reduction on MNIST and CIFAR-10 compared to standard reparameterization, with minimal computational overhead. While the improvement is consistent across architectures, we find limited benefits when combined with strong entropy regularization, suggesting the technique is most effective in high-temperature regimes. Code is available at [anonymous link].",
    "id": 1046
  },
  {
    "title": "Faster SGD with Randomized Coordinate Momentum: A Two-Line Update Rule That (Sometimes) Works",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Stochastic gradient descent (SGD) remains the workhorse for large-scale optimization despite extensive research into momentum-based variants. We propose Randomized Coordinate Momentum (RCM), a simple modification that applies momentum updates to randomly selected coordinates while keeping others unchanged. This yields a two-line update rule requiring no additional hyperparameters beyond standard momentum coefficients. We show that RCM achieves comparable convergence to full-gradient momentum on strongly convex problems while reducing per-iteration computational cost by the coordinate selection ratio \u03b1 \u2208 (0,1]. For non-convex deep learning tasks, our ImageNet experiments with ResNet-50 show 15-20% faster wall-clock training time compared to SGD+momentum at \u03b1=0.5, though gains diminish for smaller batch sizes. Theoretical analysis reveals convergence rates that match SGD up to constant factors, but our bounds depend on coordinate-wise Lipschitz constants that may be difficult to estimate in practice. While our method lacks the strong theoretical guarantees of variance-reduced methods and the robust performance of Adam variants, RCM provides a practical trade-off between computational efficiency and convergence speed when memory bandwidth is the primary bottleneck. Code is available at anonymous.url/RCM.",
    "id": 1047
  },
  {
    "title": "Meta-Learning with Learned Loss Functions: A Unified Framework via Implicit Differentiation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose a meta-learning framework that jointly learns task-specific parameters and the loss function itself, unifying recent approaches that use learned metrics or regularizers. Our method treats the loss function as a parameterized neural network whose gradients are computed via implicit differentiation through the inner optimization loop. We prove convergence guarantees under Lipschitz smoothness assumptions, extending classical meta-learning theory. On few-shot classification benchmarks, our approach achieves 2-3% improvements over MAML and prototypical networks, though gains diminish with larger meta-training sets. Experiments on text classification and molecular property prediction show consistent but modest benefits. While our framework is general, the learned loss functions often lack interpretability and can exhibit unstable training dynamics. We provide a PyTorch implementation and ablations showing the importance of loss architecture choices.",
    "id": 1048
  },
  {
    "title": "Improved Generalization Bounds for Low-Rank Neural Networks via Iterative Matrix Compression",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We investigate the generalization properties of neural networks trained with iterative low-rank matrix compression, where weight matrices are periodically compressed using truncated SVD during training. While low-rank constraints have recently emerged as an effective regularization technique, existing generalization bounds either fail to capture the benefits of iterative compression or require unrealistic assumptions about the data distribution. We derive new PAC-Bayesian bounds that exploit the cumulative compression trajectory, revealing a tighter dependence on the effective rank rather than the nominal parameter count. Our analysis extends the classic PAC-Bayes framework by incorporating compression-induced perturbations in the prior, yielding bounds that decrease with the number of compression iterations under mild assumptions. Empirically, we verify our theoretical insights on standard benchmarks including CIFAR-10/100 and ImageNet subsets, showing 5-15% improvement in bound tightness compared to existing approaches. However, we note that the practical gap between our bounds and actual generalization performance remains substantial, suggesting fundamental limitations in current PAC-Bayesian techniques for realistic architectures.",
    "id": 1049
  },
  {
    "title": "Variance-Reduced Zeroth-Order Optimization with Adaptive Step-Size Selection via Gaussian Process Bandits",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Rosenfeld, E."
    ],
    "abstract": "We study derivative-free optimization for black-box functions with limited function evaluations. While zeroth-order methods typically rely on fixed step-sizes or simple decay schedules, we propose an adaptive approach that learns good step-sizes online using a Gaussian process surrogate model. Our method combines variance-reduced gradient estimates with a bandit-style step-size selection mechanism that balances exploration and exploitation. We provide theoretical analysis showing O(d/T) convergence rate for smooth non-convex objectives, matching existing variance-reduced zeroth-order methods. Empirically, we demonstrate modest improvements over baselines on hyperparameter tuning tasks, with average speedups of 1.3x on neural architecture search benchmarks. However, our method introduces additional hyperparameters and computational overhead that may limit practicality. We discuss potential extensions and limitations of our approach.",
    "id": 1050
  },
  {
    "title": "Gradient Orthogonalization for Better Transfer Learning in Neural Networks",
    "authors": [
      "Chen, L.",
      "Ramos, J.",
      "Singh, P."
    ],
    "abstract": "Fine-tuning pre-trained models often suffers from catastrophic interference when adapting to new tasks, particularly in low-data regimes. We propose Gradient Orthogonalization during Transfer (GOT), a simple regularization technique that encourages gradient directions from new tasks to remain orthogonal to directions important for the original pre-training task. GOT adds minimal computational overhead by projecting gradients during optimization without requiring additional forward passes. Our method achieves 2-3% improvement over standard fine-tuning on 6 out of 10 benchmark datasets, particularly excelling in few-shot image classification. However, improvements diminish when using larger fine-tuning datasets or when tasks are highly dissimilar from pre-training. We provide theoretical analysis showing GOT corresponds to bounding the minimum eigenvalue of the joint Hessian, though our bounds are loose for deeper networks. Experiments on BERT and ResNet architectures demonstrate our method is architecture-agnostic, but gains for language tasks are modest. Code and experiments are reproducible with our PyTorch implementation. While GOT shows promise for mitigating interference, more sophisticated analysis is needed to understand when orthogonal gradients are truly optimal versus simply trading off between tasks.",
    "id": 1051
  },
  {
    "title": "ReLoRA: Revisiting Low-Rank Adaptation with Iterative Re-initialization",
    "authors": [
      "Thompson, L.",
      "Kim, J.",
      "Anderson, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a standard approach for efficiently fine-tuning large language models, but its effectiveness diminishes as the rank increases. We observe that while LoRA initially captures dominant features, it suffers from subspace saturation that limits expressivity. To address this, we propose ReLoRA, a simple iterative re-initialization scheme that periodically resets the low-rank matrices while preserving learned representations through a knowledge distillation objective. Our method involves training for K steps, then computing a warm restart using the current adapted weights as soft targets, allowing the low-rank decomposition to explore new subspaces. Across 4 language understanding benchmarks and 2 model scales, ReLoRA achieves a modest 2.4% average improvement over standard LoRA at rank 16-64, with diminishing returns beyond rank 128. Surprisingly, we find that random re-initialization without distillation performs comparably on 3/4 tasks, suggesting the benefit may stem primarily from noise injection. While our results indicate potential for extending LoRA's capacity, we acknowledge the computational overhead is doubled and theoretical understanding remains limited. The method may serve as a practical baseline for future work on adaptive rank selection.",
    "id": 1052
  },
  {
    "title": "LoRA-MoE: Low-Rank Adaptation Meets Mixture of Experts for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Thompson, L.",
      "Kumar, A.",
      "Chen, J."
    ],
    "abstract": "We present LoRA-MoE, a method that combines Low-Rank Adaptation (LoRA) with Mixture of Experts (MoE) for parameter-efficient fine-tuning of large language models. While LoRA reduces memory costs through low-rank updates, we observe that different downstream tasks benefit from specialized subnetworks. By treating LoRA modules as experts and learning sparse routing functions, we achieve a more adaptive fine-tuning paradigm. Our method adds only 0.1% additional parameters per task beyond standard LoRA, while improving average performance across 15 classification benchmarks by 2.3%. However, we find the gains are task-dependent: improvements are concentrated in tasks with domain mismatch from pretraining, while similar-domain tasks see minimal benefit. Compared to standard LoRA, our approach shows 15% faster convergence in early training but equivalent final perplexities. Analysis reveals that learned routing weights correlate with task similarity measures, though routing degeneracy occurs in 30% of settings. Code and models are available at anonymous-url.",
    "id": 1053
  },
  {
    "title": "Understanding the Role of Batch Normalization in Transformer Training: A Signal Propagation Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Batch normalization (BN) has been largely supplanted by layer normalization (LN) in modern Transformer architectures, leading to the common belief that BN is incompatible with attention mechanisms. We revisit this assumption through the lens of signal propagation theory, providing a theoretical framework that characterizes when BN can stabilize Transformer training. Our analysis reveals that careful placement of BN layers after attention blocks, combined with specific initialization schemes, can match or exceed LN performance when sequence lengths are moderate (\u2264512 tokens). We validate our theoretical findings on a range of NLP tasks, including machine translation and language modeling, achieving comparable perplexity to LN baselines while reducing activation memory by 15-20% in practice. However, we observe performance degradation for longer sequences, suggesting fundamental limitations in applying BN to Transformers. While our theoretical contributions are limited to simplified settings and our experimental scope restricts evaluation to encoder-decoder architectures, our work challenges prevailing assumptions about normalization choices in Transformers and provides practical guidelines for practitioners willing to trade a small accuracy loss for reduced memory consumption.",
    "id": 1054
  },
  {
    "title": "Efficient Gradient Compression Through Adaptive Structured Sparsity with Learned Thresholding",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Takashi, S."
    ],
    "abstract": "Gradient compression is critical for distributed training, yet existing methods often sacrifice convergence quality for communication efficiency. We propose ASTER, an adaptive structured sparsity framework that learns compression thresholds during training. Our key insight is that the optimal sparsity pattern varies across layers and training phases, motivating a learnable gating mechanism that dynamically adjusts compression ratios. ASTER employs a block-structured sparsity pattern coupled with a lightweight meta-network that predicts compression thresholds based on gradient statistics. Through experiments on ResNet-50 and Transformer models, we achieve 12-18x compression with \u22640.3% accuracy loss on ImageNet and comparable perplexity on IWSLT14. While our method shows consistent improvements over static compression baselines, we observe sensitivity to hyperparameter initialization and diminished benefits on small-batch settings. Our ablation reveals that threshold learning contributes 60-70% of the performance gains, with the remaining improvements coming from the structured sparsity pattern. The framework adds minimal computational overhead, requiring <2% additional training time. Code is available at [URL].",
    "id": 1055
  },
  {
    "title": "Gradient Surgery Revisited: Do Adaptive Optimizers Really Need Hessian-Aware Preconditioning?",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Obermayer, K."
    ],
    "abstract": "Adaptive gradient methods like Adam and RMSprop are widely used in deep learning, yet their theoretical justification remains limited. We revisit the question of whether these methods benefit from Hessian-aware preconditioning, focusing on the role of gradient noise and curvature information. By analyzing the update rules through a second-order lens, we show that Adam's adaptive learning rates can be interpreted as proximal updates with a diagonal Hessian approximation. This insight leads to a simple modification: we rescale the Adam update by the inverse of a smoothed Hessian diagonal, estimated efficiently using the Hutchinson method with random projections. Experiments on Vision Transformers and small-scale language models show modest improvements in convergence speed (3-7%) and final accuracy (0.2-0.5%), but only when training with large batch sizes (>2048). However, we find that these gains disappear when using gradient clipping or smaller batch sizes, suggesting the benefit is primarily from re-scaling large gradient norms rather than accurate curvature modeling. Our theoretical analysis is restricted to quadratic objectives, and we acknowledge that the practical impact in standard training settings is limited. The work provides a refined understanding of when Hessian information helps adaptive methods, though we recognize the contribution is incremental rather than transformative.",
    "id": 1056
  },
  {
    "title": "Improving Convergence of AdamW via Layer-wise Learning Rate Warmup for Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Vision Transformers (ViTs) often exhibit unstable training dynamics when fine-tuned with adaptive optimizers like AdamW, particularly on smaller datasets. We propose LayerAdam, a simple modification to AdamW that applies layer-wise learning rate warmup schedules based on each layer's Lipschitz constant. Our method estimates layer sensitivities using a single forward pass on 1% of training data, then sets warmup durations proportional to these estimates. On ImageNet-1K, LayerAdam achieves 0.7% top-1 accuracy improvement over baseline AdamW when fine-tuning DeiT-Small and Swin-Tiny models. However, the benefits diminish for larger architectures (ViT-Base) and are inconsistent across different hyperparameter settings. While the approach introduces negligible computational overhead, we find that similar improvements can often be achieved through more extensive grid search of standard warmup schedules. Our theoretical analysis provides convergence bounds under simplified assumptions that may not hold in practice. Code will be released upon acceptance.",
    "id": 1057
  },
  {
    "title": "Improving Transformer Efficiency Through Adaptive Token Dropping with Learnable Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "We propose Adaptive Token Dropping (ATD), a method for reducing computational cost in Transformer models by dynamically skipping the computation for less important tokens during inference. Unlike prior approaches that use fixed heuristics or static thresholds, ATD learns data-dependent dropping thresholds through a lightweight meta-network that predicts token importance based on local attention patterns. Our method achieves a 20-40% reduction in FLOPs on standard NLP benchmarks with minimal impact on downstream task performance (average 0.3 BLEU drop on WMT'16 and 0.7% accuracy drop on GLUE). While ATD shows consistent improvements over naive approaches on most tasks, we observe substantial variance across domains\u2014performance degrades significantly on tasks requiring long-range dependencies (up to 5% accuracy drop on LRA tasks). These results suggest ATD is most effective for input-length-sensitive applications where speed is prioritized over perfect accuracy. Theoretical analysis indicates our dropping strategy approximates an upper bound on attention entropy, though our bounds rely on strong assumptions that may not hold in practice. Code and models will be released upon acceptance.",
    "id": 1058
  },
  {
    "title": "LoRaS: Low-Rank Adaptation with Scheduled Dropout for Efficient Language Model Fine-tuning",
    "authors": [
      "Chen, J.",
      "Garcia, M.",
      "Thompson, L."
    ],
    "abstract": "We propose LoRaS, an extension to Low-Rank Adaptation (LoRA) that incorporates scheduled dropout to improve parameter efficiency while maintaining downstream performance. While LoRA has emerged as a popular parameter-efficient fine-tuning method for large language models, we empirically observe that its performance degrades on multi-task benchmarks compared to full fine-tuning. Our key insight is that the fixed low-rank matrices in LoRA may overfit to specific tasks. LoRaS introduces learnable dropout rates that decay throughout training, combined with a novel initialization scheme that preserves pretrained representations. Experiments on GLUE and SuperGLUE benchmarks show LoRaS achieves 93.2% of full fine-tuning performance with 0.1% parameters, improving over standard LoRA by 1.3 F1 points on average. However, we find LoRaS provides diminishing returns on larger models (>7B parameters) and struggles with few-shot scenarios (<100 examples). Theoretical analysis reveals LoRaS effectively controls the rank of adapted matrices, though our proof techniques require strong assumptions about gradient independence. Code and checkpoints are available at \\url{github.com/anonymous/loras}.",
    "id": 1059
  },
  {
    "title": "LoRA-Drop: Structured Pruning of Low-Rank Adaptation Weights via Gradient-Informed Thresholding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "We propose LoRA-Drop, a simple yet effective method for reducing inference latency in models fine-tuned with Low-Rank Adaptation (LoRA). Our key insight is that the magnitude of LoRA gradients during fine-tuning provides a reliable signal for identifying redundant low-rank components. By dynamically applying a threshold-based pruning strategy guided by these gradients, we can eliminate up to 40% of LoRA parameters with minimal accuracy loss. Our method operates post-training and requires no hyperparameter tuning beyond a single pruning threshold. Experiments on GLUE benchmark tasks using RoBERTa-base demonstrate average speedups of 1.3\u00d7 during inference while maintaining 97% of the original fine-tuned accuracy. However, we observe that gains diminish on larger models, with T5-3B showing only 1.1\u00d7 speedup and 2% accuracy drop. While LoRA-Drop provides a practical approach to efficient deployment, we acknowledge that our gradient-based pruning criterion may not generalize across all downstream tasks, particularly those with limited training data. Code is available at [github url].",
    "id": 1060
  },
  {
    "title": "Revisiting Weight Averaging for Improving Generalization in Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, T."
    ],
    "abstract": "Weight averaging techniques like stochastic weight averaging (SWA) have shown promise for improving generalization in deep learning, but their effectiveness varies significantly across architectures and datasets. We conduct a systematic empirical study of weight averaging variants and propose simple modifications to improve robustness. Our key insight is that the alignment between weight trajectories in parameter space determines averaging effectiveness, which we quantify using a novel distance metric based on layer-wise cosine similarity. We introduce Cyclic SWA (CSWA), which adjusts averaging frequency based on trajectory alignment, achieving 1-2% accuracy improvements over standard SWA on CIFAR-10/100 and ImageNet under consistent hyperparameters. However, we find these gains diminish on larger architectures like ViT-B/16 and ResNet-50 with advanced augmentations. Our analysis reveals that weight averaging primarily helps when the base optimizer's trajectory exhibits low-frequency oscillations, suggesting its benefits may be redundant with well-tuned learning rate schedules. While our method provides reliable improvements for medium-scale vision tasks, the computational overhead and memory requirements make it impractical for modern training pipelines. We release our implementation and hyperparameters for reproducibility, but note that gains are modest and task-specific.",
    "id": 1061
  },
  {
    "title": "LoRA-FT: Mitigating Catastrophic Forgetting in Parameter-Efficient Fine-Tuning through Low-Rank Attention Fusion",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods like LoRA have gained popularity for adapting large language models, yet they suffer from catastrophic forgetting when sequentially fine-tuned on multiple tasks. We propose LoRA-FT, a simple yet effective modification that combines low-rank adaptation with an attention-based fusion mechanism to preserve previously learned knowledge. Our approach maintains LoRA's parameter efficiency while introducing task-specific attention gates that selectively activate relevant low-rank adaptations based on input characteristics. We evaluate LoRA-FT on standard NLP benchmarks using T5-large and Llama2-7B models across five diverse tasks. Experimental results show modest improvements over vanilla LoRA (avg. +1.3% task performance, +2.8% catastrophic forgetting reduction), particularly beneficial in low-data regimes. While the gains are incremental and computational overhead increases by 15% due to the attention mechanism, our ablation studies reveal that the fusion component contributes meaningfully to performance retention. The method addresses a practical limitation of existing PEFT approaches, though we acknowledge that the improvements are task-dependent and may not justify the additional complexity in all scenarios. Code and experiments are available at [URL withheld for anonymity].",
    "id": 1062
  },
  {
    "title": "Improving Transformer Training Stability Through Layer-Wise Learning Rate Warmup",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "M\u00fcller, S."
    ],
    "abstract": "We propose a simple modification to transformer training that improves optimization stability without architectural changes. Our method applies layer-specific learning rates that warm up at different rates, with lower layers warming up faster than upper layers. This approach is motivated by observations that gradient norms vary substantially across transformer layers, particularly during early training. We evaluate our method on Wikitext-103 language modeling and GLUE fine-tuning tasks, showing modest improvements in perplexity (0.5-1.2% relative) and downstream accuracy (0.3-0.8% absolute) over standard warmup procedures. While the improvements are incremental rather than transformative, our method reduces training instability observed in 15% of random seeds across experimental settings, suggesting practical benefits for reproducibility. The approach adds minimal computational overhead and can be integrated into existing training pipelines with <10 lines of code. However, we find limited benefits on larger-scale experiments (e.g., GPT-2 medium), raising questions about the method's scaling properties. Our contributions are primarily empirical rather than theoretical, and while the method shows promise for small-to-medium scale applications, further analysis is needed to understand the mechanism of action. Code is available at [anonymous link].",
    "id": 1063
  },
  {
    "title": "LoRA-VM: Memory-Efficient Vector Quantization for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, S.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its memory requirements can still be prohibitive for large-scale models. We present LoRA-VM, a method that combines vector quantization with low-rank adaptation to further reduce memory usage. Our approach quantizes the rank decomposition matrices using a learned codebook, achieving up to 3\u00d7 memory reduction compared to standard LoRA while maintaining comparable downstream performance. We introduce an adaptive quantization scheme that selectively applies more aggressive quantization to less sensitive layers, identified through a simple gradient-based analysis. Experiments on GPT-2, LLaMA-7B, and BERT demonstrate that LoRA-VM matches LoRA's performance within 2-3% across GLUE, commonsense reasoning, and generation tasks. However, we observe larger performance drops (>5%) on tasks requiring fine-grained reasoning. Our ablation studies reveal that quantization resolution and layer selection strategy significantly impact results. While LoRA-VM offers clear memory advantages, our method introduces additional hyperparameters and requires careful tuning. We provide open-source implementations and comprehensive hyperparameter guidelines to support reproducibility. Code is available at [URL withheld for review].",
    "id": 1064
  },
  {
    "title": "Momentum-Aware Perturbation Scheduling: A Simple Extension to SAM for Improved Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a popular regularizer for improving generalization in deep neural networks, yet its reliance on uniform perturbation magnitudes across training iterations may limit effectiveness. We propose Momentum-Aware Perturbation Scheduling (MAPS), a lightweight modification to SAM that adaptively adjusts the perturbation radius based on per-parameter momentum histories. Our method introduces two scalar hyperparameters that modulate how aggressively the perturbation budget increases or decreases as a function of gradient stability. Extensive experiments on CIFAR-10/100 and ImageNet with ResNet-18 and ViT-Tiny architectures demonstrate that MAPS achieves 0.3-1.2% improvements over vanilla SAM at comparable computational cost, with particularly pronounced gains in low-data regimes. While our theoretical analysis is limited to quadratic objectives and the improvements on language modeling tasks (IWSLT14 De-En, Wikitext-103) are marginal (0.1-0.4 BLEU/perplexity), we believe the simplicity and minimal overhead of MAPS makes it a practical drop-in enhancement for practitioners using SAM. Code is available at [redacted for submission].",
    "id": 1065
  },
  {
    "title": "Gradient Norm Aware Optimization: A Simple Scaling Method for Improving Training Stability",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Gradient Norm Aware Optimization (GNAO), a lightweight modification to standard optimizers that adaptively scales update magnitudes based on local gradient statistics. While adaptive optimizers like Adam and RMSProp normalize updates using second moment estimates, we show that tracking gradient norms across mini-batches provides a computationally cheaper alternative that stabilizes training. Our method adds minimal computational overhead (\u22482% increase in wall-clock time) and requires no additional hyperparameters beyond the base optimizer. Through experiments on CIFAR-10/100 and smaller Transformer models, GNAO demonstrates slight improvements in convergence speed and final accuracy compared to vanilla SGD/Adam baselines. Theoretical analysis suggests GNAO can be viewed as a diagonal Hessian approximation under mild assumptions. However, we find the practical benefits diminish on larger models (>100M parameters), possibly due to gradient norm homogenization in later training stages. While our results are positive, we acknowledge the improvements are incremental and specific to certain training regimes. The method may be most useful for practitioners facing stability issues with small-to-medium sized models where additional computational cost is prohibitive.",
    "id": 1066
  },
  {
    "title": "Scheduled Sampling with Learned Mixing Rates for Neural Machine Translation",
    "authors": [
      "Liu, K.",
      "Thompson, B.",
      "Chen, J."
    ],
    "abstract": "We propose MixRate, a simple extension to scheduled sampling that learns instance-specific mixing rates between ground truth and model predictions during training. While scheduled sampling has shown modest improvements in sequence-to-sequence models, its linear decay schedules often mismatch the model's actual error rates across training. Our method uses a small auxiliary network to predict optimal mixing rates based on decoder hidden states and attention distributions. Experiments on WMT'16 English-Romanian and IWSLT'14 German-English show 0.3-0.7 BLEU improvements over standard scheduled sampling, with larger gains on longer sequences (>40 tokens). However, we find diminishing returns on high-resource language pairs and observe that learned rates converge to near-scheduled behavior in later training stages. Ablation studies reveal most gains come from adapting rates to sequence position rather than instance-specific features. While MixRate introduces minimal computational overhead, the improvements are incremental and may not justify added complexity for well-tuned baselines. Our PyTorch implementation and pretrained models are available at [anonymous-link].",
    "id": 1067
  },
  {
    "title": "Faster but Not Always Better: When Adaptive Learning Rates Hurt Convergence in Federated Optimization",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, J."
    ],
    "abstract": "Adaptive optimization methods have shown remarkable success in centralized training, but their behavior in federated learning remains poorly understood. We investigate when and why adaptive federated optimizers like FedAdam and FedYogi can perform worse than simple FedAvg. Through theoretical analysis of a quadratic model, we prove that adaptive methods can exhibit worse convergence rates when client data heterogeneity exceeds a problem-dependent threshold. Our empirical study across 5 datasets and 3 model architectures confirms this phenomenon, showing FedAdam underperforms FedAvg by up to 12% in certain heterogeneous settings, contradicting prior work suggesting universal benefits. While our theoretical results are limited to quadratic objectives, we propose FedScale, a simple scaling heuristic that empirically mitigates the degradation in 70% of tested cases. This work highlights the need for more nuanced guidelines when selecting federated optimizers beyond simply adopting successful centralized methods.",
    "id": 1068
  },
  {
    "title": "Momentum Meets Memory: A Practical Analysis of Variance-Reduced Gradient Methods with Adaptive Restart",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Chen, S."
    ],
    "abstract": "We analyze the training dynamics of variance-reduced stochastic gradient methods when augmented with momentum and adaptive restart strategies. While theoretical guarantees exist for convex settings, practical performance in deep learning remains poorly understood. We conduct extensive empirical studies across 8 transformer and CNN architectures, examining how variance-reduction interacts with momentum schedules. Our key finding is that while variance reduction initially accelerates convergence, benefits plateau beyond moderate batch sizes (\u2265256) due to increased memory costs. We propose MOMO-VR, a memory-efficient variant that selectively applies variance-reduction to coordinate directions with high gradient variance, reducing memory overhead by 60-80% with minimal impact on convergence. On ImageNet and C4 datasets, MOMO-VR achieves 1.2-1.4x speedup over standard SGD-Momentum early in training, though final performance converges to similar levels. Our implementation requires <30 lines of additional PyTorch code. While we provide convergence analysis for convex quadratics, extending theoretical guarantees to the non-convex regime remains an open challenge. Experiments suggest MOMO-VR is most beneficial in resource-constrained settings where memory bandwidth, not computation, is the primary bottleneck.",
    "id": 1069
  },
  {
    "title": "Lookahead Gradient Descent: When Two Steps Are Better Than One",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Lookahead Gradient Descent (LGD), a simple modification to standard gradient descent that uses gradients from future parameter values to guide current updates. LGD maintains two copies of the parameters: a fast weight that takes a tentative gradient step, and a slow weight that computes the actual update using the gradient at the tentative fast position. Despite being motivated by theoretical insights from convex optimization, we find that LGD provides modest but consistent improvements over Adam and SGD on small-scale vision and language tasks, achieving 2-3% better accuracy on CIFAR-10 and 1-2 BLEU point gains on IWSLT14. Our analysis reveals LGD acts like an adaptive learning rate method that reduces step sizes near sharp minima, though this effect diminishes on very large models. While the theoretical analysis is limited to quadratic objectives and the computational overhead is 1.5x baseline methods, we believe LGD offers an interesting perspective on how future-gradient information can be leveraged in practice. Code and experiments are available at [anonymous URL].",
    "id": 1070
  },
  {
    "title": "Improving Transformer Efficiency Through Block-Sparse Attention with Learnable Patterns",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, J."
    ],
    "abstract": "The quadratic complexity of self-attention in Transformers limits their application to long sequences. While sparse attention mechanisms have shown promise, most rely on fixed, hand-crafted patterns that may not be optimal for all tasks. We present LASER (Learnable Attention Sparsity via Entropy Regularization), a method that learns task-specific sparse attention patterns during training. LASER introduces a differentiable sparsity mask learned through entropy-regularized optimization, allowing masks to adapt to different heads and layers while maintaining hardware efficiency. Our approach achieves 2.1\u00d7 speedup during inference compared to standard attention on the Long Range Arena benchmark, with competitive accuracy (82.3% vs 83.1% for full attention). However, we observe that learned patterns vary significantly across tasks, and our method shows diminishing returns on shorter sequences (< 1k tokens). Ablations reveal that the entropy regularization coefficient is highly sensitive, requiring careful tuning for each dataset. While LASER provides practical speedups for long-sequence applications like document classification, its benefits are limited for tasks with dense attention requirements. Code and pre-trained models are available at [URL withheld for submission].",
    "id": 1071
  },
  {
    "title": "Revisiting Momentum Schedulers for Transformer Training: A Systematic Analysis and Simple Baseline",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Adaptive optimizers like Adam have become standard for training transformers, but recent work suggests that carefully tuned SGD with momentum can achieve competitive performance. We conduct a large-scale empirical study of momentum hyperparameter schedules across 25 transformer architectures and 8 language modeling benchmarks. While we do not propose fundamentally new algorithms, our systematic analysis reveals that a simple cosine decay schedule for momentum parameters consistently outperforms constant settings by 0.8-1.2 perplexity points in most cases. Surprisingly, this improvement holds even when learning rates are well-tuned, suggesting momentum schedules play an underappreciated role. We provide theoretical intuition through a quadratic approximation analysis, showing how momentum decay can improve convergence near minima. However, we find the benefits diminish for larger models (>1.3B parameters), where adaptive methods retain their advantage. Our PyTorch implementation adds only 3 lines of code to standard SGD. Though incremental, these results challenge the prevailing practice of fixing momentum values and may offer practitioners a simple way to improve transformer training with existing optimizers.",
    "id": 1072
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique via Random Coordinate Pruning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Dropout, a regularization method that randomly prunes gradient coordinates during backpropagation to improve generalization in deep neural networks. Inspired by the success of dropout in activations, our approach randomly sets a subset of gradient components to zero at each training step, effectively creating an ensemble of networks with different optimization trajectories. We provide theoretical analysis showing that gradient dropout is equivalent to adding a noise-dependent regularizer to the loss function, converging to the same solution as vanilla SGD but with potentially better generalization bounds. Experiments on CIFAR-10 and ImageNet show modest improvements over standard training: +0.8% accuracy on ResNet-50 and +1.2% on EfficientNet-B0, though results vary significantly across architectures. While we observe consistent variance reduction in validation loss, absolute performance gains are not always statistically significant. The method adds negligible computational overhead and requires only a single hyperparameter (drop probability), though optimal values appear dataset and architecture-dependent (p\u2208[0.1,0.3]). Our approach provides a lightweight alternative to more complex regularizers, but limitations include unclear behavior on small datasets and convergence observations only for moderately deep networks.",
    "id": 1073
  },
  {
    "title": "Gradient Amplification: A Simple Post-Training Technique for Improving Model Calibration",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "While neural networks achieve remarkable accuracy on downstream tasks, modern architectures exhibit poor calibration - particularly when domain shift or data corruption is present. We present Gradient Amplification (GA), a post-training calibration technique that amplifies the influence of uncertain predictions by scaling gradients within a small neighborhood around the test distribution. Our method requires only a single forward pass with modified activations and no retraining, making it computationally attractive compared to temperature scaling or ensemble approaches. We evaluate GA on image classification (CIFAR-100, ImageNet), text classification (MNLI, SST-2), and tabular datasets under Gaussian noise and adversarial corruptions. Across 15 evaluation settings, GA improves calibration by 8-15% relative to the baseline (measured via ECE) while maintaining accuracy. Notably, GA performs comparably to temperature scaling on natural data but achieves 2-3\u00d7 better calibration under distributional shift. However, we observe that effectiveness diminishes for extremely deep networks (>50 layers) and small datasets (<5k examples), suggesting applications may be limited to medium-scale systems. Our code and pre-trained models are available at [anonymized].",
    "id": 1074
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: A Lightweight Approach to Mitigating Catastrophic Forgetting in Sequential Learning",
    "authors": [
      "Kumar, S.",
      "Liu, Y.",
      "Johnson, M."
    ],
    "abstract": "Continual learning remains challenged by catastrophic forgetting, where learning new tasks degrades performance on previously learned ones. While regularization-based methods show promise, they often require expensive computations or impractical memory growth. We propose Adaptive Memory Gradient Surgery (AMGS), a method that selectively modifies gradient directions during backpropagation to maintain prior task performance while learning new tasks. AMGS computes per-parameter importance scores using a running average of gradient magnitudes, then applies targeted gradient projections that minimally interfere with previously learned representations. Our approach requires only 5% additional memory overhead compared to standard training and introduces minimal computational cost. Experiments on Split-MNIST, CIFAR-100, and a new streaming visual recognition benchmark show 12-18% improvement over naive fine-tuning, matching or slightly exceeding more complex methods like EWC and PackNet. However, AMGS underperforms recent replay-based methods by 3-5% on challenging task sequences. Our results suggest gradient manipulation alone may be insufficient for complex continual learning scenarios, but AMGS provides a practical baseline for applications with strict memory constraints.",
    "id": 1075
  },
  {
    "title": "Gradient Descent with Preconditioners Learned from Similar Tasks: A Meta-Optimization Approach",
    "authors": [
      "Chen, Y.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Task-Adaptive Preconditioned Gradient Descent (TAP-GD), a meta-learning approach that learns to generate preconditioning matrices for gradient descent from historical optimization trajectories. Unlike existing preconditioning methods that rely solely on local curvature information, TAP-GD uses a small neural network trained on similar tasks to predict preconditioners that accelerate convergence. Our method combines insights from second-order optimization with meta-learning: given a new task sampled from a related distribution, the learned preconditioner adapts to the loss landscape geometry while maintaining computational efficiency. We evaluate TAP-GD on image classification and regression benchmarks spanning 8 datasets with varying similarity to the meta-training distribution. Results show modest improvements over standard optimizers (4-12% faster convergence) and competitive performance with stronger baselines like Adam and L-BFGS, particularly when tasks exhibit high feature similarity. However, performance degrades substantially when test tasks diverge from the training distribution. While our approach introduces an interesting new perspective on combining meta-learning with optimization, its practical impact appears limited to specific task distributions. Code and experiments are available at anonymized-url.github.io.",
    "id": 1076
  },
  {
    "title": "Momentum-Based Gradient Updates with Lookahead Constraints for Improved Generalization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, V."
    ],
    "abstract": "We propose Lookahead Momentum Constraints (LMC), a simple modification to standard momentum-based optimizers that enforces consistency between short-term and long-term gradient directions. By constraining the momentum buffer to align with gradients computed from future parameters, LMC reduces optimization instability while maintaining convergence guarantees for convex objectives. Our method adds minimal computational overhead (O(d) where d is parameter dimension) and can be integrated into existing optimizers with less than 10 lines of code. Experiments on CIFAR-10, CIFAR-100, and ImageNet show consistent 0.5-1.2% accuracy improvements over Adam and SGD with momentum across various architectures, though gains diminish with stronger data augmentation. While we establish theoretical convergence bounds under Lipschitz assumptions, we acknowledge these assumptions may not hold for deep networks. Ablation studies reveal that LMC's benefits are most pronounced in underparameterized regimes and small-batch settings. We provide PyTorch code at [anonymized].",
    "id": 1077
  },
  {
    "title": "Gradient Descent with Exponentially Increasing Step Sizes: A Sharp Analysis of the Edge of Stability Phenomenon",
    "authors": [
      "Liu, S.",
      "Kim, J.",
      "Ramakrishnan, R."
    ],
    "abstract": "Recent observations reveal that gradient descent can achieve edge of stability behavior when step sizes exceed the conventional theoretical limits. We provide a theoretical justification for this phenomenon by analyzing gradient descent with exponentially increasing step sizes. Our key finding is that for quadratic objectives, the iterates exhibit a transient phase where the loss decreases exponentially despite increasing step sizes, followed by a sharp divergence when the step size reaches a critical threshold. We prove that this critical threshold is exactly 2/L for L-smooth quadratic functions, matching empirical observations. While our analysis is limited to quadratic objectives, we demonstrate through extensive experiments on neural networks that similar patterns emerge, suggesting our theory may partially explain the edge of stability. However, we also identify cases where our theoretical predictions fail, particularly for networks with batch normalization and residual connections, highlighting limitations of our quadratic approximation. Our contribution provides a partial theoretical foundation for this empirically observed phenomenon, though extensions to general non-convex optimization remain open challenges.",
    "id": 1078
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Structured Sparsity and Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, A."
    ],
    "abstract": "We present LoRA-Lite, a parameter-efficient fine-tuning method that combines low-rank adaptation with structured sparsity to reduce memory requirements for downstream adaptation of large language models. While LoRA has demonstrated effectiveness in reducing trainable parameters, we observe that its memory footprint remains substantial due to gradient accumulation during training. Our approach introduces a learnable sparsity mask applied to LoRA's low-rank matrices, eliminating 60-80% of parameters while maintaining task performance. We implement this via a differentiable top-k operation during training, followed by hard masking during inference. Experiments on GLUE and SuperGLUE benchmarks using RoBERTa-Large and T5-XL show that LoRA-Lite achieves 94-97% of full fine-tuning performance while using 35% less GPU memory than standard LoRA. However, we note that performance degradation becomes significant (>10%) on tasks requiring complex reasoning. Analysis reveals that sparsity patterns vary considerably across tasks and layers, suggesting that our fixed sparsity ratio may be suboptimal. While LoRA-Lite provides clear memory benefits for resource-constrained practitioners, we acknowledge that more sophisticated sparsity schedules could yield further improvements. Code and pre-trained adapters will be released upon acceptance.",
    "id": 1079
  },
  {
    "title": "Gradient Descent with Annealed Noise Injection: A Simple Alternative to Momentum for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "We propose Noise-Injected Gradient Descent (NIGD), a simple modification to standard gradient descent that adds carefully scheduled Gaussian noise to parameter updates instead of using momentum terms. By annealing the noise variance according to a cosine schedule, we observe improved convergence on deep neural network training tasks compared to vanilla SGD, while requiring minimal hyperparameter tuning. Our theoretical analysis shows that NIGD achieves similar convergence rates to SGD with momentum for quadratic objectives, and we demonstrate empirically that it can escape sharp minima more effectively on simple toy problems. Experiments on CIFAR-10 with ResNet-18 show a 0.8% improvement in test accuracy over standard SGD, though these gains diminish on larger datasets like ImageNet. While our method is not intended to replace existing optimizers, it provides a lightweight alternative for practitioners seeking to avoid momentum hyperparameter tuning. Our code is available at github.com/anonymous/nigd.",
    "id": 1080
  },
  {
    "title": "LoRA-Lo: Memory-Efficient Low-Rank Adaptation with Learned Sparsity Patterns",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Johnson, M."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its memory footprint during training remains problematic for consumer hardware. We propose LoRA-Lo, which introduces learned sparsity patterns to reduce memory usage while maintaining model quality. Our key insight is that the low-rank matrices in LoRA exhibit structured sparsity that can be predicted from the frozen pre-trained weights. We train a lightweight convolutional network to predict binary masks for the LoRA matrices at each layer, reducing activations by 30-40% during backward passes. Experiments on instruction-tuning Llama-2 models (7B and 13B) show LoRA-Lo achieves comparable performance to full LoRA on MT-Bench and MMLU benchmarks while reducing peak memory usage by 23-35%. However, we observe a small but consistent degradation on reasoning-heavy tasks (GSM8K drops by 2.1%). Our method adds minimal computational overhead (<5% training time) and requires no architectural changes to existing models, making it compatible with current LoRA implementations. While the gains are modest and our experimental scope is limited, we believe this approach could benefit practitioners with resource constraints. Code will be released upon acceptance.",
    "id": 1081
  },
  {
    "title": "Gradient Descent with Iterative Model Surgery: A Simple Approach to Improving Neural Network Generalization",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, A."
    ],
    "abstract": "We propose Iterative Model Surgery (IMS), a training technique that periodically removes seemingly unnecessary neurons based on their activation variance and weight magnitudes during gradient descent. Motivated by observations that small subsets of neurons often dominate network behavior, IMS selectively prunes low-variance neurons and continues training the remaining parameters. Unlike previous pruning methods that typically require full training followed by structured removal, IMS performs lightweight surgery every few epochs without compromising training dynamics. We evaluate IMS on ResNet-18 and Vision Transformer models for CIFAR-10, ImageNet-1k, and GLUE benchmarks. Results show 2-5% relative accuracy improvements on low-data regimes (10-20% of training data) and marginal gains in standard settings, with 10-15% reduction in parameter count. While the method is simple to implement and requires minimal hyperparameter tuning, improvements appear inconsistent across architectures and tasks. Theoretical analysis reveals IMS behaves similarly to implicit regularization with an additional sparsity-promoting term, though the connection remains heuristic. Our findings suggest selective pruning during training can provide limited but genuine benefits, particularly in data-constrained scenarios, though results fall short of state-of-the-art regularization techniques.",
    "id": 1082
  },
  {
    "title": "Beyond Momentum: A Comparative Study of State-Dependent Learning Rate Schedules for Stochastic Optimization",
    "authors": [
      "Kim, J.",
      "Liu, S.",
      "Dubois, Y.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a family of optimization algorithms that adapt learning rates based on the local curvature of the loss surface, estimated through inexpensive Hessian-vector products. Our method, Curvature-Adaptive Step-size (CAS), generalizes popular optimizers like Adam and RMSprop by making the learning rate state-dependent rather than history-dependent. We prove convergence guarantees under standard smoothness assumptions and demonstrate improved performance on a subset of CIFAR-10/100 and ImageNet experiments, particularly for wide ResNet architectures and Vision Transformers. However, despite its theoretical soundness, CAS shows marginal gains over AdamW in large-scale language modeling tasks and introduces computational overhead that may limit practical deployment. Our extensive ablations reveal the sensitivity of CAS to hyperparameter choices and suggest that while curvature-adaptive schedules offer benefits in specific regimes, their advantages are not universal. We release code for reproducing our experiments.",
    "id": 1083
  },
  {
    "title": "Improved Convergence Rates for SGD with Adaptive Restart Schedules on Strongly Convex Objectives",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose AR-SGD, an adaptive restart schedule for stochastic gradient descent that automatically adjusts restart frequencies based on observed gradient norms. While restart schemes have shown empirical benefits for SGD, theoretical understanding remains limited beyond deterministic settings. Our method monitors the exponentially decaying average of squared gradient norms to trigger restarts, eliminating the need for pre-specified schedules. We prove that AR-SGD achieves an O(log(T)/T) convergence rate for strongly convex objectives, improving upon the standard O(1/T) rate under certain gradient noise conditions. Experiments on logistic regression and neural network training demonstrate modest improvements over tuned baseline schedules, with 5-15% reductions in validation error across 4 benchmark datasets. However, the approach requires careful tuning of two hyperparameters (restart threshold and decay rate) and shows inconsistent gains when moving to larger-scale problems. Our theoretical analysis relies on bounded gradient assumptions that may not hold in practice. While providing a principled approach to adaptive restarting, the complexity overhead and limited empirical gains compared to well-tuned constant schedules suggest AR-SGD may be most useful when manual tuning is infeasible.",
    "id": 1084
  },
  {
    "title": "Improving Transformer Efficiency through Targeted Attention Pruning with Learned Importance Thresholds",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Okafor, G."
    ],
    "abstract": "Transformer models achieve state-of-the-art results across many domains but suffer from quadratic complexity in sequence length, limiting their applicability to long sequences. While various pruning techniques have been proposed, most apply uniform sparsity patterns or require extensive hyperparameter tuning for each task. We propose Adaptive Attention Pruning (AAP), a method that learns task-specific importance thresholds for attention head pruning during training. AAP introduces lightweight gating modules that predict which attention heads contribute most to the loss, enabling dynamic pruning decisions without manual tuning. Our experiments on GLUE, WikiText-103, and Long Range Arena demonstrate 15-30% reduction in FLOPs with <2% accuracy degradation compared to dense baselines. While our approach shows consistent improvements over uniform pruning baselines, we find performance gaps widen on tasks requiring fine-grained reasoning. AAP achieves competitive results to prior work while requiring no task-specific hyperparameter search, though our gains diminish on larger models (>1B parameters). Our code is available at [url withheld for review].",
    "id": 1085
  },
  {
    "title": "Gradual Domain Adaptation via Curriculum Learning with Learned Difficulty Measures",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Chen, B."
    ],
    "abstract": "Domain adaptation remains challenging when source and target distributions differ substantially. We propose a curriculum-based approach that gradually adjusts model parameters by scheduling data from a sequence of constructed intermediate domains. Unlike existing gradual adaptation methods that rely on hand-crafted interpolation strategies, we learn the difficulty measure for scheduling automatically using a small validation set from the target domain. Our method trains a teacher network to predict which samples from the intermediate domain are most beneficial for the current student model state. Experiments on digit recognition and object classification benchmarks show 2-4% improvement over standard domain adaptation baselines, particularly when source-target gaps are large. While our approach provides interpretable scheduling behavior and requires minimal tuning, we acknowledge that the technique achieves smaller gains when abundant labeled target data is available. Code and pre-trained models will be released.",
    "id": 1086
  },
  {
    "title": "Improving Transformer Training Efficiency Through Layer-Wise Learning Rate Warmup",
    "authors": [
      "Kim, S.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer training typically uses global learning rate schedules that apply uniformly across all layers. We propose layer-wise learning rate warmup (LiLaRW), a simple modification that assigns different warmup durations to different transformer layers based on their position. Our method is motivated by the observation that deeper layers in transformers exhibit more unstable gradient norms during early training. We empirically demonstrate that LiLaRW reduces the required training steps by 8-15% across various language modeling and machine translation tasks while maintaining comparable downstream performance. On the WMT'14 English-German translation benchmark, our method achieves comparable BLEU scores to standard training with 12% fewer gradient updates. We provide theoretical analysis showing that layer-specific warmup can improve convergence under certain assumptions about Hessian structure. While our approach shows consistent improvements over standard training, the gains are modest and the theoretical guarantees only hold under restrictive conditions. Our code and hyperparameter configurations are available at [anonymized URL].",
    "id": 1087
  },
  {
    "title": "Gradient Surgery in Federated Learning: When Does Averaging Work?",
    "authors": [
      "Kumar, S.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "Federated learning systems face optimization challenges from non-IID data distributions across clients, but theoretical understanding of when simple gradient averaging performs well remains limited. We analyze convergence bounds for federated gradient descent under data heterogeneity, introducing a new measure we call the 'gradient compatibility index' (GCI) that quantifies alignment between local and global objective gradients. Through theoretical analysis, we show that federated averaging achieves comparable convergence rates to centralized training when GCI exceeds a problem-dependent threshold. We conduct experiments on CIFAR-10 and Shakespeare datasets under various heterogeneity settings, demonstrating that our GCI bounds are tight for linear models but loose for deeper networks. While our theory provides insight into when standard federated methods suffice, the practical utility of GCI is limited by computational requirements and assumptions of convexity. Our work suggests that gradient surgery techniques may be unnecessary for many realistic federated scenarios, though several questions remain open regarding non-convex optimization and communication efficiency.",
    "id": 1088
  },
  {
    "title": "Variance-Reduced Policy Gradients with Adaptive Batch Sizes for Sample-Efficient Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Kim, J.",
      "Singh, A."
    ],
    "abstract": "Policy gradient methods face a fundamental trade-off between sample complexity and variance in gradient estimates. While variance reduction techniques like SVRG and SARAH have shown promise in supervised learning, their adaptation to reinforcement learning remains underexplored. We propose VRPGB, a variance-reduced policy gradient algorithm that dynamically adjusts batch sizes based on gradient variance estimates. Our method combines mini-batch Monte Carlo estimation with periodic full-batch corrections, achieving O(1/\u221aT) convergence while reducing variance by up to 40% compared to vanilla REINFORCE on continuous control tasks. The adaptive batch mechanism automatically increases sample collection when gradient variance exceeds a learned threshold, eliminating manual tuning across environments. Experiments on MuJoCo benchmarks demonstrate improved sample efficiency on 6 out of 8 environments, albeit with increased computational overhead during full-batch corrections. While our theoretical analysis provides convergence guarantees under standard assumptions, we acknowledge limitations in extending these results to non-convex policy spaces. Our approach offers a practical compromise between sample efficiency and implementation simplicity, though gains diminish with larger networks (>5M parameters).",
    "id": 1089
  },
  {
    "title": "Training Transformers with Binary Attention Masks via Asymmetric Relaxation",
    "authors": [
      "Liu, Q.",
      "Krishnan, S.",
      "Zhao, H."
    ],
    "abstract": "We study the problem of inducing sparse attention patterns in Transformer models without architectural modifications. Rather than learning dense attention weights, we propose to learn binary attention masks that deterministically select which tokens to attend to. Our key insight is to treat the discrete optimization problem as a bilevel relaxation: we maintain relaxed continuous parameters during forward passes but commit to binary masks via rounded thresholding during backpropagation. This asymmetric relaxation allows gradient flow while encouraging mask sparsity through an auxiliary L0 regularization term. Additionally, we introduce a curriculum that gradually decreases the temperature parameter governing mask stochasticity, transitioning from relaxed training to discrete inference. Experiments on language modeling and text classification tasks show 2-4x reduction in attention computation with <1% degradation on WikiText-103 and GLUE benchmarks. However, we observe performance gaps in tasks requiring fine-grained token interactions, suggesting our approach works best for long sequences with hierarchical structure. While our method provides consistent memory savings and maintains convergence guarantees under convex objectives, we acknowledge our theoretical analysis only holds for simplified linear attention variants. Code is available at https://anonymous.com/binary-attention.",
    "id": 1090
  },
  {
    "title": "Improving Adversarial Robustness Through Layer-Wise Gradient Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Jones, M."
    ],
    "abstract": "While adversarial training remains the dominant approach for improving neural network robustness, we propose an alternative that regularizes gradients at each layer during standard training. Our method, Layer-Wise Gradient Regularization (LWGR), penalizes the L2 norm of gradients with respect to intermediate activations, encouraging smoother feature representations. Unlike adversarial training, LWGR requires no additional forward passes or perturbation generation. We evaluate LWGR on CIFAR-10 and ImageNet, achieving 42.1% and 38.7% robust accuracy under \u2113\u221e attacks (\u03b5=8/255) respectively with ResNet-50 models. While these results fall short of state-of-the-art adversarial training baselines (51.2% and 44.8%), LWGR demonstrates 2.3\u00d7 faster training and enables better clean accuracy (+4.2% on CIFAR-10). Our analysis reveals LWGR primarily affects early layers, suggesting a complementary role to adversarial training rather than a replacement. Code and pre-trained models are available at anonymous-link.github.io/lwgr.",
    "id": 1091
  },
  {
    "title": "Reinforcement Learning with Precomputed Value Functions: When Offline Planning Meets Online Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Ishikawa, T."
    ],
    "abstract": "We propose a hybrid reinforcement learning framework that leverages precomputed value functions to accelerate online learning in environments with expensive reward computation. Our method pre-computes state-action value estimates using a lightweight proxy model during idle time, then integrates these estimates as soft constraints during policy optimization. While this approach introduces approximation errors due to model mismatch, we show empirically that it achieves 2-3x speedup in wall-clock time on continuous control tasks without significant performance degradation. The key insight is that partial planning with imperfect models can be more efficient than pure online methods. We validate our method on MuJoCo locomotion tasks and a robotic manipulation simulator, demonstrating comparable returns to SAC while reducing training time by 60% on average. However, our approach shows sensitivity to the quality of proxy models and performs poorly in highly non-stationary environments. This work suggests a practical trade-off between computational efficiency and optimality when reward evaluation is costly, though theoretical guarantees remain limited.",
    "id": 1092
  },
  {
    "title": "LoFiSGD: Memory-Efficient Low-Fidelity Gradient Compression for Large-Scale Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We present LoFiSGD, a gradient compression scheme that aggressively quantizes neural network gradients to 1-2 bits during distributed training while maintaining convergence properties. Our method builds upon existing quantization techniques but introduces a novel \"fidelity-aware\" update rule that adaptively adjusts compression levels based on gradient norms. We demonstrate LoFiSGD on ResNet-50 and Transformer architectures, achieving up to 8.5\u00d7 communication reduction compared to full-precision training with <1.5% accuracy degradation on CIFAR-10 and WMT'14. Theoretical analysis shows convergence under standard assumptions for non-convex objectives. While our compression ratios match state-of-the-art methods, we acknowledge that the observed training instabilities on larger models (e.g., BERT-Large) and requirement for careful hyperparameter tuning may limit practical adoption. Our PyTorch implementation is provided for reproducibility.",
    "id": 1093
  },
  {
    "title": "Gradient Surgery via Curvature-Aware Pruning: Improving Optimization in Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We present a simple yet surprisingly effective technique for improving gradient descent in deep networks by selectively pruning gradient components based on local curvature estimates. Our method periodically identifies gradient directions with high curvature (sharp minima directions) and zeroes out their contribution during updates. This gradient surgery is inspired by recent work on sharpness-aware minimization but requires no additional forward passes and adds minimal computational overhead. We prove that our method converges for smooth non-convex objectives under standard assumptions. Experiments on CIFAR-10 and ImageNet show modest improvements over standard SGD+momentum (0.3-0.8% accuracy gains), particularly beneficial for larger networks. While the gains are smaller than reported by sharpness-aware methods, our approach is significantly simpler to implement and tune. However, we observe the benefits diminish with strong regularization techniques like MixUp and CutMix. Theoretically, we show our method can be viewed as implicit regularization, though the connection to generalization remains unclear. Our results suggest curvature-aware gradient manipulation may provide a lightweight alternative to more complex optimizers, though the practical impact is network and dataset dependent.",
    "id": 1094
  },
  {
    "title": "Gradient Surgery with Memory: A Simple Baseline for Continual Learning in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We present Memory-Augmented Gradient Surgery (MAGS), a lightweight approach to mitigate catastrophic forgetting in continual learning scenarios. Instead of storing previous model parameters or replaying data, MAGS maintains a small episodic memory of gradients computed from previous tasks and performs gradient surgery during SGD updates. When updating the network on a new task, MAGS directly modifies gradients that conflict with stored gradient memories, preserving knowledge from earlier tasks with minimal computational overhead. We derive theoretical conditions under which MAGS maintains performance bounds and empirically demonstrate its effectiveness on standard continual learning benchmarks (Split-CIFAR-100, TinyImageNet). While MAGS achieves comparable or better performance than more complex approaches like PackNet and Progressive Neural Networks on 5-10 task sequences, we observe diminishing returns with longer sequences and instability in certain hyperparameter regimes. Our results suggest gradient surgery provides a surprisingly strong baseline for short to medium-length task sequences, though questions remain about the method's scalability and sensitivity to the episodic memory size.",
    "id": 1095
  },
  {
    "title": "Reinforcement Learning with Sample Reuse via Adaptive Trust-Region Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We consider the problem of sample-efficient policy optimization in reinforcement learning where off-policy samples can be reused to accelerate training. While importance sampling ratios are commonly used to correct for distribution mismatch, existing methods either suffer from high variance or rely on conservative clipping that limits sample reuse. We propose Adaptive Reuse Trust-Region Policy Optimization (AR-TRPO), which dynamically adjusts the trust-region constraint based on estimated mismatch between behavior and target policies. Our key insight is that samples from policies with similar state visitation distributions contribute similarly to the policy gradient, allowing for more aggressive reuse when distributions are close. We derive a practical threshold selection rule using kernel-based density ratio estimation and demonstrate theoretically that AR-TRPO achieves comparable regret bounds to standard trust-region methods while potentially reducing sample complexity by up to 40% in favorable cases. Empirical evaluation on continuous control benchmarks shows consistent improvements over TRPO and PPO baselines, though gains are modest on some environments. Our ablation studies reveal sensitivity to hyperparameter choices, particularly the kernel bandwidth and confidence threshold parameters.",
    "id": 1096
  },
  {
    "title": "Lookahead Gradient Descent with Adaptive Momentum: A Unified Framework for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose Lookahead Gradient Descent with Adaptive Momentum (LGAM), a simple modification to standard gradient descent that combines elements of lookahead optimization and adaptive momentum schemes. Our method maintains two sets of parameters: fast weights that perform local gradient updates and slow weights that periodically sync with the fast weights using a learned momentum coefficient. While LGAM does not achieve theoretical improvements over vanilla SGD in worst-case scenarios, we empirically demonstrate 2-5% relative improvement in final accuracy across various ResNet architectures on CIFAR-10/100 and ImageNet, particularly in settings with small batch sizes and aggressive learning rates. Ablation studies suggest the benefits primarily come from improved stability during early training phases rather than faster convergence. Theoretically, we prove convergence to first-order stationary points under standard smoothness assumptions, though our rate matches existing bounds. Code is available at anonymous-url.github.io/LGAM.",
    "id": 1097
  },
  {
    "title": "Gradient Surgery as Implicit Regularization: When Does Model Stitching Fail?",
    "authors": [
      "Liu, C.",
      "Chen, K.",
      "Ambekar, H.",
      "Rodriguez, M."
    ],
    "abstract": "Model stitching has emerged as a practical approach for combining pretrained networks, yet its theoretical understanding remains limited. We show that successful stitching between two networks depends not just on architectural compatibility, but on the implicit regularization induced by their training procedures. Through analysis of the Neural Tangent Kernel, we demonstrate that networks with similar gradient flow dynamics can be stitched with minimal performance loss, while others produce significant degradation. Our experiments on CIFAR-10 and ImageNet reveal a surprising failure mode: even when two networks achieve similar accuracy, stitching fails when their parameter trajectories diverged significantly during training. We propose a simple diagnostic based on gradient cosine similarity to predict stitching success without test-time evaluation, achieving 0.78 ROC-AUC across 50 network pairs. While our theoretical analysis provides new insights, it relies on the NTK approximation and assumes sufficiently wide networks. The empirical findings, particularly regarding real-world architectures like ResNets and Vision Transformers, raise questions about the applicability of our theory to practical settings. Our work suggests that understanding implicit regularization in deep learning requires considering not just the final solution, but the entire optimization trajectory.",
    "id": 1098
  },
  {
    "title": "Revisiting Data Augmentation in Contrastive Learning: A Frequency Domain Perspective",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Singh, V.",
      "Garcia, M."
    ],
    "abstract": "Recent advances in self-supervised contrastive learning have achieved impressive performance across vision and language tasks, with data augmentation playing a crucial role in generating positive pairs. While most work focuses on designing augmentation policies in the spatial/input domain, we investigate how augmentations affect the frequency spectrum of learned representations. Through systematic analysis of ImageNet and CIFAR-10 pretraining, we find that standard augmentation strategies create systematic biases in high-frequency components that persist across different model architectures. We propose Spectral MixUp (SMix), a simple augmentation technique that combines frequency-domain mixing with standard spatial augmentations. SMix improves ImageNet linear evaluation by 0.8% and 1.3% on ResNet-50 and ViT-B respectively over SimCLR baselines, with particularly strong gains (3.2%) on few-shot settings. However, our gains diminish on more recent methods like MoCo v3 and BYOL, suggesting our approach may have limited applicability to state-of-the-art frameworks. Our empirical study reveals that frequency-aware augmentations provide small but consistent improvements while requiring negligible additional compute, though we acknowledge the theoretical understanding remains incomplete.",
    "id": 1099
  },
  {
    "title": "Memory-Efficient Backpropagation Through Time via Low-Rank Gradient Approximation",
    "authors": [
      "Li, K.",
      "Chen, H.",
      "Reddy, S.",
      "Johnson, T."
    ],
    "abstract": "Training recurrent models on long sequences remains computationally challenging due to the quadratic memory growth of backpropagation through time (BPTT). While gradient checkpointing and reversible architectures reduce memory usage, they introduce significant computational overhead. We propose a simpler approach that compresses gradient tensors during backpropagation using low-rank matrix approximations. Our method exploits the observation that gradients in recurrent networks exhibit strong singular value decay after processing long sequences. By projecting gradients to their top-k singular vectors at each timestep, we reduce memory usage by 60-75% with minimal computational overhead. We demonstrate competitive perplexity on Penn Treebank language modeling and Wikitext-103 using LSTMs and Transformers, though our gains diminish with larger models. Analysis reveals our approximation works best when weight matrices have small effective rank, suggesting limited applicability to highly overparameterized networks. Code is available at [anonymized for submission].",
    "id": 1100
  },
  {
    "title": "Gradient Diversification: A Simple Regularization Technique for Improving Transformer Training Stability",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.M.",
      "Kim, H."
    ],
    "abstract": "We propose Gradient Diversification (GD), a lightweight regularization method that encourages gradient orthogonality across transformer layers during training. Motivated by observations of gradient similarity collapse in large language models, GD adds a penalty term to the loss function that maximizes the cosine distance between gradient vectors of adjacent layers. Our method requires only a single hyperparameter and adds minimal computational overhead (<5% increase in training time). We evaluate GD on language modeling tasks using GPT-2 and T5 architectures, where it achieves comparable perplexity to baseline models while reducing gradient norm explosion by 23-31%. Additionally, GD shows modest improvements in few-shot transfer learning performance on SuperGLUE tasks (+1.2 average score). While our experiments are limited to encoder-decoder and decoder-only language models, our results suggest gradient diversification may provide a simple alternative to more complex stabilization techniques. Code will be made available upon acceptance.",
    "id": 1101
  },
  {
    "title": "Iterative Gradient Surgery: A Simple Extension to Adam for Improved Generalization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Despite the widespread success of adaptive optimizers like Adam, these methods often converge to sharp minima that generalize poorly compared to stochastic gradient descent (SGD) with momentum. We propose Iterative Gradient Surgery (IGS), a lightweight modification to Adam that periodically applies gradient projection to encourage flatter minima. At each surgical step, IGS computes the principal eigenvector of the loss Hessian and projects gradients away from this direction, effectively smoothing the optimization trajectory. Our method requires only a single additional Hessian-vector product per surgery step and introduces a single hyperparameter controlling surgery frequency. Experiments on CIFAR-10/100 and ImageNet show IGS improves test accuracy by 1-2% over standard Adam while maintaining training speed. Ablations reveal the benefits primarily emerge from early-stage flattening rather than continued intervention. However, we find performance gains diminish on larger models (ViT-Large) and are sensitive to batch size. While IGS provides a practical alternative to SGD for practitioners who prefer adaptive optimizers, our theoretical analysis remains incomplete\u2014specifically, we lack convergence guarantees when the surgical frequency varies non-monotonically. Code is available at anonymous-ICML-2025/IGS.",
    "id": 1102
  },
  {
    "title": "LoRA-Soup: Improving Parameter-Efficient Fine-Tuning via Gradient-Based Merging of Low-Rank Adapters",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its performance remains sensitive to rank selection and initialization. We propose LoRA-Soup, a simple approach that trains multiple LoRA adapters with different ranks and hyperparameters, then merges them using gradient-based weight averaging. Our method requires only 15% additional memory compared to standard LoRA while achieving comparable or better performance across 8 NLP benchmarks. We provide theoretical motivation showing that gradient-based merging approximates the optimal linear combination of adapters under mild assumptions. Experiments on GPT-2 (1.5B) and Llama-2 (7B) demonstrate improvements of 2.1% average accuracy over strongest baselines, with particularly strong gains on low-resource settings. However, we observe diminishing returns on larger models (13B+ parameters) and tasks with abundant training data. While our approach adds computational overhead during training, the inference cost remains identical to standard LoRA. Our code will be available upon acceptance.",
    "id": 1103
  },
  {
    "title": "LoRA-X: Cross-Modal Adaptation via Low-Rank Residual Alignment",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods like LoRA have shown promise for adapting large language models, but their effectiveness across modalities remains understudied. We propose LoRA-X, an extension that aligns low-rank adaptations between vision and language models through a lightweight residual bridge. Our method introduces cross-modal attention matrices that share 0.01% of total parameters while enabling knowledge transfer between modalities. Experiments on 5 vision-language tasks show 2-3% improvements over vanilla LoRA baselines on average, with particularly strong gains on cross-modal retrieval (+4.2% R@1). However, we observe inconsistent performance across different model scales and data regimes, with comparable performance (\u00b11%) to full fine-tuning only on larger datasets. Theoretical analysis provides convergence guarantees under simplified assumptions but relies on strong distributional alignment conditions. While LoRA-X offers a practical trade-off between parameter efficiency and cross-modal adaptation, our empirical results suggest the method's benefits are most pronounced when sufficient cross-modal training data is available. Code and models will be released upon acceptance.",
    "id": 1104
  },
  {
    "title": "Gradient Projection Memory: Mitigating Catastrophic Forgetting through Low-Rank Subspace Regularization",
    "authors": [
      "Chang, L.",
      "Mukherjee, S.",
      "Johnson, T."
    ],
    "abstract": "We propose Gradient Projection Memory (GPM), a method for continual learning that maintains low-rank subspaces of past gradients to reduce catastrophic forgetting without storing raw data. While similar to existing methods like orthogonal gradient descent and parameter importance regularization, GPM projects gradients onto specially constructed subspaces derived from the top-k singular vectors of accumulated gradient matrices. Our approach shows competitive performance on standard benchmarks (average accuracy increase of 3.7% over baselines on Split-CIFAR-100), though we acknowledge that our gains diminish as task complexity increases and our theoretical analysis remains limited to toy distributions. We provide extensive ablations on subspace dimensionality and memory budget, with code to reproduce experiments using standard PyTorch implementations. Our results suggest that projecting gradients onto carefully chosen low-dimensional subspaces can provide a lightweight alternative to replay-based methods, though we note that performance remains below state-of-the-art methods on more challenging benchmarks like Split-CIFAR-100 with 20 tasks.",
    "id": 1105
  },
  {
    "title": "LoRA-Plus: Improved Parameter-Efficient Fine-Tuning via Learned Scaling Factors",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular method for parameter-efficient fine-tuning of large language models, but its performance often lags behind full fine-tuning, particularly on tasks requiring significant distribution shifts. We introduce LoRA-Plus, a simple extension that learns task-specific scaling factors for the low-rank matrices during adaptation. Our approach adds only 0.001% additional parameters while providing the optimizer greater flexibility in adapting the pretrained representations. Through extensive experiments on the GLUE and SuperGLUE benchmarks, LoRA-Plus achieves a 1.2% average improvement over standard LoRA, closing approximately 30% of the gap between LoRA and full fine-tuning. However, our gains diminish on larger models (>7B parameters) and are less pronounced in few-shot settings. Theoretical analysis reveals that learned scaling factors effectively increase the expressive power of the low-rank subspace, though we acknowledge our results may be limited to encoder models and English NLP tasks. Code will be made available upon acceptance.",
    "id": 1106
  },
  {
    "title": "Revisiting Regularization in Neural Networks: A Study of Weight Decay Schedules Under Data Augmentation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "We investigate the interaction between weight decay schedules and data augmentation in training deep neural networks. While weight decay is widely used as a regularizer, its effectiveness when combined with modern data augmentation pipelines remains poorly understood. We propose a simple cosine-annealing schedule for weight decay coefficients that adapts to the effective data size introduced by augmentation. Through experiments on CIFAR-10, CIFAR-100, and ImageNet subsets, we show modest improvements (0.8-1.2% accuracy gains) over fixed weight decay baselines. Our theoretical analysis suggests these gains arise from better alignment between regularization strength and the implicit increase in dataset size from augmentation. However, we find performance gains diminish with stronger augmentation policies or larger datasets, indicating limited scalability. Our results provide practical scheduling guidelines for practitioners, but also highlight fundamental limitations in regularization methods that do not explicitly account for augmentation-induced distribution shifts. Code and pretrained models are available at anonymous-url.",
    "id": 1107
  },
  {
    "title": "Gradient Surgery with Topological Constraints: Mitigating Catastrophic Forgetting via Persistent Homology",
    "authors": [
      "Liu, H.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Catastrophic forgetting remains a fundamental challenge in continual learning, where sequentially trained neural networks lose performance on previous tasks. While existing regularization-based approaches provide some protection, they often rely on heuristic similarity measures between tasks. We propose TopoGrad, a novel regularization framework that leverages persistent homology to identify critical gradient directions for preserving task-specific topological features. Our method computes persistent barcodes across intermediate network representations, then constructs gradient projections that minimally interfere with these topological invariants. On the Split-CIFAR-100 benchmark, TopoGrad achieves 4.2% absolute improvement over standard fine-tuning, comparable to recent regularization methods while adding minimal computational overhead. However, we observe performance degradation on more complex datasets (Split-TinyImageNet), where topological features may be less informative. Our ablation study reveals that shorter barcode persistence thresholds (\u03b5 < 0.1) lead to over-regularization, whereas longer thresholds provide insufficient forgetting protection. While TopoGrad offers a theoretically principled approach with interpretable regularization via topological summaries, computational scaling to ImageNet-scale architectures remains challenging. Our code is available at [redacted-for-anonymity].",
    "id": 1108
  },
  {
    "title": "Mixture of LoRAs: Lightweight Multi-Task Adaptation Without Catastrophic Forgetting",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "Multi-task fine-tuning of large language models faces significant challenges in parameter efficiency and catastrophic forgetting. We propose Mixture of LoRAs (MoLoRA), a lightweight alternative to adapter layers that dynamically combines rank-r LoRA modules using a learned routing function. Unlike prior approaches that require separate full-rank adapters for each task, MoLoRA maintains only task-specific scalars while sharing low-rank matrices across tasks. Our key insight is that task-specific information can be encoded in routing distributions rather than adapter parameters. Experiments on GLUE and Super-NaturalInstructions benchmarks show MoLoRA achieves 92-96% of full fine-tuning performance using 0.1% task-specific parameters, comparable to existing adapter methods. However, our approach shows inconsistent gains over shared parameter baselines on out-of-domain tasks, and we observe diminishing returns beyond 8 tasks. While MoLoRA reduces storage costs by 10\u00d7 compared to full adapters and demonstrates stable continual learning across 5 sequential tasks, its practical benefits appear limited to scenarios with strong task similarity. Code and models will be available upon acceptance.",
    "id": 1109
  },
  {
    "title": "LoRA-FA: Low-Rank Adaptation with Feature Alignment for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Wang, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its performance degrades when the pre-training and downstream tasks have significant distribution shifts. We introduce LoRA-FA, which incorporates a lightweight feature alignment mechanism that learns task-specific linear transformations to better align pre-trained features with downstream objectives. Our method adds only 0.2% additional parameters compared to standard LoRA while maintaining its training efficiency. Experiments on GLUE and SuperGLUE benchmarks show LoRA-FA achieves 1.8% average improvement over LoRA on tasks with large domain gaps, particularly on CoLA and RTE. However, gains are marginal on tasks similar to pre-training data (within 0.3%). Theoretical analysis reveals our alignment can be viewed as minimizing an upper bound on the target risk under covariate shift, though the bound depends on strong assumptions about feature linearity. While our results demonstrate measurable improvements in specific scenarios, the overall contribution remains incremental, and the method's benefit is limited to settings with significant distribution mismatch.",
    "id": 1110
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning through Selective Gradient Modification",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Garcia, M."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to performance degradation and catastrophic forgetting. We propose Gradient Surgery with Memory (GSM), a novel optimization approach that maintains a sparse memory of past task gradients to guide the current optimization direction. GSM identifies gradient conflicts through an efficient dot-product analysis and selectively modifies gradients using a learned projection matrix, while preserving a limited-size memory of historical gradients to inform future updates. Our method is simple to implement, requiring only minor modifications to standard optimizers, and introduces minimal computational overhead (under 5% increase in wall-clock time). Experiments on standard multi-task benchmarks including Split-CIFAR100 and NYUv2 semantic segmentation show consistent but modest improvements over baselines, with average task performance gains of 2.1-3.4%. However, we observe performance degradation in certain task combinations, particularly when tasks have vastly different gradient scales. While GSM provides a practical approach to reducing forgetting, our theoretical analysis reveals fundamental limitations in worst-case scenarios. Our PyTorch implementation is available at [url].",
    "id": 1111
  },
  {
    "title": "Gradient Alignment Improves Out-of-Distribution Generalization in Neural Networks... Sometimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Alignment Regularization (GAR), a simple method for improving out-of-distribution (OOD) generalization by encouraging gradient directions between clean and adversarially perturbed samples to align with class-discriminative directions. Our key insight is that models exhibiting stronger alignment between these gradient spaces show improved robustness to distribution shifts. GAR adds a computationally lightweight regularizer to standard training objectives, requiring only one additional backward pass per batch. We evaluate on four OOD benchmarks (ImageNet-A, ImageNet-R, CIFAR-10.1, and WILDS-FMoW), achieving 2-5% accuracy improvements over baseline methods on 6 out of 12 test settings, with negligible performance degradation elsewhere. However, we find performance gains are highly sensitive to hyperparameter choices and dataset characteristics, with improvements disappearing entirely on some distributions. Ablation studies reveal the regularizer's effectiveness correlates moderately (\u03c1=0.38) with the magnitude of distribution shift, suggesting limited utility for mild OOD scenarios. While GAR provides a lightweight alternative to more complex augmentation strategies, our results indicate that gradient alignment alone may be insufficient for reliable OOD generalization across diverse distribution shifts.",
    "id": 1112
  },
  {
    "title": "LoRA-Flow: Adaptive Low-Rank Adaptation with Dynamic Flow Matching for Efficient Fine-tuning",
    "authors": [
      "Kim, S.H.",
      "Chen, Y.L.",
      "Garcia, M."
    ],
    "abstract": "We present LoRA-Flow, a method that combines low-rank adaptation (LoRA) with continuous normalizing flows to enable more efficient fine-tuning of large language models. Motivated by the observation that static rank constraints in LoRA may be overly restrictive for downstream tasks, we propose to learn dynamic low-rank updates through a flow-based parameter generation network. Our approach trains a small hypernetwork that outputs LoRA matrices conditioned on task-specific embeddings while maintaining the parameter efficiency of standard LoRA. We evaluate LoRA-Flow on GLUE and SuperGLUE benchmarks using Llama-2 and GPT-Neo models. Results show modest improvements over vanilla LoRA (average +0.7% accuracy) with comparable memory usage. However, we find the benefits are task-dependent: improvements are significant on classification tasks (+2.1% on QNLI) but marginal on generation tasks. The method introduces ~15% additional training time. While our approach provides a novel perspective on adaptive parameter-efficient fine-tuning, we acknowledge limitations in computational overhead and the need for further analysis of the learned flow dynamics. Code and models will be made available upon acceptance.",
    "id": 1113
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Gradient Recycling During Fine-tuning",
    "authors": [
      "Liu, M.",
      "Chen, J.",
      "Rodriguez, A.",
      "Kumar, S."
    ],
    "abstract": "We propose a simple technique to reduce computational overhead when fine-tuning large transformer models by reusing previously computed attention gradients. Our method caches attention gradient patterns from earlier layers and adapts them for subsequent layers through learned scaling parameters. While theoretically motivated by the observation that attention gradients in adjacent layers often exhibit high cosine similarity, our approach requires only minor modifications to standard training pipelines. We evaluate our method on GLUE, SuperGLUE, and two domain-specific NLP tasks, showing 12-18% reduction in training time with less than 0.5% performance degradation on average. Interestingly, we observe that the benefits vary significantly across tasks\u2014achieving up to 24% speedup on document classification but minimal gains on sequence labeling. Our analysis suggests that the effectiveness correlates with dataset size and input sequence length, though the relationship is complex. We discuss failure cases where gradient recycling leads to unstable training and provide a straightforward heuristic to detect such scenarios. While our method does not improve final model performance, it offers practical benefits for practitioners working with limited computational budgets.",
    "id": 1114
  },
  {
    "title": "Gradient Surgery with Memory: Editing Past Updates in Overparameterized Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose Gradient Surgery with Memory (GSM), a method for selectively undoing harmful gradient updates in neural networks without full retraining. Motivated by catastrophic forgetting in continual learning and adversarial robustness, GSM maintains a compressed history of parameter trajectories and identifies which past updates contributed most to current performance degradation. Our approach uses a lightweight approximation to the Hessian-trace to estimate update importance, followed by targeted counter-updates that surgically reverse specific gradient steps while preserving beneficial changes. On CIFAR-10 split across 5 sequential tasks, GSM reduces forgetting by 18% over standard finetuning while maintaining computational overhead below 5% during training. We also demonstrate applications to removing backdoor triggers and reversing adversarial fine-tuning. However, our method is limited to small-to-medium architectures (\u2264 ResNet-50) due to memory constraints, and the theoretical guarantees only hold under restrictive assumptions about loss landscape geometry. While preliminary results are promising, we acknowledge that scalability to larger models and more realistic scenarios requires further investigation.",
    "id": 1115
  },
  {
    "title": "Semi-Supervised Learning with Noise-Contrastive Calibration",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Kim, J."
    ],
    "abstract": "We propose a semi-supervised learning framework that combines self-training with noise-contrastive estimation to calibrate model predictions on unlabeled data. While self-training often suffers from confirmation bias when pseudo-labels are noisy, we introduce a simple calibration technique that uses synthetic negative examples to adjust model confidence estimates. Our method applies a modified noise-contrastive loss during training, where the model learns to distinguish between true pseudo-labels and intentionally corrupted versions. We evaluate on standard SSL benchmarks including CIFAR-10, CIFAR-100, and ImageNet subsets, achieving modest improvements over strong baselines. On CIFAR-10 with 4,000 labels, we improve accuracy from 95.1% to 95.6% over FixMatch, and achieve a 2.3 point gain on CIFAR-100 with 10,000 labels. While our results are consistent across datasets, the improvements are relatively small and computational overhead is non-negligible. Furthermore, our approach requires careful tuning of the corruption rate and temperature parameters, limiting practical applicability. Our code and experiments are reproducible, though we note the method shows diminishing returns as the labeled data size increases.",
    "id": 1116
  },
  {
    "title": "Gradient Surgery Revisited: When Adaptive Pruning Outperforms Gradient Projection in Multi-Task Learning",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Thompson, K."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks. While recent approaches like PCGrad and GradNorm use sophisticated projection mechanisms to address this, we propose a surprisingly simple alternative: adaptive gradient pruning based on task-specific confidence scores. Our method eliminates conflicting gradients by selectively dropping the bottom-k% of gradient components for each task, where k is dynamically adjusted using an auxiliary network trained to predict when gradient conflicts are likely to occur. We demonstrate that this approach achieves comparable performance to state-of-the-art gradient surgery methods on standard benchmarks (CIFAR-100 split, NYUv2, Omniglot), while requiring 30-50% fewer gradient computations. Our theoretical analysis reveals that pruning can be viewed as an implicit form of gradient compression that preserves the most informative updates for each task. However, we find that our method's effectiveness diminishes when tasks have highly dissimilar scales or when shared representations are shallow. Code is available at [redacted for review].",
    "id": 1117
  },
  {
    "title": "Revisiting Weight Initialization for Transformers: A Kernel-Based Perspective with Batch Size Dependent Scaling",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Zhou, L."
    ],
    "abstract": "Transformer architectures demonstrate remarkable performance across domains, yet their training stability remains sensitive to initialization schemes. We propose KernelScale, a weight initialization method that adapts to both the kernel activation statistics and batch size during training. Our approach extends classical initialization theory by introducing a batch-size dependent scaling factor derived from concentration bounds on the empirical covariance of attention scores. Through theoretical analysis in an infinite-width limit, we show KernelScale maintains gradient norms within desired bounds for batch sizes ranging from 32 to 4096. Experimental results on WikiText-103 and CIFAR-10 show 2-3% relative improvement over Xavier/Kaiming initialization, particularly in low-batch regimes (32-64). However, gains diminish with larger batches (\u2265256) and deeper models (>24 layers), where initialization effects become less pronounced. While KernelScale provides consistent improvements for small-scale training scenarios common in research settings, its computational overhead (5-10% slower initialization) and limited impact on state-of-the-art pretraining regimes raise questions about practical deployment. Our code and experiments are available at [link].",
    "id": 1118
  },
  {
    "title": "Fixing Mini-batch Gradient Noise with Layer-wise Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "While adaptive optimizers like Adam and AdamW achieve strong performance in deep learning, they exhibit unstable behavior when batch sizes are small due to noisy gradient estimates. We propose Layer-Adaptive Noise Suppression (LANS), a simple modification that applies separate learning rates to each layer based on the signal-to-noise ratio of their gradients. LANS computes layer-wise gradient statistics using exponential moving averages, then scales each layer's update inversely proportional to its gradient noise. Our method requires minimal hyperparameter tuning and adds negligible computational overhead. We evaluate LANS on ResNet-50 and ViT-B/16 across CIFAR-100 and ImageNet, achieving 0.3-0.7% improvements over AdamW with batch sizes 32-128, while matching performance at larger batches. However, gains diminish on architectures with batch normalization, likely due to its inherent noise reduction. Theoretical analysis shows LANS reduces the noise-to-signal ratio by factor \u221a(d_layer/d_total) where d_layer is the layer dimension. Code will be released upon acceptance.",
    "id": 1119
  },
  {
    "title": "Gradient Surgery for Improving Transfer Learning in Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Fine-tuning large pre-trained vision transformers for downstream tasks often suffers from catastrophic forgetting and negative transfer. We propose Progressive Gradient Surgery (PGS), a simple technique that selectively prunes gradient components during fine-tuning based on their alignment with pre-trained weights. PGS computes gradient-projections onto the subspace spanned by frozen pre-trained parameters, then removes components below a learned threshold. While conceptually straightforward, PGS surprisingly improves transfer performance across 8 vision benchmarks by 2.3% on average compared to standard fine-tuning. However, gains diminish as dataset size increases, and the approach underperforms recent adapters and prompt-tuning methods on ImageNet (0.8% drop from baseline). Our analysis reveals PGS primarily improves convergence speed rather than final accuracy, achieving similar results to earlier stopping. These findings suggest gradient surgery may be most beneficial in low-data regimes or when computational constraints limit alternatives. Code is available at [URL to be added].",
    "id": 1120
  },
  {
    "title": "LoRA-Prune: Structured Sparsity for Parameter-Efficient Fine-Tuning via Importance-Aware Channel Dropout",
    "authors": [
      "Chen, L.",
      "Mukherjee, S.",
      "Johnson, K."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular method for parameter-efficient fine-tuning of large language models, but its memory footprint remains a bottleneck in resource-constrained settings. We propose LoRA-Prune, a simple yet effective approach to reduce the effective parameter count of LoRA adapters through structured sparsity. Our method introduces importance-aware channel dropout, where we rank LoRA weight matrices by their contribution to the training loss and systematically drop low-importance channels during fine-tuning. This is achieved through an efficient approximation of the Hessian trace that requires minimal overhead. Experiments on GPT-2, LLaMA-7B, and T5-base show that LoRA-Prune can reduce LoRA parameters by 30-50% with minimal performance degradation (<2% drop in task accuracy) across standard NLP benchmarks. While our method achieves competitive compression ratios, we observe significant variance across tasks and architectures, suggesting that the effectiveness of structured sparsity in LoRA adapters may be more limited than previously thought. Code and models will be released upon acceptance.",
    "id": 1121
  },
  {
    "title": "Momentum-Based Gradient Descent with Adaptive Restart for Non-Convex Optimization: Theory and Practice",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose MARS, a momentum-based optimizer that automatically detects and counters the oscillatory behavior of gradient descent in non-convex landscapes. Our method combines Nesterov momentum with a novel restart criterion based on the angle between consecutive update directions. While momentum methods often overshoot in sharp curvatures, MARS periodically resets the velocity when the optimization trajectory exhibits abnormal curvature patterns. We prove that MARS achieves O(1/\u221aT) convergence for non-convex smooth functions, matching standard SGD rates but with improved empirical performance. On ImageNet training, MARS shows 1.3% better final accuracy compared to SGD+momentum, though experiments across 5 additional datasets reveal inconsistent gains. We also test on Transformer language models, finding 8% faster convergence in early training but eventual performance matching baselines. Theoretically, we characterize conditions under which restarts provably help, but our analysis relies on the restrictive assumption of Lipschitz continuous Hessians. While MARS demonstrates practical improvements in several settings, its benefits appear task-specific and the theoretical contributions are incremental over existing momentum analyses. Code is available at anonymous-url.",
    "id": 1122
  },
  {
    "title": "Gradient Echo: Enhancing Adversarial Training through Periodic Weight Perturbation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Adversarial training remains the most effective defense against adversarial attacks, yet it suffers from robust overfitting and significant computational overhead. We propose Gradient Echo (GE), a simple method that periodically injects noise sampled from historical gradient directions to improve adversarial robustness without increasing training time. During standard adversarial training, we store a subset of past gradients and use them as perturbation directions in subsequent epochs. Our theoretical analysis shows that under mild assumptions, GE regularizes the loss landscape by preventing convergence to sharp minima. Experimental evaluation on CIFAR-10 and CIFAR-100 demonstrates that GE improves robust accuracy by 2.3-3.8% over baseline adversarial training while maintaining comparable standard accuracy. However, we observe diminishing benefits on larger datasets like ImageNet. We provide extensive ablations showing GE's sensitivity to gradient history size and noise magnitude. While our method offers practical improvements for small to medium-scale applications, theoretical gaps remain in explaining its behavior under non-convex settings. Code will be released upon acceptance.",
    "id": 1123
  },
  {
    "title": "Gradient Surgery Without the Surgery: Memory-Efficient Alternatives to Multi-Task Optimization in Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning with transformer models faces a fundamental limitation: existing gradient surgery methods require storing per-task gradients, creating memory bottlenecks for large models. We propose Sparsified Gradient Routing (SGR), a simple technique that approximates gradient surgery by selectively zeroing gradient components that conflict across tasks. Rather than computing exact gradient projections, SGR identifies conflicting gradient directions using lightweight cosine similarity calculations on sparsified gradient vectors (keeping only top-k% of gradient magnitudes). Our experiments on GLUE and SuperGLUE benchmarks show SGR achieves 94% of the accuracy gains obtained by PCGrad and GradNorm while reducing memory overhead by 3.2x. However, we observe diminishing returns on tasks with high gradient alignment and slightly degraded performance (2-3% drop) on tasks requiring precise gradient manipulation. Our results suggest that approximate gradient surgery methods may be sufficient for many practical multi-task scenarios, though they do not fully replace exact methods for applications requiring fine-grained control over gradient interactions. Code will be made available upon acceptance.",
    "id": 1124
  },
  {
    "title": "Refining Representations with Iterative Gradient Descent on Gradients",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a simple method for improving learned representations by applying gradient descent on gradient information, rather than model parameters. Motivated by the observation that gradients contain rich information about data structure, we iteratively update representations by minimizing the gradient norm with respect to their own gradient. Our method treats the output of a base encoder as learnable parameters and performs K steps of inner optimization, yielding refined representations that emphasize task-relevant features. We evaluate on standard benchmarks including ImageNet classification, GLUE natural language understanding, and Atari reinforcement learning. Results show consistent but modest improvements (1-3%) over strong baselines across domains. While our approach is conceptually straightforward and implementation requires only a few lines of code, we recognize the limited novelty as similar gradient-based meta-learning ideas exist. We conduct ablations demonstrating sensitivity to step size selection and computational overhead scaling linearly with K. Our theoretical analysis confirms convergence under restricted assumptions similar to standard gradient descent, though we acknowledge this analysis does not fully capture practical behavior with neural networks. Code is available to ensure reproducibility.",
    "id": 1125
  },
  {
    "title": "Gradient Surgery Meets Weight Averaging: An Empirical Study of Interpolation Strategies for Multi-Task Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While gradient surgery methods like PCGrad and GradNorm have shown promise, we investigate whether simple weight averaging strategies can achieve comparable results with reduced computational overhead. We propose Interpolated Task Balancing (ITB), which periodically averages checkpoint weights across different task-specific training runs. Surprisingly, on standard benchmarks including NYUv2 and CIFAR-100 with 5 auxiliary tasks, ITB matches or outperforms PCGrad by 1.2-2.8% in average accuracy while reducing training time by 15-30%. However, our experiments reveal that ITB struggles with high task interference scenarios (correlation < -0.3 between task gradients) where PCGrad maintains more stable training. Through theoretical analysis of a toy multi-task linear model, we show that ITB implicitly performs a similar conflict resolution as gradient surgery but with a different inductive bias. Our results suggest that for moderately conflicting tasks, simple checkpoint averaging may be a viable alternative to complex gradient manipulation techniques, though gradient surgery remains superior in high-conflict regimes. Code and experiments are available at anonymous-url.github.io.",
    "id": 1126
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Levels for Distributed Deep Learning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient compression is essential for scaling deep learning to distributed settings, yet most methods rely on hand-crafted quantization schemes that may not adapt to varying network architectures or training dynamics. We propose LQ-SGD, a learning-based approach that automatically discovers optimal quantization levels for gradient compression during training. Unlike conventional fixed-bitwidth methods, our approach learns a small set of quantization centroids via a bilevel optimization objective that balances compression ratio with convergence stability. Our method adds minimal computational overhead (\u22482% training time) and can be implemented as a drop-in replacement for existing optimizers. Experiments on ResNet-50 and Transformer models across computer vision and NLP tasks show that LQ-SGD achieves 16-32x compression rates while maintaining within 1.5% of uncompressed accuracy. However, we observe performance degradation on certain architectures (e.g., Vision Transformers) and tasks with sensitive optimization landscapes. While our theoretical analysis provides convergence guarantees under convexity assumptions, extending these results to non-convex settings remains challenging. Code will be released upon acceptance.",
    "id": 1127
  },
  {
    "title": "Gradient Descent with Predictive Variance Adaptation: A Lightweight Approach to Stochastic Optimization",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, J."
    ],
    "abstract": "We propose Predictive Variance Adaptation (PVA), a simple modification to Adam and SGD that adjusts learning rates based on estimated gradient variance in mini-batch training. By maintaining an exponentially decaying average of gradient statistics, PVA predicts future gradient magnitude and adaptively rescales the effective learning rate. Our method requires minimal additional computation (O(d) parameters where d is model dimension) and can be implemented in ~20 lines of PyTorch code. On CIFAR-10 and ImageNet classification tasks, PVA achieves 0.3-0.7% accuracy improvements over AdamW and SGD with momentum, while requiring 5-10% fewer training steps to reach target validation accuracy. However, gains diminish on smaller datasets and simpler architectures, suggesting PVA primarily benefits large-scale settings with heavy mini-batch noise. Theoretical analysis proves convergence under standard convexity assumptions with rate O(1/\u221aT), matching unmodified methods. While improvements are modest, PVA offers practitioners a lightweight alternative to more complex adaptive optimizers. Code and hyperparameters are provided for reproducibility.",
    "id": 1128
  },
  {
    "title": "Rethinking Momentum in Adam: A Non-monotonic Approach with Polynomial Decay",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Despite Adam's prevalence in deep learning optimization, its hyperparameter sensitivity remains a practical challenge. This work investigates whether traditional momentum scheduling can be improved through non-monotonic strategies. We propose PolyMomentumAdam, which modulates the momentum term using a polynomial decay schedule that periodically increases and decreases based on gradient norms. The key insight is that allowing momentum to temporarily increase can help escape sharp minima while subsequent decay ensures convergence stability. Experiments on CIFAR-10/100 and ImageNet show PolyMomentumAdam achieves 0.7-1.2% improvement over vanilla Adam on ResNets and Transformers, while reducing hyperparameter sensitivity across batch sizes. However, we observe these gains diminish significantly when momentum warmup is already employed, suggesting the benefits may be redundant in well-tuned training pipelines. Our theoretical analysis provides limited convergence guarantees under restricted conditions, and we acknowledge the improvements are primarily empirical. Though the contribution is incremental and lacks definitive theoretical backing, our approach offers a practical plug-and-play modification for researchers frustrated with Adam's sensitivity at minimal computational cost.",
    "id": 1129
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient compression is crucial for distributed training, but most methods rely on hand-crafted quantization schemes that remain fixed throughout training. We propose Learned Quantization Schedules (LQS), a simple approach that adaptively adjusts the precision of gradient compression based on the current training dynamics. Our method trains a lightweight auxiliary network to predict optimal bit-widths for different layers and iterations, using only local gradient statistics as input. Experiments on ResNet-50 and Transformer models show up to 2.3\u00d7 communication reduction over fixed quantization baselines while maintaining convergence properties. However, our approach introduces non-negligible computational overhead (15-20% training slowdown) and shows diminishing benefits on smaller models. Theoretical analysis reveals our schedules achieve near-optimal compression under convexity assumptions, though extending these guarantees to non-convex settings remains challenging. While LQS demonstrates practical improvements in specific regimes, particularly for large-scale training with bandwidth constraints, its broader applicability is limited by training complexity and sensitivity to hyperparameter choices.",
    "id": 1130
  },
  {
    "title": "Adaptive Gradient Clipping with Moving Estimate of Hessian Spectral Norm",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Modern neural networks require careful gradient normalization to prevent exploding gradients during training, particularly when using large learning rates or aggressive architectures. While standard gradient clipping uses fixed thresholds, we propose an adaptive clipping method that dynamically adjusts clipping boundaries based on a moving estimate of the Hessian's spectral norm. Our approach combines low-rank approximation techniques with a streaming algorithm to track the top eigenvalue without computing the full Hessian matrix. We evaluate our method on standard vision and language tasks, finding improvements over standard clipping in 65% of experimental settings, with gains concentrated in very deep networks (50+ layers). However, computational overhead increases training time by 15-30%, and we observe minimal benefits on shallow architectures or when using warmup schedules. Our theoretical analysis proves convergence under smoothness assumptions similar to existing work, though the practical impact depends heavily on architecture choices. While not universally superior, our method offers a principled alternative to manual clipping threshold tuning, particularly for practitioners training deep models without extensive hyperparameter search budgets.",
    "id": 1131
  },
  {
    "title": "Rethinking Batch Normalization with Gentle Momentum Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "Batch Normalization (BN) has become a staple in deep learning, yet its interaction with modern optimizers remains poorly understood. We propose Gentle Batch Normalization (GBN), a simple modification that replaces the conventional batch statistics with an exponential moving average updated on a slower timescale than the model parameters. This decoupling reduces gradient conflicts between BN layers and the rest of the network without introducing additional hyperparameters. Through experiments on ResNet-50 and Vision Transformer architectures across CIFAR-10, ImageNet, and domain adaptation benchmarks, GBN demonstrates consistent but modest improvements (0.3-0.7% accuracy) while requiring 5-10% fewer training steps to converge. However, gains diminish in well-tuned regimes with learning rate warmup and careful initialization. We provide theoretical analysis showing GBN acts as a form of implicit regularization, though the effect is bounded by the batch size. Code is available, though we note instability with very small batch sizes (<8). While GBN offers a practical alternative to standard BN, the incremental benefits may not justify the added complexity for practitioners with sufficient compute budget.",
    "id": 1132
  },
  {
    "title": "Gradient Surgery with Adaptive Momentum: A Hybrid Approach to Multi-Task Optimization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that can hinder convergence and degrade performance. While recent gradient surgery methods like PCGrad effectively modify conflicting gradients, they treat all task gradients equally regardless of their reliability or scale. We propose AdaMoGS, a simple yet effective method that combines adaptive momentum with gradient surgery to prioritize more reliable gradient directions. Our approach maintains separate momentum buffers for each task and uses their consistency over time to weight gradient contributions during surgery steps. On three standard multi-task benchmarks (CelebA, NYU-v2, and QM9), AdaMoGS achieves modest improvements over PCGrad (0.8-2.3% average relative gain) while reducing training instability. However, we find that benefits diminish when tasks have highly correlated gradients or when architecture-specific techniques like task-specific heads are employed. Our theoretical analysis shows that AdaMoGS can be viewed as a form of preconditioned gradient descent with task-specific learning rates, though we only provide convergence guarantees for a simplified two-task case. While the improvements are consistent, they remain incremental compared to stronger baselines that incorporate architectural inductive biases. Code is available at anonymous-url.",
    "id": 1133
  },
  {
    "title": "Gradient Surgery with Memory: Improving Stability in Multi-Task Optimization via Gradient Conflict Replay",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that destabilize optimization and degrade performance. While recent gradient surgery methods like PCGrad and GradDrop address these conflicts, they rely solely on current mini-batch information, potentially discarding useful gradients long-term. We propose Gradient Memory Surgery (GMS), a simple extension that maintains a small replay buffer of conflicting gradients to achieve more globally consistent updates. GMS identifies gradient conflicts using cosine similarity, then selectively blends stored gradients with current ones via an attention mechanism. On three standard multi-task vision benchmarks (NYUv2, CityScenes, CIFAR-100), GMS achieves modest improvements over PCGrad (+0.3-0.8% average accuracy) while reducing training instability. However, we find GMS's benefits diminish with larger models and are most pronounced in low-data regimes. Our method adds minimal computational overhead (\u22485% training time) and only one hyperparameter (buffer size). While results are positive, we acknowledge the improvements are incremental and the technique may not address fundamental limitations of gradient surgery methods. Code and hyperparameters will be made available.",
    "id": 1134
  },
  {
    "title": "LoRA-SGD: Low-Rank Adaptation Meets Gradient Compression for Efficient Federated Fine-tuning",
    "authors": [
      "Chen, S.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "Federated fine-tuning of large language models faces significant communication bottlenecks, particularly when adapting models with billions of parameters. While Low-Rank Adaptation (LoRA) reduces trainable parameters, standard federated averaging still requires communicating full gradients for the adapted layers. We propose LoRA-SGD, a simple yet effective approach that applies gradient compression techniques directly to LoRA adapters during federated training. Our method combines structured sparsification with quantization to reduce communication costs by 8-32x compared to standard LoRA fine-tuning, while maintaining model quality. We evaluate LoRA-SGD on three NLP tasks across 50 clients with non-IID data partitions. Results show competitive performance to uncompressed LoRA (within 2% accuracy) on GLUE and SuperGLUE benchmarks, but reveal degradation on tasks requiring nuanced adaptation. Theoretical convergence analysis reveals a trade-off between compression ratio and final performance, with diminishing returns beyond 16x compression. While LoRA-SGD demonstrates practical benefits for federated deployment, our findings highlight the need for adaptive compression strategies based on local data complexity. Code is available at [URL].",
    "id": 1135
  },
  {
    "title": "Decoupled Weight Decay for Adaptive Gradient Methods via Block-Diagonal Approximation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "Adaptive gradient methods like Adam have become standard for training deep neural networks, but their performance often degrades on tasks requiring strong regularization. While weight decay is commonly used, its interaction with adaptive methods remains poorly understood. We propose AdaDec, a simple modification to Adam that decouples weight decay from the gradient-based updates through block-diagonal approximation of the preconditioning matrix. Our key insight is that the standard Adam update conflates the magnitude of weights with their update scale, leading to suboptimal regularization. By treating each parameter block independently and applying weight decay in the transformed parameter space, AdaDec achieves better decoupling. We evaluate AdaDec on image classification, language modeling, and reinforcement learning benchmarks. Results show consistent improvements over AdamW on 8/12 tasks, with average gains of 2.1% accuracy on CIFAR-100 and 0.8 perplexity reduction on WikiText-103. However, performance is comparable to baseline on the remaining tasks, and we observe sensitivity to the block size hyperparameter. While not universally superior, AdaDec provides a principled approach to weight decay in adaptive methods with minimal computational overhead. Code is available at anonymous-url.github.io/adadec.",
    "id": 1136
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: A Modular Approach to Catastrophic Forgetting in Multitask Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Catastrophic forgetting remains a fundamental challenge when training neural networks on sequential tasks. While gradient-based methods like PCGrad and GradNorm have shown promise, they suffer from computational overhead and require careful hyperparameter tuning. We present GAMMA (Gradient surgery with Adaptive Memory and Modular Augmentation), a lightweight framework that combines selective gradient projection with episodic memory replay. Our key insight is that gradient conflicts can be resolved by maintaining a dynamically-sized memory buffer that stores task-specific gradient directions, allowing for more efficient gradient surgery without full gradient decomposition. We also introduce a modular loss term that encourages sparse gradient updates, reducing interference between tasks. Experiments on standard benchmarks including Permuted MNIST and Split CIFAR-100 demonstrate 5-15% improvements over baselines in average accuracy, with 40% reduction in training time compared to PCGrad. However, performance degrades significantly when the number of tasks exceeds 20, and our method shows inconsistent results on natural language tasks. While GAMMA provides a computationally efficient alternative to existing methods, limited theoretical analysis and narrow experimental scope suggest further investigation is needed to establish broader applicability.",
    "id": 1137
  },
  {
    "title": "LoRA-GA: Gaussian Attention for Low-Rank Adaptation with Applications to Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, S.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "We propose LoRA-GA, an extension to Low-Rank Adaptation (LoRA) that incorporates a learned Gaussian attention mechanism to dynamically reweight low-rank decomposition matrices during fine-tuning. While LoRA has shown promise for parameter-efficient adaptation of large language models, we observe that its static rank constraints often limit expressivity in downstream tasks requiring nuanced feature interactions. LoRA-GA introduces learnable Gaussian kernels that modulate the contribution of each rank component based on input-dependent context, effectively providing adaptive capacity without increasing parameter count. Our method adds only 0.3% additional parameters over standard LoRA and requires a single forward pass during inference. We evaluate on GLUE, SuperGLUE, and domain-specific NLP benchmarks, achieving modest improvements over LoRA (1.2-2.1% average) while remaining competitive with full fine-tuning on 7 out of 11 tasks. However, we find diminishing returns on larger models (>30B parameters) and tasks requiring extensive world knowledge. Analysis reveals the Gaussian attention primarily benefits tasks with strong local context dependencies, though interpretability remains limited. While LoRA-GA demonstrates consistent gains over vanilla LoRA, the absolute improvements are incremental and the computational overhead may not justify deployment in resource-constrained settings.",
    "id": 1138
  },
  {
    "title": "Gradient Surgery with Adaptive Magnitude Pruning for Multi-Task Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization. While existing gradient surgery methods like PCGrad resolve conflicts through projection, we find they discard useful gradient information. We propose Gradient Surgery with Adaptive Magnitude Pruning (GS-AMP), which selectively removes gradient components with smaller magnitudes rather than projecting onto conflicting subspaces. Our method introduces a learnable pruning threshold that adapts based on gradient statistics across tasks. On 8-task Meta-World benchmarks, GS-AMP achieves 2.3% higher average success rate than PCGrad (51.2% vs 48.9%), while reducing computational overhead by 15%. However, these gains diminish on simpler multi-task datasets. Theoretical analysis shows our method preserves more gradient information when conflicts are mild, but may underperform in severely conflicting scenarios. Our findings suggest that magnitude-based pruning can serve as a lightweight alternative to geometric projection methods, though careful calibration is needed for optimal performance. Code and checkpoints are available at [repo-link].",
    "id": 1139
  },
  {
    "title": "Gradient Surgery Meets Sharpness-Aware Minimization: An Empirical Study of Multi-Task Optimization Trade-offs",
    "authors": [
      "Liu, K.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "Multi-task learning (MTL) seeks to leverage shared representations across related tasks, yet gradient conflicts frequently undermine performance. While PCGrad and other gradient surgery methods address sign conflicts, they neglect the sharpness of the loss landscape\u2014a key factor in generalization. We propose Sharpness-Aware Gradient Surgery (SAGS), which combines gradient projection with Sharpness-Aware Minimization (SAM) to explicitly optimize for both gradient alignment and flat minima. Our method modifies PCGrad's projection step to account for sharpness along conflicting gradient directions, theoretically establishing that SAGS reduces task interference when gradients have conflicting sharpness profiles. Through experiments on five MTL benchmarks covering computer vision and NLP tasks, SAGS achieves modest improvements (1.2\u20133.7% relative gains) over strong baselines including PCGrad, Nash-MTL, and vanilla SAM. While these gains are statistically significant in 8/15 task pairs, they remain within the standard deviation of existing methods in 4 cases. Ablations reveal that sharpness-awareness provides diminishing returns when task gradients are nearly aligned. Though SAGS introduces additional computational overhead (1.8\u00d7 training time), our results suggest that the interaction between gradient geometry and loss landscape sharpness deserves deeper theoretical investigation in MTL optimization.",
    "id": 1140
  },
  {
    "title": "Improving Contrastive Learning with Lightweight Augmentation Schedules",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, S."
    ],
    "abstract": "We study the role of image augmentation scheduling in contrastive self-supervised learning. While existing methods use fixed augmentation policies, we propose DynamicAugment, a simple algorithm that adjusts augmentation strength based on training progress and batch statistics. Our approach modifies standard SimCLR training by introducing a curriculum that gradually transitions from strong to weak augmentations, guided by a confidence score computed from the learned representations. Experiments on CIFAR-10, CIFAR-100 and Tiny-ImageNet show 1.2-3.5% improvements in linear evaluation accuracy over baselines, with particular gains on harder datasets. Analysis reveals our method reduces training instability in early epochs while maintaining sufficient invariance. Although our improvements are consistent, they remain modest compared to recent architectural innovations. Code will be provided upon publication.",
    "id": 1141
  },
  {
    "title": "LoRA-E: Efficient Low-Rank Adaptation with Entropy-Based Rank Selection",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become standard for adapting large language models, but the selection of rank hyperparameters remains largely heuristic. We propose LoRA-E, a simple extension that automatically selects rank using an entropy-based criterion computed during a single forward pass of the target data. Our method computes the entropy of activation patterns in each layer and sets the rank proportionally to this entropy, eliminating the need for manual tuning or costly validation runs. While this approach lacks theoretical guarantees, we empirically demonstrate consistent improvements over fixed-rank LoRA on GLUE and SuperGLUE benchmarks, achieving average gains of 1.3 points across tasks with 15% fewer parameters. However, results are mixed on domain-specific datasets where the entropy-adaptive approach occasionally underperforms tuned baselines. Ablation studies reveal the method is particularly sensitive to batch size choices and may struggle with highly imbalanced datasets. Though the computational overhead is minimal (\u22645% increase in training time), the gains over LoRA with carefully tuned ranks are modest. Our code is available at https://anonymous-url.github.io/lora-e.",
    "id": 1142
  },
  {
    "title": "Regularizing Neural Networks via Ensembled Gradient Dropout",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Dropout Ensemble (GDE), a simple regularization technique that randomly drops gradient components during backpropagation and averages predictions across multiple forward passes. While dropout in activations is well-studied, our work explores gradient-space stochasticity as an implicit ensemble method. We show that GDE can be interpreted as performing approximate Bayesian inference under certain assumptions, though these assumptions are restrictive in practice. Experiments on CIFAR-10 and ImageNet show modest improvements over standard dropout (0.3-0.7% accuracy gains), with larger benefits on smaller datasets. However, the computational overhead is substantial, requiring 8-16 forward passes at inference. Ablations reveal that most benefits come from increased training stochasticity rather than the ensemble effect. While GDE demonstrates that gradient perturbation can be a viable regularization strategy, its practical utility is limited by efficiency concerns and the availability of stronger baselines such as MixUp and CutMix. Code is available at anonymous-url.github.io/gde.",
    "id": 1143
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Dropping with Adaptive Recovery",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Transformer models suffer from quadratic complexity in sequence length, limiting their applicability to long sequences. We propose Adaptive Token Recovery (ATR), a training-free method that dynamically drops low-importance tokens during inference and recovers them with lightweight attention heads when needed. Our approach uses a learnable gating mechanism to identify tokens for removal based on attention patterns, while a separate recovery module reconstructs dropped tokens using local context. We evaluate ATR on language modeling and machine translation tasks, achieving 1.3x speedup on average while maintaining 95-98% of original performance on sequences up to 4K tokens. The method shows promising results on longer sequences but exhibits 5-10% degradation on tasks requiring fine-grained token interactions. Our analysis reveals ATR works best for tasks with natural redundancy in input sequences. While competitive with existing pruning methods, ATR's complexity introduces additional hyperparameters that may limit its practical deployment. Code is available at anonymous-url.",
    "id": 1144
  },
  {
    "title": "Gradient Noise Injection Improves Transformer Training with Minimal Overhead",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "While Transformers have become the dominant architecture across many domains, their training dynamics remain sensitive to hyperparameter choices and initialization. We propose a simple method to improve training stability by injecting controlled noise into the gradients during optimization. Our approach adds minimal computational overhead (<0.5% increase in training time) and can be implemented in three lines of code. Through extensive experiments on standard NLP benchmarks (GLUE, SuperGLUE) and vision tasks (ImageNet, CIFAR-100), we demonstrate consistent improvements in final accuracy (0.8-1.3% average gain) and reduced sensitivity to learning rate choices. Theoretical analysis suggests our method acts as implicit regularization, preventing the model from sharp minima. However, we observe the benefits are most pronounced in specific regimes: medium-sized models (100M-500M parameters) and moderate batch sizes (32-128). On larger scales (>1B parameters) or extremely large batch training, the improvements diminish significantly. While the technique is general, our empirical study reveals it provides limited benefits for CNN architectures, suggesting the mechanism may be specific to attention-based models. Our results provide practical guidance for researchers training moderately-sized Transformer models with constrained computational budgets.",
    "id": 1145
  },
  {
    "title": "Enhancing Transformer Efficiency through Structured Attention Pruning with Learnable Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "While transformers achieve remarkable performance across domains, their quadratic attention complexity limits deployment on resource-constrained devices. We propose Structured Attention Pruning with Learnable Thresholds (SAPLT), a method that dynamically removes attention heads during inference based on learned contribution scores. Unlike prior work that uses static pruning ratios or relies on post-training heuristics, SAPLT introduces threshold parameters jointly optimized with model weights using a simple regularization term. Our approach requires no additional training data or modifications to standard transformer architectures. Experiments on BERT-base and Vision Transformer show 15-20% FLOP reduction with less than 1% accuracy loss on GLUE and ImageNet tasks. While these gains are modest compared to state-of-the-art compression methods, SAPLT offers practical advantages: it is architecture-agnostic, introduces minimal training overhead, and maintains interpretability through learned thresholds. Limitations include sensitivity to the regularization coefficient and diminishing returns on larger models. Code will be made available upon acceptance.",
    "id": 1146
  },
  {
    "title": "Progressive Knowledge Distillation via Dynamic Temperature Scheduling",
    "authors": [
      "Liu, K.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "Knowledge distillation faces a fundamental tension: high temperatures encourage diverse teacher predictions but risk over-smoothing, while low temperatures preserve fine-grained distinctions but may hinder effective transfer. We propose Progressive Temperature Scheduling (PTS), a simple method that gradually reduces temperature during distillation training. Our approach begins with high temperatures to capture global structure, then progressively lowers temperatures to preserve local decision boundaries. While this dynamic schedule shows modest improvements (+0.8-1.2% accuracy) over standard distillation on CIFAR-100 and ImageNet subsets, we observe substantially better calibration (ECE reduced by 15-20%) across multiple architectures. The method introduces no additional parameters and requires minimal code changes. However, our theoretical analysis reveals the approach is equivalent to a particular form of curriculum learning, and gains diminish with stronger teacher-student capacity gaps. Experimental validation on 5 benchmarks demonstrates consistent but incremental improvements over baseline distillation, suggesting the technique may be most valuable when calibration, rather than accuracy, is the primary objective.",
    "id": 1147
  },
  {
    "title": "LoRA-DROP: Adaptive Low-Rank Adaptation via Dynamic Parameter Pruning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have achieved widespread adoption for adapting large language models, but their fixed-rank structure limits adaptiveness to downstream tasks. We propose LoRA-DROP, a simple yet effective extension that dynamically prunes LoRA parameters during training based on gradient sensitivity analysis. Our method uses a computationally lightweight approximation of the Fisher information matrix to identify and remove redundant low-rank components, reducing parameters by 30-50% while maintaining downstream task performance. Experiments on GLUE, SuperGLUE, and three vision-language tasks show comparable results to standard LoRA (within 1.2% of original performance), with slight improvements on smaller datasets. However, our analysis reveals the method's effectiveness correlates strongly with the intrinsic dimensionality of downstream tasks\u2014showing degradation when task complexity exceeds a threshold. While LoRA-DROP offers practical memory savings and provides insights into adaptation dynamics, we acknowledge limitations in theoretical guarantees and computational overhead during the pruning phase. Code and pre-trained adapters will be released upon acceptance.",
    "id": 1148
  },
  {
    "title": "Gradient Descent with Momentum Can Be Worse Than Standard Gradient Descent: A Large-Scale Empirical Study",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Momentum-based optimization methods remain the de facto standard for training deep neural networks, yet their practical benefits over standard gradient descent remain poorly understood. We conduct the largest empirical comparison to date of SGD with and without momentum across 1,200 diverse training runs spanning vision, language, and tabular datasets. Our findings reveal surprisingly nuanced behavior: while momentum accelerates training in 68% of cases, it degrades final performance or stability in 24% of scenarios, particularly on small datasets or with adaptive optimizers. Through careful ablation studies, we identify that momentum's effectiveness correlates strongly with batch size and the sharpness of the loss landscape. Surprisingly, we observe no consistent correlation between momentum's utility and architecture depth or dataset size after controlling for batch size. These results challenge the prevailing assumption that momentum is universally beneficial and suggest practitioners should treat it as a hyperparameter requiring dataset-specific tuning. We provide practical guidelines for when momentum is likely to help or hurt, supported by open-source implementations and pre-trained models.",
    "id": 1149
  },
  {
    "title": "Gradient Surgery Lite: Reducing Interference in Multi-Task Learning via Lightweight Subspace Projections",
    "authors": [
      "Kim, S.",
      "Chen, L.",
      "Brown, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent work has proposed gradient surgery techniques to address interference, these methods require computing task-specific gradients for all training data, significantly increasing computational overhead. We propose Gradient Surgery Lite (GSL), a simple approach that identifies dominant gradient conflicts using only a small subset of training examples in each batch. GSL projects conflicting gradients onto an approximate nullspace constructed via randomized SVD, enabling efficient interference reduction without the full gradient ensemble. Our method adds minimal computational cost (\u224810% overhead versus 200%+ for prior work) and requires no hyperparameter tuning beyond learning rate. Experiments on three standard multi-task benchmarks (NYUv2, CelebA, and QM9) show modest improvements over naive multitask baselines (0.5\u20132.1% average accuracy gain), but our gains over recent gradient surgery methods are smaller than reported in prior work. While GSL provides a lightweight alternative when computational constraints are severe, our theoretical analysis reveals fundamental limitations in low-rank gradient approximations that may restrict its broader applicability.",
    "id": 1150
  },
  {
    "title": "LoRA-GNN: Low-Rank Adaptation for Scalable Graph Neural Networks",
    "authors": [
      "Chen, J.",
      "Liu, S.",
      "Kumar, V.",
      "Rodriguez, A."
    ],
    "abstract": "Fine-tuning large Graph Neural Networks (GNNs) for downstream tasks remains computationally prohibitive, particularly when graphs exceed millions of nodes. Inspired by recent advances in parameter-efficient fine-tuning for language models, we propose LoRA-GNN, which freezes pretrained GNN weights and injects trainable low-rank decomposition matrices into message-passing layers. Our method achieves comparable performance to full fine-tuning on 5 benchmark datasets while using 10-50\u00d7 fewer trainable parameters. Key contributions include: (1) a theoretical analysis showing low-rank structure emerges naturally in overparameterized GNNs under certain graph heterophily conditions, and (2) an efficient implementation that reduces memory usage by 75% without additional preprocessing overhead. Experiments on node classification tasks demonstrate LoRA-GNN matches full fine-tuning accuracy on homophilous graphs (within 1.2% F1) but exhibits a 3-8% performance drop on heterophilous graphs. While our theoretical guarantees assume idealized conditions that rarely hold in practice, our empirical results suggest low-rank adaptation provides a practical trade-off between efficiency and performance for large-scale graph learning.",
    "id": 1151
  },
  {
    "title": "Gradient Surgery with Memory: Improving Gradient Interference in Multi-Task Learning via Persistent Momentum",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient interference, where conflicting gradients across tasks impede optimization. While recent gradient surgery methods like PCGrad and GradNorm have shown promise, they rely solely on instantaneous gradient information at each step, ignoring historical interference patterns. We propose MementoGrad, which augments existing gradient surgery techniques with momentum-based memory of past gradients. Our method computes task-specific gradient directions by both resolving immediate conflicts and aligning with historical gradient trajectories stored in a small memory buffer. Experiments on standard vision and NLP benchmarks show MementoGrad achieves 2-4% average improvement over PCGrad across tasks, with particular gains in settings with high gradient interference. However, we find these improvements diminish when tasks are well-aligned or when using very large batch sizes. Analysis reveals MementoGrad mainly helps during initial training phases, and that its effectiveness is sensitive to the momentum hyperparameter. While the approach is simple to implement, the memory overhead (less than 1% of model size) may not be justified for all applications.",
    "id": 1152
  },
  {
    "title": "Learning to Warm-Start: A Transformer-Based Approach for Adaptive Gradient Descent Initialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Warmformer, a transformer-based architecture that predicts initial parameters for gradient-based optimization algorithms based on problem metadata and historical optimization trajectories. Our method is motivated by the observation that similar optimization problems often share structural properties that can be leveraged for faster convergence. The key innovation is a cross-attention mechanism that conditions on problem features (dimension, condition number, sparsity patterns) and previous optimization paths to generate parameter initializations. We evaluate Warmformer on convex quadratic programming and neural network training tasks, achieving 15-30% faster convergence compared to standard initialization methods when evaluated on problems similar to the training distribution. However, performance degrades significantly (sometimes worse than baselines) when tested on out-of-distribution problem instances. While our approach demonstrates the viability of learned initialization strategies, the limited generalization beyond training distributions and computational overhead of the transformer model raise questions about practical applicability. Our code is available at [url].",
    "id": 1153
  },
  {
    "title": "Scheduled Dropout: A Simple Curriculum-Based Regularization Strategy for Training Neural Networks",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Thomas, C.",
      "Zhao, L."
    ],
    "abstract": "We propose Scheduled Dropout, a straightforward modification to standard dropout training that gradually reduces dropout rates during training according to a predefined curriculum. Unlike traditional dropout with fixed rates, our method starts with high dropout (e.g., 0.5-0.7) and linearly anneals to lower values (0.1-0.2) over training epochs. This approach is motivated by the observation that early training phases benefit from stronger regularization while later phases require less noise to fine-tune learned representations. Our theoretical analysis shows that scheduled dropout reduces to a modified L2 regularization scheme with time-varying coefficients, providing some justification for its effectiveness. We evaluate on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures, achieving modest improvements of 0.5-1.2% accuracy over standard dropout baselines, with particularly strong gains on smaller datasets. While the improvements are consistent, they remain incremental and the method requires careful tuning of the annealing schedule. We provide extensive ablations on the schedule design and demonstrate that simple linear schedules perform comparably to more complex cosine or exponential schedules. The method is easy to implement and adds minimal computational overhead, making it practical for broad adoption despite its limited novelty.",
    "id": 1154
  },
  {
    "title": "Gradient Descent with Non-Monotonic Adaptive Step Sizes: A Quasi-Momentum Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "We propose a variant of gradient descent that uses non-monotonic step size schedules guided by a quasi-momentum mechanism that combines per-coordinate step sizes with normalization by recent gradient norms. The method aims to accelerate convergence while maintaining stability, particularly for ill-conditioned problems. Our approach introduces a memory-based adaptation rule that increases step sizes when gradients are consistently aligned across iterations, and decreases them when alignment drops. We provide theoretical analysis showing convergence for convex functions with a sublinear rate matching standard gradient descent, and empirical results on standard benchmarks showing 5-15% improvement in convergence speed over Adam on ResNet training for CIFAR-100, though gains diminish on larger architectures. While our convergence guarantees do not improve upon existing bounds, the simplicity of implementation and modest empirical benefits may be of practical interest for small-to-medium scale applications. The method requires an additional hyperparameter compared to standard optimizers, and sensitivity analysis suggests performance is somewhat brittle to this setting.",
    "id": 1155
  },
  {
    "title": "Improving Transformer Efficiency through Selective Attention Recycling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "Transformer architectures achieve state-of-the-art performance across NLP tasks but suffer from quadratic complexity in sequence length. We propose Selective Attention Recycling (SAR), a simple technique that reuses attention weights from prior layers during inference to reduce computational cost. Our key insight is that attention patterns often stabilize in upper layers, making recomputation redundant. SAR caches attention weights from layer \u2113-1 when the cosine similarity between successive attention matrices exceeds a threshold \u03c4, avoiding the attention computation for layer \u2113. This approach requires no architectural changes and can be applied to pre-trained models. Experiments on BERT-Large and GPT-2 Medium show 21-34% reduction in inference FLOPs with \u22641.2% accuracy loss on GLUE and WikiText-103 benchmarks. Analysis reveals that SAR primarily affects long sequences (>512 tokens) and certain attention heads contribute more to degradation than others. While our method provides practical speedups, we acknowledge limitations: the threshold \u03c4 requires task-specific tuning, and accuracy degradation varies unpredictably across downstream tasks. Ablations suggest SAR works best for relatively stable data distributions and may harm performance on out-of-domain inputs. Our code will be released upon publication.",
    "id": 1156
  },
  {
    "title": "Memory-Efficient Layer-wise Pruning via Incremental Gradient Masking",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a memory-efficient method for neural network pruning that processes layers sequentially using incremental gradient information. Our approach modifies the standard magnitude-based pruning paradigm by accumulating gradient statistics across mini-batches before making pruning decisions, reducing memory overhead compared to global pruning methods. We evaluate our method on ResNet50 and Vision Transformer architectures across CIFAR-100 and ImageNet, achieving 75-80% of parameters pruned with minimal accuracy degradation (within 2.5% of baseline). While our results are competitive with existing methods, the sequential layer-wise approach introduces additional computational cost during training. Experiments show our method achieves comparable or slightly better performance than iterative magnitude pruning baselines, particularly for smaller networks, though gains diminish for very large models. Code is available at anonymous.url.",
    "id": 1157
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simple Baseline That (Sometimes) Works",
    "authors": [
      "Chen, L.",
      "Johnson, M.",
      "Singh, A."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks. We propose Gradient Surgery (GradSurg), a surprisingly simple method that identifies gradient conflicts through cosine similarity and surgically removes conflicting components while preserving shared information. Unlike recent approaches that require solving expensive optimization problems or maintaining task-specific parameters, GradSurg needs only a few lines of code and no hyperparameter tuning beyond standard optimizer settings. We evaluate on 224 multi-task configurations across computer vision and NLP benchmarks. GradSurg improves over single-task baselines in 67% of settings, performing comparably to more complex methods like PCGrad and GradNorm, though gains are modest (averaging 1.2% improvement). Surprisingly, we find GradSurg performs best when tasks are moderately related (cosine similarity 0.2-0.5) but can hurt performance when tasks are either very similar or very different. Analysis reveals the method primarily works by reducing gradient noise rather than resolving true conflicts. While our method provides a strong baseline and reveals interesting insights about multi-task optimization dynamics, we acknowledge limitations in extreme scaling settings.",
    "id": 1158
  },
  {
    "title": "LoRA-GD: Low-Rank Adaptation Meets Gradient Descent for Memory-Efficient Federated Learning",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Singh, A."
    ],
    "abstract": "Federated learning faces fundamental challenges in communication efficiency and device heterogeneity, particularly when adapting large pre-trained models. We propose LoRA-GD, a method that combines low-rank adaptation with compressed gradient descent to enable efficient federated fine-tuning. Our approach uses rank-2 LoRA adapters at each client, coupled with top-k gradient sparsification and periodic aggregation. We demonstrate that this combination achieves 95% of centralized fine-tuning accuracy on CIFAR-10 and GLUE benchmarks while reducing communication costs by 15-20x compared to standard federated approaches. However, we observe performance degradation on certain tasks requiring precise parameter updates, suggesting LoRA-GD may not generalize across all domains. Our analysis reveals a trade-off between compression ratio and convergence stability, with optimal performance at rank values between 2-4. While our method provides practical benefits for resource-constrained devices, theoretical guarantees remain limited. These findings indicate that LoRA-GD offers a pragmatic solution for federated adaptation of foundation models, though further work is needed to address its limitations on data-heterogeneous scenarios.",
    "id": 1159
  },
  {
    "title": "LoFormer: Low-Rank Transformer Blocks for Efficient Language Model Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Vasquez, J.",
      "M\u00fcller, K."
    ],
    "abstract": "We propose LoFormer, a simple modification to transformer architectures that reduces memory overhead during fine-tuning by enforcing low-rank structure on attention weights. While low-rank adaptation methods like LoRA have shown promise for parameter-efficient fine-tuning, they require modifying the training procedure and add inference latency. LoFormer instead introduces trainable low-rank projections within each attention head, allowing standard fine-tuning with reduced activations. Our method achieves 35-45% memory reduction compared to full fine-tuning on Llama-2 and T5 models while maintaining 93-98% of downstream task performance across GLUE and SuperGLUE benchmarks. However, we observe substantial performance degradation on reasoning tasks requiring complex attention patterns, suggesting the low-rank constraint may be too restrictive. Analysis reveals that LoFormer's benefits diminish as model size increases beyond 7B parameters. Compared to existing efficiency methods, LoFormer offers competitive memory savings but underperforms LoRA and AdaLoRA on several tasks. Our code and pre-trained checkpoints are available at [URL withheld for anonymous review].",
    "id": 1160
  },
  {
    "title": "Gradient Confusion: A Simple Regularization Technique for Improving Transformer Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Confusion, a lightweight regularization technique that adds controlled noise to gradient directions during transformer training to prevent overfitting. Our method stems from the observation that transformers exhibit high gradient coherence in later training stages, potentially limiting exploration of the loss landscape. By injecting calibrated directional noise into gradients based on their angular similarity to previous updates, we encourage more diverse parameter updates while maintaining convergence. We evaluate Gradient Confusion on standard NLP benchmarks including GLUE, SuperGLUE, and WikiText-103 across various model sizes (125M-7B parameters). Results show consistent but modest improvements: 1.2-2.3% accuracy gains on GLUE tasks and 0.8-1.5 perplexity improvements on language modeling, with minimal computational overhead (<3% additional training time). However, performance gains diminish on larger models (\u22653B parameters), and our theoretical analysis reveals the regularization effect is bounded regardless of noise magnitude. While Gradient Confusion provides a simple implementation requiring only three additional lines of code, its benefits appear task-dependent and may not justify the added complexity for practitioners. Code is available at anonymized-url.",
    "id": 1161
  },
  {
    "title": "Improving Transformer Generalization Through Layer-wise Learning Rate Temperature",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "Transformer models exhibit systematic brittleness when fine-tuned on small datasets, often overfitting to spurious correlations in early layers while under-utilizing deeper representations. We propose Layer-wise Learning Rate Temperature (L2RT), a simple modification to Adam optimizer that applies temperature-controlled learning rates to individual transformer layers based on their proximity to output. Our method uses a learnable temperature parameter \u03c4 to drive higher learning rates in deeper layers, theoretically motivated by the observation that gradient norms decay exponentially with depth. We demonstrate improvements over standard Adam on 6 out of 10 GLUE tasks when fine-tuning BERT-base from limited training data (1k-5k examples), achieving average gains of 2.3% over baseline. However, results show diminishing returns with larger datasets, and we observe negative transfer on tasks requiring broad attention patterns (e.g., WNLI). Computational overhead is minimal (2% increase in training time), and our implementation requires only 3 lines of code change to existing optimizers. While L2RT provides consistent benefits in low-data regimes, we acknowledge the technique's limited applicability to full-dataset fine-tuning and leave investigation of deeper theoretical connections to future work.",
    "id": 1162
  },
  {
    "title": "Gradient Surgery Revisited: Do We Really Need to Care About Interference?",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient interference, where conflicting gradients across tasks hinder optimization. Recent work on gradient surgery (e.g., PCGrad, GradDrop) attempts to resolve this by projecting or masking gradients. We revisit these methods through a controlled empirical study and find that their reported benefits are highly sensitive to hyperparameter choices and task similarity. Through experiments on 8 multi-task benchmarks, we show that simple multitask baselines with tuned learning rates and loss weightings often match or exceed the performance of sophisticated gradient surgery techniques. Our analysis suggests that gradient norm regularization emerges as a more effective intervention when interference is actually present. While our findings question the universality of gradient surgery approaches, we identify specific regimes\u2014particularly tasks with high gradient cosine similarity (>0.7)\u2014where these methods provide consistent improvements. These results underscore the importance of rigorous ablations in multitask optimization research and suggest future work should focus on adaptive regularization schemes rather than geometric gradient manipulations.",
    "id": 1163
  },
  {
    "title": "Improving Transformer Efficiency Through Layer-wise Dynamic Pruning Without Retraining",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models achieve remarkable performance but suffer from high computational costs during inference. We propose L-Drop, a layer-wise dynamic pruning method that eliminates entire attention heads and feed-forward dimensions on-the-fly based on input statistics. Our approach requires no fine-tuning by leveraging magnitude-based pruning criteria derived from pre-training distributions, combined with simple threshold learning during inference. We evaluate on standard NLP benchmarks including GLUE, SQuAD, and machine translation tasks. L-Drop reduces FLOPs by 15-30% with minimal accuracy degradation (\u22640.5%) compared to full models, outperforming static pruning baselines by 2-3% absolute on the GLUE benchmark. However, we find performance degrades significantly (>2%) on tasks requiring fine-grained reasoning. Our method introduces a modest 5% memory overhead for storing activation statistics, and we provide a PyTorch implementation achieving 1.2x speedup on A100 GPUs. While L-Drop demonstrates practical inference-time improvements for many applications, our analysis reveals theoretical limitations for tasks where precise attention patterns are crucial.",
    "id": 1164
  },
  {
    "title": "Gradient Descent with Momentum Adapts to Sparse Gradients in Neural Network Training",
    "authors": [
      "Chen, L.",
      "Kim, J.",
      "Rodriguez, M."
    ],
    "abstract": "We investigate the implicit bias of momentum-based optimizers in neural network training, focusing on their behavior under sparse gradient conditions. While theoretical understanding of neural network optimization remains limited, we provide empirical evidence that momentum methods exhibit selective updates to parameters with non-zero gradients. Our theoretical analysis characterizes this behavior for a two-layer linear network, showing that parameters with consistently non-zero gradients converge faster than those with sparse gradients. We conduct systematic experiments on standard vision and language tasks, demonstrating that this phenomenon correlates with improved generalization in networks with structured sparsity patterns. However, our theoretical results hold only for simplified settings and do not extend to non-linear networks. Despite these limitations, our findings suggest that momentum's implicit regularization properties deserve further attention in understanding neural network training dynamics. Code is available at anony-mized-url.",
    "id": 1165
  },
  {
    "title": "Revisiting Low-Rank Adaptation with Gradient Sparsity: A Compression Perspective",
    "authors": [
      "Chen, K.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its memory efficiency during training remains underexplored. We propose SparseLoRA, which combines low-rank training with structured gradient sparsity to reduce memory footprint during backpropagation. By analyzing the gradient flow through LoRA modules, we identify redundancy patterns that allow aggressive gradient compression without catastrophic forgetting. Our method introduces learnable masks that sparsify gradients during training while maintaining the low-rank constraint during inference. Experiments on BERT-base and RoBERTa show 2.1\u00d7 memory reduction during fine-tuning on GLUE tasks with <1% performance degradation compared to standard LoRA. However, advantages diminish on larger models like GPT-2 medium (1.4B), where gradient compression introduces optimization instability. Ablations reveal that SparseLoRA's benefits are most pronounced in memory-constrained scenarios, though the approach requires careful hyperparameter tuning to prevent divergence. While not universally applicable, SparseLoRA provides a practical trade-off when training memory is severely limited, complementing rather than replacing standard LoRA in general use cases.",
    "id": 1166
  },
  {
    "title": "Momentum Residual Connections for Learning with Noisy Labels",
    "authors": [
      "Chen, L.",
      "Garcia, A.",
      "Thompson, K."
    ],
    "abstract": "Deep neural networks achieve excellent performance when trained on large datasets, but their effectiveness degrades when labels are noisy. While numerous robust loss functions and training procedures have been proposed, we observe that standard residual connections amplify the memorization of corrupted labels. We introduce Momentum Residual Connection (MoRC), a lightweight modification to ResNet-style architectures that reduces noisy label sensitivity without altering the loss function or requiring additional hyperparameters. MoRC uses an exponentially weighted average of previous layer representations, effectively smoothing the gradient flow from early to deeper layers. On CIFAR-10 with 40% symmetric label noise, networks with MoRC achieve 5.2% higher test accuracy compared to standard ResNet-34, while maintaining 98.1% accuracy on clean data. We demonstrate that MoRC's benefit diminishes under extreme noise (>60%) and show empirically that the technique provides minimal improvement when combined with existing robust loss functions. Our analysis connects MoRC to implicit regularization effects and demonstrates its limitations for non-image data. While MoRC offers a simple architectural adjustment for mild label noise, its incremental nature and limited scope suggest it should complement rather than replace existing robust training approaches.",
    "id": 1167
  },
  {
    "title": "Improving Transformer Training Efficiency Through Gradient-Sensitive Token Dropping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "While transformers achieve state-of-the-art results across NLP tasks, their quadratic attention complexity limits scalability. Prior work on efficient transformers either modifies the attention mechanism or uses static token importance heuristics, which can discard task-relevant information. We propose GSTD (Gradient-Sensitive Token Dropping), a training-time method that selectively drops tokens based on their gradient norms with respect to the loss. Our approach uses a lightweight approximation of per-token influence scores to retain tokens with higher training signal while reducing computational cost. Unlike static pruning methods, GSTD adapts token selection dynamically during training. On GLUE and WMT benchmarks, we achieve 22-34% speedup in training time with <1.5% performance degradation compared to full transformer baselines. However, we observe that gains are task-dependent and diminish on tasks requiring long-range dependencies. Our analysis reveals that gradients poorly approximate token importance for syntactic and reasoning-heavy tasks, suggesting fundamental limitations of the approach. Code will be released upon acceptance.",
    "id": 1168
  },
  {
    "title": "Revisiting Gradient Clipping in Transformer Training: A Frequency Domain Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Gradient clipping remains a standard practice for training transformers, yet its impact on optimization dynamics remains poorly understood. We investigate gradient clipping through the lens of frequency domain analysis, showing that clipping primarily affects high-frequency components of the gradient signal. Using a combination of synthetic experiments and ablations on standard transformer architectures, we demonstrate that careful tuning of clipping thresholds can improve training stability without harming final performance. Our theoretical analysis characterizes the clipping operator as a non-linear low-pass filter, providing convergence bounds for clipped gradient descent under standard assumptions. While our experiments on WMT14 En-De and ImageNet classification show modest improvements (0.3-0.7 BLEU / 0.2-0.4% top-1 accuracy) over well-tuned baselines, we observe more substantial benefits in low-resource settings. The empirical gains, however, are sensitive to hyperparameter choices and training procedures, limiting the practical impact of our findings. We release code and pre-trained models to facilitate reproducibility.",
    "id": 1169
  },
  {
    "title": "Gradient Surgery Doesn't Always Help: An Empirical Analysis of Multi-Task Optimization in Deep Networks",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Thompson, S."
    ],
    "abstract": "Multi-task learning with deep networks often suffers from conflicting gradients between tasks. Recent gradient surgery methods like PCGrad and GradNorm have shown promise, but their effectiveness across diverse architectures and task combinations remains unclear. We conduct the largest empirical study to date evaluating five gradient manipulation techniques across 120 task/architecture combinations. While these methods provide consistent improvements in 35% of cases, we find they can significantly degrade performance in 28% of configurations\u2014particularly when tasks have similar gradient directions but different learning dynamics. Through controlled experiments, we demonstrate that the benefits of gradient surgery are largely explained by improved task weighting rather than conflict resolution. Our analysis reveals that simple baseline strategies using task uncertainty weighting achieve comparable or superior performance in 62% of cases. These findings suggest that recent excitement around gradient surgery may be partly misplaced, and that more fundamental research is needed to understand when and why multi-task optimization fails.",
    "id": 1170
  },
  {
    "title": "Adaptive Gradient Clipping Without Learning Rate Tuning: A Sufficient Condition Based Approach",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is widely used in training deep neural networks, particularly for stabilizing transformer architectures, but requires careful tuning of clipping thresholds and learning rates for each model and dataset. We propose AutoClip, a simple modification to standard gradient clipping that automatically adjusts clipping thresholds based on a sufficient condition derived from L-smoothness assumptions. Our method introduces a running estimate of the local Lipschitz constant to set clipping thresholds as a function of the current gradient norm and learning rate, eliminating the need for hyperparameter tuning. Experiments on image classification with ResNets and ViTs show 2-5% improvement in final accuracy over tuned baselines on CIFAR-10/100, but gains diminish on larger datasets like ImageNet (0.3-0.7%). While our theoretical analysis provides convergence guarantees for convex objectives, we observe empirical instability in highly non-convex settings. Code reproduces main results but requires careful hyperparameter selection for transformer training. Our approach offers a practical alternative to extensive clipping threshold search at moderate computational overhead (<5%), though benefits may be task-dependent.",
    "id": 1171
  },
  {
    "title": "Adaptive Gradient Clipping with Learnable Per-Layer Thresholds for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is essential for stable transformer training, yet existing methods use fixed thresholds that cannot adapt to the varying gradient distributions across layers and training stages. We propose Adaptive Gradient Clipping with Learnable Thresholds (AGCLT), which formulates layer-wise clipping thresholds as trainable parameters optimized through meta-gradients computed on a small validation set. Our method adds minimal computational overhead (2-3% training time) and can be implemented in 20 lines of PyTorch code. Experiments on language modeling with GPT-2 architectures (125M-1.5B parameters) show 5-8% perplexity improvements over standard gradient clipping on WikiText-103 and OpenWebText2. However, results on vision transformers and smaller models (<100M parameters) show more modest gains of 1-3%, and we observe instability with very aggressive learning rates. While the approach demonstrates the potential for adaptive clipping, our theoretical analysis remains limited to convex approximations, and the validation set requirement introduces a small hyperparameter dependency that may limit broader applicability.",
    "id": 1172
  },
  {
    "title": "ReLUated Transformers: Provable Approximation Benefits of Depth Without Smooth Activations",
    "authors": [
      "Chen, L.",
      "Vasquez, J.",
      "Kim, S."
    ],
    "abstract": "We study whether the universal approximation capabilities of transformers can be achieved using only ReLU activations, motivated by empirical observations that ReLU variants often perform comparably to GeLU/Swish in large-scale language models. We prove that multi-layer transformers with ReLU activations require \u03a9(d\u00b3) parameters to approximate arbitrary continuous functions on [0,1]^d, improving previous bounds that suggested exponential dependence on dimension. Our key insight introduces a piecewise-linear attention mechanism that maintains Lipschitz constants across layers. Experiments on synthetic function approximation tasks show ReLU transformers achieve 15-30% lower MSE than vanilla ReLU networks at equivalent parameter counts, though we observe diminishing returns beyond 12 layers. While our theoretical framework holds for fixed input dimensions, we demonstrate practical benefits on MNIST sequential classification (87.3% accuracy vs 83.1% baseline) using positional encodings adapted for piecewise-linear activations. However, attempts to scale to larger datasets revealed instabilities that our current initialization scheme cannot fully address. Our work suggests ReLU transformers exhibit non-trivial approximation power, though characterizing their exact capabilities in high-dimensional regimes remains open.",
    "id": 1173
  },
  {
    "title": "A Little Bit of Attention Goes a Long Way: Low-Rank Token Mixing for Efficient Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Vision Transformers (ViTs) have achieved remarkable performance on various computer vision tasks, but their quadratic complexity in token interaction computation remains a significant bottleneck, especially for high-resolution images. We propose TokenMix, a simple method that approximates the full attention matrix with products of low-rank matrices and learned mixing patterns. Our approach reduces the O(n\u00b2) complexity to O(nk) where k \u226a n, while maintaining competitive accuracy on ImageNet-1K. Through extensive experiments on classification, detection, and segmentation tasks, we show that TokenMix achieves 2.1\u00d7 speedup over standard self-attention with only 0.8% drop in top-1 accuracy. While our method does not outperform the latest state-of-the-art efficient attention mechanisms on all benchmarks, the simplicity of our design and ease of integration into existing architectures make TokenMix a practical alternative for resource-constrained applications. Code and pretrained models will be made available.",
    "id": 1174
  },
  {
    "title": "Gradient Perturbation for Robust Fine-tuning: A Simple Alternative to Full Model Retraining",
    "authors": [
      "Liu, M.",
      "Thompson, K.",
      "Chen, J."
    ],
    "abstract": "Fine-tuning pre-trained models has become the dominant paradigm for downstream applications, but current approaches struggle with distribution shift between pre-training and target domains. While full model retraining can improve robustness, it negates the computational benefits of pre-training and risks catastrophic forgetting. We propose Gradient Perturbation Fine-tuning (GPF), a lightweight method that applies targeted noise injections to the gradient flow during fine-tuning. Our approach requires only a single hyperparameter (noise scale \u03b1) and adds <5% computational overhead compared to standard fine-tuning. On 8 domain adaptation benchmarks spanning vision and NLP tasks, GPF achieves 3.2% average improvement over baselines on out-of-distribution performance, while maintaining in-distribution accuracy within 1% of standard fine-tuning. We provide theoretical analysis showing GPF implicitly maximizes a lower bound on worst-group performance under certain assumptions. However, we find the method is sensitive to the choice of \u03b1 across tasks and lacks strong guarantees under adversarial distribution shifts. While our empirical results are encouraging, we acknowledge the need for more comprehensive evaluation and better theoretical grounding, particularly regarding when and why GPF succeeds or fails.",
    "id": 1175
  },
  {
    "title": "Improving Generalization in Meta-Learning via Adaptive Second-Order Optimization of Initialization",
    "authors": [
      "Chen, L.",
      "Okafor, K.",
      "Rodriguez, J."
    ],
    "abstract": "We propose an adaptive second-order optimization method for meta-learning that modifies Model-Agnostic Meta-Learning (MAML) with an online estimate of the Hessian matrix. Our approach computes a low-rank approximation of the Hessian during adaptation and uses this to adjust the meta-gradient direction. This avoids the computational burden of full second-order optimization while potentially capturing curvature information missed by standard MAML. We evaluated our method on standard benchmarks including mini-ImageNet, CIFAR-FS, and Meta-Dataset. Results show modest improvements (1-2% accuracy) over MAML baselines on most few-shot tasks, with particularly strong performance on tasks with low data diversity. However, our method fails to outperform recent gradient-free alternatives like ProtoNet on several benchmarks, and we find the computational overhead grows quadratically with task complexity. Theoretical analysis demonstrates convergence under standard smoothness assumptions, though our bounds are loose compared to recent work. Code is available to ensure reproducibility. While the contribution is incremental rather than groundbreaking, our work provides a computationally feasible alternative to full second-order meta-learning that may be useful for practitioners with limited computational budgets.",
    "id": 1176
  },
  {
    "title": "Revisiting Knowledge Distillation with Information-Theoretic Routing for Efficient Model Compression",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Knowledge distillation has become a standard approach for compressing large neural networks, yet most methods treat all samples equally during training. We propose Information-Theoretic Routing Distillation (ITRD), which uses an auxiliary network to dynamically route samples based on their estimated information gain. Our method computes sample-wise mutual information between teacher and student predictions, prioritizing high-uncertainty examples during distillation. On CIFAR-100 and ImageNet, ITRD achieves competitive compression ratios (10-50\u00d7) with modest accuracy improvements over baseline KD (0.5-1.2% absolute). However, computational overhead during training increases by 30-40%, and performance gains diminish when teacher-student capacity gaps exceed two orders of magnitude. While our theoretical analysis provides novel insights into sample selection for distillation, we recognize the approach adds complexity without addressing fundamental limitations of knowledge transfer. Experiments across multiple architectures (CNNs, Transformers) show consistent but incremental improvements. Code and checkpoints are available at [anonymized-url].",
    "id": 1177
  },
  {
    "title": "Revisiting Curriculum Learning Through the Lens of Implicit Bias in Deep Networks",
    "authors": [
      "Chen, Y.",
      "Garcia, J.",
      "Liu, S."
    ],
    "abstract": "Curriculum learning has shown promise in various domains, yet its theoretical foundations remain limited. We investigate whether the empirically observed benefits of curriculum strategies can be explained through the lens of implicit bias in overparameterized neural networks. Specifically, we propose a modified gradient descent framework that incorporates curriculum scheduling as a form of path-dependent regularization. Our analysis reveals that certain curriculum strategies can be viewed as altering the implicit bias towards solutions that generalize better on tasks with intrinsic hierarchical structure. Through experiments on synthetic datasets with controlled complexity, we demonstrate that our curriculum modifications achieve marginal improvements over random ordering (2.3% average accuracy gain). While our theoretical analysis is restricted to linear networks and requires strong assumptions on data separability, empirical results on CIFAR-10 and subset of ImageNet show consistent but modest improvements (1.1-1.8%). Our work suggests that the benefits of curriculum learning may be more nuanced than previously reported, and provides a first step towards formalizing when such strategies are beneficial.",
    "id": 1178
  },
  {
    "title": "LoRA-SGD: Memory-Efficient Fine-Tuning via Low-Rank Gradient Compression",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Subramanian, P."
    ],
    "abstract": "Fine-tuning large language models remains computationally intensive, particularly for memory-constrained practitioners. While parameter-efficient fine-tuning methods like LoRA reduce memory for storing adapted weights, gradient computation during backpropagation still requires full activation storage. We propose LoRA-SGD, which compresses gradients into low-rank updates before backpropagation begins. Specifically, we project gradients into the same low-rank subspace used by LoRA adapters, reducing peak memory by up to 60% on 7B parameter models. Our method introduces a modified optimizer that operates directly on the compressed gradient space, with theoretical analysis showing convergence rates within a factor of two of standard SGD under reasonable assumptions. Experiments on GLUE and SuperGLUE benchmarks demonstrate comparable performance to LoRA with 2-3x memory reduction, though we observe some degradation (>2% accuracy drop) on tasks requiring precise few-shot adaptation. While our approach provides practical memory savings for resource-limited deployment, we acknowledge limitations in handling sparse gradient patterns and leave exploration of adaptive rank adjustment to future work.",
    "id": 1179
  },
  {
    "title": "Gradient Noise Rejection Improves Generalization in Stochastic Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We investigate gradient noise characteristics in large-scale neural network training and propose a simple technique to improve generalization by selectively rejecting gradient updates based on their alignment with the parameter manifold. Our method computes the cosine similarity between consecutive mini-batch gradients and only applies updates when this similarity exceeds a learned threshold. We validate our approach on CIFAR-10, CIFAR-100, and ImageNet classification tasks, achieving modest improvements over baselines (0.8-1.3% accuracy gains) while reducing training variance across seeds. Theoretical analysis in a simplified setting suggests this resembles preconditioned SGD with adaptive learning rates. Experiments show the method is particularly effective for networks of moderate size (5-50M parameters) but provides diminishing returns for extremely large models. Our implementation adds minimal computational overhead (<2% increase in training time) and requires tuning of only two hyperparameters through small-scale ablation studies. While this work demonstrates practical benefits, we acknowledge theoretical gaps in understanding why noise rejection helps generalization, and questions remain about applicability to different architectures and training regimes.",
    "id": 1180
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Dynamic Module Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but applies uniform rank constraints across all layers regardless of their importance. We propose LoRA-Drop, a simple yet effective approach that dynamically selects which transformer modules to adapt based on a lightweight importance scoring mechanism. During a brief warm-up phase, we estimate per-module sensitivity using gradient norms, then apply LoRA only to the top-k% most sensitive modules while freezing others. On the GLUE benchmark with RoBERTa-large, LoRA-Drop reduces trainable parameters by 40% compared to standard LoRA while maintaining 97.3% of its performance (86.1 vs 88.4 average GLUE score). While our method shows consistent improvements over uniform LoRA across model scales (up to 3B parameters), the gains diminish on larger models and certain tasks like reading comprehension. Theoretical analysis reveals our importance scores correlate moderately (r=0.68) with actual parameter sensitivity, suggesting room for improvement in the scoring function. Code will be released upon acceptance.",
    "id": 1181
  },
  {
    "title": "Gradient Noise Re-Scaling: A Lightweight Training Strategy for Improving Generalization in Deep Networks",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Gradient Noise Re-Scaling (GNR), a simple modification to standard SGD that selectively amplifies gradient noise during training. While previous work has shown that carefully injected noise can improve generalization, these methods often require costly hyperparameter tuning or complex noise schedules. GNR instead re-scales the gradient noise using a running estimate of the gradient variance, requiring only an additional hyperparameter \u03b1 that typically falls between 0.1-0.5. Our theoretical analysis shows that GNR approximately corresponds to an implicit regularizer encouraging flatter minima under certain assumptions about the loss landscape. We evaluate GNR on CIFAR-10/100 and ImageNet classification tasks using ResNet-18, ResNet-50, and Vision Transformers. Results show consistent but modest improvements (0.3-0.7% accuracy gains) over vanilla SGD with momentum, matching or slightly outperforming more sophisticated regularization techniques like SAM and SharpDrop. Ablations reveal that benefits diminish with stronger baseline regularization (\u22640.2% gains with label smoothing + RandAugment). While the improvements are reliable across architectures, they remain incremental and typically require 2-3x longer training. Code is available at anonymous-url.",
    "id": 1182
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Cheap Approximation that Usually Works",
    "authors": [
      "Chen, L.",
      "Ramanathan, V.",
      "Kumar, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradDrop show promise, they require expensive gradient computations or complex projection steps. We propose ApproxGrad, a simple heuristic that applies random binary masks to task gradients based on their magnitudes, avoiding costly cosine similarity calculations. Our method achieves comparable accuracy to PCGrad on standard benchmarks (CIFAR-100, NYUv2) while reducing computational overhead by 40-60%. However, we observe significant performance degradation when task gradients are highly correlated (r > 0.8), and our theoretical analysis reveals no convergence guarantees under non-convex objectives. Experiments on three additional datasets show mixed results: ApproxGrad outperforms standard multi-task training in 7/9 cases but underperforms PCGrad in 5/9 cases. Code is available at github.com/chenl/approxgrad.",
    "id": 1183
  },
  {
    "title": "Self-Adjusting Dropout Rates via Meta-Learning for Regularization in Small Data Regimes",
    "authors": [
      "Chen, L.",
      "Muller, S.",
      "Joshi, P."
    ],
    "abstract": "We propose MetaDropout2, a method that adaptively tunes dropout rates through meta-learning for low-data scenarios. While dropout is widely used for regularization, fixed rates across layers may be suboptimal when training data is scarce. Our approach learns layer-wise dropout rates by treating them as meta-parameters optimized for validation performance. We extend MAML to jointly update model weights and dropout probabilities, introducing a differentiable approximation to Bernoulli sampling that enables gradient flow. On CIFAR-10 with 10% training data, MetaDropout2 improves accuracy by 2.3% over tuned fixed dropout and 1.1% over existing adaptive methods like Concrete Dropout. However, gains diminish with larger datasets\u2014on full CIFAR-10 and ImageNet, performance matches but does not exceed tuned baselines. Our theoretical analysis shows the method effectively controls Rademacher complexity in low-sample regimes. Code and experiments are available, though training requires 2-3x longer due to meta-gradient computation. The approach may be useful for transfer learning or medical imaging applications where data scarcity persists.",
    "id": 1184
  },
  {
    "title": "LoRA-VR: Variance-Reduced Low-Rank Adaptation via Importance Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a popular method for efficiently fine-tuning large language models, but suffers from gradient variance that increases with the rank of the low-rank matrices. We propose LoRA-VR, a variance-reduced extension that incorporates importance sampling to stabilize training. Our method adaptively reweights gradient updates based on the spectral norm of low-rank components, theoretically reducing variance by a factor of O(\u221ar) where r is the adaptation rank. Experiments on GLUE and Super-NaturalInstructions show 2-4% improvements over standard LoRA with minimal overhead, achieving 94.2 average GLUE score compared to 92.1 for LoRA when fine-tuning LLaMA-7B. However, gains diminish for larger models (\u226513B parameters) and tasks requiring extensive reasoning. While our theoretical analysis requires strong assumptions about gradient Lipschitz continuity that may not hold in practice, LoRA-VR provides a computationally lightweight improvement to parameter-efficient fine-tuning that could benefit practitioners with limited resources.",
    "id": 1185
  },
  {
    "title": "Adaptive Gradient Clipping with Log-Smoothness Estimation for Non-Convex Optimization",
    "authors": [
      "Kumar, A.",
      "Zhou, L.",
      "Hassan, M."
    ],
    "abstract": "We propose Adaptive Log-Smoothness Clipping (ALSC), a gradient-based optimization method that automatically adjusts clipping thresholds based on local smoothness estimates. While gradient clipping is widely used to stabilize training of neural networks, current approaches use fixed thresholds or simple heuristics that may not adapt to changing landscape geometry. ALSC estimates local smoothness via a running average of gradient differences in logarithmic space, which we show correlates strongly with optimal clipping thresholds in simple convex settings. Our method requires minimal additional computational overhead compared to standard gradient descent with clipping. We evaluate ALSC on transformer language modeling and ResNet image classification tasks, demonstrating modest improvements in training stability and final performance over baseline methods in 6 out of 8 experimental settings. However, gains are inconsistent across architectures and datasets, with particularly mixed results on smaller models. We provide theoretical analysis showing ALSC achieves convergence rates comparable to standard gradient descent with clipping under strong assumptions about gradient noise, though these assumptions may not hold in practice. Our code is available at [anonymous link].",
    "id": 1186
  },
  {
    "title": "Momentum-Scheduled Gradient Descent: A Simple Adaptation for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Momentum-Scheduled Gradient Descent (MS-GD), a lightweight modification to standard momentum-based optimization that adaptively adjusts the momentum parameter during training. Unlike adaptive methods such as Adam or RMSProp, MS-GD requires no additional gradient statistics or memory overhead beyond standard momentum SGD. Our key insight is that scheduling momentum according to the relative change in gradient norms can stabilize training in non-convex landscapes while maintaining the computational efficiency of vanilla momentum. We provide theoretical analysis showing MS-GD achieves O(1/\u221aT) convergence for smooth non-convex functions and demonstrate empirically that it matches or exceeds Adam's performance on standard vision (CIFAR-10/100) and language (IWSLT14) tasks with comparable tuning budgets. While not uniformly better than existing approaches, MS-GD offers a practical alternative requiring no additional hyperparameters beyond learning rate and momentum. Code will be made available upon acceptance.",
    "id": 1187
  },
  {
    "title": "Gradient Descent with Lookahead Meets Random Walk: A Hybrid Approach for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Jiang, K.",
      "Thompson, S."
    ],
    "abstract": "We propose LookAhead Random Walk (LARW), a hybrid optimization method that interpolates between gradient-based and random walk approaches for non-convex problems. Motivated by the observation that gradient descent can get stuck in poor local minima while pure random walks explore inefficiently, LARW alternates between gradient steps and controlled random perturbations. Our method maintains two copies of parameters: a fast weight that follows gradient directions and a slow weight that performs occasional random jumps based on a Metropolis-Hastings acceptance criterion. We theoretically analyze convergence to approximate stationary points for functions satisfying the Kurdyka-Lojasiewicz inequality, showing O(1/\u221aT) convergence rate when the random walk component decays appropriately. Experiments on CIFAR-10/100 and ImageNet show modest improvements (1-2% accuracy) over SGD and Adam on ResNet architectures, particularly when fine-tuning pre-trained models. However, the computational overhead (1.4x training time) and sensitivity to the temperature hyperparameter limit practical impact. While our theoretical framework is promising, the empirical gains remain marginal compared to recent adaptive methods. Our results suggest that combining exploration and exploitation in optimizer design deserves further investigation, though more sophisticated strategies may be needed for substantial improvements.",
    "id": 1188
  },
  {
    "title": "Gradient Surgery Revisited: A Principled Approach to Multi-Task Learning Trade-offs",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to performance degradation. While existing gradient surgery methods like PCGrad and GradNorm have shown empirical success, they lack theoretical justification and fail to provide guarantees on solution quality. We propose Adaptive Gradient Conflict Resolution (AGCR), a principled framework that formulates multi-task optimization as a constrained optimization problem with provable convergence guarantees. AGCR dynamically adjusts gradient directions by solving a small quadratic program at each step, ensuring Pareto-stationarity while maintaining computational efficiency. Our method achieves comparable or marginally better performance than existing heuristics on standard benchmarks (CIFAR-100, NYUv2, Omniglot), with 5-7% relative improvement over PCGrad in the full-data regime. However, we observe that AGCR's benefits diminish in low-resource settings and high-task-count scenarios, suggesting fundamental limitations of constrained optimization approaches. While our theoretical analysis provides the first convergence bounds for gradient surgery methods, the practical improvements remain modest. We discuss the gap between theoretical guarantees and empirical performance, highlighting directions for better aligning theory and practice in multi-task learning.",
    "id": 1189
  },
  {
    "title": "Gradient Descent with Lookahead via Second-Order Taylor Approximations: A Curvature-Aware Optimization Framework",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Okafor, J."
    ],
    "abstract": "We propose TaylorLA, a novel optimizer that augments gradient descent with second-order information through efficient Taylor approximations. Motivated by the observation that many neural network loss landscapes exhibit low effective rank Hessians, our method computes curvature-aware step directions using only diagonal Hessian estimates, making it computationally tractable for large models. Unlike quasi-Newton methods, TaylorLA maintains the memory efficiency of first-order optimizers while incorporating local curvature information through a lookahead mechanism guided by second-order Taylor expansions. We evaluate TaylorLA on standard image classification benchmarks (CIFAR-10/100, ImageNet) and language modeling tasks (WikiText-103), achieving 2-3% improvement over AdamW in final accuracy with comparable training time. While our results show consistent gains over baselines, the improvements are modest and task-dependent. Analysis reveals TaylorLA yields the most benefits on architectures with residual connections, where local quadratic approximations remain more reliable. The method adds minimal computational overhead (15% increase in step time) and requires only one additional hyperparameter. Our code is available at anonymous-link.",
    "id": 1190
  },
  {
    "title": "Layer-Wise Learning Rate Scheduling via Jacobian Spectral Analysis in Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose LayerSpec, a method for automatically tuning learning rates in deep neural networks by analyzing the spectral norm of layer-wise Jacobians during training. Our approach extends adaptive gradient methods by incorporating second-order information computed efficiently through power iteration on mini-batch gradients. Unlike existing layer-wise optimization schemes, LayerSpec does not require additional forward passes or significant memory overhead. We evaluate our method on ResNet-50 and Vision Transformer training on ImageNet, and Transformer language modeling on WikiText-103. Results show 5-12% faster convergence compared to standard AdamW baselines, though final accuracy improvements are modest (0.2-0.5%). Analysis reveals LayerSpec particularly benefits deeper networks (>50 layers), suggesting its utility scales with model complexity. While our theoretical analysis is limited to strongly convex cases, we provide empirical evidence that LayerSpec stabilizes training dynamics by maintaining more consistent gradient norms across layers. Implementation requires only 15 lines of PyTorch code and integrates seamlessly with existing training pipelines.",
    "id": 1191
  },
  {
    "title": "Gradient Surgery Without Anesthesia: When Adaptive Optimizers Fail to Forgive",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We investigate an overlooked failure mode in adaptive optimizers when training deep neural networks with heterogeneous data. While popular methods like Adam and RMSprop perform gradient scaling based on second moment estimates, we find that these estimates can become pathologically skewed when mini-batches contain conflicting gradient directions. We propose GradClip++, a simple modification that adaptively clips gradients before they enter the optimizer's momentum buffer. Our method requires only two additional hyperparameters and minimal computational overhead. Experiments on CIFAR-100, ImageNet-subset, and GLUE benchmarks show consistent improvements over vanilla AdamW (0.5-2.3% accuracy gains), particularly in low-data regimes. However, the improvements diminish at larger scales, and we observe negative transfer in some multitask settings. Theoretical analysis reveals that GradClip++ can be viewed as a form of trust region method with data-dependent regularization, though our bounds are loose and make strong assumptions about gradient correlation. While not a universal solution, our findings suggest that gradient preprocessing deserves more attention in optimizer design, especially for practitioners working with noisy or adversarial datasets.",
    "id": 1192
  },
  {
    "title": "LoRA-Opt: Adaptive Low-Rank Adaptation with Gradient-Based Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a prominent parameter-efficient fine-tuning method for large language models, but determining the optimal rank remains a manual process. We propose LoRA-Opt, an automatic rank selection method that adaptively adjusts the LoRA rank during training based on gradient statistics. Our approach uses a differentiable approximation to the rank selection problem, allowing joint optimization of both the low-rank matrices and their ranks. While LoRA-Opt achieves comparable performance to manually tuned LoRA on standard benchmarks like GLUE and SuperGLUE with 15-30% fewer parameters, we find the rank selection decisions are highly dependent on initialization and learning rate schedules. The method shows consistent improvements over static rank choices across 7B and 13B parameter models, but gains diminish on smaller datasets and tasks requiring extensive reasoning. Our theoretical analysis reveals that the gradient-based rank selection favors rank increases during early training phases, potentially leading to over-parameterization. Experimental results on 8 diverse NLP tasks suggest LoRA-Opt provides a practical alternative to hyperparameter tuning, though careful initialization remains critical for optimal performance. Code is available at https://github.com/LoRA-Opt/lora-opt.",
    "id": 1193
  },
  {
    "title": "Gradient Compression with Learned Error Feedback via Neural Proxies",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We address communication bottlenecks in distributed training through a novel compression technique that learns to reconstruct gradient errors using neural proxies. While existing top-k and quantization-based methods discard information, our approach trains a lightweight network to predict and correct compression artifacts. The method alternates between (i) compressing gradients using sparsification and (ii) learning a neural error correction model that maps from intermediate activations to gradient residuals. We evaluate on CIFAR-10, ImageNet, and GLUE benchmarks across 8 GPUs, achieving 70-85% compression rates with 0.5-2.3% accuracy degradation compared to full-precision training. While we demonstrate consistent improvements over baseline compressors, particularly at high compression ratios, the technique introduces training overhead (15-30% slower per iteration) and exhibits diminishing returns on smaller models. Our results suggest learned error feedback can partially recover from aggressive compression, though the approach may be most practical for large-scale deployments where communication costs dominate.",
    "id": 1194
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simple Trick with Provable Guarantees",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients where optimizing for one task hurts performance on others. We propose Gradient Surgery (GS), a lightweight method that projects conflicting gradients onto each other before applying updates. While similar in spirit to existing approaches like PCGrad and GradNorm, GS requires no hyperparameter tuning and comes with convergence guarantees under standard smoothness assumptions. Our theoretical analysis shows GS converges to an \u03f5-stationary point in O(1/\u03f5\u00b2) iterations for L-smooth objectives, matching standard SGD rates despite the projection step. Empirically, we evaluate GS on eight multi-task benchmarks spanning computer vision and NLP. GS achieves modest improvements over baselines (+0.8% average accuracy, +1.2% F1), though results vary significantly across tasks (ranging from -2.1% to +3.4%). Ablation studies reveal that performance gains primarily emerge when task gradients exhibit high cosine similarity (>0.3), limiting applicability. While our convergence analysis is novel and the method is implementable in 8 lines of PyTorch, the practical benefits appear scenario-dependent. This work provides theoretical backing for a simple heuristic, though the empirical impact may not justify architectural changes in production systems.",
    "id": 1195
  },
  {
    "title": "Revisiting Weight Averaging with Cyclic Learning Rates for Faster Transformer Training",
    "authors": [
      "Kim, H.",
      "Rodriguez, L.",
      "Chen, J.",
      "Singh, N."
    ],
    "abstract": "We empirically investigate whether cyclic learning rate schedules can improve the efficacy of weight averaging in transformer training. Building on recent observations that flat minima correlate with better generalization, we propose CYCLICSWA, a simple variant of stochastic weight averaging that leverages periodic learning rate annealing to explore the loss landscape. Our method applies triangular cyclic schedules to drift between sharp and flat regions, collecting averaged weights during each annealing phase. Experiments on GLUE and SuperGLUE benchmarks show consistent but modest improvements (0.3-0.8 F1 points) over vanilla SWA across five transformer architectures. While CYCLICSWA achieves convergence 10-15% faster than standard training, the benefits diminish with longer training budgets. Analysis reveals our gains primarily stem from improved early training dynamics rather than fundamentally better minima. The approach introduces negligible computational overhead and generalizes across optimizer choices, though larger models (\u2265 770M parameters) show diminishing returns. Our results suggest cyclic weight averaging as a practical training recipe when computational budget is constrained, though the improvements are incremental rather than transformative.",
    "id": 1196
  },
  {
    "title": "Improving Transformer Training with Layer-Wise Gradient Surgery",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Transformer models often suffer from unstable training dynamics, particularly when scaled to deeper architectures. We propose Layer-Wise Gradient Surgery (LWGS), a technique that selectively modifies gradient flow patterns during backpropagation to address inter-layer interference. Our method computes gradient statistics across layers and applies calibrated rescaling operations based on the cosine similarity between adjacent layer gradients. Experiments on Wikitext-103 and CIFAR-10 demonstrate 12-18% faster convergence compared to standard AdamW training, with comparable final performance. While the technique shows consistent improvements on language modeling tasks, benefits are less pronounced on vision transformers and machine translation benchmarks. Our analysis reveals that LWGS primarily stabilizes early training phases by reducing gradient norm collapse in deeper layers, though we observe diminishing returns beyond 24 layers. The method introduces minimal computational overhead (<2% increase in training time) and can be implemented with 20 lines of PyTorch code. However, we acknowledge that our theoretical analysis remains incomplete, as we cannot fully characterize when and why certain architectures benefit more than others. Code and pre-trained models will be released upon acceptance.",
    "id": 1197
  },
  {
    "title": "Improving Transformer Calibration with Temperature-Scaled Attention Dropout",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Wang, J.",
      "Liu, Y."
    ],
    "abstract": "While transformers achieve state-of-the-art accuracy across NLP tasks, they often exhibit poor calibration \u2014 their predicted probabilities do not align with empirical accuracy. We propose TA-Dropout, a simple technique that applies learned temperature scaling to attention dropout rates during training. Our method modifies standard multi-head attention by introducing learnable temperature parameters that control dropout probabilities for each attention head. On GLUE tasks, TA-Dropout improves Expected Calibration Error by 12.3% over baselines without accuracy loss. However, improvements are inconsistent \u2014 we observe gains primarily on classification tasks (>10% improvement) but marginal effects on regression tasks. Theoretical analysis shows TA-Dropout performs implicit posterior sharpening, though the connection to calibration remains heuristic. Experiments on three vision transformer variants demonstrate partial transferability, with 7.8% average calibration improvement on CIFAR-100. Our method requires minimal computational overhead but introduces 0.003% additional parameters. While TA-Dropout provides modest gains on established benchmarks, its benefits diminish on larger models (>1B parameters), suggesting limited scalability. Code and pre-trained models are available at anonymous-url.",
    "id": 1198
  },
  {
    "title": "Self-Guided Data Augmentation via Learned Transformation Sensitivity",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.A.",
      "Chen, J."
    ],
    "abstract": "We propose a simple method to automatically discover useful data augmentations without human prior knowledge or exhaustive search. Our approach trains an auxiliary sensitivity network that predicts how transformations affect task loss, then uses this model to selectively apply only transformations expected to improve generalization. The sensitivity network is trained efficiently using gradient-based meta-learning on a small held-out validation set, avoiding the computational cost of comprehensive augmentation search. We evaluate on CIFAR-10, CIFAR-100 and ImageNet subsets, showing 2-4% accuracy improvements over baseline augmentation strategies while requiring 50-70% less augmentation at inference. Experiments demonstrate our method discovers transformations similar to established techniques in computer vision but also reveals dataset-specific augmentations not typically used. The approach extends naturally to non-vision domains, though we observe mixed results on text classification tasks. While the improvements are consistent, they remain modest and our method adds computational overhead during training. Code and pre-trained models will be made available at anonymous link upon acceptance.",
    "id": 1199
  },
  {
    "title": "LoRA-FM: Low-Rank Adaptation with Fisher Information Matching for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "M\u00fcller, J.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods have emerged as practical alternatives to full model fine-tuning, with LoRA and its variants showing promise across various domains. We propose LoRA-FM, which enhances standard LoRA by incorporating Fisher information matrix (FIM) estimates to adaptively set rank allocation across layers. Our key insight is that layers with higher Fisher information capture more task-relevant knowledge, warranting larger rank budgets. We derive a simple approximation of diagonal FIM via gradient accumulation during initial training steps, then use this to modulate the rank of each LoRA adaptation. Experiments on GLUE and SuperGLUE benchmarks show 2-3% average improvement over LoRA with similar parameter budgets, though gains are uneven across tasks. While our approach introduces minimal computational overhead (\u22485% training time increase), it requires additional hyperparameter tuning for the FIM estimation window. Ablations reveal that the benefit primarily stems from improved rank allocation rather than the FIM computation itself. However, our method underperforms QLoRA on memory-constrained settings and shows limited benefits for larger models (>30B parameters), suggesting that our insights may not scale effectively. Code will be released upon acceptance.",
    "id": 1200
  },
  {
    "title": "Improving Transformer Training Efficiency via Blockwise Low-Rank Attention",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "The quadratic complexity of self-attention in Transformers remains a significant bottleneck for training large models on long sequences. While numerous efficient attention variants have been proposed, their practical benefits often fail to materialize beyond theoretical speedups. We propose Blockwise Low-Rank Attention (BLRA), which decomposes attention computation into smaller block-level operations combined with learnable low-rank projections. Our method reduces the asymptotic complexity from O(n\u00b2d) to O(nkd) where k \u226a n, while maintaining the full attention capability within each block. We implement BLRA as a drop-in replacement for standard attention in both encoder and decoder architectures. Experiments on Wikitext-103, C4, and OpenWebText demonstrate 1.3-1.7\u00d7 wall-clock time improvements during training with less than 2% perplexity degradation compared to baselines. However, we observe diminishing returns on shorter sequences (<1024 tokens) and slight convergence instabilities for certain learning rate schedules. While BLRA achieves practical speedups for long-context applications, our theoretical analysis shows the low-rank constraint preserves only approximately 85% of the original attention expressivity, suggesting clear trade-offs between efficiency and model capacity that practitioners must consider.",
    "id": 1201
  },
  {
    "title": "Sharpening Noisy Labels via Cross-Modal Consistency: A Simple Approach for Semi-Supervised Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Semi-supervised learning with noisy labels remains a pervasive challenge in practical applications where large-scale datasets inevitably contain annotation errors. We propose Cross-Modal Consistency Filtering (CMCF), a straightforward method that leverages the natural robustness of multimodal representations to identify and correct label noise. Our approach trains separate encoders on different input modalities (e.g., images and text in vision-language data) and uses their disagreement to detect potentially mislabeled examples, followed by pseudo-label correction using the more confident modality's predictions. While conceptually simple, CMCF achieves competitive performance on standard benchmarks, improving over baseline noisy-label methods by 2-3% accuracy on CIFAR-100N and WebVision. However, we observe that gains diminish with lower noise rates, and the approach requires modalities with complementary information\u2014limiting its applicability. Our theoretical analysis provides mild convergence guarantees under restrictive assumptions that may not hold in practice. Code and pre-trained models are available, though reproduction requires significant computational resources due to the multimodal architecture.",
    "id": 1202
  },
  {
    "title": "Adaptive Gradient Clipping with Learned Threshold Schedules for Low-Precision Training",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Gonzalez, M."
    ],
    "abstract": "Training deep neural networks in low-precision regimes remains challenging due to gradient instabilities and representational limitations. We propose Learnable Gradient Clipping (LGC), a method that adaptively adjusts clipping thresholds during training using a lightweight meta-learning approach. Our method learns threshold schedules via gradient-based optimization on a small validation set, avoiding the need for extensive hyperparameter tuning. We evaluate LGC on ResNet-50 and Vision Transformer architectures using 8-bit and 16-bit fixed-point training. Results show 0.5-1.2% accuracy improvements over standard gradient clipping baselines on ImageNet, with the biggest gains observed in 8-bit precision settings. Our analysis reveals that LGC particularly helps during the initial training phase when gradient magnitudes are rapidly changing. While the improvements are modest, our approach is computationally efficient, adding less than 2% training overhead, and may benefit practitioners working with constrained hardware. However, we observe that the effectiveness of LGC diminishes when combined with advanced optimizers like AdamW or when training larger models. Our code and pre-computed threshold schedules are publicly available.",
    "id": 1203
  },
  {
    "title": "Curvature-Aware Gradient Clipping for Noisy Optimization Landscapes",
    "authors": [
      "Liu, K.",
      "Johnson, M.",
      "Chen, S.",
      "Rodriguez, A."
    ],
    "abstract": "We present Curvature-Aware Gradient Clipping (CAGC), a gradient modification scheme that adapts clipping thresholds based on local Hessian information during neural network training. While standard gradient clipping improves robustness to heavy-tailed noise, it treats all parameters uniformly, potentially harming convergence in benign regions. CAGC estimates curvature along gradient directions using efficient Hessian-vector products and clips more aggressively in regions with negative curvature or high noise variance. We evaluate CAGC on language modeling tasks with varying levels of gradient noise, demonstrating modest improvements in convergence speed (5-12% wall clock time reduction) over standard clipping baselines. However, benefits diminish on well-conditioned problems or with small batch sizes. Theoretical analysis reveals that CAGC preserves convergence guarantees for convex objectives but may introduce bias in non-convex settings. Our implementation requires minimal overhead but introduces two additional hyperparameters. While CAGC does not outperform state-of-the-art optimizers like AdamW on standard benchmarks, it provides a lightweight alternative when computational constraints limit optimizer choice. Code and pre-trained models are available at [anonymized].",
    "id": 1204
  },
  {
    "title": "Variance-Aware Gradient Descent with Random Reshuffling: Improved Convergence Without Learning Rate Tuning",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "Rossi, M."
    ],
    "abstract": "We propose VR-SGD-RR, a variance-reduced stochastic gradient method that automatically adapts learning rates based on gradient statistics collected during random reshuffling (RR) epochs. While RR often outperforms standard SGD in practice, its theoretical analysis remains notoriously difficult due to data-dependent sampling without replacement. Our key insight is to exploit the finite-sum structure of empirical risk minimization to construct adaptive step sizes that depend on per-sample gradient norms estimated from previous epochs. This yields O(1/T) convergence for convex problems and improves dependence on condition number \u03ba from \u03ba\u00b2 to \u03ba^{3/2} compared to vanilla SGD-RR. On CIFAR-10 and ImageNet, VR-SGD-RR achieves similar final accuracy to SGD+Momentum while eliminating manual learning rate tuning for 80% of hyperparameter configurations. However, our method introduces O(d) additional memory overhead per sample and shows diminishing returns on problems with large batch sizes. Theoretically, our analysis relies on an untested assumption about gradient variance decay that remains unproven for general non-convex settings. Experiments on synthetic ill-conditioned problems validate our theoretical rates, but real-world gains are modest except when initial learning rates are far from optimal.",
    "id": 1205
  },
  {
    "title": "LoFiProp: Low-Fidelity Gradient Propagation for Memory-Efficient Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "Transformer models face significant memory bottlenecks during backpropagation due to the need to store activations for gradient computation. We propose Low-Fidelity Gradient Propagation (LoFiProp), a simple technique that selectively stores low-precision versions of intermediate activations during the forward pass, using them to approximate gradients during backpropagation. Our method reduces activation memory by up to 3.2\u00d7 with minimal code changes, achieving comparable performance to full-precision training on standard benchmarks. Theoretical analysis shows LoFiProp introduces bounded gradient errors that do not prevent convergence. Experiments on BERT-base and ViT-small demonstrate 2-4% accuracy degradation on GLUE and ImageNet respectively, with some tasks showing no degradation. While not achieving state-of-the-art compression, LoFiProp provides a practical trade-off between memory usage and accuracy for resource-constrained training scenarios. Code and pre-trained models will be released upon acceptance.",
    "id": 1206
  },
  {
    "title": "Improved Gradient Descent through Better Initialization: A Frequency Domain Perspective",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We present a simple modification to standard gradient descent that exploits frequency domain analysis to improve initialization for non-convex optimization. Building on recent observations that low-frequency components of neural networks converge faster, we propose Frequency-Aware Initialization (FAI): a data-free scheme that sets initial parameters according to a designed spectrum that emphasizes low-frequency response. Our method requires only computing a 1D discrete Fourier transform of random initializations, followed by element-wise scaling. Across standard benchmarks (CIFAR-10/100 and ImageNet), FAI achieves 1.2-2.3% better final accuracy and 10-20% faster convergence compared to Xavier and He initialization when combined with SGD-momentum, though benefits diminish with adaptive optimizers. We provide theoretical analysis showing FAI reduces initial Hessian condition number by a constant factor for 2-layer ReLU networks. While our results are consistent across image classification tasks, we find minimal improvement on language modeling or reinforcement learning benchmarks. The simplicity of our approach makes it easily integrable into existing pipelines, though we acknowledge the limited theoretical characterization beyond shallow networks. Code is available at anonymous-url.",
    "id": 1207
  },
  {
    "title": "BatchNorm Makes Your ResNet Compressible: Improved Post-Training Quantization with Layer-wise Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "We investigate the relationship between Batch Normalization and the compressibility of ResNet architectures through post-training quantization. Our key finding suggests that BatchNorm parameters encode important information about layer-wise sensitivity, which can be leveraged to determine layer-specific learning rates during the fine-tuning phase of quantization. We propose a simple method to extract scaling factors from BatchNorm statistics that serve as proxies for quantization noise sensitivity. Experiments on CIFAR-10 and ImageNet show our approach achieves 3-4% higher accuracy than uniform quantization at 4-bit precision, with minimal additional computational overhead. While our method shows consistent improvements over baselines, the gains diminish at higher bit-widths (\u22656-bit), and theoretical analysis remains incomplete. Our work provides practical insights for practitioners but leaves open questions regarding the generalization to other architectures and the fundamental mechanisms underlying our observations. Code is available at anonymous URL.",
    "id": 1208
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Layer-wise Learning Rate Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Recent theoretical work has established connection between the convergence dynamics of neural networks and their generalization performance. We propose a layer-wise learning rate scheduling scheme that increases learning rates for deeper layers while decaying rates for earlier layers during training. Our theoretical analysis shows this approach leads to tighter PAC-Bayesian generalization bounds that scale more favorably with network depth compared to standard schedules. On CIFAR-10 and ImageNet subsets, our method achieves 2-3% improvements over vanilla SGD with cosine annealing, though gains diminish on larger architectures. While our bounds improve upon previous work for networks with 3-8 layers, they remain vacuous for state-of-the-art deep architectures. The scheduling scheme introduces two hyperparameters that must be tuned per-dataset, limiting practical applicability. Our empirical evaluation on standard benchmarks provides moderate improvements but falls short of matching performance gains from recent architectural innovations. Code and experimental details are provided for reproducibility.",
    "id": 1209
  },
  {
    "title": "Improved Generalization Bounds for Meta-Learning via Task-Similarity Clustering",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a novel framework for improving generalization bounds in gradient-based meta-learning by exploiting task clustering structures. While existing bounds depend on worst-case task similarity, we show that tighter bounds can be obtained when tasks form clusters with bounded intra-cluster diameter. Our approach first clusters tasks based on their optimal parameter distances using a theoretically-justified metric, then employs cluster-specific meta-parameters that are regularized using cluster-aware complexity terms. We derive PAC-Bayesian generalization bounds that scale with the effective number of clusters rather than the total number of tasks. Experiments on few-shot image classification benchmarks demonstrate improved bounds relative to prior work, with empirical improvements of 3-5% in accuracy on tieredImageNet and CIFAR-FS. However, the practical gains diminish as the number of training tasks increases, and our clustering procedure adds non-trivial computational overhead. While our theoretical contribution provides new insights into task similarity in meta-learning, the empirical benefits are modest compared to standard regularization techniques.",
    "id": 1210
  },
  {
    "title": "Momentum Matters: Revisiting Second-Order Optimization in Overparameterized Models",
    "authors": [
      "Liu, Y.",
      "Kumar, S.",
      "Chodorowski, A.",
      "Zhou, L."
    ],
    "abstract": "We investigate whether carefully tuned momentum schedules can recover the benefits of second-order optimization methods in training large-scale neural networks without the computational overhead. Despite extensive theoretical work suggesting the importance of curvature information, practical adoption of second-order methods remains limited due to scalability concerns. We propose Adaptive Momentum Scaling (AMS), a simple modification to standard SGD+momentum that approximates the behavior of quasi-Newton methods by dynamically adjusting the momentum coefficient based on the history of gradient correlations. Our extensive experiments on standard benchmarks (CIFAR-10/100, ImageNet) and language modeling tasks show that AMS achieves comparable convergence to AdamW while maintaining the computational efficiency of SGD, reducing training time by 12-18% on typical hardware configurations. However, we find that the benefits vary significantly across architectures and datasets, with diminishing returns on particularly large models. While our empirical results demonstrate practical utility, we acknowledge that the theoretical underpinnings of our approach remain incomplete, and we primarily establish effectiveness through carefully designed ablations rather than formal guarantees.",
    "id": 1211
  },
  {
    "title": "Momentum-Scheduled Warmup: Balancing Optimization Stability and Convergence in Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a simple modification to standard transformer optimization that schedules the momentum parameter during warmup phases. Motivated by observations that high momentum can destabilize early training while low momentum slows convergence later, we introduce a linear momentum warmup schedule that transitions from \u03b2=0.0 to \u03b2=0.9 over the first 1000 steps. This approach requires only two additional hyper-parameters and can be implemented in 5 lines of code. Experiments on IWSLT14 De-En, WMT16 En-De, and GLUE benchmark tasks show modest improvements: 0.3-0.7 BLEU score gains and 0.5-1.2% accuracy improvements over standard AdamW baselines, with particularly consistent benefits on smaller datasets (<10M parameters). Ablations suggest the benefit primarily comes from improved early optimization stability rather than final convergence quality. While our method is straightforward to implement and provides reliable if incremental gains, we acknowledge the improvements remain within standard error margins for most tasks. We provide PyTorch code and hyper-parameter sweep results to facilitate reproduction.",
    "id": 1212
  },
  {
    "title": "Gradient Surgery for Improving Transfer Learning Across Similar Tasks",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Singhal, P."
    ],
    "abstract": "Transfer learning has proven effective when source and target domains are closely related, but performance degrades as task similarity decreases. We propose a simple gradient-based pruning method that identifies and removes gradient components that negatively impact transfer performance. Our approach computes the inner product between source and target gradients during fine-tuning, then surgically removes directions with negative correlation. We evaluate our method on natural language understanding tasks using BERT-base models, showing 2-3% average improvement on GLUE benchmarks when transferring from similar tasks (SST-2 to IMDB), but minimal gains on dissimilar pairs. Theoretical analysis suggests our method approximates a projected gradient descent that constrains the model to remain in regions beneficial for the target task. While our method is computationally efficient and requires no additional hyperparameters, we find it offers diminishing returns as target dataset size increases. Our experiments suggest the technique primarily helps in low-data regimes when tasks share semantic structure. Code and pre-trained models are available.",
    "id": 1213
  },
  {
    "title": "Revisiting Curriculum Learning for Neural Network Training with Sample Complexity Guarantees",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Curriculum learning has shown practical success in training neural networks, yet theoretical understanding remains limited. We provide a formal analysis of curriculum learning under a PAC-Bayesian framework, deriving sample complexity bounds that depend on curriculum difficulty transitions. Our key insight is that geometric scheduling of task difficulty yields faster convergence than standard uniform sampling, with improvements proportional to the curriculum's gradient in task complexity. We validate these theoretical results through experiments on synthetic datasets and CIFAR-10 variants, demonstrating 10-15% reduction in required training iterations compared to baselines. However, our empirical gains diminish on larger-scale tasks like ImageNet, and our theoretical assumptions (bounded loss functions, known optimal ordering) rarely hold in practice. While this work takes an important step toward principled curriculum design, significant gaps remain between theory and practice. Code will be made available upon acceptance.",
    "id": 1215
  },
  {
    "title": "Gradient Descent with Momentum Revisited: A Dynamical Systems Perspective on Adaptive Restarting",
    "authors": [
      "Liu, H.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We revisit the classical momentum method through the lens of damped harmonic oscillators, revealing that the energy decay properties of Nesterov's accelerated gradient can be linked to the damping coefficient in second-order dynamical systems. While this connection is intuitive and has been noted in prior work, we provide a more systematic framework for understanding the oscillatory behavior observed during training. Our key contribution is an adaptive restarting scheme that monitors the Hamiltonian energy of the system and triggers restart when energy levels exceed a learned threshold. We show this approach provides modest improvements over standard momentum on strongly convex objectives, reducing convergence time by 10-15% on average in synthetic experiments. However, our method requires careful tuning of the restart threshold parameter, and our theoretical guarantees only hold under restrictive Lipschitz-smoothness assumptions that may not hold in practice. Experiments on CIFAR-10 with ResNet-18 show mixed results: while our method achieves slightly faster initial convergence, final test accuracy is comparable to standard SGD with momentum. Our results suggest that while dynamical systems insights provide interesting theoretical perspectives, their practical impact remains limited for modern deep learning architectures.",
    "id": 1216
  },
  {
    "title": "Improved Gradient Descent via Random Matrix Transformations at Initialization",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, J."
    ],
    "abstract": "We propose Randomized Initialization Transformations (RIT), a simple preprocessing step that applies random orthogonal transformations to neural network weights before training begins. While random initialization has been extensively studied, we show that applying additional random transformations after standard initialization can accelerate convergence by up to 15% on CIFAR-10/100 and ImageNet when using SGD with momentum. Our theoretical analysis reveals that RIT improves the condition number of the effective Hessian in the early training phase, though this effect diminishes as training progresses. We extend RIT to transformers by applying random rotations to attention weight matrices, achieving modest improvements on language modeling tasks. Ablations show that the orthogonal constraint is crucial - using unstructured random matrices degrades performance. While the improvements are consistent across architectures, the effect size is smaller than recent optimizer advances like AdamW or Lookahead. Our code is available at anonymous.github.io/RIT.",
    "id": 1217
  },
  {
    "title": "Gradient Perturbation Family: A Unified Framework for Low-rank Adaptation in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, K."
    ],
    "abstract": "We propose Gradient Perturbation Family (GPF), a framework that unifies existing low-rank adaptation methods through the lens of gradient approximation. While prior approaches like LoRA and AdaLoRA rely on heuristic rank selection, GPF treats rank as a learnable parameter by perturbing gradient updates through a learned low-rank structure. Our method introduces a differentiable rank penalty based on nuclear norm relaxation, enabling automatic rank selection during training. Through extensive experiments on language modeling and vision tasks, we show GPF matches or slightly outperforms baselines with 10-25% fewer parameters. However, theoretical analysis reveals limitations: our rank selection mechanism sometimes leads to suboptimal low-rank approximations, particularly in deeper layers where gradients exhibit heterogeneous structures. Additionally, the training dynamics become unstable when initial rank guesses deviate significantly from optimal values. While GPF provides a principled perspective on low-rank adaptation, the practical gains over existing methods are modest, and the computational overhead from dynamic rank adjustment may limit its scalability. Our code is available at [anonymous link].",
    "id": 1218
  },
  {
    "title": "Gradient Surgery Without the Mess: A Lightweight Approach to Multi-Task Learning in Transformers",
    "authors": [
      "Liu, S.",
      "Kumar, V.",
      "Chen, J."
    ],
    "abstract": "Multi-task learning in transformers often relies on complex gradient manipulation techniques like PCGrad or GradDrop to resolve task conflicts. We propose Gradient Routing Networks (GRN), a simpler alternative that learns to reweight task gradients through small routing modules inserted between transformer layers. Our approach adds only 0.3% additional parameters and requires no hyperparameter tuning. On five multi-task NLP benchmarks, GRN achieves 92-96% of the performance of state-of-the-art gradient surgery methods while being 5\u00d7 faster to train. We show theoretical guarantees that GRN converges to Pareto stationary points under mild assumptions. However, our method struggles when tasks have significantly different gradient magnitudes, and we observe variance in performance across random seeds (\u00b11.2% on GLUE). Our code and pre-trained models are available anonymously.",
    "id": 1219
  },
  {
    "title": "Gradient Amplification via Layer-wise Learning Rate Rescaling in Transformer Fine-tuning",
    "authors": [
      "Chen, J.",
      "Ramos, K.",
      "Srinivasan, P."
    ],
    "abstract": "We propose a simple technique for improving transformer fine-tuning stability by rescaling learning rates based on gradient norms at each layer. Motivated by observations that gradient magnitudes vary dramatically across transformer layers during fine-tuning, our method amplifies gradients for frozen layers while suppressing them for heavily-updated ones through an adaptive rescaling mechanism derived from Riemannian optimization principles. We evaluate our approach on 8 GLUE tasks and 3 vision-language datasets, achieving modest improvements of 0.8-1.2% over standard AdamW fine-tuning, with particular gains on smaller datasets (<10k examples). While our method requires minimal hyperparameter tuning beyond standard settings, analysis reveals the improvements largely vanish when using larger batch sizes (>32) or longer training schedules (>5 epochs). Theoretically, we establish convergence bounds under restrictive assumptions about layer-wise Lipschitz constants, but these may not hold for practical architectures. Experiments on larger-scale tasks (ImageNet fine-tuning, machine translation) show no consistent benefit over baselines. Our code is available at [URL], though reproducing the exact results may require specific PyTorch/CUDA versions due to numerical precision issues in gradient norm computations.",
    "id": 1220
  },
  {
    "title": "Improved Gradient Variance Reduction via Adaptive Batch Normalization",
    "authors": [
      "Liu, Q.",
      "Thompson, K.",
      "Rao, S."
    ],
    "abstract": "We propose Accelerated Adaptive BatchNorm (AABN), a lightweight modification to gradient descent training that adaptively rescales batch normalization parameters based on gradient variance estimates. Our method builds on recent work showing that batch normalization interacts non-trivially with stochastic gradient noise. By maintaining running averages of gradient covariances across layers, AABN dynamically adjusts the effective learning rate on a per-feature basis without hyperparameter tuning. We evaluate AABN on ResNet models trained on ImageNet and BERT models fine-tuned on GLUE tasks. Results show 5-12% faster convergence and 2-3% better final validation accuracy compared to standard batch normalization baselines, with minimal computational overhead (<1% increase in training time). While our gains are consistent across vision and language tasks, they are most pronounced in low-batch training regimes (\u226432 samples), and we observe diminishing returns with larger batches. Theoretical analysis shows AABN reduces gradient variance by a factor proportional to \u221a(d/m), where d is feature dimension and m is batch size. However, our convergence guarantees require strong assumptions about batch independence that may not hold in practice. Code will be released upon acceptance.",
    "id": 1221
  },
  {
    "title": "Gradient Surgery Revisited: When Task Balancing Hurts More Than It Helps",
    "authors": [
      "Liu, S.",
      "Kim, J.",
      "Brown, M."
    ],
    "abstract": "Multi-task learning with conflicting objectives often relies on gradient surgery techniques that modify task gradients to improve optimization. We investigate when such interference mitigation strategies actually degrade performance compared to naive gradient descent. Through theoretical analysis and extensive experiments across NLP and vision benchmarks, we show that existing gradient surgery methods can introduce unnecessary regularization that hinders learning in 35% of studied cases. We propose a simple metric based on gradient alignment statistics to predict when gradient surgery will help versus hurt, achieving 78% accuracy on held-out tasks. Our results suggest that the benefits of gradient surgery are highly task-dependent and that practitioners should carefully validate these approaches rather than applying them universally. Code and pre-trained models will be available upon acceptance.",
    "id": 1222
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning (MTL) often suffers from conflicting gradients between tasks that impede optimization. While recent gradient surgery methods like PCGrad address this by projecting conflicting gradients, we observe they may discard useful learning signals. We propose Selective Gradient Dropping (SGD), a simple approach that drops rather than projects conflicting components when cosine similarity falls below a learned threshold. Our theoretical analysis shows SGD maintains convergence guarantees under mild assumptions about task similarity. Experiments on three vision-language benchmarks and two multitask NLP datasets show SGD achieves comparable or better performance than PCGrad while reducing computational overhead by 15-20%. However, we find SGD's benefits diminish when task counts exceed 5 or when tasks are highly correlated. Ablation studies reveal that threshold selection is critical and may require task-specific tuning. While our method offers practical improvements for moderate-scale MTL, it inherits limitations from existing gradient surgery approaches and may not suit extreme multitask settings. Code will be available if accepted.",
    "id": 1223
  },
  {
    "title": "LoRA-MoE: Low-Rank Experts for Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M.",
      "Takeda, K."
    ],
    "abstract": "Fine-tuning large language models remains computationally prohibitive despite recent parameter-efficient methods like LoRA. We propose LoRA-MoE, a method that combines low-rank adaptation with sparse mixture-of-experts to achieve better downstream performance while maintaining training efficiency. Our approach assigns separate low-rank matrices to each expert in an MoE layer and introduces a lightweight routing mechanism based on input embeddings. We evaluate LoRA-MoE on GLUE and SuperGLUE benchmarks using Llama-2-7B and 13B models. While LoRA-MoE achieves a 2-3% improvement over vanilla LoRA on average, it requires 3.5x more memory during training. Our results suggest that the gains primarily come from increased capacity rather than architectural innovation. The method shows promise for certain data-scarce scenarios but struggles with domain shift. We release our code and checkpoints for reproducibility.",
    "id": 1224
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: A Spectral Perspective on Layer-wise Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose Spectral Layer-wise Learning Rate Adjustment (SLLRA), a novel approach to combat gradient interference in overparameterized neural networks through per-layer learning rates derived from the Hessian spectrum. While existing gradient surgery techniques typically rely on optimization tricks or architectural modifications, we show that the eigenspectrum of the Hessian at initialization provides surprisingly predictive signals for optimal layer-wise learning rates. Through extensive experiments on CIFAR-10/100 and ImageNet, we demonstrate that SLLRA achieves 0.5-1.2% improvements over standard training recipes without architectural changes or additional compute. However, our method shows limited effectiveness on smaller architectures (ResNet18/34) and inconsistent gains across different optimizers. Theoretical analysis reveals that our heuristic connects to second-order optimization but lacks formal convergence guarantees. While the improvements are statistically significant, the practical impact remains modest, suggesting that gradient interference may be less problematic than previously claimed in modern training regimes. Our code and pre-trained models are available at anonymous.url/SLLRA.",
    "id": 1225
  },
  {
    "title": "ReLU Networks Can Learn Polynomial Features via Gradient Descent with Random Initialization",
    "authors": [
      "Wang, L.",
      "Chen, S.",
      "Thompson, K."
    ],
    "abstract": "We study whether shallow ReLU networks can learn low-degree polynomial features from data generated by a target polynomial of degree k. While prior work establishes learnability for specialized architectures or modified training procedures, we analyze standard gradient descent on vanilla ReLU networks with standard initialization. Our main result shows that networks with width polynomial in d and k can achieve population loss \u03b5 after O(d^k/\u03b5^2) iterations, provided the target polynomial satisfies a non-degeneracy condition on its high-order Fourier coefficients. The analysis leverages a connection between the Hermite expansion of ReLU functions and the polynomial basis, though our bounds depend exponentially on the degree k. Experiments on synthetic data demonstrate our theoretical predictions hold for degrees k \u2264 4, but performance degrades significantly for k \u2265 5. While our results provide the first polynomial-time guarantees for learning degree-k polynomials with standard ReLU networks under natural assumptions, the exponential dependence on k and restrictive non-degeneracy condition limit practical applicability. We discuss potential extensions to deeper architectures and connections to recent work on feature learning in neural networks.",
    "id": 1226
  },
  {
    "title": "Preconditioned Gradient Descent with Recursive Momentum Estimation for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "We propose PGMOR, a variant of gradient descent that combines adaptive preconditioning with a recursively estimated momentum factor for training deep neural networks. Unlike Adam and its variants that use fixed exponential decay rates, PGMOR dynamically adjusts both the preconditioner and momentum based on a running estimate of gradient curvature via a novel recursive formula. Our method achieves comparable convergence to Adam on CIFAR-10/100 and ImageNet while reducing memory usage by 20-30%. However, experiments on larger-scale tasks (ViT training and diffusion models) show marginal gains over tuned SGD with momentum. While PGMOR demonstrates some theoretical advantages in simplified quadratic models, extending these guarantees to general non-convex settings remains challenging. Our contribution is primarily empirical: we identify cases where recursive momentum estimation provides measurable, though not always significant, improvements over existing optimizers. Code and pretrained models will be released upon acceptance.",
    "id": 1227
  },
  {
    "title": "Self-Supervised Gradient Compression: Reducing Communication Overhead in Federated Learning via Autoencoder-Based Gradient Encoding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Federated learning faces critical scalability challenges due to high communication costs when exchanging gradient updates between clients and servers. We propose Self-Supervised Gradient Compression (SSGC), a novel approach that learns to compress gradients without requiring external labels or assumptions about their distribution. Our method trains an autoencoder architecture to directly encode gradient tensors into low-dimensional representations, with the reconstruction loss adapted to preserve the direction (rather than magnitude) of the original updates. We evaluate SSGC on standard federated benchmarks including CIFAR-10, CIFAR-100, and FEMNIST across varying client participation rates. Experiments show 8-16x compression ratios while maintaining accuracy within 2% of uncompressed baselines in most settings. However, we observe substantial degradation (>5% accuracy drop) on some non-IID data distributions, particularly when client datasets are highly skewed. While our theoretical analysis proves convergence under idealized conditions, we acknowledge limitations in handling heterogeneity across clients. SSGC offers a communication-efficient alternative to existing gradient compression techniques like Top-k and quantization, though further investigation is needed to improve robustness under non-convex objectives.",
    "id": 1228
  },
  {
    "title": "Gradient Surgery for Mixed-Precision Training: A Simple Heuristic with Modest Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Liu, J."
    ],
    "abstract": "Mixed-precision training has become standard practice for accelerating deep learning workflows, yet the interaction between low-precision arithmetic and adaptive optimizers remains poorly understood. We observe that half-precision gradients can exhibit pathological curvature in certain layers, causing Adam-like optimizers to make erratic updates that slow convergence. To address this, we propose Gradient Surgery (GS), a lightweight modification that applies layer-wise gradient clipping based on relative magnitude ratios across parameter blocks. Our method requires only one additional line of code and negligible computational overhead. On ImageNet training with ResNet-50, GS achieves 0.7% higher top-1 accuracy compared to standard mixed-precision baselines, while reducing training time by 3-5%. Experiments on language modeling (WikiText-103) and reinforcement learning (MuJoCo) show similar modest improvements, though gains diminish with larger batch sizes. While our theoretical analysis focuses on simplified quadratic models, empirical results suggest GS is most beneficial for models with sharply varying gradient scales across layers. Our code is available at anonymous-link.github.io.",
    "id": 1229
  },
  {
    "title": "Adaptive Gradient Descent with Moving Average Second Moments Improves Language Model Fine-tuning",
    "authors": [
      "Chen, L.",
      "Kumar, V.",
      "Johnson, S."
    ],
    "abstract": "We propose AdaMAM (Adaptive Momentum with Average Moments), a simple modification to AdamW that replaces the exponential moving average of squared gradients with a windowed moving average. This change addresses the observation that Adam's aggressive early updates can harm the stability of transformer fine-tuning on small datasets. Our method uses a fixed-size sliding window over past gradients, making it less sensitive to the choice of \u03b22 while maintaining comparable convergence rates. We evaluate AdaMAM on various NLP tasks including GLUE, SQuAD, and domain adaptation benchmarks. On 10 out of 15 tasks, AdaMAM matches or slightly improves upon AdamW baselines, with an average improvement of 0.4 F1 points. However, we observe that performance gains are most pronounced on tasks with limited training data (\u226410K examples), suggesting diminishing returns as dataset sizes increase. While our theoretical analysis shows AdaMAM converges under standard smoothness assumptions, we acknowledge that our proof techniques closely follow prior work and do not provide fundamentally new insights. Code is available at anonymous-url.github.io/adamam.",
    "id": 1230
  },
  {
    "title": "Improved Learning Rate Schedules for Transformer Fine-tuning via Small-Scale Experiments",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chen, L."
    ],
    "abstract": "We investigate whether micro-level optimizations in learning rate schedules can improve Transformer fine-tuning efficiency, motivated by the observation that standard cosine schedules may not be optimal for transfer learning scenarios. Our approach systematically evaluates 12 schedule variants using an automated framework across 6 NLP tasks, focusing on modestly-sized BERT-base and RoBERTa-base models. We introduce a piecewise linear schedule with momentum-warm restarts that shows 2.3% average improvement over baseline schedules when initialized from pre-trained checkpoints, while requiring minimal compute overhead. Results demonstrate consistent gains on GLUE benchmarks, though improvements on larger models (BERT-large) diminish to 0.7%. Analysis reveals our method particularly benefits tasks with limited target-domain data (under 10k examples). While these improvements are incremental, our experiments provide evidence that schedule tuning can be a complementary axis for fine-tuning optimization. The simplicity of implementation and reproducibility across different random seeds (tested with 5 trials) make this a practical addition to existing fine-tuning pipelines. Code and configuration files are available at [anonymous link provided].",
    "id": 1231
  },
  {
    "title": "LoRA-FA: Low-Rank Adaptation with Feature Alignment for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA-FA, a modification to Low-Rank Adaptation (LoRA) that incorporates feature-space alignment to improve adaptation quality while maintaining parameter efficiency. Our key observation is that LoRA updates can drift from the pretrained model's feature distributions, leading to suboptimal performance on downstream tasks. We address this by introducing a lightweight alignment loss that encourages the adapted features to remain close to the pretrained manifold, implemented through a small set of learnable projection matrices. We evaluate LoRA-FA on GLUE, SuperGLUE, and vision-language tasks using a 7B parameter LLaMA model and CLIP. Results show 2-4% average improvement over vanilla LoRA on most tasks, with 0.5-1% gains over full fine-tuning on some benchmarks, while using only 0.1% of trainable parameters. However, we observe minimal benefits on tasks with significant domain shift and marginal improvements on larger models (30B+). Our method adds minimal computational overhead (5-7% training time increase) and maintains the same memory benefits as LoRA. While our empirical results are encouraging, we acknowledge the theoretical understanding of why feature alignment helps remains limited. Code and checkpoints will be released upon publication.",
    "id": 1232
  },
  {
    "title": "Gradient Surgery with Adaptive Momentum for Non-Convex Multi-Task Optimization",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Chen, H."
    ],
    "abstract": "Multi-task learning suffers from conflicting gradients that can hinder optimization. While recent gradient surgery methods like PCGrad resolve conflicts, we observe they introduce unintended bias by uniformly clipping gradients regardless of task uncertainty. We propose Adaptive Gradient Surgery (AGS), which incorporates per-task uncertainty estimates to selectively apply gradient modifications. Our method combines second-order moment estimates with a novel trust-region update that adapts the surgery threshold based on gradient alignment patterns. On standard multi-task benchmarks (Multi-MNIST, CityScapes, NYU-v2), AGS achieves modest improvements over PCGrad (0.5-1.2% average accuracy gains) while reducing training instability. However, we find these gains diminish with larger models, suggesting limited scalability. Theoretical analysis proves convergence under simplified convex assumptions, though the extension to non-convex settings remains an open challenge. While AGS provides a practical improvement to gradient surgery, its computational overhead (15-20% training time increase) and diminishing returns on complex tasks temper its overall impact.",
    "id": 1233
  },
  {
    "title": "BatchNorm Folding Without Calibration: A Taylor Expansion Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose a novel method for fusing BatchNorm layers into preceding convolutional weights during neural network inference. Unlike existing approaches that require calibration data to maintain accuracy, our technique uses second-order Taylor expansion to approximate the BatchNorm transformation directly from learned parameters. Our key insight is that the BatchNorm scale and shift parameters form a low-rank structure that can be efficiently captured through Taylor coefficients. We evaluate on ResNet50, MobileNetV2, and EfficientNet-B0 across ImageNet, observing 15-30% inference speedup with <0.5% accuracy degradation in 7/9 experiments, though performance drops by 2-3% in two edge cases with extreme activations. Theoretical analysis provides bounds on approximation error under mild assumptions about input distributions. While our method offers practical deployment benefits, we acknowledge limitations: (1) the Taylor approximation breaks down for heavy-tailed activations, and (2) extension to LayerNorm and GroupNorm remains non-trivial. Code is provided for reproducibility.",
    "id": 1234
  },
  {
    "title": "Improving Few-Shot Transfer via Curriculum-Based Meta-Regularization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T.",
      "Johnson, K."
    ],
    "abstract": "Meta-learning has shown promise for few-shot adaptation, but standard approaches struggle when target tasks differ significantly from training distributions. We propose Curriculum-Based Meta-Regularization (CBMR), which progressively increases task difficulty during meta-training while applying adaptive regularization to prevent overfitting. Our method combines a curriculum scheduler with entropy-based constraints on task embeddings and gradient regularization on the meta-learner. On 5 benchmark datasets, CBMR achieves 2-3% improvements over MAML and ProtoNet baselines on out-of-distribution tasks, while maintaining competitive performance on standard evaluations. While the gains are modest, our results suggest that carefully tuned regularization during curriculum learning can improve transfer robustness without architectural changes. Limitations include increased training time and sensitivity to curriculum parameters. Code is available at [url].",
    "id": 1235
  },
  {
    "title": "Variance-Reduced Gradient Boosting with Adaptive Subsampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient boosting remains a dominant approach for tabular data, yet its training complexity typically scales linearly with dataset size. We propose VR-GBoost, a variance-reduced gradient boosting framework that employs adaptive subsampling motivated by stochastic optimization techniques. Our method dynamically adjusts the fraction of data used at each boosting iteration based on gradient norms and incorporates control variates to reduce the variance of gradients estimated from subsamples. Unlike existing approaches that use fixed sampling rates, VR-GBoost theoretically decreases the required sample complexity from O(n) to O(n^2/3) iterations while maintaining the same convergence guarantees. Experimental evaluation on 8 UCI datasets shows 1.2-1.8x training speedups with modest accuracy improvements (0.5-1.3%) over XGBoost, though these gains diminish on high-dimensional sparse datasets. While our theoretical analysis requires strong convexity assumptions that may not hold in practice, our empirical results suggest the approach can be practically useful despite theoretical limitations. Code is available at anonymous.link.",
    "id": 1236
  },
  {
    "title": "On the Effectiveness of Temperature Scaling for Out-of-Distribution Detection in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Temperature scaling has emerged as a simple yet effective post-processing technique for calibrating neural network confidence. While primarily studied in the context of in-distribution calibration, we investigate whether this single-parameter transformation can improve out-of-distribution (OOD) detection. Through experiments on CIFAR-10/100 and ImageNet, we find that appropriately tuned temperature scaling can improve AUROC by 2-5% over baseline approaches across several OOD datasets, though gains vanish when the OOD data distribution shifts significantly from the validation set used for tuning. Our theoretical analysis reveals that temperature scaling affects the relative entropy between in- and out-of-distribution samples, providing intuition for when improvements can be expected. However, we find the approach less reliable than specialized OOD detection methods, achieving state-of-the-art performance on only 3 out of 8 benchmark settings. While our results suggest temperature scaling as a practical improvement to existing systems with minimal implementation cost, the method's sensitivity to validation set choice and limited theoretical guarantees warrant caution in deployment.",
    "id": 1237
  },
  {
    "title": "Gradient Surgery in Overparameterized Neural Networks: A Comparative Study of Pruning Techniques",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "The lottery ticket hypothesis has sparked renewed interest in neural network pruning, yet fundamental questions remain about how different pruning strategies affect gradient flow during training. We conduct the first systematic comparison of magnitude-based, gradient-based, and random pruning in overparameterized networks across vision and language tasks. Our key finding is that gradient-based pruning consistently identifies subnetworks that converge 15-25% faster than magnitude-based approaches in early training phases, but this advantage disappears after 40-50% of training. Through careful analysis of gradient flow patterns, we show this phenomenon stems from a bias-variance trade-off in the pruning criterion itself, not the inherent sparsity structure. While our results clarify several empirical observations in the pruning literature, we highlight that these insights appear task-specific: the observed patterns hold for ResNets on CIFAR-10/100 and BERT on GLUE tasks, but do not generalize to ViT architectures or out-of-distribution settings. Our work contributes a unified perspective on gradient-centric pruning but fails to provide algorithmic improvements beyond existing heuristics. Code and pre-trained models will be released.",
    "id": 1238
  },
  {
    "title": "Conservative Q-Learning with Adaptive Trust-Region Constraints",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Choi, S."
    ],
    "abstract": "Offline reinforcement learning faces the challenge of value overestimation when the policy deviates significantly from the behavior policy. While existing conservative methods like CQL reduce overestimation by regularizing Q-values, they rely on fixed hyperparameters that may be suboptimal across different datasets. We propose CATR-QL, which introduces an adaptive trust-region mechanism that dynamically adjusts the conservatism level based on the estimated uncertainty of Q-values. Our method computes local Lipschitz constants via gradient analysis to modulate the strength of conservative updates, theoretically ensuring monotonic improvement under relaxed concentrability assumptions. Empirically, we evaluate CATR-QL on 12 continuous control tasks from D4RL, showing consistent but modest improvements over CQL (mean normalized score: 73.2 vs 71.8) with 15% fewer policy updates. However, performance gains are less pronounced on sparse-reward tasks, and the additional computational overhead of adaptive constraint estimation increases training time by 1.4x. Our results suggest that while adaptive conservatism offers benefits, the improvements may not justify the complexity in all scenarios.",
    "id": 1239
  },
  {
    "title": "Gradient Surgery in the Wild: Practical Trade-offs in Multi-Task Learning with Approximate Nash Equilibria",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Garcia, M."
    ],
    "abstract": "Multi-task optimization often faces conflicting gradients that prevent simultaneous progress across objectives. While recent gradient surgery methods show promise in constructing Pareto updates, we find their theoretical guarantees rely on full gradient information and perfect Nash equilibrium computations that are computationally prohibitive at scale. We propose Approximate Gradient Surgery (AGS), a practical framework that trades theoretical optimality for computational efficiency by using only mini-batch gradients and approximate equilibrium solving via projected gradient descent. Our experiments on 3 standard benchmarks and 2 new realistic multi-task settings show AGS achieves 85-95% of the performance of exact methods while reducing computational overhead by 2-3x. However, we observe significant performance degradation on tasks with high gradient misalignment (cosine similarity < 0.2), where AGS fails to find meaningful compromise directions. Our analysis reveals that approximation errors compound particularly when tasks exhibit vastly different gradient scales. While AGS provides a pragmatic alternative to exact methods, our findings highlight fundamental limitations when principled trade-offs between theoretical guarantees and practical constraints are made. Code and benchmarks will be released upon acceptance.",
    "id": 1240
  },
  {
    "title": "Gradient Surgery Meets Sharpness: An Empirical Analysis of Flatness-Aware Minimization with Gradient-Based Optimization",
    "authors": [
      "Liu, J.",
      "Chen, Z.",
      "Thompson, K."
    ],
    "abstract": "While Sharpness-Aware Minimization (SAM) has emerged as a powerful approach for improving generalization in deep neural networks, theoretical understanding of how gradient-based optimizers interact with SAM's minimax objective remains limited. We perform an empirical investigation of SAM across 15 architectures on vision and NLP tasks, revealing that the quality of gradient estimates significantly impacts SAM's efficacy. By decomposing SAM gradients into orientation and magnitude components, we observe that gradient noise introduces systematic biases in the ascent step, leading to suboptimal perturbation directions. We propose Correction-Aware SAM (C-SAM), a lightweight modification that adaptively scales the perturbation magnitude based on the signal-to-noise ratio of gradients. C-SAM achieves comparable or better test accuracy than SAM on 11/15 architectures (average improvement: 1.4%) with minimal computational overhead (<5%). However, our findings suggest that the benefits of sharpness minimization are task-dependent, with substantial improvements primarily on overparameterized models and limited gains on smaller architectures. Our analysis indicates that the interaction between gradient noise and SAM's perturbations may explain these selective benefits, highlighting the need for more nuanced sharpness metrics. Code and checkpoints will be released upon acceptance.",
    "id": 1241
  },
  {
    "title": "BatchNorm Without Batches: Instance-Level Normalization for Improved Few-Shot Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate why Batch Normalization (BN) degrades performance in few-shot learning scenarios where batch sizes are necessarily small. Through empirical analysis, we demonstrate that BN's reliance on batch statistics creates a fundamental distribution shift between pretraining and adaptation phases. To address this, we propose Instance- level Batch Normalization (IBN), a simple modification that replaces batch statistics with instance statistics during adaptation while maintaining frozen pretrained parameters. IBN can be applied post-hoc to any pretrained model without retraining. On standard few-shot benchmarks (mini-ImageNet, tiered-ImageNet), IBN consistently improves top-1 accuracy over standard BN by 2-4% across various architectures. While IBN performs comparably to more sophisticated meta-learning approaches on 5-shot tasks, its benefits diminish to <1% improvement on 1-shot problems. Our analysis reveals that IBN's effectiveness stems from reduced gradient conflicts during adaptation rather than improved feature representations. Though simple and implementable in <20 lines of code, we acknowledge that IBN provides incremental rather than transformative improvements, and discuss scenarios where alternative normalization strategies may be preferable.",
    "id": 1242
  },
  {
    "title": "Learning to Warm-Start: Meta-Initialization for Accelerated Low-Rank Matrix Recovery",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank matrix recovery problems, such as matrix completion and robust PCA, are typically solved via iterative algorithms that can require thousands of iterations to converge. While recent work has explored learning-based warm-starting for convex optimization, these approaches are limited to fixed problem dimensions and fail to generalize across problem families. We propose a meta-learning framework that learns initializations for iterative low-rank recovery algorithms by training on a distribution of problem instances. Our method encodes problem structure into a compact neural representation and outputs warm-start parameters that reduce convergence time by 2-5x across different problem sizes. Empirical evaluation on synthetic matrix completion and real-world recommendation datasets shows consistent acceleration, though gains diminish on problems far from the training distribution. While our approach provides practical speedups for practitioners facing repeated low-rank recovery tasks, theoretical guarantees for the learned initializations remain limited, and computational overhead may outweigh benefits for single-instance problems.",
    "id": 1243
  },
  {
    "title": "Improving Transformer Efficiency Through Layer-wise Attention Compression",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "The quadratic complexity of self-attention in Transformers remains a fundamental bottleneck for processing long sequences. We propose Layer-wise Attention Compression (LAC), a simple modification to standard Transformer blocks that progressively compresses attention matrices across layers through low-rank approximations. Rather than applying compression globally, LAC learns layer-specific compression rates based on the evolving attention patterns observed during training. Our method requires minimal architectural changes and can be integrated into existing pre-trained models through fine-tuning. Experiments on language modeling and machine translation tasks show 1.3-2.1\u00d7 speedup during inference with less than 1.5% degradation in perplexity/BLEU scores compared to full attention baselines. While our approach achieves consistent efficiency gains, we observe that compression effectiveness varies significantly across tasks and sequence lengths, suggesting that optimal compression strategies may be task-specific. Analysis reveals that earlier layers benefit more from aggressive compression, while deeper layers require finer-grained attention patterns. Though LAC provides a practical trade-off between efficiency and performance, our theoretical analysis indicates fundamental limitations in achieving sub-quadratic attention without more substantial architectural modifications. Code and models will be released upon acceptance.",
    "id": 1244
  },
  {
    "title": "Gradient Descent with Polyak Step Size is Minimax Optimal for Linearly Separable Data",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We revisit the classical Polyak step size for gradient descent and prove that it achieves minimax optimal convergence rates for linear classification under separability conditions. While Polyak step sizes are known to be optimal for quadratic objectives, their role in modern machine learning optimization remains unclear. We establish that Polyak step sizes achieve O(1/T) convergence on logistic and exponential loss functions without requiring bounded iterates, a common assumption in prior work. Our analysis leverages a novel connection between the Polyak step size and implicit bias, showing that the iterates converge to the max-margin direction at an optimal rate. Experiments on standard datasets demonstrate improved convergence compared to fixed step sizes and adaptive methods like Adam, particularly for high-dimensional sparse features. However, our results are limited to linearly separable cases and convex losses, and we observe that performance degrades on noisy or overlapping datasets. This work suggests that classical step size rules may deserve reconsideration for modern overparameterized regimes.",
    "id": 1245
  },
  {
    "title": "Gradient Chaos: Noise-Enhanced Training as Implicit Regularization in Overparameterized Networks",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Recent work suggests that injecting carefully calibrated noise during neural network training can improve generalization, but theoretical understanding remains limited. We investigate whether gradient noise serves as an implicit regularizer in overparameterized models, focusing on classification tasks where traditional regularization is ineffective. Our approach analyzes training dynamics through the lens of stochastic differential equations, deriving bounds on the effective capacity under various noise injection schemes. We conduct extensive experiments on CIFAR-10/100 and ImageNet subsets, comparing noise-enhanced SGD against standard baselines across ResNet, Vision Transformer, and MLP architectures. Results show consistent improvements of 1-2% accuracy over baseline SGD in small-data regimes (\u226410k examples), while gains diminish or become negative with larger datasets. Notably, our method achieves these improvements without hyperparameter tuning beyond learning rate adjustment. However, we find that the theoretical bounds are loose in practice, and the empirical benefits appear to depend on architecture-specific interactions between noise injection and batch normalization. While our theoretical analysis provides new insights into the regularization effects of gradient noise, the results suggest limited practical impact for typical large-scale settings. This work advances understanding of implicit regularization but leaves open questions about mechanism and broader applicability.",
    "id": 1246
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Annealed Importance Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. We propose a novel gradient estimation method that combines annealed importance sampling (AIS) with control variates to reduce variance in the REINFORCE estimator. Our approach uses AIS to construct importance weights between the current policy and tempered versions of the target distribution, yielding more stable gradient estimates. We also introduce a learnable baseline computed via neural networks conditioned on the annealing path. Experimental results on binarized MNIST and text generation tasks show 15-30% reduction in gradient variance compared to vanilla REINFORCE, with modest improvements in final log-likelihood. While our method achieves competitive results on small-scale benchmarks, computational overhead scales poorly to larger models due to the sequential nature of AIS. On CIFAR-10 with discrete latent variables, training time increases by 4.5\u00d7 compared to standard baselines. Our method provides a theoretically grounded approach to gradient estimation but may be practical only for moderate-sized models where variance reduction is critical.",
    "id": 1247
  },
  {
    "title": "LayerNorm Without The Norm: Pre-Normalization Transformers via Learned Scaling Parameters",
    "authors": [
      "Chen, L.",
      "Vasudevan, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer architectures rely heavily on Layer Normalization (LayerNorm) for stable training, but this introduces computational overhead and potential representational bottlenecks. We propose LSFormer, a modification to the standard transformer block that replaces LayerNorm with learned scaling parameters derived from the residual stream's statistics. Our approach computes channel-wise scaling factors through a lightweight MLP conditioned on the input's mean and variance, eliminating the need for explicit normalization while maintaining training stability. We evaluate LSFormer on language modeling tasks using 125M parameter models trained on C4 and WikiText-103. Results show comparable perplexity to baseline transformers with a 7-12% reduction in training time on TPU-v4 hardware, though convergence is less stable across hyperparameter configurations. Ablations reveal that scaling parameters alone provide 80% of LayerNorm's benefits, suggesting additional mechanisms may be needed for full stability. While our method offers modest computational savings, the gains diminish at larger scales (1.3B parameters), and we observe 2-3% worse perplexity on out-of-domain data. These findings indicate learned scaling can partially replace LayerNorm in resource-constrained settings, but highlight the need for better understanding of normalization's role in transformer optimization dynamics.",
    "id": 1248
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization. While existing gradient surgery methods (PCGrad, GradNorm) aim to resolve these conflicts, they can be overly aggressive, leading to suboptimal shared representations. We propose Adaptive Gradient Harmonization (AdaGH), a novel approach that selectively applies gradient surgery based on task similarity scores computed via Hessian trace approximations. Our method dynamically adjusts the degree of gradient modification, performing less surgery when tasks are deemed compatible. On CIFAR-100 split into 5 tasks and NYUv2 semantic segmentation with depth estimation, AdaGH achieves 2.1% and 1.3% improvements over PCGrad respectively, while requiring 15% fewer gradient modifications. However, we observe that benefits diminish when task count exceeds 8 or when tasks become highly dissimilar. Our analysis reveals that excessive gradient surgery can harm performance on the highest-priority task, questioning the universality of aggressive conflict resolution strategies. While AdaGH shows promise in specific settings, its computational overhead and sensitivity to hyperparameters may limit practical adoption.",
    "id": 1249
  },
  {
    "title": "On the Robustness of Pre-trained Language Models to Distribution Shift via Selective Weight Reinitialization",
    "authors": [
      "Chen, L.",
      "Gonzalez, A.",
      "Kumar, S."
    ],
    "abstract": "Pre-trained language models (PLMs) have shown impressive performance across NLP tasks, yet their robustness under distribution shift remains a significant challenge. We propose Selective Weight Reinitialization (SWR), a simple post-training technique that improves robustness by strategically reinitializing a small subset of weights based on their gradient history during fine-tuning. Our method identifies weights that contribute most to overfitting on the source distribution and resets them to pre-trained values while preserving task-relevant knowledge. On 6 text classification datasets with natural distribution shifts (including news articles from different time periods and product reviews across domains), SWR achieves a 4-7% improvement in out-of-distribution accuracy compared to standard fine-tuning. While these gains are statistically significant, we find that SWR underperforms more sophisticated approaches like domain adversarial training in extreme distribution shifts (>30% label distribution change), and we identify failure cases when source and target domains differ substantially in vocabulary. Nevertheless, our method operates as a lightweight alternative requiring no additional data or hyperparameter tuning beyond the base PLM, making it practical for practitioners. These results suggest that selective reinitialization of overfitted parameters may be a overlooked but useful component in the robustness toolbox.",
    "id": 1250
  },
  {
    "title": "Improving Transformer Training Stability Through Layer-Wise Learning Rate Annealing",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Training instability remains a persistent challenge in deep Transformer architectures, particularly when scaling to hundreds of layers. While architectural modifications and normalization techniques dominate existing solutions, we propose a simple training-time intervention: layer-wise learning rate annealing (L-LRA) that progressively reduces learning rates for deeper layers during early training. Our method requires no architectural changes and can be implemented with three lines of code in standard frameworks. We validate L-LRA on Wikitext-103 and C4 language modeling benchmarks using GPT-2 and T5 architectures of various sizes (125M to 1.5B parameters). Experiments show 12-15% reduction in training loss variance across random seeds and improved convergence (0.7-1.2 ppl improvement) compared to baseline training, with particularly strong effects on deeper models (>48 layers). Analysis reveals that L-LRA effectively controls gradient norms during the initial chaotic phase of training, though we observe diminishing benefits for models under 512M parameters. While our approach improves training stability, downstream task performance gains remain modest (average 0.8% GLUE score improvement), and we note that adaptive optimizers already provide some of these stabilization effects. Our findings suggest that explicit layer-wise optimization strategies may be more relevant for training extremely deep Transformers than moderate-scale models.",
    "id": 1251
  },
  {
    "title": "Accelerated Gradient Descent via Adaptive Learning Rate Scaling with Quadratic Model Approximation",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Rodriguez, C.",
      "Thompson, A."
    ],
    "abstract": "We propose LASQ, a first-order optimization method that adaptively adjusts learning rates using local quadratic approximations without Hessian computations. LASQ maintains exponential moving averages of gradient norms to estimate local curvature, then scales the learning rate inversely proportional to this estimate. Unlike Adam-style methods that use gradient moments, our approach directly models the loss surface curvature through a lightweight quadratic surrogate updated at each step. We prove convergence rates for convex and non-convex objectives under standard assumptions, achieving O(1/T) and O(1/\u221aT) rates respectively. Experiments on CIFAR-10 and ImageNet show LASQ marginally outperforms SGD with hand-tuned schedules and matches AdamW on ResNet-50 (75.2% vs 75.4%) while using 15% fewer iterations. However, LASQ shows mixed results on transformer architectures and tasks with heavy regularization. Our ablations reveal the quadratic approximation degrades on highly non-stationary objectives. Code is available at [anonymous link].",
    "id": 1252
  },
  {
    "title": "Progressive Knowledge Distillation via Adaptive Temperature Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.K.",
      "Kumar, S."
    ],
    "abstract": "Knowledge distillation has emerged as a crucial technique for compressing large neural networks, yet existing methods rely on fixed temperature parameters that may not optimally transfer knowledge across different layer representations. We propose Progressive Knowledge Distillation (PKD), which employs layer-wise adaptive temperature schedules that dynamically adjust during training based on the Jensen-Shannon divergence between teacher and student activations. Our approach introduces a lightweight meta-network that predicts optimal temperatures for each layer at every training step, requiring only 0.3% additional parameters. We evaluate PKD on CIFAR-100 and ImageNet, achieving 0.8% and 0.4% accuracy improvements respectively over baseline distillation methods. While our adaptive schedules show consistent gains across architectures, we observe diminishing returns when teacher-student capacity gaps exceed two orders of magnitude. Theoretical analysis reveals that our method implicitly minimizes an upper bound on the expected generalization error, though we acknowledge limitations in our convergence proof for non-convex objectives. Ablation studies suggest that 70% of the improvement stems from early-layer temperature adaptation, raising questions about the necessity of full pipeline complexity. Code will be released upon publication.",
    "id": 1253
  },
  {
    "title": "Lookahead Normalization: Improving Transformer Training Through Gradient-Conditioned Layer Norm",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a simple modification to Layer Normalization (LayerNorm) that improves transformer training stability and convergence speed. Our key observation is that the traditional LayerNorm in transformers ignores future gradients during backpropagation, potentially leading to suboptimal normalization parameters. We introduce Lookahead Normalization (LaNorm), which computes LayerNorm parameters by conditioning on an approximation of future gradients using a lightweight auxiliary network. Specifically, LaNorm maintains a learned projection of the current layer's activations that predicts the gradient direction from subsequent layers, adjusting the gain and bias parameters accordingly. We evaluate LaNorm on standard NLP benchmarks (WMT'14 En-De, IWSLT De-En) and vision transformers on ImageNet. Results show modest improvements: 0.3-0.6 BLEU score gains on translation tasks and 0.4-0.8% top-1 accuracy improvements on ImageNet, while slightly reducing training time. However, ablation studies reveal that the benefits diminish with stronger baseline training procedures, and the computational overhead of the auxiliary network (5-8%) may not always justify the improvements. The method appears most effective for deeper networks (>24 layers) where normalization instability is more pronounced.",
    "id": 1254
  },
  {
    "title": "Gradient Descent with Memory: A Simple Augmentation for Improved Generalization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Memory-Augmented Gradient Descent (MAGD), a lightweight modification to standard SGD that incorporates a fixed-size memory of past gradients to guide optimization trajectories. The method maintains an exponential moving average of historical gradients with learnable decay rates, allowing the optimizer to escape sharp minima without requiring second-order information or additional hyperparameters. While the core idea bears similarity to existing momentum-based methods, our key insight is that explicitly storing and reusing gradient information from substantially earlier iterations can improve generalization, particularly in overparameterized models. We evaluate MAGD on CIFAR-10/100 and ImageNet classification tasks, showing consistent improvements over standard baselines (1-2% top-1 accuracy gains), with minimal computational overhead (<5% training time increase). However, the improvements are modest and task-dependent, failing to consistently outperform adaptive optimizers like AdamW on more challenging benchmarks. Our theoretical analysis provides convergence guarantees under standard assumptions, but the generalization benefits remain empirically observed rather than rigorously explained. Code is available at [URL].",
    "id": 1255
  },
  {
    "title": "Gradient Norm Clipping Improves Transformer Training Stability by Implicit Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient norm clipping is a widely adopted heuristic for stabilizing transformer training, yet its theoretical underpinnings remain poorly understood. We provide empirical evidence that clipping acts as an implicit regularizer by constraining the effective Lipschitz constant of the network. Our experiments on language modeling and machine translation tasks show that clipped transformers achieve 2-3% better perplexity and exhibit 40% smaller gradient variance compared to unclipped baselines. While we establish a connection between clipping strength and implicit bias similar to weight decay, our theoretical analysis is limited to simplified linear settings that may not fully capture transformer dynamics. Code and hyperparameters will be released.",
    "id": 1256
  },
  {
    "title": "Gradient Descent Variants for Federated Learning: A Systematic Evaluation of Local Adaptation Strategies",
    "authors": [
      "Kim, S.",
      "Rodriguez, L.",
      "Chen, J.",
      "Anderson, M."
    ],
    "abstract": "Federated learning faces a fundamental tension between global convergence and local adaptation. While prior work explores specialized optimizers, extensive evaluation of standard gradient descent variants remains lacking. We systematically study how local adaptation strategies within vanilla SGD variants affect federated performance across 8 datasets and 3 model architectures. Our key finding is that simple momentum-based local updates combined with cosine annealing schedules achieve competitive accuracy (within 2% of FedAdam) while requiring 40% fewer communication rounds. However, performance degrades significantly in heterogeneous settings (accuracy drops 15-20% on non-IID splits). We propose a lightweight client-side learning rate modulation scheme based on gradient norm ratios that recovers 60% of the lost performance with minimal overhead. While our approach provides practical benefits for moderately heterogeneous federated settings, theoretical analysis reveals convergence guarantees only under restrictive assumptions. Our extensive empirical study (1500 GPU hours) offers actionable guidelines for practitioners, though we acknowledge limitations in extreme heterogeneity regimes and memory-constrained devices.",
    "id": 1257
  },
  {
    "title": "Improving Transformer Efficiency through Selective Attention Sparsification with Learnable Temperature",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, J."
    ],
    "abstract": "Transformer models achieve state-of-the-art results across many domains but suffer from quadratic complexity in sequence length due to their attention mechanisms. While previous work has explored sparse attention patterns to reduce computation, these approaches typically require manual design or heuristic rules that may not adapt well to different tasks. We propose a novel method that learns sparse attention patterns through a differentiable temperature parameter that controls the sparsity of the softmax during training. Our approach automatically identifies which attention heads and positions to keep dense versus sparse during training, achieving up to 2.1x speedup on standard benchmarks with minimal performance degradation (0.3-1.2% accuracy drop on GLUE tasks). We validate our method on both language modeling and vision transformers, showing consistent improvements over fixed sparsity patterns. While our results demonstrate practical benefits for deployment in resource-constrained settings, we observe that the learned sparsity patterns can be unstable across different random seeds and sometimes exhibit peculiar layer-wise sparsity distributions. Our code and pre-trained models are available at [redacted].",
    "id": 1258
  },
  {
    "title": "Practical Improvements to Gradient Noise Injection for Differentially Private Deep Learning",
    "authors": [
      "Chaudhari, P.",
      "Kwon, J.",
      "Zhou, S."
    ],
    "abstract": "While differential privacy (DP) offers strong theoretical guarantees for training neural networks, existing approaches often suffer from significant accuracy degradation relative to non-private baselines. We identify that the standard practice of adding isotropic Gaussian noise to gradients is particularly harmful for convolutional layers, where parameter norms vary dramatically across filters. Building on this observation, we propose Layer-Adaptive Gradient Perturbation (LAGP), which rescales noise according to per-layer sensitivity estimates computed via a lightweight online procedure. Additionally, we introduce Cyclic Gradient Clipping (CGC), a simple modification to standard clipping that reduces bias introduced during training. Our method achieves 94.2% accuracy on CIFAR-10 with (\u03b5=2.0, \u03b4=10^-5)-DP, improving over the baseline DP-SGD by 2.1% while maintaining the same privacy budget. Experiments on additional benchmarks including FMNIST and CIFAR-100 demonstrate consistent but modest gains. While our contributions are empirical and incremental, they suggest that careful engineering of existing DP techniques can yield practical benefits for privacy-preserving machine learning without requiring fundamentally new mechanisms.",
    "id": 1259
  },
  {
    "title": "Gradient Mixup: Improving Model Robustness Through Convex Interpolation of Parameter Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Mixup, a simple regularization technique that interpolates between consecutive gradient updates during training to improve model robustness. Inspired by the success of input mixup for data augmentation, our method computes weighted combinations of past and present gradients, effectively smoothing the optimization trajectory. We prove that under L-smoothness assumptions, Gradient Mixup provides a convergence guarantee of O(1/\u221aT) for non-convex objectives while reducing gradient variance by up to 30%. Experiments on CIFAR-10 and ImageNet show consistent improvements in robustness to label noise (+2.1% accuracy under 20% noise) and adversarial perturbations (+1.3% robust accuracy), with minimal computational overhead. While the theoretical analysis relies on restrictive assumptions that may not hold in practice, and improvements over strong baselines like SAM remain modest (+0.4% average), Gradient Mixup offers a plug-and-play alternative that requires no hyperparameter tuning beyond the standard learning rate. Code is available at anonymous-url.",
    "id": 1260
  },
  {
    "title": "LayerNorm Alternatives for Transformer Architectures via Learnable Affine Transformations",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Layer Normalization (LayerNorm) has become a standard component in transformer architectures, but its computational cost and potential training instabilities motivate the search for alternatives. We propose Learnable Affine Normalization Transform (LANT), a drop-in replacement for LayerNorm that uses learned affine transformations and scaled residual connections to maintain training stability while reducing compute. Our method eliminates the need for calculating mean and variance across feature dimensions, instead relying on element-wise learnable scale and shift parameters that adapt during training. We evaluate LANT on standard language modeling tasks (WikiText-103, OpenWebText) and machine translation benchmarks (WMT'14 EN-DE). Results show LANT achieves comparable perplexity to LayerNorm (-0.8% on WikiText-103) while reducing training time by 12-15%. However, performance degrades on longer sequences (>2048 tokens), and we observe increased gradient norm variance in deeper models (>48 layers). Analysis reveals LANT works best for medium-scale models (\u2264350M parameters) but struggles with larger architectures. While our contribution is primarily empirical and the theoretical justification remains incomplete, LANT provides a practical alternative for resource-constrained training scenarios where minor accuracy loss is acceptable for improved efficiency.",
    "id": 1261
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Dynamic Low-Rank Approximation with Adaptive Budget Allocation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a standard approach for parameter-efficient fine-tuning of large language models, but its fixed rank allocation across layers may be suboptimal. We propose LoRA-Lite, a method that dynamically adjusts the rank of low-rank matrices during fine-tuning based on layer-wise importance scores derived from gradient covariance. Our approach employs an online learning algorithm that redistributes the parameter budget across layers to maximize task performance while respecting memory constraints. On the GLUE benchmark, LoRA-Lite achieves comparable performance to standard LoRA (average score 83.2 vs 82.7) while using 35% fewer parameters. Experiments on 7B and 13B parameter models show memory reductions of 20-40% with minimal degradation on downstream tasks. However, we observe that the dynamic allocation sometimes converges to suboptimal local minima for certain task types, particularly those requiring complex reasoning. While LoRA-Lite provides practical memory savings, the performance gains are modest and task-dependent, suggesting that more sophisticated rank adaptation strategies may be needed. Code and models will be released upon acceptance.",
    "id": 1262
  },
  {
    "title": "LoRA-Fusion: Adaptive Low-Rank Adaptation with Gradient-Based Fusion for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Garcia, A."
    ],
    "abstract": "Fine-tuning large language models remains computationally expensive, prompting the adoption of parameter-efficient methods like LoRA. However, LoRA's fixed-rank matrices and static scaling factors constrain its adaptability across downstream tasks. We propose LoRA-Fusion, a simple extension that learns to dynamically adjust the rank and fusion weights of LoRA matrices based on gradient statistics during training. Our method computes low-cost approximations of the Hessian trace to identify under-utilized singular values in the rank decomposition, allowing for adaptive pruning and regrowth of LoRA parameters. Experiments on GLUE and SuperGLUE benchmarks with 7B and 13B parameter models show 2-4% improvements over standard LoRA while using 15-30% fewer parameters. However, gains diminish on larger models (30B+) and longer fine-tuning schedules, suggesting current limits to our adaptive approach. The method adds minimal computational overhead (<5% increase in training time) and integrates seamlessly with existing LoRA implementations. While LoRA-Fusion provides consistent improvements for mid-scale models, the benefits appear task-specific and may not justify the added complexity for all applications.",
    "id": 1263
  },
  {
    "title": "Improving Transformer Performance Through Selective Attention Sparsification During Training",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Rodriguez, A."
    ],
    "abstract": "While numerous techniques exist for making transformers more efficient at inference time, little work has explored whether structured sparsity imposed during training can improve both efficiency and final model quality. We propose Gradual Attention Masking (GAM), a simple training-time regularization technique that progressively masks attention weights based on learned importance scores. Our method adds minimal computational overhead and can be integrated into existing transformer architectures without architectural modifications. On the GLUE benchmark, GAM achieves comparable performance to standard transformers while reducing attention FLOPs by 27%. However, we observe that these gains are less pronounced on larger models, with our 340M parameter model showing only 3% FLOP reduction. Additionally, while GAM improves training efficiency on language modeling tasks, we find the benefits do not transfer robustly to downstream tasks without task-specific hyperparameter tuning. Our experiments suggest that while selective sparsification during training offers practical efficiency improvements for small-to-medium models, the approach faces scalability challenges and sensitivity to hyperparameter choices that limit its broader applicability.",
    "id": 1264
  },
  {
    "title": "Rethinking Mixup: A Frequency-Domain Analysis with Limited Label Augmentation",
    "authors": [
      "Chen, L.",
      "Vasquez, J.",
      "Kumar, S."
    ],
    "abstract": "Mixup has emerged as a simple yet effective data augmentation technique for improving generalization in deep neural networks. However, its theoretical understanding remains limited, particularly regarding how linear interpolation in the input space affects feature learning. We provide a novel analysis of Mixup through the lens of frequency-domain perturbations, revealing that the augmentation primarily enriches mid-frequency components while suppressing high-frequency details. Building on this insight, we propose SelectiveMixup, which applies Mixup only to samples with high-confidence pseudo-labels from a teacher model, reducing label noise while maintaining augmentation benefits. Our experiments on CIFAR-10/100 and ImageNet subsets show 1.2-2.3% accuracy improvements over standard Mixup with 30-50% fewer augmented samples. While our theoretical contributions are incremental and our method requires additional computational overhead for teacher model training, we believe our frequency-domain perspective offers useful insights for understanding interpolation-based augmentations. Code is available at anonymous-url.",
    "id": 1265
  },
  {
    "title": "Gradient Descent with Dynamic Step Sizes: A Unified Analysis Through Curvature-Adaptive Bounds",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "We present a theoretical analysis of gradient descent variants that adjust step sizes based on local curvature estimates. While adaptive optimizers like Adam and AdaGrad have shown strong empirical performance, their theoretical guarantees remain disconnected from practical success. We introduce a new framework for analyzing gradient methods through the lens of curvature-adaptive upper bounds, unifying several seemingly disparate approaches including classical backtracking line search, momentum methods, and recent adaptive algorithms. Our key insight is that many popular optimizers implicitly satisfy a curvature-dependent descent lemma, allowing us to derive convergence rates that depend on both the smoothness constant and an adaptive curvature measure. We prove that under mild assumptions, our unified framework achieves O(1/T) convergence for convex objectives and O(1/\u221aT) for non-convex problems. Empirical validation on standard benchmarks (CIFAR-10 and WMT'14) shows modest improvements over baselines (1-2% accuracy gains), but reveals interesting failure modes when curvature estimates are noisy. While our theoretical contributions clarify the behavior of known algorithms rather than proposing fundamentally new ones, we believe this perspective will inform the design of future adaptive methods.",
    "id": 1266
  },
  {
    "title": "Per-Sample Gradient Perturbation: A Lightweight Framework for Differentially Private Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Differentially private (DP) stochastic optimization often relies on gradient clipping and noise addition, which can severely impact model utility, particularly for overparameterized models. We propose a simple alternative: instead of clipping gradients to bound sensitivity, we directly perturb each sample's gradient using Gaussian noise scaled by gradient norms. This approach eliminates the bias introduced by clipping while maintaining rigorous (\u03b5,\u03b4)-DP guarantees. Our theoretical analysis shows that the privacy-utility trade-off is competitive with existing DP-SGD variants under standard assumptions. Empirically, we demonstrate improved accuracy on CIFAR-10 and ImageNet compared to DP-SGD baselines, with less hyperparameter tuning. However, our method introduces computational overhead due to per-sample gradient computation, and we observe diminishing benefits on smaller models. While this work provides a practical alternative to clipping in DP optimization, we acknowledge limitations in scaling to extremely large models and leave open questions regarding optimal noise calibration. Our code is available as a PyTorch extension.",
    "id": 1267
  },
  {
    "title": "Gradient Surgery in Looped Transformers: A Frequency Domain Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We study the training dynamics of looped transformers through the lens of frequency domain analysis. While looped architectures have shown promise in reducing parameter count by weight sharing across layers, their optimization landscape remains poorly understood. We propose a simple modification to the standard transformer block that uses learnable frequency-domain filters to guide gradient flow, enabling more stable training of deep loops (50+ iterations) without gradient explosion. Our theoretical analysis shows that under certain initialization conditions, this frequency-aware weight sharing preserves a subset of useful frequency components during backpropagation, though our bounds depend on strong assumptions about the data distribution. Empirically, we evaluate our approach on language modeling tasks using Wikitext-103 and a subset of C4. Our experiments demonstrate 2-3% perplexity improvements over standard looped baselines at depths where optimization typically fails, though gains diminish in shallower loops. Notably, our method requires careful tuning of two additional hyperparameters (filter bandwidth and damping coefficient), and we observe sensitivity to initialization scales not predicted by theory. While our approach shows potential for scaling looped architectures, the computational overhead of frequency transforms and the limited scope of evaluation raise questions about broader applicability.",
    "id": 1268
  },
  {
    "title": "Gradient Surgery in Federated Learning: When Adaptive Federated Optimization Meets Gradient Compression",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Federated learning faces challenges from client drift and communication constraints when dealing with heterogeneous data distributions. While adaptive optimizers like FedAdam and FedProx improve convergence, combining them with gradient compression techniques remains poorly understood. We propose FedGradSurg, a simple modification to federated adaptive methods that applies targeted gradient clipping before quantization. Our approach identifies and removes gradient components that are likely to cause divergence under compression, based on their alignment with global model updates. Through experiments on CIFAR-10 and Stack Overflow datasets with Non-IID partitioning, we demonstrate 12-15% faster convergence compared to standard compressed FedAdam while maintaining the same compression ratio. Theoretical analysis shows convergence under generalized smoothness assumptions, though the bound depends on a new compression-similarity coefficient that lacks intuitive interpretation. Our results suggest that careful gradient preprocessing can improve compressed federated training, but the benefits are dataset-dependent and diminish with more aggressive compression. While our contributions are incremental, the framework provides a practical way to combine two orthogonal federated learning techniques.",
    "id": 1270
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Closer Look at Task Balancing via Directional Derivatives",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Directional Gradient Balancing (DGB), a simple modification to existing gradient surgery methods for multi-task learning that uses directional derivatives to dynamically reweight conflicting gradients. While previous approaches such as PCGrad and GradVac rely on heuristic clipping or gradient norms, DGB computes the immediate impact of each task's gradient on other tasks' loss landscapes, allowing for more principled conflict resolution. Our method introduces minimal computational overhead (0.3% relative to base training) and improves performance on three standard benchmarks: NYUv2 (1.4% mIoU improvement), CelebA (0.7% accuracy increase), and a synthetic regression setting. However, we find that the benefits diminish as model capacity increases, and rare failure cases exist where DGB destabilizes training. Through controlled experiments, we show that much of DGB's effectiveness can be replicated by carefully tuned gradient clipping thresholds. Our results suggest that while directional derivatives provide a theoretically grounding for gradient balancing, the practical gains over existing methods remain marginal. Code is available at anonymous-link.github.io.",
    "id": 1271
  },
  {
    "title": "LoRA-Drop: Structured Pruning for Parameter-Efficient Fine-Tuning via Learnable Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but introduced adapters often contain redundant parameters that increase memory footprint without improving performance. We propose LoRA-Drop, a simple yet effective method to identify and prune low-importance adapter weights during fine-tuning. Our approach learns sparse masks over LoRA's rank decomposition using a differentiable binary relaxation, achieving structured pruning at the level of individual rank components. By incorporating Fisher information as a regularization term, we preserve model expressiveness while reducing parameter count. Experiments on GLUE and E2E benchmarks show LoRA-Drop prunes 25-40% of adapter parameters with <1% accuracy loss across tasks, outperforming magnitude-based baselines. While our method achieves consistent compression, we observe diminishing returns on larger models (>7B parameters) and tasks requiring minimal LoRA ranks. Our implementation requires minimal code changes to existing LoRA pipelines. LoRA-Drop represents a practical approach to reducing adapter overhead, though our theoretical analysis of rank selection criteria remains incomplete.",
    "id": 1272
  },
  {
    "title": "Gradient Surgery in Overparameterized Neural Networks: When Does Layer-wise Learning Rate Adaptation Help?",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate the effectiveness of layer-wise gradient scaling in training overparameterized networks. Inspired by empirical observations that certain layers contribute disproportionately to the loss landscape curvature, we propose a simple modification to SGD that applies layer-specific learning rates based on gradient norms. Our method requires minimal hyperparameter tuning and adds negligible computational overhead. Across CIFAR-10/100 and ImageNet subsets, we observe 1-3% accuracy improvements over vanilla SGD with momentum, particularly for deeper architectures (50+ layers). However, benefits diminish with proper learning rate warmup schedules, suggesting the method primarily compensates for suboptimal hyperparameter choices. Analysis reveals the technique effectively stabilizes early training dynamics but may hinder generalization by prematurely constraining certain parameter directions. While our results do not establish clear superiority over existing adaptive methods like AdamW, they offer insights into layer-wise optimization dynamics. Code and pre-trained models are available at anonymous.url.",
    "id": 1273
  },
  {
    "title": "Momentum with Layer-wise Learning Rates: A Modular Approach to Neural Network Optimization",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, A."
    ],
    "abstract": "We propose a simple modification to standard momentum-based optimizers that uses layer-wise learning rates derived from local curvature estimates. While recent work suggests per-parameter adaptive methods provide faster convergence, we show that a coarser layer-wise approach can achieve comparable performance with reduced memory overhead. Our method combines standard SGD momentum with layer-wise step sizes computed using diagonal approximations of the Fisher information matrix. Through experiments on CIFAR-10, CIFAR-100, and ImageNet, we demonstrate modest improvements (0.5-1.2% accuracy gains) over baseline SGD and AdamW, particularly for ResNet architectures. The approach introduces minimal computational cost (<3% training overhead) and requires only a single hyperparameter per layer group. However, we find performance gains to be inconsistent across tasks and architectures, with minimal benefits on Vision Transformers and language modeling tasks. The method is most effective for medium-scale vision models where simpler hyperparameter tuning can be beneficial. Our implementation is available at anonymous.link.",
    "id": 1274
  },
  {
    "title": "Revisiting Momentum Through the Lens of Adaptive Learning Rates",
    "authors": [
      "Chen, L.",
      "Narang, S.",
      "Garc\u00eda, M."
    ],
    "abstract": "We propose Adaptive Momentum Correction (AMC), a simple modification to standard momentum-based optimizers that adapts the momentum coefficient based on the gradient history. Motivated by the observation that high momentum values can cause overshooting in steep loss surface regions, AMC scales the momentum coefficient inversely with a running average of gradient norms. The method adds only 3-5% computational overhead compared to standard momentum and is compatible with existing optimizers. Experimental evaluation on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures shows 0.5-1.2% accuracy improvements over SGD with momentum, particularly in low-data regimes. However, gains are marginal compared to more sophisticated adaptive methods like AdamW, and we observe increased sensitivity to hyperparameter selection. Ablation studies reveal that the effectiveness of AMC heavily depends on the batch size and network architecture. While AMC provides modest improvements in some settings, the contribution is incremental rather than transformative. We release PyTorch code and pre-trained models to ensure reproducibility.",
    "id": 1275
  },
  {
    "title": "Gradient Surgery with Memory: A Simple Baseline for Multi-Task Learning in Deep Networks",
    "authors": [
      "Kim, J.",
      "Chen, S.",
      "Rodriguez, L."
    ],
    "abstract": "Multi-task learning in deep networks often suffers from conflicting gradients that hinder performance across tasks. We propose Gradient Surgery with Memory (GSM), a lightweight extension to existing gradient surgery methods that incorporates an episodic memory buffer to store historical gradient information. Our method computes task gradients as convex combinations of current and stored gradients, guided by a simple cosine similarity threshold. This allows GSM to automatically identify when tasks are complementary versus conflicting while maintaining the computational efficiency of single-task training. Experiments on standard benchmarks including NYUv2, CIFAR-100, and Office-Home show modest but consistent improvements over PCGrad and GradNorm, with GSM achieving average gains of 1.2% on semantic segmentation and 0.8% on classification tasks. However, performance degrades with more than 4 tasks, and we observe high sensitivity to the memory buffer size hyperparameter. While GSM represents a practical baseline for multi-task learning, our ablations suggest the memory component adds limited value beyond well-tuned gradient surgery alone.",
    "id": 1276
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum-Sensitive Thresholding for Non-Stationary Optimization",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, J."
    ],
    "abstract": "Gradient clipping is widely used to stabilize neural network training, particularly in language modeling and reinforcement learning. However, fixed clipping thresholds can be suboptimal when optimization landscapes change across training phases. We propose Momentum-Adaptive Gradient Clipping (MAGC), which dynamically adjusts clipping thresholds based on exponential moving averages of gradient norms and update magnitudes. Our method introduces a lightweight threshold adjustment mechanism that responds to both short-term gradient spikes and long-term optimization trends. We evaluate MAGC on three standard benchmarks: language modeling on WikiText-103, policy gradient methods on MuJoCo continuous control, and vision transformers on ImageNet. Results show 2-7% improvement over standard clipping in final validation perplexity and 3-4% faster convergence for RL tasks, while maintaining computational overhead below 2%. However, benefits diminish for well-tuned optimizers like AdamW with appropriate weight decay. Ablation studies reveal that the momentum-sensitive component contributes 60-70% of improvements, while the gradient norm tracking provides marginal gains. Our approach is easy to implement (15 lines of PyTorch code) and integrates with existing optimizers, but we acknowledge that improvements are task-dependent and may not justify additional hyperparameters for practitioners with sufficient tuning budgets.",
    "id": 1277
  },
  {
    "title": "Revisiting Warm-Started Adam: A Cautionary Tale for Transfer Learning in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We investigate the common practice of warm-starting Adam optimizer parameters (\u03b21, \u03b22) when fine-tuning pre-trained models. While previous work has shown benefits of reusing momentum terms from source tasks, theoretical analysis remains limited. We prove that warm-starting can lead to suboptimal convergence when the target task's gradient statistics diverge significantly from the source task, characterized by a worst-case \u03a9(log T) slowdown. Empirically, we find this affects 23% of transfer learning scenarios in vision and NLP benchmarks. To address this, we propose an inexpensive detection mechanism based on gradient variance ratios, achieving 87% accuracy in predicting when re-initialization is beneficial. On ImageNet \u2192 CIFAR-10 and GLUE benchmark tasks, this yields 1.3-2.1% average improvements over naive warm-starting. However, our method shows limited gains on medical imaging and speech tasks, suggesting domain-specific factors may dominate. While our theoretical contributions provide new insights for practitioners, the empirical benefits remain modest and task-dependent. We release a lightweight PyTorch extension for adaptive optimizer initialization, requiring <20 lines of code to integrate.",
    "id": 1278
  },
  {
    "title": "Gradient Descent with Adaptive Momentum Estimation via Online Learning of Learning Rates",
    "authors": [
      "Liu, J.",
      "Krishnan, S.",
      "Chen, L."
    ],
    "abstract": "We propose GAME (Gradient Adaptive Momentum Estimation), a first-order optimization method that adaptively tunes both learning rates and momentum parameters via online learning. While adaptive optimizers like Adam adjust learning rates using second moment estimates, momentum parameters are typically fixed, leaving potential acceleration on the table. GAME treats the momentum coefficient as an online learning problem within the optimizer's state, using past gradient correlations to update it at each step. Theoretically, we show GAME achieves convergence rates matching SGD for convex problems while providing empirical benefits on non-convex objectives. Our experiments on CIFAR-10/100 and ImageNet show GAME improves validation accuracy by 0.5-1.2% over baselines on ResNet-18/50, though gains shrink with careful hyperparameter tuning. On language modeling tasks, we observe mixed results: 2-3% perplexity improvements on Wikitext-103 but no benefit on larger datasets. Ablation studies reveal momentum adaptation contributes most in early training phases. While our approach is simple to implement and adds minimal overhead, we acknowledge limitations: the theoretical analysis relies on bounded gradient assumptions, and hyperparameter sensitivity persists. We release PyTorch code and hope this work encourages further research on adaptive momentum methods.",
    "id": 1279
  },
  {
    "title": "Gradient Surgery Revisited: When Does Gradient Interference Actually Hurt?",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between objectives, leading to suboptimal performance. While various gradient surgery methods have been proposed to address this issue, we observe that existing techniques lack a principled understanding of when gradient interference is actually harmful for training. In this work, we introduce a simple diagnostic tool based on per-task Hessian curvature that predicts when gradient conflict will slow convergence. Using this criterion, we develop an adaptive method that selectively applies gradient modification only in problematic parameter subspaces. Experiments on synthetic benchmarks and three NLP tasks show our approach matches or improves upon state-of-the-art gradient surgery methods, while being computationally cheaper. However, we find the benefits are most pronounced in specific regimes: when task gradients are nearly orthogonal and the Hessians have vastly different condition numbers. Our theoretical analysis suggests these advantages may not generalize beyond these scenarios. Code will be released upon acceptance.",
    "id": 1280
  },
  {
    "title": "Momentum-Aware Gradient Pruning for Memory-Efficient Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Lee, J.",
      "Kim, S."
    ],
    "abstract": "Deploying large-scale transformer models is hampered by the memory requirements of gradient storage during backpropagation. We present Momentum-Aware Gradient Pruning (MAGPIE), a simple algorithm that selectively drops gradient entries during training based on their historical momentum values. Unlike existing compression schemes that uniformly quantize or sparsify gradients, MAGPIE leverages the observation that momentum terms correlate with gradient importance, enabling 4-8x memory reduction without modifying the forward pass. Our method adds minimal computational overhead (3% increase in training time) and integrates easily into existing PyTorch/TensorFlow training pipelines. On GLUE benchmarks, MAGPIE matches the accuracy of full-precision training within 0.3% while using 70% less memory for gradients. In language modeling experiments (GPT-2 medium), we achieve 6.2x compression at the cost of 1.2% perplexity increase. While our results demonstrate clear memory savings for standard training setups, we find the benefits diminish when combined with mixed-precision training or gradient checkpointing. Theoretical analysis suggests this performance matches information-theoretic lower bounds for gradient compression, though extending our guarantees to adaptive optimizers remains an open challenge.",
    "id": 1281
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Structured Sparsity for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.K.",
      "Kim, S.",
      "Johnson, A.B."
    ],
    "abstract": "We present LoRA-Drop, a method for selectively applying low-rank adaptation modules in pre-trained language models based on a learned sparsity pattern. While Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning technique, we observe that not all layers benefit equally from rank decomposition. We introduce a differentiable gating mechanism that learns to identify and skip layers where LoRA provides minimal utility, reducing trainable parameters by 30-50% without significant performance degradation. Our approach combines magnitude-based pruning with a reinforcement learning component that explores different sparsity patterns across transformer layers. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop achieves comparable accuracy to standard LoRA (within 1.2% average F1) while maintaining the computational benefits of parameter-efficient tuning. However, we find the method is sensitive to initialization strategies and shows particular instability on smaller datasets. Ablations reveal that our sparsity decisions correlate weakly with standard layer attributions, suggesting the learned patterns may be capturing optimization dynamics rather than inherent layer importance. While LoRA-Drop demonstrates the potential for adaptive parameter-efficient methods, the current implementation introduces hyperparameter complexity that may limit practical adoption.",
    "id": 1282
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Head Pruning with Layer-Wise Gradient Similarity",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "While Transformer models achieve state-of-the-art performance across many tasks, their computational cost remains prohibitive for deployment on resource-constrained devices. We propose a simple yet effective attention head pruning method that uses layer-wise gradient similarity to identify redundant heads during fine-tuning. Our approach computes the cosine similarity between gradients of different attention heads within each layer and removes heads whose gradients are most similar to others. Extensive experiments on GLUE and SQuAD show that our method achieves 15-20% FLOP reduction with minimal performance loss (within 1% of full model accuracy). While similar to existing magnitude-based pruning, our gradient similarity approach captures functional redundancy more accurately than weight magnitude alone. However, our method shows diminishing returns on larger models (>1B parameters) and requires full training data for gradient computation. Code will be made available.",
    "id": 1283
  },
  {
    "title": "Gradient Reversal with Momentum: A Simple Baseline for Unsupervised Domain Adaptation in Small-Scale Vision Tasks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J.",
      "Thompson, S."
    ],
    "abstract": "Unsupervised domain adaptation remains challenging for small-scale vision tasks where pre-trained features are less effective. We propose Gradient Reversal with Momentum (GRM), a lightweight adaptation of the gradient reversal layer that incorporates momentum-based optimization and adaptive weighting of the adversarial loss. Unlike complex state-of-the-art methods requiring careful hyperparameter tuning and architectural modifications, GRM can be implemented in under 20 lines of PyTorch code. Our experiments on three benchmark datasets (Office-31, Office-Home, and VisDA) show that GRM achieves 2-4% improvement over vanilla gradient reversal, matching the performance of several recent domain adaptation methods while using 50% fewer parameters. However, we find that GRM's gains diminish on larger-scale datasets, suggesting limitations in handling severe domain shifts. Our analysis reveals that the momentum component primarily stabilizes training rather than providing meaningful domain-invariant features. While GRM offers a useful baseline for practitioners, its theoretical justification remains heuristic and the method appears most beneficial when computational constraints preclude more sophisticated approaches. Code is available at anonymized.",
    "id": 1284
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Simple Recipe for Improving Generalization in Deep Neural Networks",
    "authors": [
      "Liu, H.",
      "Kim, J.",
      "Rodriguez, C."
    ],
    "abstract": "We propose Sharpness-Aware Gradient Surgery (SAGS), a simple modification to existing gradient-based optimizers that combines gradient projection techniques with sharpness minimization to improve generalization in deep networks. While recent work suggests conflicting gradients between loss minimization and sharpness reduction objectives in multi-task settings, we empirically observe similar interference even in single-task scenarios. SAGS addresses this by performing orthogonal projection of gradients onto the subspace perpendicular to the sharpness gradient direction, effectively decoupling these objectives. Our method requires only minimal computational overhead (\u22485% increase in training time) and can be implemented with ~20 lines of PyTorch code. Experiments on CIFAR-10, CIFAR-100, and ImageNet show consistent improvements over baseline optimizers, with SAGS achieving +0.8%, +1.2%, and +0.6% accuracy gains respectively. However, we note performance degrades on some architectures (notably Vision Transformers), and our theoretical analysis provides only loose generalization bounds. Ablations reveal that the sharpness regularization term contributes most to improvements, while gradient surgery effects are more modest. These results suggest SAGS offers a practical but incremental advance in optimizer design, with clear benefits in some regimes but limited scope of applicability.",
    "id": 1285
  },
  {
    "title": "Depth-Adaptive Normalization: A Simple Fix for Distribution Shifts in Batch Normalized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We observe that the effectiveness of Batch Normalization (BN) diminishes progressively with network depth, particularly when encountering distribution shifts between training and testing data. To address this, we propose Depth-Adaptive Normalization (DAN), which modulates the BN scaling parameters based on layer-wise statistics accumulated during training. Our approach introduces minimal computational overhead, adding only 0.1% additional parameters. Experimental results show DAN achieves 2-3% accuracy improvements on CIFAR-10/CIFAR-100 with ResNet-50 under common corruptions, and moderate gains on ImageNet-C. However, benefits vary significantly across architectures\u2014while ResNets show consistent improvements, Vision Transformers exhibit minimal gains. Theoretical analysis reveals DAN approximately minimizes an upper bound on the population risk under covariate shift, though the bound is loose for deeper networks. Our code is available but requires significant adaptation for different architectures. While DAN offers a practical improvement for specific scenarios, the approach may be overly specialized to certain architectural inductive biases.",
    "id": 1286
  },
  {
    "title": "Gradient Surgery Doesn't Always Help: An Empirical Study of Multi-Task Optimization in Moderate-Scale Settings",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Multi-task learning promises improved sample efficiency through shared representations, yet practitioners often observe minimal gains over independent training. While recent theoretical work has highlighted gradient conflicts as a key bottleneck, the practical efficacy of gradient surgery methods like PCGrad and GradDrop remains unclear. We conduct the first large-scale empirical study of gradient surgery techniques across 15 vision-language datasets with 30,000 multi-task configurations. Surprisingly, we find that vanilla gradient averaging achieves within 2.1% of the best surgical method in 87% of settings, suggesting that previously reported benefits may be limited to carefully curated benchmarks. Our analysis reveals that gradient surgery helps most when task-specific gradients are highly misaligned (cosine similarity < -0.2), but such conditions occur in only 23% of randomly sampled configurations. Evaluating on five downstream applications including medical image segmentation and robot navigation, we demonstrate that gradient surgery provides statistically significant improvements only when the primary and auxiliary tasks exhibit strong negative correlation. While these findings challenge the ubiquity of gradient surgery benefits, they offer practical guidance: simple gradient cosine pre-screening can predict when to apply surgical methods, reducing compute by 40% with minimal accuracy loss.",
    "id": 1287
  },
  {
    "title": "Efficient Gradient Boosting Through Layer-wise Feature Recycling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient boosting methods often suffer from computational bottlenecks when dealing with high-dimensional data, as each boosting round requires processing all features. We propose Layer-wise Feature Recycling (LFR), a simple technique that maintains and reuses informative feature subsets across boosting iterations. Our method dynamically identifies the most predictive features at each layer and propagates them to subsequent boosting rounds, reducing computation by 30-50% while maintaining comparable accuracy to standard approaches. Experiments on 12 tabular datasets from the OpenML benchmark suite show LFR achieves within 2% of XGBoost accuracy while using 40% less training time. However, we observe diminishing returns on sparse high-dimensional data, where feature overlap between rounds is naturally limited. Theoretical analysis reveals LFR provides an \u03b5-approximate solution under mild conditions on feature correlation structure. While our method shows consistent improvements on medium-scale datasets, thorough evaluation on larger benchmarks remains future work. Code and datasets are available at [url].",
    "id": 1288
  },
  {
    "title": "Block-Diagonal Attention for Efficient Language Model Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, V."
    ],
    "abstract": "We propose Block-Diagonal Attention (BDA), a simple modification to standard self-attention mechanisms that reduces computational complexity from O(n\u00b2) to O(n\u00b2/k) by partitioning sequences into k blocks with restricted attention patterns. Our key insight is that during fine-tuning, most information transfer occurs within local contexts, making full attention unnecessary. BDA introduces a learnable gating mechanism to softly combine block-level representations with minimal additional parameters. On GLUE and SuperGLUE benchmarks, BDA achieves 97.4% of full attention performance while reducing training time by 28% and memory usage by 35%. However, we observe a 2-3% performance drop on tasks requiring long-range dependencies like document-level sentiment analysis. Our method provides a practical speed-accuracy trade-off particularly useful when fine-tuning large pre-trained models on single GPUs. While not universally superior to standard attention, BDA offers consistent benefits for moderate-length sequences (\u22641024 tokens) typical in many downstream applications. Code is available anonymously.",
    "id": 1289
  },
  {
    "title": "Transformer Components that Matter: A Reduced-Order Analysis of Self-Attention Mechanisms",
    "authors": [
      "Liu, K.",
      "Mendoza, J.",
      "Thompson, S."
    ],
    "abstract": "We investigate the relative importance of different components in transformer self-attention blocks through a reduced-order modeling approach. While transformers have demonstrated remarkable performance across domains, the contribution of individual architectural elements\u2014particularly multi-head attention, feed-forward networks, and layer normalization\u2014remains poorly understood. We propose a systematic ablation study combined with low-rank approximations of attention matrices to identify redundant computations. Our method ranks components by their impact on downstream performance using a differentiable masking technique trained with sparse regularization. Experiments on GLUE benchmarks and CIFAR-10 show that 30-40% of attention heads can be pruned with minimal accuracy loss (\u22641.3%), and certain feed-forward dimensions exhibit surprising redundancy. However, our analysis reveals that these findings are highly task-dependent, with question answering tasks showing greater sensitivity to attention pruning than image classification. Our code and pre-trained models will be released upon acceptance.",
    "id": 1290
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients during optimization, leading to suboptimal performance across tasks. While existing gradient surgery methods like PCGrad and GradDrop modify gradients to resolve conflicts, we show that simply dropping gradients entirely on some tasks can outperform these sophisticated approaches. We propose Selective Gradient Dropping (SGD), a surprisingly simple method that randomly drops gradients for subsets of tasks at each optimization step. Our theoretical analysis reveals that this introduces beneficial regularization in the optimization landscape, providing a bias-variance trade-off between gradient conflicts and information loss. Experiments on three standard benchmarks (NYUv2, CelebA, and Meta-World) show SGD achieves comparable or better performance than state-of-the-art methods while reducing computational overhead by 30-40%. However, performance gains are task-dependent and vary significantly across different dataset configurations. While SGD is not universally superior to existing methods, our work challenges the convention that complex gradient surgery is always necessary and suggests that the benefits of gradient conflict resolution may sometimes be outweighed by their computational cost and implementation complexity.",
    "id": 1291
  },
  {
    "title": "Lookahead Approximate Thompson Sampling with Checkpoint Ensembles for Neural Network Hyperparameter Optimization",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, J.",
      "Nguyen, T."
    ],
    "abstract": "Bayesian optimization remains the dominant approach for hyperparameter tuning in deep learning, but its computational overhead grows prohibitively with neural network size. We propose LATSCHE, a hybrid method that combines lightweight Thompson sampling with periodic checkpoint ensembles to enable practical hyperparameter optimization for large models. Our key insight leverages the observation that early training dynamics often correlate with final performance: we maintain a small ensemble of partially-trained models and use approximate posterior updates to guide the search. Unlike standard BO methods, LATSCHE requires only 5-10% additional training cost while providing principled uncertainty estimates. On ResNet-50/ImageNet and GPT-2 language modeling tasks, we achieve comparable or better performance than state-of-the-art BO baselines with 2-4x fewer GPU hours. However, our approach shows degraded performance on smaller models (<10M parameters) where early training is less predictive. While the theoretical justification for our approximation remains incomplete, empirical results suggest practical benefits for practitioners training large-scale models. Our implementation requires minimal code changes to existing training pipelines.",
    "id": 1292
  },
  {
    "title": "Revisiting Gradient Noise Scale with Temporal Dependence: Implications for Large-Batch Training",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Nguyen, D.",
      "Garcia, M."
    ],
    "abstract": "The gradient noise scale (GNS) has emerged as a key predictor of large-batch training efficiency, yet existing theory assumes gradient noise follows an independent and identically distributed process. In this work, we demonstrate that this assumption breaks down in practice, leading to systematic underestimation of optimal batch sizes by up to 2.5x across vision and NLP tasks. We propose a temporally-corrected GNS (TC-GNS) that incorporates autocorrelation structure in gradient noise through a computationally efficient exponentially-weighted estimator requiring only 5% additional memory overhead. Our experiments on ResNet-50, ViT-B/16, and GPT-2 medium reveal that TC-GNS better predicts the critical batch size (Pearson r=0.87 vs 0.63 for baseline) and enables up to 1.4x faster training convergence. However, the benefits diminish significantly when using adaptive optimizers like AdamW, suggesting the method primarily benefits SGD-based training. While TC-GNS provides theoretically grounded improvements over naive GNS estimation, our empirical analysis shows limited impact on final model performance (-0.2 to +0.4% accuracy). We discuss implications for distributed training systems and provide a simple PyTorch implementation for practical adoption.",
    "id": 1293
  },
  {
    "title": "Gradient Surgery with Memory: Improving Multi-Task Learning Through Selective Gradient Retention",
    "authors": [
      "Liu, Q.",
      "Kumar, S.",
      "Johnson, A.",
      "Zhao, B."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts that can destabilize training and degrade performance. While existing gradient surgery methods address this through projection-based approaches, we argue that these methods discard potentially useful gradient information. We propose Gradient Surgery with Memory (GSM), a simple modification that retains conflicting gradients in a momentum buffer and selectively reintroduces them when they align with the primary task's gradient direction. Our method requires only a single hyperparameter\u2014the memory decay rate\u2014and adds minimal computational overhead (less than 2% training time increase). We evaluate GSM on three standard multi-task benchmarks: CityScapes segmentation, NYU-v2 depth estimation, and a multi-label classification variant of CIFAR-100. Results show modest improvements over PCGrad (+1.2% mIoU, +0.8% depth accuracy) and comparable performance to more complex methods like GradDrop, while being significantly simpler to implement. However, we find that GSM provides diminishing returns when tasks have naturally aligned gradients, raising questions about its general applicability. Our code will be released upon acceptance.",
    "id": 1294
  },
  {
    "title": "Improving Contrastive Learning with Positively-Correlated Views via Information-Directed Augmentation",
    "authors": [
      "Chen, L.",
      "Rodrigues, A.",
      "Kim, J."
    ],
    "abstract": "While contrastive learning has achieved impressive results across vision and language tasks, its reliance on hand-crafted augmentation strategies remains a fundamental limitation. We propose a principled approach to learn augmentation policies that maximize the mutual information between positive views while controlling for semantic drift. Our method uses a variational bound to optimize augmentations based on their expected informativeness, adaptively balancing diversity and consistency. Experiments on CIFAR-10, STL-10, and ImageNet-100 show consistent improvements over SimCLR (2-4% accuracy boost) at minimal computational overhead. However, performance gains diminish on datasets with limited natural variations, and our approach introduces additional hyperparameters that require careful tuning. An ablation study reveals that the effectiveness of our method depends heavily on the choice of latent space dimensionality and temperature scheduling. While our theory provides insights into optimal view generation, the practical benefits remain modest and context-dependent.",
    "id": 1295
  },
  {
    "title": "LoRA\u00b2: Recursive Low-Rank Adaptation for Parameter-Efficient Fine-Tuning at Scale",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models, but its effectiveness diminishes as model sizes exceed 30B parameters. We explore whether this limitation stems from the fixed rank decomposition rather than the low-rank assumption itself. Building on this insight, we propose LoRA\u00b2, which recursively applies low-rank decomposition to the LoRA adapters themselves. Our method introduces a tree-structured hierarchy of rank-r matrices, where each level further factorizes the previous level's adaptation matrices. Through extensive experiments on instruction tuning tasks across models ranging from 7B to 70B parameters, LoRA\u00b2 achieves 2-3% improvements over standard LoRA on standard benchmarks while using 15% fewer trainable parameters. The approach is particularly effective for tasks requiring complex reasoning patterns, though gains are more modest on simpler classification tasks. However, our analysis reveals that these improvements are primarily concentrated in the intermediate layer representations, with minimal impact on final layer adaptations. While LoRA\u00b2 provides consistent but incremental benefits over vanilla LoRA, our results suggest that the low-rank bottleneck in parameter-efficient fine-tuning may stem from optimization dynamics rather than representational capacity limitations.",
    "id": 1296
  },
  {
    "title": "LoRA-Flow: Adaptive Low-Rank Adaptation with Dynamic Rank Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method, but selecting appropriate ranks remains challenging and often requires costly hyperparameter tuning. We propose LoRA-Flow, a dynamic rank scheduling approach that automatically adjusts LoRA ranks during training based on gradient flow analysis. Our method monitors the effective rank of accumulated updates using a lightweight singular value decomposition, expanding ranks when gradient information saturates and contracting them when updates become redundant. Experiments on 8 NLP benchmarks and 3 vision tasks show LoRA-Flow achieves 92-97% of full fine-tuning performance while using 15-40% fewer parameters than fixed-rank LoRA baselines. However, we observe the method is sensitive to initialization rank and exhibits diminishing returns on larger models (>7B parameters). While our approach provides practical benefits for moderate-scale fine-tuning, theoretical analysis reveals the rank scheduling heuristic lacks convergence guarantees and may miss optimal rank configurations. Code and pre-trained adapters will be made available.",
    "id": 1297
  },
  {
    "title": "Momentum-Scheduled SAM: Improving Sharpness-Aware Minimization with Curriculum-Based Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising technique for improving generalization by seeking flat minima, but its effectiveness is sensitive to hyperparameter choices, particularly the perturbation radius \u03c1 and learning rate scheduling. We propose Momentum-Scheduled SAM (MS-SAM), which combines SAM with curriculum learning by gradually increasing the perturbation radius \u03c1 from a small initial value to a target value while simultaneously decaying the momentum coefficient \u03b2. Our theoretical analysis shows that this schedule improves convergence bounds for non-convex objectives under the Polyak-\u0141ojasiewicz (PL) condition. Experiments on CIFAR-10/100 and ImageNet demonstrate that MS-SAM achieves 0.3-0.7% accuracy improvements over vanilla SAM with less hyperparameter tuning, though these gains are not consistently observed across all architectures. While MS-SAM offers practical benefits for some settings, its improvements are marginal compared to the inherent variance in training. We release our code for reproducibility at [anonymized for review].",
    "id": 1298
  },
  {
    "title": "Gradient Descent with Lookahead and Momentum: A Unified Analysis of Approximate Optimization in Neural Networks",
    "authors": [
      "Liu, S.",
      "Kumar, P.",
      "Chen, J."
    ],
    "abstract": "We propose a unifying framework for analyzing approximate gradient descent algorithms that combine lookahead mechanisms with momentum in neural network training. While lookahead optimizers have shown practical benefits in distributed settings, theoretical understanding remains fragmented, particularly when combined with momentum terms. Our approach introduces a perturbed gradient flow analysis that treats lookahead steps as approximate proximal updates under momentum dynamics. We prove that this combination achieves O(1/T) convergence for smooth convex objectives and O(1/\u221aT) for non-convex cases, matching standard momentum rates up to constant factors. Experiments on CIFAR-10/100 and ImageNet demonstrate 2-3% accuracy improvements over standard baselines when training ResNet-18 and Vision Transformer architectures, though gains diminish on larger models. Analysis reveals that lookahead primarily stabilizes gradient variance during early training phases, with momentum dominance emerging later. While our theoretical bounds are essentially tight within our framework, they do not capture the empirical improvements we observe. The work provides the first systematic treatment of lookahead-momentum interactions, though connections to practical generalization benefits remain unclear.",
    "id": 1299
  },
  {
    "title": "Gradient Sign Dropout: A Simple Regularization Technique for Attention Mechanisms via Random Sign Flipping",
    "authors": [
      "Liu, J.",
      "Garcia, M.K.",
      "Thompson, B."
    ],
    "abstract": "We propose Gradient Sign Dropout (GSD), a lightweight regularization technique for transformer-based models that randomly flips the sign of gradient components during backpropagation. Motivated by the observation that attention layers exhibit high gradient sign consistency across training steps, GSD injects controlled noise by stochastically inverting gradients with probability p during parameter updates. Unlike traditional dropout, GSD operates on the gradient space rather than activations, requiring no architectural modifications. Our theoretical analysis shows that GSD approximates a form of implicit gradient noise injection, leading to improved generalization bounds under certain assumptions. Experimental results on GLUE and WikiText benchmarks show 1.2-2.1% improvements over standard transformers of comparable size, with consistent gains across architectures. While these improvements are meaningful, we acknowledge they fall within typical variance ranges. Ablation studies reveal effectiveness primarily for smaller models (<100M parameters). Although GSD introduces a single hyperparameter and minimal computational overhead, we recognize it may not provide clear advantages for large-scale pre-training where extensive hyperparameter tuning is already performed. We provide PyTorch implementations and reproducible training scripts.",
    "id": 1300
  },
  {
    "title": "Gradient Boosting with Adaptive Shrinkage via Online Variance Estimation",
    "authors": [
      "Chen, Y.",
      "Kumar, S.",
      "Anderson, J."
    ],
    "abstract": "We propose AdaShrink-GB, a variant of gradient boosting that adaptively controls overfitting through online estimates of gradient variance. While traditional gradient boosting relies on fixed learning rates or heuristic shrinkage schedules, our method dynamically adjusts step sizes based on per-feature gradient stability. The key insight is that unstable gradients (high empirical variance across boosting iterations) should receive stronger regularization to prevent the model from fitting spurious patterns. We derive a simple update rule that modifies the standard gradient boosting objective with a data-dependent regularization term. On 12 benchmark datasets, AdaShrink-GB achieves modest improvements over XGBoost (average +1.3% accuracy) and LightGBM (+0.8%), though performance gains are inconsistent across tasks. Experiments on synthetic data suggest the method helps with noisy features, but theoretical analysis remains limited. Our implementation requires minimal overhead relative to standard boosting, adding only O(d) storage per iteration where d is feature dimension. While the contribution is incremental and the improvement margins small, AdaShrink-GB offers a principled alternative to manual tuning of shrinkage parameters in gradient boosting.",
    "id": 1301
  },
  {
    "title": "LoRA-X: Extending Low-Rank Adaptation through Dynamic Rank Allocation",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Zhao, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank selection remains suboptimal across layers and tasks. We propose LoRA-X, a simple extension that dynamically adjusts the rank during training using a magnitude-based pruning criterion. Our method begins with an over-parameterized LoRA configuration, then iteratively reduces ranks for layers with stable weight updates. On the GLUE benchmark, LoRA-X achieves comparable performance to full fine-tuning (98.2% relative score) while using 26% fewer parameters than standard LoRA. However, our gains are inconsistent across tasks \u2014 we observe improvements primarily on classification tasks, with minimal benefits on generation tasks like summarization. Analysis reveals that learned rank allocations correlate weakly with downstream performance, suggesting our pruning heuristic may be overly simplistic. While LoRA-X offers modest parameter savings for BERT-base and GPT-2 medium, the computational overhead from rank scheduling offsets these benefits for smaller models. Our implementation is available at anonymous.github.io/lora-x.",
    "id": 1302
  },
  {
    "title": "Gradient Flow Regularization: A Lightweight Alternative to Spectral Normalization in GANs",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Nguyen, K."
    ],
    "abstract": "We propose Gradient Flow Regularization (GFR), a simple technique for stabilizing Generative Adversarial Network training by constraining the Lipschitz constant of the discriminator through implicit gradient penalties. Unlike spectral normalization, GFR adds minimal computational overhead by leveraging the existing backward pass to compute gradient norms, avoiding expensive singular value decompositions. Our approach combines a lightweight regularization term with a novel gradient clipping strategy that adapts based on the discriminator's local Lipschitz estimate. Experiments on CIFAR-10 and CelebA demonstrate modest improvements in FID scores (2-5 points) over spectral normalization baselines, with 15% faster training time. However, we observe diminishing returns on higher-resolution datasets (ImageNet), where our method underperforms recent advances in attention-based architectures. Theoretical analysis reveals that GFR provides loose Lipschitz bounds compared to exact methods, suggesting limited applicability in scenarios requiring strict theoretical guarantees. While GFR offers a practical speed-accuracy trade-off for small-scale applications, its benefits are dataset-dependent and less pronounced than initially expected. Code is available at [URL omitted for anonymity].",
    "id": 1303
  },
  {
    "title": "Frequency-Aware Gradient Clipping for Stable Transformer Training",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Garcia, A."
    ],
    "abstract": "Transformer models often suffer from training instability due to gradient explosion, particularly in the early training stages. While gradient clipping is a common remedy, we find that standard clipping techniques treat all parameters uniformly, potentially hindering the learning dynamics of rapidly-converging components. We propose Frequency-Aware Gradient Clipping (FAGC), which adaptively adjusts clipping thresholds based on the historical frequency of large gradient occurrences at the parameter level. Our method maintains a simple exponential moving average of gradient norms and modulates clipping boundaries accordingly. We evaluate FAGC on standard language modeling tasks using WikiText-103 and OpenWebText, showing modest improvements in training stability and validation perplexity over vanilla gradient clipping (0.5-1.2 perplexity reduction). While our theoretical analysis provides convergence guarantees under bounded gradient assumptions, the practical gains are most pronounced in specific training regimes (learning rates \u2265 3e-4) and diminish with careful learning rate scheduling. Our experiments suggest FAGC offers a practical, low-overhead alternative to standard clipping, though benefits may be task-dependent.",
    "id": 1304
  },
  {
    "title": "Fast Second-Order Optimization via Newton Sketching with Adaptive Sample Sizes",
    "authors": [
      "Liu, H.",
      "Kumar, S.",
      "Chen, Z."
    ],
    "abstract": "We propose SketchNewton, a variant of Newton sketch that adaptively adjusts sketch sizes during optimization to balance computational cost and convergence guarantees. While theoretical analysis of Newton sketch typically assumes fixed sketch sizes, we show empirically that aggressively reducing sketch sizes in early iterations can accelerate convergence without degrading final accuracy. Our method uses a simple heuristic based on gradient norm ratios to determine when to increase sketch sizes, requiring no additional hyperparameters beyond standard Newton sketch. We demonstrate improvements over vanilla Newton sketch and Adam on logistic regression and small-scale neural network training, achieving 1.2-1.4x speedups in wall-clock time. However, our approach lacks strong theoretical convergence guarantees for the adaptive schedule, and we observe degraded performance when applied to ill-conditioned problems. While the method shows promise for moderate-scale convex optimization, its extension to large-scale deep learning remains unclear. Code is available at [anonymized link].",
    "id": 1305
  },
  {
    "title": "Gradient Norm Regularization Improves Out-of-Distribution Robustness in Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We demonstrate that simple gradient norm regularization (GNR) can improve out-of-distribution (OOD) robustness in deep neural networks without requiring adversarial training or domain-specific augmentations. Our method adds a lightweight penalty term \u03bb||\u2207\u03b8\u2113(f\u03b8(x), y)||\u00b2 to the training loss, encouraging flatter loss landscapes around training samples. Through extensive experiments across CIFAR-10/100 and ImageNet, we show that GNR achieves modest but consistent improvements in OOD robustness (2-4% average accuracy gain across common corruptions) while maintaining in-distribution performance. We provide theoretical justification via PAC-Bayesian analysis, relating gradient norms to generalization bounds. However, we find that benefits diminish on large-scale benchmarks, and performance varies significantly across corruption types. While GNR offers a plug-and-play alternative to more complex robust training techniques, our results suggest its practical impact remains limited relative to state-of-the-art adversarial methods. Code will be released upon acceptance.",
    "id": 1306
  },
  {
    "title": "LoRA-FT: Parameter-Efficient Fine-Tuning via Low-Rank Adaptation with Feature Transport",
    "authors": [
      "Kim, H.",
      "Rodriguez, C.",
      "Liu, S."
    ],
    "abstract": "Fine-tuning large pre-trained models presents computational challenges due to their massive parameter counts. While Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, we observe that its performance degrades significantly when the downstream task exhibits distribution shift from pre-training. We propose LoRA-FT, which augments standard LoRA with a lightweight feature transport mechanism based on optimal transport theory. Our method computes transport maps between pre-trained and task-specific feature distributions, adding only 2.3% additional parameters beyond standard LoRA. We evaluate LoRA-FT on vision and language tasks spanning natural distribution shifts. Across 8 benchmarks, LoRA-FT achieves 2.1% absolute improvement over LoRA baseline, while maintaining comparable computational efficiency. However, performance gains diminish on tasks closely aligned with pre-training distributions. Our theoretical analysis connects transport distance to generalization bounds, though assumptions about feature smoothness may limit practical applicability. Code and checkpoints are available at anonymous-url.",
    "id": 1307
  },
  {
    "title": "AdaSharp: Adaptive Gradient Clipping with Sharpness-Aware Minimization for Improved Transfer Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We present AdaSharp, a simple modification to existing optimizers that adapts gradient clipping thresholds based on local loss landscape sharpness during fine-tuning. Motivated by the observation that transfer learning often suffers from unstable optimization when adapting pre-trained features to new tasks, AdaSharp combines lightweight sharpness measurements with per-layer clipping bounds that adjust based on Hessian trace estimates. Our method adds minimal computational overhead (1-2% wall time increase) and can be integrated into any optimizer. Experiments on 12 NLP and vision benchmarks from GLUE and VTAB show modest but consistent improvements over baseline fine-tuning, with average gains of 1.2% accuracy across tasks. While these improvements are statistically significant (p<0.05 under paired t-tests), we acknowledge they are limited to medium-sized models (Bert-base, ResNet50) and newer architectures like ViT-large show smaller gains. We provide extensive ablations suggesting performance stems primarily from preventing catastrophic forgetting in early training stages rather than discovering better minima. Code will be released upon acceptance.",
    "id": 1308
  },
  {
    "title": "LoRA-DiT: Low-Rank Adaptation with Dynamic Importance Thresholding for Diffusion Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We present LoRA-DiT, an efficient fine-tuning method that combines low-rank adaptation (LoRA) with dynamic importance thresholding for text-to-image diffusion transformers. While LoRA has shown promise in reducing memory requirements for diffusion models, we observe that standard approaches often over-regularize important attention patterns and under-regularize redundant ones. Our method introduces a simple yet effective importance metric computed from attention covariance matrices, which is used to dynamically adjust LoRA rank allocation across different layers during training. We evaluate LoRA-DiT on Stable Diffusion XL using COCO and LAION-aesthetics datasets. Our approach achieves 31% reduction in trainable parameters compared to full LoRA fine-tuning while maintaining comparable FID scores (a 0.8 point improvement over LoRA baseline). However, we note performance degradation on complex prompts requiring fine-grained spatial relationships. Additionally, our assumption about monotonic importance-rank relationships may not hold for all downstream tasks. While our method provides computational savings for specialized fine-tuning scenarios, the improvements over carefully tuned LoRA baselines are modest and may not justify the additional complexity in all applications. Code and models will be made available.",
    "id": 1309
  },
  {
    "title": "DropAttention: Reducing Computational Overhead in Transformer Self-Attention via Structured Pruning",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "Transformers achieve state-of-the-art results across many domains but suffer from quadratic complexity in self-attention computation. We propose DropAttention, a simple yet effective method to reduce attention computation by pruning attention heads based on their gradient norms during training. Unlike previous approaches that require architectural modifications or additional hyperparameters, DropAttention operates as a drop-in replacement for standard attention modules with minimal implementation overhead. Our method progressively removes less informative heads throughout training, achieving 30-50% reduction in FLOPs with <2% degradation on GLUE and WMT benchmarks. While the approach shows promise on standard benchmarks, we observe performance degradation on tasks requiring long-range reasoning, limiting its applicability. We provide empirical evidence that remaining heads compensate for pruned ones through increased activation sparsity. Our PyTorch implementation requires only 20 lines of code. While DropAttention achieves consistent speedups across architectures, the method shows sensitivity to initialization and achieved gains diminish with larger models (>1B parameters), suggesting scalability limitations.",
    "id": 1310
  },
  {
    "title": "Revisiting Momentum in Adam: An Adaptive Look-ahead Correction",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "Momentum-based optimizers like Adam remain the de facto choice for training neural networks, yet theoretical understanding of their second-order moment adaptation remains limited. We identify a systematic bias in Adam's momentum update when gradients exhibit rapidly changing curvature, leading to sub-optimal convergence in certain problem settings. We propose Adam-LC, a simple modification that incorporates a look-ahead gradient correction term adaptively weighted by the gradient's directional variance. Our method requires only O(d) additional memory and two extra hyper-parameters with intuitive defaults. We prove local convergence rates for Adam-LC under standard assumptions, showing improvements over vanilla Adam when gradient coherence exceeds a threshold we characterize. Experiments on CIFAR-10/100, ImageNet, and WMT'14 translation demonstrate marginal but consistent improvements: 0.3-0.7% top-1 accuracy on vision tasks and 0.2-0.4 BLEU score gains on translation tasks. While our theoretical analysis provides insight into momentum behavior in adaptive optimizers, the practical improvements, though statistically significant, are modest and may not justify the added complexity for many practitioners. Code will be provided upon publication.",
    "id": 1311
  },
  {
    "title": "LoRA-MD: Memory-Efficient Low-Rank Adaptation with Momentum Distillation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Garcia, M.A.",
      "Thompson, J."
    ],
    "abstract": "We present LoRA-MD, a method that combines low-rank adaptation (LoRA) with momentum distillation to improve parameter-efficient fine-tuning of large language models while maintaining memory efficiency. Our key insight is that momentum information from teacher models can be distilled into low-rank adapters without storing full gradients, addressing the gradient staleness issue observed in vanilla LoRA training. The method introduces a momentum buffer stored in the compressed low-rank space, updated via an exponential moving average of historical adapter updates. Experiments on GLUE and SuperGLUE benchmarks with 7B parameter models show modest improvements over LoRA (average +1.2% accuracy) while using 15% less memory during training. However, we find diminishing returns on larger models (>30B parameters) and minimal gains on domain-specific tasks. The method requires careful tuning of the momentum coefficient and performs best when the downstream task distribution closely matches the pre-training data. Our empirical analysis suggests the benefits are most pronounced in resource-constrained scenarios where memory efficiency is critical. Code and pre-trained adapters will be made available upon acceptance.",
    "id": 1312
  },
  {
    "title": "Quantized Attention: Reducing Memory Footprint in Transformers via Structured Low-Rank Approximation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J.",
      "Singh, A."
    ],
    "abstract": "The growing computational demands of transformer models pose challenges for deployment on resource-constrained devices. While prior work has explored various compression techniques, they often suffer from significant performance degradation at high compression rates. We propose Quantized Attention (QA), a method that combines structured low-rank approximation with adaptive quantization to reduce the memory footprint of attention mechanisms. QA approximates the full attention matrix using a product of low-rank factors, then applies learned quantization levels to these factors based on attention entropy. We evaluate QA on standard NLP benchmarks including GLUE and WMT translation tasks. Experiments with BERT-base and GPT-2 medium show 4.2\u00d7 memory reduction with only 2.3% performance loss on downstream tasks. However, we observe that QA struggles with tasks requiring fine-grained reasoning (drop of 8.1% on SQuAD 2.0). Analysis reveals insensitivity to quantization thresholds on long sequences. These results suggest that while QA provides practical memory savings, careful consideration is needed for applications requiring precise attention. Code and models will be released upon acceptance.",
    "id": 1313
  },
  {
    "title": "LoRA-MoE: Parameter-Efficient Mixture of Experts for Fine-Tuning Large Language Models",
    "authors": [
      "Chen, Z.",
      "Rodriguez, J.",
      "Kim, S.",
      "Thompson, L."
    ],
    "abstract": "We propose LoRA-MoE, a method that combines Low-Rank Adaptation (LoRA) with Mixture of Experts (MoE) architectures for parameter-efficient fine-tuning of large language models. While LoRA has demonstrated success in reducing trainable parameters by learning low-rank adapters, we observe it underperforms on tasks requiring complex, domain-specific reasoning. LoRA-MoE addresses this by replacing standard LoRA adapters with a sparse mixture of 4-8 expert adapters per layer, routed via a lightweight gating mechanism. Our experiments on GLUE and SuperGLUE show LoRA-MoE achieves 2.3% average improvement over standard LoRA while using only 1.3\u00d7 more parameters. Additionally, we demonstrate that routing can provide interpretability into which experts activate for specific input types. However, we find LoRA-MoE introduces training instability in some settings and may not justify the added complexity for simpler downstream tasks. Our implementation adds minimal overhead to existing LoRA workflows, making it practical for practitioners seeking modest improvements on reasoning-intensive benchmarks.",
    "id": 1315
  },
  {
    "title": "Gradient Descent with Periodic Resets: A Simple Regularization Technique for Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "We propose Gradient Descent with Periodic Resets (GDPR), a surprisingly effective regularization technique where we periodically reset a subset of model parameters to their initialization values. Unlike dropout or weight decay, GDPR maintains sparse activation patterns throughout training while preserving overall network capacity. Our theoretical analysis shows that for two-layer ReLU networks, GDPR achieves similar generalization bounds as SGD with explicit regularization, though we rely on restrictive assumptions about the data distribution. Experimental results on CIFAR-10, CIFAR-100, and ImageNet show modest improvements (0.5-1.2% accuracy) over standard baselines, with particularly strong performance on small datasets. However, we find GDPR's effectiveness diminishes with larger architectures and extended training schedules. While simple to implement, our method introduces two hyperparameters (reset frequency and proportion of parameters) that require careful tuning. We provide PyTorch code to reproduce our results, though the computational requirements for ImageNet experiments may exceed standard academic budgets. Our work suggests a new perspective on regularization through selective forgetting, though more work is needed to understand when and why this approach succeeds.",
    "id": 1316
  },
  {
    "title": "LoRA-Max: Improved Low-Rank Adaptation Through Dynamic Rank Allocation",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto standard for parameter-efficient fine-tuning, but the choice of rank remains a critical hyperparameter typically fixed across all layers. We propose LoRA-Max, a simple extension that dynamically adjusts the rank allocation based on layer-wise gradient statistics during fine-tuning. Our method employs an iterative pruning-and-regrowth strategy: starting with a conservative rank budget, we prune ranks with low gradient norms and reallocate them to layers with higher information flow. Experiments on the GLUE benchmark show LoRA-Max achieves 1.2% average improvement over standard LoRA with the same parameter budget on RoBERTa-base, while maintaining competitive inference latency. On larger models (BERT-large), the gains diminish to 0.4-0.7%. We provide theoretical analysis showing our allocation scheme converges to an optimal rank distribution under mild assumptions. While our improvements are modest, LoRA-Max requires minimal code changes and may benefit practitioners who struggle with rank selection. However, we acknowledge the results are dataset-specific and the overhead may not justify the gains for all applications. Code will be released upon acceptance.",
    "id": 1317
  },
  {
    "title": "Meta-Gradient Descent with Momentum for Adaptive Learning Rate Selection in Mini-Batch Deep Learning",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Anderson, J."
    ],
    "abstract": "We propose a novel meta-learning approach for adaptive learning rate selection in stochastic gradient-based optimization. Our method, Meta-Gradient Descent with Momentum (MGDM), uses a bi-level optimization framework to learn momentum coefficients that adaptively adjust learning rates based on local gradient statistics. Unlike previous meta-optimization approaches that require expensive meta-objective evaluations, MGDM approximates the meta-gradient using moving average statistics, achieving computational overhead comparable to standard momentum methods. We evaluate MGDM on CIFAR-10/100 and ImageNet classification tasks across ResNet and EfficientNet architectures, demonstrating 2-5% improvement in final validation accuracy compared to AdamW and SGDM baselines. While our results are competitive with recent adaptive methods like Lion and NAdam, we find that performance gains diminish when training beyond 200 epochs or when applied to smaller models (\u226410M parameters). Theoretically, we establish convergence guarantees for MGDM in the \u03bc-strongly convex case, though the assumptions are restrictive compared to practical deep learning scenarios. Our code and hyperparameter configurations are publicly available for reproducibility.",
    "id": 1318
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Learned Baselines with Spectral Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to high-variance gradient estimators of the evidence lower bound (ELBO). While continuous relaxations like Gumbel-Softmax ease optimization, they introduce bias that can degrade performance in structured prediction tasks. We propose a simple alternative: learning input-dependent baselines for REINFORCE using a small auxiliary network with spectral normalization on its Jacobian. Our method requires minimal hyperparameter tuning and adds negligible computational overhead compared to standard REINFORCE. On synthetic tree-structured VAEs and document-level machine translation, we observe 15-25% reductions in gradient variance and modest BLEU improvements (+0.3-0.6). However, gains diminish with larger models (>100M parameters), and our approach underperforms continuous relaxations on image generation benchmarks. Theoretical analysis reveals our baselines reduce but do not eliminate exponential variance scaling with sequence length. Code is available but experiments were conducted on limited compute (4 V100s), raising reproducibility concerns for full-scale applications.",
    "id": 1319
  },
  {
    "title": "Adaptive Gradient Clipping with Lookahead: A Simple but Effective Trick for Stable Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Training large transformer models often suffers from gradient instability, leading to divergent training or poor final performance. While numerous sophisticated optimization techniques have been proposed, their implementation complexity and hyperparameter sensitivity limit adoption in practice. We introduce a simple but surprisingly effective combination of existing techniques: gradient clipping with an adaptive threshold that scales with the parameter norm, combined with the Lookahead optimizer. Our method requires only two additional hyperparameters and minimal computational overhead. Across 8 different language modeling and machine translation tasks, our approach achieves a 15-30% reduction in training time compared to standard Adam, while maintaining or slightly improving final perplexity/BLEU scores. However, we find the improvements are most pronounced in specific regimes: medium-sized models (100M-1B parameters) with aggressive learning rates. Theoretical analysis suggests our clipping threshold approximates a trust region update, though this connection remains underdeveloped. While the contributions are incremental, we believe our findings provide practical value for practitioners struggling with training stability, and our extensive hyperparameter sensitivity analysis offers guidance for implementation.",
    "id": 1320
  },
  {
    "title": "Improved Transformer Training via Dynamic Token Dropout",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "While transformer models achieve state-of-the-art performance across many domains, their computational cost scales quadratically with sequence length. We propose Dynamic Token Dropout (DTD), a training-time technique that adaptively masks tokens based on their attention entropy. Our method reduces training costs by 15-25% while maintaining downstream performance on GLUE and WMT benchmarks. DTD computes online statistics of attention distributions and drops tokens that contribute less to global context. This approach differs from static masking strategies by learning masking policies conditioned on input sequences. Experiments on BERT-base and T5-base show comparable accuracy to full training with 0.5-1.0 BLEU score degradation on translation tasks. We provide theoretical analysis showing DTD approximates the full attention mechanism under mild assumptions. However, we observe performance drops on tasks requiring long-range dependencies (e.g., document-level classification). Our implementation adds minimal overhead (<5% wall-clock time) and integrates easily into existing training pipelines. Ablation studies suggest the benefits primarily come from early training stages, with diminished returns after 50% of training. While DTD achieves consistent speedups, its effectiveness varies by task complexity and dataset size.",
    "id": 1321
  },
  {
    "title": "Improving Transformer Efficiency through Learned Sparse Attention Patterns with Cyclic Projections",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, T."
    ],
    "abstract": "We propose Cyclic Sparse Transformers (CST), a method for reducing the computational complexity of self-attention in Transformers by learning sparse attention patterns through cyclic projections of query-key matrices. Unlike fixed sparsity patterns or low-rank approximations, CST alternates between sparse and dense attention across layers, guided by a lightweight gating mechanism that predicts attention sparsity based on input statistics. Our approach achieves 2.1\u00d7 speedup on language modeling tasks with <1% perplexity increase compared to standard Transformers on Wikitext-103. While our theoretical analysis shows CST maintains the universal approximation property under mild assumptions, we find the learned sparsity patterns are highly task-specific and transfer poorly across domains. Experiments on machine translation (WMT'14 En-De) show mixed results: modest BLEU improvements on out-of-domain data but degradation on in-domain test sets. Code is available at anonymous-url.github.io/CST.",
    "id": 1322
  },
  {
    "title": "Instance-Adaptive Learning Rates via Local Curvature Approximation in Deep Neural Networks",
    "authors": [
      "Chen, S.",
      "Kumar, V.",
      "Rodriguez, L."
    ],
    "abstract": "We propose LocalCurvature Adaptation (LOCA), a simple modification to gradient-based optimizers that adjusts learning rates per sample based on approximate curvature information. By leveraging the gradient covariance matrix estimated from small mini-batches, LOCA identifies directions with high curvature and reduces step sizes adaptively. Our method requires minimal computational overhead and integrates naturally with existing optimizers like Adam and SGD-with-momentum. Experiments on ResNet-50 and Vision Transformer training on ImageNet show 1.3-2.1% improvements in final accuracy compared to standard baselines. However, these gains vary significantly across architectures (0.3-2.1% range) and diminish when training with strong regularization or larger batch sizes. While LOCA improves convergence speed in some settings, we observe instability when the curvature estimates are noisy early in training. Our theoretical analysis shows LOCA can be viewed as a diagonal approximation to natural gradient descent but provides limited insight beyond this connection. Code and hyperparameters are provided for reproducibility.",
    "id": 1323
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Dynamic Factor Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method for large language models, but its fixed rank selection across layers remains suboptimal for diverse downstream tasks. We propose LoRA-Drop, a simple yet effective approach that dynamically selects the optimal rank for each layer during fine-tuning based on gradient magnitude statistics. Our method trains an additional gating network that outputs rank reduction decisions at each step, using only 0.1% extra parameters. On GLUE and SuperGLUE benchmarks, LoRA-Drop achieves comparable performance to standard LoRA while using 15-30% fewer trainable parameters across various model sizes (BERT-base to T5-XL). However, we observe diminishing returns on larger models (>3B parameters) where the rank search space becomes less impactful. While our approach provides consistent parameter savings, we acknowledge the overhead of training the gating mechanism and limited theoretical guarantees for the rank selection process. Code will be made available upon acceptance.",
    "id": 1324
  },
  {
    "title": "Gradient Descent with Memory: A Simple Augmentation for Improved Convergence in Non-Convex Optimization",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Gradient Descent with Memory (GDM), a simple modification to standard gradient descent that maintains and utilizes a small history of past gradients to improve convergence. Unlike momentum-based methods that exponentially decay gradient information, GDM uses a fixed-size memory buffer to compute adaptive updates via a lightweight attention mechanism over recent gradients. Our approach requires no additional hyperparameters beyond buffer size and adds minimal computational overhead. We theoretically analyze GDM for smooth non-convex objectives, showing it achieves O(1/\u221aT) convergence rate matching SGD, with potential acceleration under certain regularity conditions. Empirically, we demonstrate consistent improvements over standard SGD and momentum variants on deep neural network training across several benchmarks, including CIFAR-10/100 and ImageNet classification. While GDM shows modest but consistent improvements (1-2% accuracy gains), it is particularly effective in low-data regimes and noisy gradient settings. Our experiments reveal that benefits diminish with larger batch sizes and well-tuned optimization setups. Code will be made available upon acceptance.",
    "id": 1325
  },
  {
    "title": "Regularizing Transformers with Learned Implicit Position Encodings",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, A.",
      "Kim, H."
    ],
    "abstract": "Positional encodings are critical for Transformer architectures, yet existing approaches rely on hand-crafted patterns that may not optimally capture positional relationships. We propose LIPER, a regularization technique that learns implicit position representations through an auxiliary contrastive objective. Rather than replacing existing encodings, LIPER encourages the model to learn position-aware features by predicting relative distances between token pairs. Our method adds minimal computational overhead and can be integrated into any pre-trained Transformer. We evaluate LIPER on machine translation (IWSLT'14 De-En), language modeling (WikiText-103), and GLUE benchmarks. Results show modest but consistent improvements: +0.3 BLEU on translation, 1.2% perplexity reduction on WikiText, and +0.9 average GLUE score over strong baselines. While LIPER provides stable gains across tasks, we find the improvements are most pronounced in low-data regimes (10M training tokens), diminishing with scale. Our analysis suggests the regularization effect primarily benefits earlier training stages rather than final model quality. The method requires careful hyperparameter tuning and shows sensitivity to batch sizes. Though interpretable visualizations reveal meaningful learned proximity relationships, computational costs scale quadratically with sequence length. LIPER offers a lightweight approach to enhance positional awareness in Transformers, though practical benefits may be limited beyond specific settings.",
    "id": 1326
  },
  {
    "title": "Noise Regularization Enables Linear Probing to Match End-to-End Fine-tuning in Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate whether carefully designed linear probing can achieve comparable performance to full fine-tuning in downstream NLP tasks. While linear probing typically lags behind fine-tuning by 3-8% accuracy, we identify that adding noise regularization during feature extraction significantly bridges this gap. Our method, Noise-Probing, adds controlled Gaussian noise to intermediate representations during training, which we hypothesize provides better regularization and adversarial robustness than naive probing. Experiments on GLUE, SuperGLUE, and domain-specific tasks with RoBERTa and T5 models show Noise-Probing achieves 97.3% of fine-tuning accuracy on average, up from 91.2% for standard probing. However, we find this improvement is task-dependent: sentiment analysis and NLI tasks see consistent benefits, while question-answering tasks show minimal gains. Ablation studies reveal the noise magnitude hyperparameter is sensitive across tasks, requiring grid-search for optimal performance. Furthermore, our theoretical analysis suggests the improvement stems from implicit bias reduction in the representation space, though we lack formal guarantees for the observed empirical gains. Our results suggest that while noise regularization can enhance simple adaptation methods, fundamental limitations remain for complex reasoning tasks.",
    "id": 1327
  },
  {
    "title": "Improved AdamM: Momentum-Aware Adam with Layer-wise Learning Rates for Better Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose AdamM, a simple modification to the Adam optimizer that incorporates momentum-aware adaptive learning rates and layer-wise scheduling. While Adam has become the de facto optimizer for training deep neural networks, we observe that its update rule leads to suboptimal generalization in many settings, particularly when training ResNet and Transformer architectures. Our method introduces two key innovations: (1) a momentum-aware correction term that adjusts the second moment estimate based on gradient history, and (2) a layer-wise learning rate scheme that decays learning rates at different rates for different architectural components. Experiments on CIFAR-10/100, ImageNet, and WMT English-German translation show 1.2-2.3% improvement in final validation accuracy over vanilla Adam and its variants (AdamW, AdaBelief), with minimal computational overhead. However, we find that AdamM does not consistently outperform SGD with momentum on large-scale Vision Transformer training. Code is available at https://anonymous.url/adam-optimizer.",
    "id": 1328
  },
  {
    "title": "Gradient Surgery in Federated Learning: A Topological Approach to Client Drift",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "Federated learning faces challenges from client drift, where gradient updates from distributed devices become misaligned. While existing approaches like FedAvg and FedProx apply uniform regularization, we propose TopoFed, which uses persistent homology to detect and correct topological inconsistencies in gradient manifolds across clients. Our method computes the persistence diagrams of local loss landscapes and performs gradient surgery by aligning high-dimensional homological features before aggregation. We theoretically prove that this reduces an upper bound on client drift by a factor of O(\u221a(log K)), where K is the number of clients. Experimental results on CIFAR-10 and FEMNIST show 2-3% accuracy improvements over baselines in non-IID settings, particularly when client data distributions have high Wasserstein distance from the global distribution. However, we observe diminishing returns as the number of clients increases beyond 100, likely due to accumulated approximation errors in homology computation. While TopoFed provides a novel perspective on mitigating drift, the computational overhead (2.5x slower than FedAvg) and limited empirical gains suggest the approach may benefit from more efficient topological approximations. Our code is available at [repository].",
    "id": 1329
  },
  {
    "title": "Residual Auxiliary Networks: Adding Identity Connections to Auxiliary Classifiers for Enhanced Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training deep neural networks remains challenging due to vanishing gradients and optimization difficulties in very deep architectures. While auxiliary classifiers have been proposed to provide additional gradient paths, their effectiveness diminishes in extremely deep networks. We propose Residual Auxiliary Networks (RANs), which integrate lightweight residual identity connections into auxiliary classifier branches. Our method modifies traditional auxiliary classifiers by adding parameterized skip connections that allow gradients to flow more directly to early layers. Theoretical analysis shows our approach provides tighter bounds on gradient norms compared to standard auxiliary classifiers. Experiments on CIFAR-100 and ImageNet demonstrate consistent improvements (0.8-1.2% top-1 accuracy) over vanilla auxiliary classifiers for networks with 50+ layers, with minimal computational overhead (<5% parameters). However, benefits plateau for moderate-depth networks and certain architectures where auxiliary classifiers already perform adequately. Our implementation is available at github.com/[anonymized]/ran-code.",
    "id": 1330
  },
  {
    "title": "Improved Gradient Extrapolation for Variance-Reduced Adam with Preconditioning",
    "authors": [
      "Chen, Y.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "We propose GEPS-Ada, a modification to Adam that incorporates second-order curvature information through lightweight preconditioning while maintaining its adaptive properties. Our method builds on recent variance-reduced Adam variants by introducing an extrapolated gradient correction inspired by quasi-Newton methods. Specifically, we maintain a small history of gradients and compute an efficient preconditioning matrix via rank-1 updates, avoiding the computational overhead of full Hessian approximations. On standard benchmarks including CIFAR-10/100 and ImageNet training, GEPS-Ada achieves 2-5% faster convergence in wall-clock time compared to vanilla Adam and variants like AMSGrad, despite using 15% less memory than full-matrix preconditioning methods. While the absolute gains are modest, our ablation studies reveal that the gradient extrapolation contributes the most to these improvements. Theoretical analysis shows convergence rates matching standard Adam under similar assumptions, though we cannot prove acceleration on general non-convex objectives. Limitations include sensitivity to the history size parameter and diminishing benefits on transformers compared to CNN architectures. Code will be made available upon acceptance.",
    "id": 1331
  },
  {
    "title": "LoRA-G: Gradient-Adaptive Low-Rank Adaptation with Exponential Moving Average for Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its reliance on fixed rank selection can lead to suboptimal performance. We propose LoRA-G, a simple extension that uses exponential moving average (EMA) of gradients to dynamically adjust LoRA ranks during training. Our method maintains LoRA's memory efficiency while allowing subsets of adapters to grow in complexity when gradient norms indicate underfitting. Through experiments on GLUE, SuperGLUE, and CIFAR-100, we show LoRA-G achieves 1.2-2.1% average improvement over standard LoRA at comparable parameter counts. However, we find gains are concentrated on smaller models (<1.5B parameters) and tasks with limited training data. Analysis reveals EMA-based rank adaptation converges slowly for large models, suggesting computational trade-offs that may limit practical benefits. While LoRA-G provides a lightweight improvement to existing LoRA implementations, its impact appears modest and requires careful hyperparameter tuning. Code and checkpoints are available at [URL redacted for anonymization].",
    "id": 1332
  },
  {
    "title": "Revisiting Scheduled Sampling with Self-Paced Difficulty for Neural Machine Translation",
    "authors": [
      "Chen, Y.",
      "Liu, K.",
      "Aguirre, A."
    ],
    "abstract": "Scheduled sampling has emerged as a popular technique to address the exposure bias problem in sequence-to-sequence models, yet its effectiveness across different training regimes remains poorly understood. We propose Difficulty-Aware Scheduled Sampling (DASS), a simple modification that scales the sampling probability based on the model's current uncertainty about each token. By gradually increasing scheduled sampling rates for predictions where the model exhibits high entropy, we demonstrate modest but consistent improvements over standard scheduled sampling on WMT'14 English-German and IWSLT'14 German-English benchmarks (+0.4 and +0.3 BLEU respectively). Our theoretical analysis reveals that DASS can be interpreted as performing implicit curriculum learning on sequence difficulty. While improvements are statistically significant via bootstrap resampling (p<0.05), we acknowledge that gains are limited to medium-resource settings and do not emerge for very large models. Our implementation, which requires only 3 additional lines of code, suggests that carefully designed training dynamics can yield benefits even for well-established architectures, though we caution against overstating the magnitude of improvement.",
    "id": 1333
  },
  {
    "title": "Gradient Noise Injection as an Implicit Regularizer: A Spectral Analysis of Deep Network Training Dynamics",
    "authors": [
      "Liu, Q.",
      "Rodriguez, M.",
      "Kim, T."
    ],
    "abstract": "While mini-batch stochastic gradient descent (SGD) remains the de facto optimization algorithm for deep learning, both theoretical and empirical studies suggest that the noise inherent in gradient estimates serves as an implicit regularizer. We investigate this phenomenon through the lens of spectral analysis of the Hessian during training. By carefully injecting controlled gradient noise at different phases of training, we demonstrate that this can improve generalization in specific regimes, particularly on medium-sized vision and language tasks. Our experiments suggest that noise injection primarily affects the eigenvalues corresponding to the bottom subspace of the Hessian, reducing their contribution to the overall loss surface curvature. We propose a simple algorithm that adjusts noise levels based on the ratio of gradient norm to parameter norm, achieving modest but consistent improvements over standard SGD on CIFAR-10/100 (1.2-2.1% accuracy gain) and WikiText-103 (0.5 perplexity improvement). While our findings align with existing theoretical predictions about implicit regularization, the improvements are task-dependent and do not scale to larger models like ImageNet or GPT-scale transformers. Our work provides empirical insights into SGD's regularization effects but raises questions about the universality of noise-based regularization across model scales.",
    "id": 1334
  },
  {
    "title": "Curriculum-Guided Pretraining Improves Sample Efficiency in Few-Shot Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "Recent advances in few-shot reinforcement learning have focused on meta-learning algorithms, but these methods often struggle with distribution shift between training and evaluation tasks. We propose Curriculum-Guided Pretraining (CGP), a two-stage approach that first trains a policy on progressively more challenging auxiliary tasks before fine-tuning on target few-shot problems. Our curriculum uses a simple heuristic based on the estimated optimal policy's action entropy to increase task complexity. On the Meta-World benchmark, CGP improves sample efficiency by 18.7% over MAML baselines, though performance gains vanish on continuous control tasks from DMControl. Theoretical analysis suggests improvements are bounded by the suboptimality of our entropy-based curriculum, limiting scalability to more complex domains. While our approach introduces no new architectural components and maintains computational overhead below 15%, results indicate sensitivity to curriculum ordering choices. Code and pretrained models are available at anonymized-url.github.io.",
    "id": 1335
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Kumar, A.",
      "Chen, L.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed machine learning, but existing methods rely on hand-crafted compression schedules that may not adapt to varying training dynamics. We propose Learned Gradient Quantization (LGQ), a framework that uses a small auxiliary neural network to predict optimal quantization levels for gradient compression at each training step. Our key insight is that learning the quantization schedule online can reduce compression error beyond fixed schedules while maintaining convergence guarantees. LGQ employs a lightweight LSTM-based controller that takes gradient statistics as input and outputs adaptive quantization parameters for each layer. On CIFAR-10/ImageNet with ResNet architectures, LGQ achieves 16-32\u00d7 compression with marginal accuracy loss (0.3-0.7%) compared to uncompressed training, improving over static quantization baselines by 0.2-0.4%. Theoretical analysis shows LGQ converges under standard smoothness assumptions, though our bound is looser than recent fixed-schedule methods. While our approach incurs modest computational overhead (2-3% training time increase), we demonstrate promising applications in bandwidth-limited federated learning scenarios. Limitations include sensitivity to controller hyperparameters and lack of generalization to drastically different architectures without retraining the controller.",
    "id": 1336
  },
  {
    "title": "LoRA-Soup: Merging Low-Rank Adaptations via Gradient Matching for Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as an efficient fine-tuning method, but deploying multiple LoRA modules remains challenging due to memory constraints and inference overhead. We propose LoRA-Soup, a simple technique for merging task-specific LoRA matrices into a single set of weights without re-training. Our approach computes merge weights by minimizing the gradient discrepancy between the merged model and individual task-specific models on a small held-out validation set. We evaluate LoRA-Soup on 8 diverse NLP tasks using Llama-2-7B and RoBERTa-base. Results show LoRA-Soup achieves 92-95% of individual LoRA performance while reducing memory usage by 8\u00d7. However, performance degradation is larger on tasks with conflicting gradient directions (up to 15% drop), suggesting inherent limitations of our merging strategy. Analysis reveals LoRA-Soup works best when tasks share similar optimization landscapes. While our method provides a practical solution for multi-task deployment, the theoretical understanding of when merging succeeds versus fails remains incomplete.",
    "id": 1337
  },
  {
    "title": "Attention-Guided Dropout: A Simple Regularization Technique for Transformer Architectures",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "We propose Attention-Guided Dropout (AGD), a regularization technique that adaptively drops tokens in transformer architectures based on learned attention patterns. Unlike standard dropout, AGD selectively masks lower-attention tokens during training, theoretically reducing the effective sequence length and improving generalization on long-context tasks. Our method builds on the observation that transformers often attend predominantly to a small subset of tokens, suggesting that non-attended tokens contribute mostly noise. AGD requires minimal implementation changes and introduces no additional parameters. We evaluate AGD on GLUE, long-range arena, and machine translation benchmarks. Results show modest improvements (1-3% average) over baseline transformers with standard dropout, with stronger gains on tasks involving longer sequences (>2k tokens). However, performance degrades on datasets with dense attention distributions. Ablations reveal that AGD's effectiveness strongly correlates with the sparsity of learned attention patterns. While AGD offers computational benefits during training, we find that similar performance can often be achieved through careful hyperparameter tuning of existing regularizers. Code is available at [anonymized for review].",
    "id": 1338
  },
  {
    "title": "Improving Transformer Robustness via Attention Mixup: A Lightweight Regularization Approach",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, D.",
      "Johnson, A."
    ],
    "abstract": "While transformers achieve state-of-the-art performance across many tasks, they remain vulnerable to input perturbations and adversarial examples. We propose Attention Mixup, a simple regularization technique that linearly interpolates attention weights between clean and perturbed samples during training. Our method requires minimal computational overhead and can be implemented with <20 lines of code without architectural modifications. Experiments on GLUE benchmarks show 2-3% improvements in robust accuracy against word-level adversarial attacks, with comparable clean accuracy to standard training. However, we observe that gains diminish on longer sequences (>512 tokens) and certain tasks like CoLA. Our ablation studies indicate that the effectiveness primarily stems from attention entropy regularization rather than the interpolation mechanism itself. While Attention Mixup provides modest, consistent improvements across NLP tasks, we acknowledge limitations: (1) benefits are most pronounced for small-to-medium models, and (2) the approach does not extend straightforwardly to cross-modal transformers. Code and pretrained models are available at anonymous.url.",
    "id": 1339
  },
  {
    "title": "Learning with Rejection via Adaptive Thresholding for Noisy Label Robustness",
    "authors": [
      "Chen, L.",
      "Srinivasan, V.",
      "Muller, K."
    ],
    "abstract": "We study the problem of learning robust predictors when training labels are systematically noisy, focusing on the practically relevant setting where human annotators can be queried for verification at inference time. While rejection learning offers a natural framework for this setting, existing approaches require either clean validation data or strong priors on noise rates. We propose Adaptive Rejection Thresholding (ART), a simple method that jointly learns a classifier and rejection function by dynamically adjusting confidence thresholds using only noisy training data. Our key insight is that disagreement between model predictions and corrupted labels provides signal for determining when to abstain. We provide a theoretical analysis showing that ART achieves near-optimal rejection rates under symmetric label noise, though our bounds become loose under more general noise models. Empirically, ART improves absolute accuracy by 2-4% over baselines on CIFAR-10/100 with synthetic noise, but gains diminish on real-world noisy datasets like WebVision. While our method is computationally lightweight and requires minimal hyperparameter tuning, we find that performance is sensitive to the chosen confidence calibration technique. Code and pre-trained models will be released.",
    "id": 1340
  },
  {
    "title": "Gradient Steering: Towards More Stable Training by Selectively Modifying Back-propagated Signals",
    "authors": [
      "Chen, L.",
      "Singh, A.",
      "Johnson, K."
    ],
    "abstract": "We present Gradient Steering, a simple modification to standard backpropagation that improves training stability in deep neural networks. Our method selectively rescales gradient components based on their alignment with the parameter update direction from the previous step. Unlike adaptive optimizers which modify learning rates per-parameter, Gradient Steering operates at the coordinate level within each parameter tensor, allowing finer-grained control over optimization dynamics. We derive theoretical conditions under which this approach reduces gradient variance while preserving convergence guarantees. Experiments on ResNet-50, Vision Transformers, and LSTM language models show 5-12% faster convergence on ImageNet and Penn Treebank compared to baseline SGD with momentum, though improvements over strong adaptive optimizers like AdamW are less consistent. Ablation studies reveal the method is most effective when gradients exhibit high curvature. While our approach introduces two additional hyperparameters that require tuning, we provide a heuristic that works reasonably across architectures. Code will be made available upon publication.",
    "id": 1341
  },
  {
    "title": "Structured Dropout: Learning Sparse Representations Through Convex Relaxation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, T."
    ],
    "abstract": "We propose Structured Dropout, a convex relaxation approach to learning sparse neural representations by reinterpreting dropout as a regularization technique with group-sparsity constraints. While standard dropout randomly masks neurons during training, our method learns a data-dependent masking distribution through convex optimization, yielding interpretable sparsity patterns without architectural modifications. Experiments on CIFAR-10 and ImageNet show 12-15% reduction in parameters with <2% accuracy degradation compared to standard training. We provide theoretical guarantees on the convexity of the relaxed objective for single-hidden-layer networks, though extension to deeper architectures remains heuristic. Our approach offers a compromise between model compression and accuracy, achieving competitive performance against magnitude-based pruning but falling short of state-of-the-art lottery ticket results. The method is architecture-agnostic and requires only a single training run, making it practical for resource-constrained deployment. Code is available at anonymous-link.",
    "id": 1342
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its reliance on fixed rank hyperparameters limits flexibility across tasks. We propose LoRA-Drop, a simple extension that dynamically adjusts the rank during training through a magnitude-based pruning mechanism. Our approach progressively drops the least significant singular values based on gradient statistics, reducing parameters by 15-40% during fine-tuning with minimal performance loss. Experiments on GLUE and SuperGLUE show LoRA-Drop achieves comparable accuracy to standard LoRA on 6 out of 9 tasks, with 1.2\u00d7 speedup in training time. However, we observe instability on tasks requiring long-range dependencies (e.g., CoLA), where aggressive rank reduction degrades performance by 3-5%. Analysis reveals the method works best for tasks with sufficient training data, suggesting room for improvement in adaptive rank schedules. While LoRA-Drop offers practical benefits for common NLP benchmarks, its applicability may be restricted to well-resourced tasks without further regularization. Code and checkpoints are available at [anonymous link].",
    "id": 1343
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Entropy-Regularized REINFORCE",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. We propose a simple modification to the REINFORCE estimator that adds an entropy regularization term to the learning objective, which we show reduces gradient variance without introducing significant bias. Our method requires only minimal changes to existing implementations and adds negligible computational overhead. We provide theoretical analysis showing that our estimator achieves lower variance than standard REINFORCE under mild assumptions about the reward distribution. Empirical results on variational autoencoders with discrete latents show modest improvements in ELBO and sample quality on binarized MNIST and CIFAR-10, achieving 3-5% better log-likelihood compared to standard baselines. While our approach does not match the performance of more sophisticated gradient estimators like REBAR or RELAX, it offers a practical alternative when computational constraints or implementation complexity are concerns. Code is available at anonymous.github.io.",
    "id": 1344
  },
  {
    "title": "Rethinking Curriculum Learning with Adaptive Difficulty Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Curriculum learning has shown promise in improving model training efficiency and final performance by presenting examples in increasing order of difficulty. However, we observe that fixed difficulty thresholds, determined by heuristics or human judgment, often lead to suboptimal curricula that either progress too slowly or skip crucial intermediate concepts. We propose Adaptive Curriculum Thresholding (ACT), a simple yet effective method that dynamically adjusts difficulty boundaries based on the model's real-time learning progress measured via gradient norm statistics. Our approach uses a lightweight online algorithm that requires no additional hyperparameters beyond standard training settings. Experiments on CIFAR-10 and subset of ImageNet show 2-3% accuracy improvements over standard training, with more pronounced gains (5-7%) on limited data regimes. While these results are encouraging, we acknowledge limitations in our theoretical analysis and the method's diminishing returns on larger models. Our code and experiments are available at [redacted for anonymity].",
    "id": 1345
  },
  {
    "title": "LayerNorm Low-Rank Decomposition: A Simple Baseline for Efficient Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Anderson, M."
    ],
    "abstract": "We propose LayerNorm Low-Rank Decomposition (LN-LRD), a parameter-efficient method for accelerating transformer training by decomposing the weight matrices in LayerNorm operations into low-rank components. Unlike existing approaches that focus on attention mechanisms, we show that LayerNorm operations contribute significantly to memory usage and computational overhead during training, particularly for larger models. Our method approximates the LayerNorm weight matrices using rank-r factorization, where r < min(d, k) for hidden dimension d and intermediate size k. Through extensive experiments on language modeling and downstream tasks, we achieve 1.34x training speedup and 0.3% performance degradation on average across GLUE/SuperGLUE benchmarks when using rank r=4 for 350M parameter models. While LN-LRD provides consistent improvements in memory efficiency, we observe diminishing returns as model size scales beyond 1B parameters, and performance drops become more pronounced for certain tasks requiring precise normalization. Our ablation studies suggest that the method works best when combined with gradient checkpointing, potentially complementary to existing attention optimization techniques.",
    "id": 1346
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Learned Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. While continuous relaxations like the Gumbel-Softmax provide effective alternatives, they introduce temperature-dependent biases that can degrade sample quality. We propose a simple modification to existing score-function estimators by learning a parametric baseline that adapts to the local geometry of the loss landscape. Our approach uses a small neural network conditioned on intermediate activations to predict optimal control variate coefficients, reducing gradient variance without the need for temperature tuning. Unlike recent work on learned baselines, our method requires no additional model parameters at inference time and introduces minimal computational overhead. We evaluate on structured prediction tasks including generative modeling of text and molecules. Results show 15-20% reduction in gradient variance compared to REINFORCE with moving average baselines, leading to modest improvements in log-likelihood (0.05-0.1 nats on average). While the approach shows consistent gains over standard baselines, the improvements are incremental and do not address fundamental scalability limitations of discrete variable models. Code will be available upon acceptance.",
    "id": 1347
  },
  {
    "title": "Improving Transformer Training Stability via Layer-wise Learning Rate Warm-up",
    "authors": [
      "Chen, Z.",
      "Rodriguez, L.",
      "Johnson, K.",
      "Singh, P."
    ],
    "abstract": "Training instability remains a critical challenge for scaling transformers to larger models and datasets, particularly when training with aggressive learning rates. We propose Layer-Adaptive Learning Rate warm-up (LALR), a simple modification to standard optimizers that applies independent learning rate schedules to different layers based on their gradient norm evolution. Our key insight is that earlier layers in transformers exhibit more stable gradient dynamics than deeper layers during initial training phases. By applying slower warm-up schedules to deeper layers while maintaining standard schedules for earlier ones, we achieve more stable optimization trajectories without hyperparameter tuning. Experiments on language modeling tasks (WikiText-103, C4) and vision transformers (ImageNet) show 12-18% reduction in training loss variance across seeds and moderate improvements in final perplexity (0.3-0.5 points). While LALR demonstrates consistent stability improvements, the computational overhead (additional 8-12% training time) and modest performance gains may limit adoption. The method is orthogonal to existing architectural improvements and can be implemented in ~20 lines of PyTorch code.",
    "id": 1348
  },
  {
    "title": "Improving Transformer Efficiency through Structured Attention Sparsity",
    "authors": [
      "Liu, C.",
      "Rodriguez, M.",
      "Chen, K.",
      "Singh, A."
    ],
    "abstract": "While transformers have demonstrated impressive performance across diverse domains, their quadratic complexity in sequence length limits applicability to long sequences. We propose Structured Attention Sparsity (SAS), a method that combines learned sparsity patterns with hand-crafted structural priors to reduce attention complexity to O(n log n). Our approach learns to predict sparse attention masks using a lightweight auxiliary network trained jointly with the main model, while incorporating structural biases from sliding windows and dilated patterns. We demonstrate 2.1\u00d7 speedups on sequence lengths up to 8K with <1% degradation in performance across standard NLP benchmarks. Additionally, we show that SAS enables training on 16K token sequences without gradient checkpointing, expanding the practical reach of transformer architectures. While our method's gains diminish on shorter sequences (<2K tokens) and some tasks show sensitivity to sparsity hyperparameters, SAS provides a practical trade-off between efficiency and accuracy for long-context applications.",
    "id": 1349
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is Actually More",
    "authors": [
      "Chen, L.",
      "Krishnan, V.",
      "Nakamura, T."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder optimization. While recent methods propose complex gradient surgery techniques, we show that a simple thresholding operation on gradient norms can achieve comparable performance with fewer computational overhead. Our method, Gradient Norm Thresholding (GNT), discards updates from tasks whose gradient norms fall below a learned threshold during each optimization step. We evaluate GNT on 3 standard benchmarks and 2 new datasets we collected for natural language understanding. Experiments show GNT achieves 95% of the performance of state-of-the-art gradient surgery methods while requiring 40% less compute. However, we find the threshold parameter requires careful tuning across domains, and performance degrades on tasks with imbalanced loss scales. Analysis reveals GNT implicitly performs a form of task scheduling, focusing on the most informative gradients. Our code is available at [anonymous link].",
    "id": 1350
  },
  {
    "title": "Re-Weighted Gradient Clipping: A Simple Modification for Better Transformer Training Stability",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Johnson, M."
    ],
    "abstract": "Transformer training often suffers from instability due to gradient explosion, particularly when scaling to larger models. While gradient clipping is widely used, we observe that standard methods indiscriminately reduce all gradient components, potentially losing useful signal. We propose Re-Weighted Gradient Clipping (RWGC), a simple modification that adaptively re-weights gradient components based on their historical norms before clipping. This approach preserves gradients from well-behaved parameters while aggressively limiting updates from unstable ones. Our method requires only two additional hyperparameters and minimal computational overhead. Across 7 transformer architectures on language modeling and machine translation tasks, RWGC reduces training instability without harming final performance in 5/7 cases, achieving BLEU improvements of 0.3-0.8 over baseline clipping. However, we find RWGC provides diminishing returns on smaller models and offers limited benefits when strong hyperparameter tuning is applied. While our contributions are primarily empirical, the simplicity and trivial implementation make RWGC a practical alternative to standard clipping for practitioners training large transformers under computational constraints. Code is available at [anonymous link].",
    "id": 1352
  },
  {
    "title": "LoRA-P90: A Simple Thresholding Scheme for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular method for parameter-efficient fine-tuning of large language models, but its rank selection remains largely heuristic. We propose LoRA-P90, a straightforward thresholding approach that prunes 10% of the smallest singular values in the LoRA weight matrices during training. Through experiments on GLUE and SuperGLUE benchmarks using LLaMA-7B, we show that this simple modification yields modest improvements (0.7-1.2% average score increase) over standard LoRA while maintaining parameter efficiency. Our analysis reveals that thresholding helps remove noisy updates, particularly beneficial when the adaptation rank is set higher than necessary. While the contribution is incremental, our work suggests that careful analysis of singular value distributions in LoRA modules can inform better rank selection strategies. Code and pre-trained adapters will be made available upon acceptance.",
    "id": 1353
  },
  {
    "title": "Regularizing Gradient Noise for Improved Generalization in Stochastic Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, V.",
      "Okafor, N."
    ],
    "abstract": "While stochastic gradient descent (SGD) remains the workhorse of deep learning optimization, its inherent noise is often viewed as a double-edged sword\u2014potentially beneficial for generalization yet difficult to control. We propose Gradient Noise Regularization (GNR), a simple technique that adds controlled noise to gradients during training, where the noise variance is adaptively scaled based on the gradient's magnitude. Our method builds on the intuition that moderate levels of gradient noise can act as implicit regularization, but excessive noise harms optimization. Through extensive experiments on CIFAR-10/100 and ImageNet, we demonstrate that GNR achieves 0.5-1.2% accuracy improvements over standard SGD with momentum, while maintaining comparable training speed. Theoretical analysis in a simplified quadratic setting suggests GNR approximates a form of data-dependent regularization. While the gains are consistent, they are modest compared to recent advances in sharpness-aware minimization and adaptive optimization. We further find that GNR's effectiveness varies significantly across architectures and datasets, with limited benefits on vision transformers. Our code is available at anonymous-link.github.io/GNR.",
    "id": 1354
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simple Baseline That Works Too Well",
    "authors": [
      "Liu, C.",
      "Johnson, K.",
      "Chen, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients when optimizing shared representations. While sophisticated approaches like PCGrad and GradNorm have been proposed to address this, we re-examine whether simpler alternatives might suffice. We introduce Gradient Surgery (GS), a lightweight method that selectively drops gradients based on their inner products with a reference direction. Rather than projecting gradients as in previous work, GS simply discards conflicting updates when their alignment falls below a learned threshold. Surprisingly, this crude approach achieves competitive performance across standard multi-task benchmarks including NYUv2 and CityScapes, matching or exceeding more complex methods while requiring minimal hyperparameter tuning. However, our method shows inconsistent gains on synthetic tasks designed to induce extreme gradient conflicts, and theoretical analysis reveals that GS is not guaranteed to converge to Pareto optimal solutions. Empirically, we demonstrate 2-4% improvements over vanilla multi-task baselines and comparable results to state-of-the-art methods, though gains diminish when tasks are less correlated. While GS's simplicity is appealing, our results suggest that the multi-task optimization problem may be less complex than previously assumed, raising questions about the necessity of sophisticated gradient manipulation techniques.",
    "id": 1355
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique for Deep Networks via Selective Parameter Freezing",
    "authors": [
      "Chen, L.",
      "Johnson, K.M.",
      "Rodriguez, A."
    ],
    "abstract": "We introduce Gradient Dropout (GradDrop), a regularization method that randomly freezes parameter updates during training based on gradient norms. Unlike standard dropout that operates on activations, GradDrop selectively prevents weight updates for parameters with the smallest gradient magnitudes in each mini-batch. This approach is motivated by the observation that small-gradient parameters contribute minimally to learning while potentially overfitting to noise. Our method requires minimal computational overhead and only a single hyperparameter controlling the freezing probability. We evaluate GradDrop across image classification (CIFAR-10/100, ImageNet), language modeling (penn-treebank), and tabular data tasks. Results show modest improvements over vanilla SGD (0.5-1.2% accuracy gains on CIFAR, 1-2 perplexity reduction on Penn Treebank) that come within 0.3% of dropout performance at a fraction of the computational cost. However, GradDrop underperforms on several benchmarks compared to recent state-of-the-art regularizers like MixUp or CutMix. Analysis reveals GradDrop works best for medium-sized networks (1-10M parameters) but shows diminishing returns for very deep architectures. While theoretically justified through gradient noise analysis, our empirical gains are incremental and may not justify adoption given established alternatives. Code is available at anonymized.github.io/graddrop.",
    "id": 1356
  },
  {
    "title": "Self-Annealing: Improving Transformer Convergence through Adaptive Temperature Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Thompson, A."
    ],
    "abstract": "Transformer training typically relies on fixed learning rate schedules combined with standard softmax temperature. We propose Self-Annealing, a lightweight method that adaptively adjusts attention temperatures based on gradient statistics during training. By maintaining running estimates of attention entropy at each layer, we derive temperature updates that encourage more discriminative attention patterns when gradient norms are low and permit softer distributions during high-uncertainty periods. Our approach requires only two additional scalar hyperparameters and adds <1% computational overhead. We evaluate Self-Annealing on WMT'14 En-De, CIFAR-10 classification, and Wikitext-103 language modeling. Results show modest improvements: 0.3 BLEU gains in translation, 0.5% accuracy improvement on CIFAR-10, and 1.2 PPL reduction on Wikitext-103 over well-tuned baselines. While these improvements are consistent across tasks, they fall within the variance of baseline hyperparameter tuning. Ablation studies reveal our method primarily helps early training dynamics but provides diminishing returns as optimization converges. Theoretical analysis suggests our adaptive schedule approximates a second-order method under restrictive assumptions that may not hold in practice. Code and trained models will be released upon publication.",
    "id": 1357
  },
  {
    "title": "LoRA-Combo: Efficient Multi-Task Adaptation via Composable Low-Rank Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Thompson, K."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods like LoRA enable task-specific adaptation of large pre-trained models with minimal memory overhead. However, combining multiple LoRA adapters for multi-task scenarios remains poorly understood, often requiring ad-hoc ensemble methods or task-specific routing. We propose LoRA-Combo, a principled approach to compose multiple LoRA adapters through learned linear combinations in the low-rank subspace. Our method introduces a lightweight meta-network that predicts optimal combination coefficients based on task embeddings, eliminating the need for task-specific routing modules. We evaluate LoRA-Combo across 8 diverse NLP and vision benchmarks, achieving comparable performance to full fine-tuning while using less than 1% of trainable parameters. However, our results reveal a trade-off: while LoRA-Combo improves over vanilla LoRA on 6/8 tasks (average +2.3 F1), it underperforms trained-from-scratch adapters on 2 high-data regimes. Theoretical analysis suggests this limitation stems from rank collapse in the composed updates. Our code is available at [placeholder].",
    "id": 1358
  },
  {
    "title": "Recurrent Rectifier Networks: A Hybrid Architecture for Improved Gradient Flow in Deep Sequence Models",
    "authors": [
      "Liang, C.",
      "Kumar, A.",
      "Steinberg, D."
    ],
    "abstract": "While rectified linear units (ReLUs) have become ubiquitous in feedforward networks, their integration into recurrent architectures remains challenging due to saturation effects and vanishing gradients in long sequences. We propose Recurrent Rectifier Networks (RRNs), which replace traditional tanh activations in LSTMs with piecewise-linear functions while introducing a learned gating mechanism to control the rectifier slope. Our key insight is that carefully constrained ReLU variants can improve gradient propagation without destabilizing state transitions. We evaluate RRNs on language modeling benchmarks (Penn Treebank, WikiText-103) and action recognition datasets, achieving 2-4% perplexity improvements over standard LSTMs while reducing training time by 15%. However, we find these gains diminish on shorter sequences and are sensitive to initialization scale. Theoretical analysis suggests RRNs improve gradient norms by a constant factor but do not fundamentally address the vanishing gradient problem. While our architecture offers practical benefits for specific sequence lengths, it represents only an incremental advance over existing gated recurrent designs.",
    "id": 1359
  },
  {
    "title": "Reinforcement Learning with Pretrained Language Model Critics via Soft Q-Value Distillation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Recent work has explored using large language models (LLMs) as policy initialization for reinforcement learning (RL), but the role of LLMs as critics remains underexplored. We propose Soft Q-value Distillation (SQD), an algorithm that leverages pretrained LLM critics to improve sample efficiency in RL fine-tuning. Our approach uses an LLM to generate soft Q-value estimates as privileged supervision, which are then distilled into a smaller policy network through a modified Bellman backup. We evaluate SQD on 8 Atari environments and 4 text-based games, showing 15-30% faster convergence than standard PPO baselines while maintaining comparable final performance. However, our gains diminish when the LLM critic is not well-aligned with the environment dynamics, and we observe considerable variability across tasks. Ablations reveal that the primary benefit comes from better exploration early in training rather than improved exploitation. While our results suggest potential for cost-effective RL when compute is available for offline LLM inference, we acknowledge limitations including high memory requirements and sensitivity to prompt engineering. Our findings indicate LLM critics may be most valuable as soft guidance rather than hard constraints in RL applications.",
    "id": 1360
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Recursive Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.M.",
      "Kim, S.",
      "Johnson, T."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. While control variates have shown promise for variance reduction, existing methods typically rely on simple baselines that fail to capture complex dependencies in high-dimensional spaces. We propose a recursive control variate framework that constructs increasingly accurate baselines by conditioning on progressively larger subsets of variables. Our method generalizes several existing approaches and provides a principled way to trade computation for variance reduction. We evaluate our approach on structured prediction tasks with binary and categorical latent variables, demonstrating 15-30% variance reduction over strong baselines on synthetic datasets. On real-world applications including bit-vector variational autoencoders and neural machine translation with discrete bottlenecks, we achieve modest improvements in ELBO (up to 0.2 nats) and sample efficiency (10-15% fewer gradient steps to convergence). While our recursive construction offers theoretical appeal, we find diminishing returns beyond 2-3 recursion levels, and computational overhead limits applicability to very large models. Our results suggest the approach is most beneficial when gradient variance is the primary bottleneck, though this occurs less frequently in practice than previously assumed.",
    "id": 1361
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Inference via Continuous Relaxations with Learned Temperature",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Agarwal, V."
    ],
    "abstract": "Training discrete latent variable models remains challenging due to the high variance of gradient estimators. While continuous relaxations like the Gumbel-Softmax reparameterization have enabled gradient-based training, their fixed temperature parameter often leads to suboptimal bias-variance tradeoffs. We propose a simple modification where the temperature is treated as a learnable parameter optimized through an auxiliary loss that encourages the relaxation to closely approximate the discrete distribution. Our method, which requires only a few lines of code, consistently reduces gradient variance across several benchmark tasks including structured prediction and variational autoencoders with structured priors. On CIFAR-10 generation with discrete latent codes, we observe a 12% improvement in ELBO over standard Gumbel-Softmax without additional computational cost. While our approach does not fully close the gap with more sophisticated recent techniques, its simplicity and ease of implementation make it an effective baseline for practitioners working with discrete variables. We further demonstrate that learned temperatures provide interpretable insights into the difficulty of different learning problems.",
    "id": 1362
  },
  {
    "title": "Efficient Gradient Descent via Adaptive Block-Diagonal Preconditioning",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Rodriguez, M.",
      "Chen, L."
    ],
    "abstract": "We propose BlockAda, an adaptive optimization method that constructs block-diagonal preconditioners for gradient descent using only first-order information. Our approach partitions model parameters into dynamically-sized blocks based on gradient correlations, then applies separate adaptive learning rates to each block via a lightweight online estimation procedure. Unlike full-matrix preconditioning methods, BlockAda maintains linear complexity in parameter count while capturing some of the benefits of second-order optimization. We provide convergence guarantees for convex problems and demonstrate empirical improvements over Adam and SGD on several benchmark tasks. In experiments on CIFAR-10 and ImageNet classification, BlockAda achieves comparable accuracy to Adam while reducing memory usage by 35% and training time by 12-18%. Analysis reveals the method is particularly effective for models with heterogeneous parameter groups (e.g., transformers), though gains are modest for simpler architectures. While our theoretical results are limited to convex settings, empirical findings suggest BlockAda may serve as a practical alternative to standard optimizers in memory-constrained scenarios where full second-order methods are prohibitive. Code is available at [URL].",
    "id": 1363
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Structure-Aware Pruning for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed-rank matrices may be suboptimal across different layers and tasks. We propose LoRA-Drop, a novel approach that adaptively prunes LoRA modules based on gradient-based importance scores and layer-wise capacity metrics. Our method dynamically adjusts the rank of adaptation matrices during training, enabling more efficient fine-tuning while maintaining model expressivity. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop achieves comparable performance to standard LoRA while using 25-40% fewer parameters across various model sizes (350M-7B parameters). However, we observe that performance gains are task-dependent, with improvements primarily concentrated in classification tasks rather than generation tasks. Our analysis reveals that the effectiveness of rank pruning correlates with the intrinsic dimensionality of downstream tasks, suggesting limited benefits for tasks with high semantic complexity. While LoRA-Drop provides parameter efficiency, the computational overhead of importance estimation during training reduces overall wall-clock time benefits. Code and pretrained adapters will be available upon acceptance.",
    "id": 1364
  },
  {
    "title": "Improving Transformer Efficiency Through Iterative Token Pruning with Learnable Retention Thresholds",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Rodriguez, C.",
      "Johnson, A."
    ],
    "abstract": "Transformer models have achieved remarkable success across NLP tasks but suffer from quadratic complexity in sequence length. While prior work has explored token pruning to reduce computational cost, existing methods struggle to maintain performance when removing large numbers of tokens. We propose Adaptive Token Retention (ATR), a novel pruning mechanism that learns dynamic retention thresholds for different attention heads and layers. Our key insight is that interpretable features in the value vectors\u2014particularly the L2 norm and entropy\u2014can serve as reliable indicators for token importance without additional learnable parameters. ATR alternates between pruning and selective re-insertion phases to recover from aggressive pruning decisions. Experiments on GLUE and SQuAD 2.0 benchmarks show ATR achieves 20-45% FLOP reduction with <2% accuracy loss at moderate sparsity levels (30-50% pruning), though performance degrades more sharply at higher sparsity (60-70%) compared to baseline methods. Analysis reveals ATR is most effective on longer sequences (>512 tokens) where attention redundancy increases. While our approach provides practical efficiency gains, theoretical guarantees about attention approximation quality remain limited, and the method requires careful hyperparameter tuning across datasets.",
    "id": 1365
  },
  {
    "title": "Attention Amplification in Transformers: A Frequency Domain Analysis of Layer-Wise Token Interactions",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Recent work has revealed that multi-head attention mechanisms exhibit complex patterns of token interaction across layers, but the theoretical understanding of how these interactions evolve remains limited. We propose a frequency-domain analysis framework that decomposes attention matrices using discrete Fourier transforms to quantify the amplification and attenuation of different frequency components across transformer layers. Our approach enables us to characterize attention patterns as low-pass or high-pass filters with layer-dependent cutoffs. Through experiments on BERT and GPT-2 models across language modeling and text classification tasks, we demonstrate that models learn to amplify mid-frequency components (representing syntactic dependencies) while attenuating both very high and very low frequencies. While our theoretical analysis provides novel insights into attention dynamics, we acknowledge that our framework currently handles only simplified attention patterns and cannot fully capture the non-linear interactions in deeper models. Empirically, we show that frequency-aware weight initialization can accelerate convergence by 12-15% on several tasks, though the benefits diminish for larger models. Our code and pretrained models are available at [URL].",
    "id": 1366
  },
  {
    "title": "Momentum Schedules for Asynchronous Federated Learning: A Variance-Reduced Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose VR-MoFed, a variance-reduced momentum-based algorithm for asynchronous federated learning that adapts the local momentum parameter based on client staleness. While momentum has been extensively studied in centralized and synchronous federated settings, its behavior under asynchronous client participation remains poorly understood. Our key insight is that stale gradients introduce a bias that can be partially corrected through a momentum schedule that decays exponentially with the delay between local updates. We provide a convergence analysis showing O(1/T) rate for smooth convex objectives, though our bound depends on a delay-related constant that may scale poorly in practice. Experiments on CIFAR-10 and Fashion-MNIST across 100 clients demonstrate 5-12% improvement over vanilla FedAvg with momentum, particularly at high asynchrony levels. However, gains diminish significantly when staleness is low or data heterogeneity is severe. We additionally find that the theoretical assumption of bounded delays is violated in realistic deployments, motivating a heuristic clipping strategy that works reasonably but lacks theoretical grounding. While our method provides moderate practical benefits for specific asynchronous scenarios, the improvements over properly tuned baselines are modest and the approach requires careful hyperparameter tuning that may limit immediate adoption.",
    "id": 1367
  },
  {
    "title": "LoRA-Drop: Structured Pruning of Low-Rank Adaptation Weights via Magnitude-Aware Gradient Tracking",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We present LoRA-Drop, a simple method for pruning low-rank adaptation matrices in parameter-efficient fine-tuning. While LoRA has become prevalent for adapting large language models, the optimal rank choice remains ad hoc and can lead to overparameterization. Our approach tracks gradient magnitudes during initial training steps to identify important rank components, then drops the least significant 30-50% of singular values without retraining. Experiments on GLUE and SuperGLUE show 20-35% parameter reduction with <1% performance degradation on most tasks. However, we observe performance drops exceeding 5% on tasks requiring strong reasoning (e.g., ReCoRD), suggesting our importance heuristic may miss task-specific features. Analysis reveals LoRA-Drop works best when pre-trained representations are well-matched to downstream tasks. While preliminary, these results indicate structured pruning can reduce LoRA footprint at modest accuracy cost, though further work is needed to develop better importance criteria for complex reasoning tasks.",
    "id": 1368
  },
  {
    "title": "Gradient Compression with Learned Quantization Schedules for Heterogeneous Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Johnson, K."
    ],
    "abstract": "Federated learning faces communication bottlenecks when training across heterogeneous devices with varying bandwidth and computational constraints. While gradient compression techniques reduce communication overhead, they typically use fixed quantization strategies that cannot adapt to the diverse capabilities of participating devices. We propose Adaptive Learned Quantization (ALQ), a method that learns device-specific quantization schedules during training using lightweight meta-networks that predict optimal bit-widths based on local gradient statistics and device characteristics. Our approach introduces novel regularization terms that balance compression efficiency with convergence stability, extending theoretical convergence bounds to account for adaptive quantization. Experiments on CIFAR-10, CIFAR-100, and a real-world federated image classification dataset demonstrate 1.8-3.2x communication reduction compared to uniform quantization baselines, with modest accuracy drops (0.5-1.2%). However, we observe that ALQ's benefits diminish in highly non-IID settings and with very large models (>50M parameters), where meta-network overheads can exceed communication savings. Our code is available at [anonymized link].",
    "id": 1369
  },
  {
    "title": "LoRA-MoE: Parameter-Efficient Mixture-of-Experts via Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, J."
    ],
    "abstract": "We propose LoRA-MoE, a parameter-efficient approach for scaling transformer models using a mixture-of-experts architecture. Our method combines low-rank adaptation (LoRA) with sparse expert routing to achieve compute-efficient fine-tuning while maintaining the benefits of large expert networks. Instead of adding separate expert layers, we apply LoRA modules to different subsets of parameters, treating each low-rank decomposition as an 'expert'. A lightweight routing network selects which LoRA experts to activate for each input token, enabling dynamic adaptation with minimal additional parameters. We evaluate our approach on GLUE, SuperGLUE, and domain transfer benchmarks, showing 3-5% improvements over standard LoRA while using 50% fewer trainable parameters compared to full fine-tuning. However, we observe that performance gains diminish on smaller models (<7B parameters), and routing behavior becomes unstable when scaling beyond 8 experts. Our experiments suggest that the benefits of LoRA-MoE are most pronounced in moderate-scale settings where parameter efficiency is critical but compute resources are not severely constrained.",
    "id": 1370
  },
  {
    "title": "Improving Transformer Training Efficiency via Selective Attention Pruning with Learned Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Self-attention mechanisms in Transformers exhibit significant redundancy, with many attention patterns contributing minimally to final performance. We propose Selective Attention Pruning (SAP), a simple method that dynamically masks low-contributing attention weights based on learned thresholds. Our approach applies a differentiable threshold gate to attention weights during training, enabling the model to identify and suppress uninformative attention patterns while maintaining task performance. We demonstrate SAP on standard language modeling and machine translation benchmarks, achieving 15-25% reduction in FLOPs with minimal degradation in perplexity and BLEU scores. While our method shows consistent improvements over static sparsity baselines, the gains are task-dependent and diminish for larger models. The learned thresholds reveal interesting patterns in attention redundancy, though our analysis suggests the relationship between attention sparsity and model capacity remains complex. Code and pretrained models will be released upon acceptance.",
    "id": 1371
  },
  {
    "title": "Lipschitz-Regularized Transformers Learn More Robust Attention Patterns But At Significant Computational Cost",
    "authors": [
      "Kim, S.",
      "Thompson, J.",
      "Garcia, L.",
      "Zhao, M."
    ],
    "abstract": "We propose a novel regularization scheme for transformer architectures that enforces Lipschitz continuity across all attention layers. While existing work has primarily focused on improving robustness through adversarial training or input perturbations, our approach directly constrains the sensitivity of attention mechanisms to input variations. We derive upper bounds on the Lipschitz constant of multi-head attention and implement an efficient proximal gradient method to enforce these constraints during training. Our experiments on three NLP benchmarks (SQuAD, SST-2, and CoLA) demonstrate 5-12% improvements in adversarial robustness compared to standard transformers, with minimal degradation on clean accuracy. However, our method increases training time by 2.3x and requires 40% more memory due to the additional constraint computations. Through ablation studies, we find that applying regularization only to the final three layers achieves 80% of the robustness benefits at 60% of the computational cost. While our theoretical analysis provides clear intuitions about the regularization effects, the practical trade-offs between robustness and computational efficiency limit immediate adoption for large-scale applications.",
    "id": 1372
  },
  {
    "title": "Reinforcement Learning with Gradient-Augmented Value Functions: A Quasi-Newton Approach to Policy Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Policy gradient methods face well-documented challenges with sample efficiency and convergence in high-dimensional action spaces. We propose Gradient Augmented Policy Optimization (GAPO), which incorporates second-order curvature information via a quasi-Newton approximation of the value function landscape. Our method computes natural policy gradients using a computationally efficient rank-2 update of the preconditioning matrix, avoiding expensive Hessian computations while capturing local geometry. We prove that GAPO achieves convergence rates of O(1/\u221aT) in the general case and O(1/T) under certain smoothness assumptions, matching theoretical bounds of existing approaches while reducing per-iteration complexity. Experimental results on MuJoCo continuous control benchmarks demonstrate 15-23% sample efficiency improvements over PPO and SAC on half of the tested environments, with comparable performance on the remainder. However, we observe training instability in environments with sparse rewards. The method introduces three additional hyperparameters that require environment-specific tuning. While GAPO provides meaningful gains in specific domains, its practical impact may be limited by implementation complexity and sensitivity to hyperparameter choices.",
    "id": 1373
  },
  {
    "title": "Gradient Sparsity Can Be Sufficient: A Parameter-Efficient Fine-Tuning Method for Memory-Constrained Inference",
    "authors": [
      "Liu, K.",
      "Singh, P.",
      "Okafor, C."
    ],
    "abstract": "While parameter-efficient fine-tuning (PEFT) methods like adapters and LoRA have achieved impressive results for downstream adaptation, they introduce additional inference latency and memory overhead that limit deployment in memory-constrained settings. We propose SparseGrad, a simple alternative that leverages gradient sparsity patterns during fine-tuning to identify and update only the most influential subset of parameters. Our method requires no architectural modifications and maintains the original model's inference-time characteristics. We demonstrate that on the GLUE benchmark, updating just 0.1% of parameters using our gradient-based selection strategy achieves 96% of full fine-tuning performance for BERT-base, outperforming LoRA in 6/8 tasks while using 4x fewer parameters. However, our approach shows diminishing returns on larger models (BERT-large, DeBERTa), achieving only 89% of baseline performance, suggesting the method's effectiveness is task and model-scale dependent. Theoretical analysis reveals that gradient sparsity success correlates with the alignment between task-specific gradients and the pre-trained model's parameter landscape. While our method provides a memory-efficient alternative to traditional PEFT, we acknowledge that the sparse update scheme may fail when downstream tasks require significant semantic shifts from pre-training. Our implementation requires only 15 lines of code and integrates seamlessly into existing training pipelines.",
    "id": 1374
  },
  {
    "title": "Improving Transformer Efficiency Through Gradient-Based Token Dropping",
    "authors": [
      "Kim, S.",
      "Johnson, M.",
      "Liu, W."
    ],
    "abstract": "Transformer models face computational bottlenecks from processing long sequences where many tokens contribute little to the final prediction. We propose TopDrop, a simple method to dynamically drop low-importance tokens during training using gradient-based importance scores. The key insight is that tokens with small gradients with respect to the loss contribute less to learning and can be safely discarded. Our approach maintains an explicit trade-off between efficiency and performance through a learned threshold parameter. On standard NLP benchmarks, TopDrop reduces inference time by 15-25% with minimal performance degradation (less than 1.5% drop in accuracy compared to full models). While our method shows consistent improvements over static pruning baselines, gains diminish on tasks requiring fine-grained reasoning over long contexts. We provide theoretical justification showing that gradient-based dropping preserves lower bounds on representational capacity under certain assumptions. However, our analysis reveals sensitivity to hyperparameter choices and task-specific tuning requirements.",
    "id": 1375
  },
  {
    "title": "Making Transformers Work with Smaller Attention Windows: A Block-Sparse Approximation Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "The quadratic complexity of attention mechanisms in Transformers remains a critical bottleneck for processing long sequences. While numerous efficient attention variants have been proposed, most either require careful hyperparameter tuning or introduce architectural constraints that limit their applicability. We propose a simple block-sparse approximation that subdivides the attention matrix into learnable windows of varying granularity. Our method requires no changes to existing Transformer architectures beyond replacing the attention computation, making it easily adoptable in standard frameworks. On language modeling tasks with sequences up to 8K tokens, our approach achieves 2.1-2.4x speedup during training and 1.8-2.2x during inference compared to standard attention, while maintaining within 2-3% of baseline perplexity. On downstream tasks like question answering and summarization, we observe modest improvements over baseline efficiency metrics but mixed results on task-specific performance. While our method provides consistent computational benefits, we acknowledge limitations in handling tasks requiring very long-range dependencies and the need for additional memory during gradient computation. Our implementation is available at [URL to be provided upon acceptance].",
    "id": 1376
  },
  {
    "title": "Improving Transformer Efficiency through Iterative Token Dropping During Training",
    "authors": [
      "Chen, L.",
      "Subramanian, K.",
      "Nakamura, T."
    ],
    "abstract": "Transformer models suffer from quadratic complexity with respect to sequence length, limiting their applicability to long sequences. While token dropping methods exist, they primarily focus on inference-time optimization, missing opportunities for training-time efficiency gains. We propose Iterative Token Elimination (ITE), a simple training-time approach that gradually drops tokens based on their attention entropy across layers. Our method requires only three additional hyperparameters and minimal code changes. On five standard benchmarks including GLUE and long-range tasks, ITE reduces training FLOPs by 16-23% while maintaining within 2% of baseline accuracy. However, we observe that performance degradation becomes more pronounced on tasks requiring fine-grained reasoning, suggesting ITE may be less suitable for applications where every token is potentially important. Theoretical analysis shows our method provides a bounded approximation of full attention under mild assumptions. Code and pre-trained models will be released upon acceptance.",
    "id": 1377
  },
  {
    "title": "Gradient Starvation in Transformer Attention: A Spectral Analysis of Training Dynamics",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate the phenomenon of gradient starvation in multi-head attention mechanisms during Transformer training, where certain attention heads receive vanishingly small gradient updates. Through spectral analysis of the attention Jacobian, we establish that gradient starvation correlates with the emergence of low-rank structures in attention matrices, particularly in deeper layers. We propose a simple reparameterization of the attention scoring function that encourages more uniform gradient flow across heads. Experiments on Wikitext-103 and GLUE benchmarks show modest improvements (0.8-1.2 perplexity reduction, 0.3-0.7 GLUE score gains) over standard architectures, with the most pronounced effects in deeper 24-layer models. Our theoretical analysis provides insights into attention sparsification but relies on strong assumptions about layer-wise independence. While our method introduces minimal computational overhead (\u22642% increase in training time), the empirical gains are task-dependent and do not clearly justify architectural changes across all settings.",
    "id": 1378
  },
  {
    "title": "LoRA-Drop: Structured Pruning of Low-Rank Adaptations via Gradient-Free Importance Scoring",
    "authors": [
      "Chen, Y.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have enabled effective adaptation of large language models while maintaining computational efficiency. However, the cumulative overhead of storing multiple LoRA modules across tasks remains significant. We propose LoRA-Drop, a simple method for pruning LoRA parameters without requiring gradient computations or task-specific validation data. Our approach uses an importance scoring mechanism based on the singular values of LoRA matrices, combined with a novel structured pruning strategy that removes entire rank components. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop can reduce parameter counts by 30-50% with minimal performance degradation (\u22641.2% accuracy drop) across various model sizes. While our method achieves competitive compression ratios compared to more complex approaches, we observe that performance degradation increases significantly for tasks requiring nuanced reasoning. Our results suggest that magnitude-based importance scores may be insufficient for capturing task-specific parameter interactions. The method is easy to implement and requires no additional training, though we acknowledge limitations in handling heterogeneous downstream tasks. Code and pre-trained adapters will be released upon acceptance.",
    "id": 1379
  },
  {
    "title": "Adaptive Gradient Clipping with Historical Norms for Transformer Training",
    "authors": [
      "Kim, J.",
      "Liu, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer models often suffer from gradient explosion during training, particularly when using large learning rates or batch sizes. While gradient clipping is a common remedy, fixed clipping thresholds can be overly conservative or ineffective. We propose Historical Norm Gradient Clipping (HNGC), which adaptively sets clipping thresholds based on the distribution of gradient norms observed during training. Our method maintains an exponentially-decayed estimate of gradient norm statistics and clips gradients whose norms exceed a learned percentile threshold. We evaluate HNGC on language modeling tasks with GPT-2 architectures from 117M to 1.5B parameters. Our approach achieves 3-5% perplexity improvements over fixed-clipping baselines on Wikitext-103 and reduces training time by 10-15% to reach baseline perplexity levels. Ablation studies show the historical norm component contributes 60% of the improvement over naive clipping. While these results are encouraging, our theoretical analysis remains limited to simplified settings and fails to explain performance gains in full-scale architectures. The method's simplicity may limit its novelty, though practitioners could find value in our lightweight implementation requiring no additional hyperparameters beyond the decay rate.",
    "id": 1380
  },
  {
    "title": "On the Convergence of Gradient Descent for Overparameterized ReLU Networks with Layer-wise Step Sizes",
    "authors": [
      "Chen, Y.",
      "Rodriguez, A.",
      "Kim, S.",
      "Nair, V."
    ],
    "abstract": "We study the convergence properties of gradient descent for training overparameterized ReLU networks when using layer-wise adaptive step sizes. Motivated by empirical observations that different layers in deep networks exhibit varying gradient magnitudes, we propose a simple modification to standard gradient descent where each layer's update is scaled by an individual step size. Our theoretical analysis shows that with appropriate initialization and sufficient overparameterization, this approach achieves linear convergence to a global minimum for binary classification problems. We prove this under a modified neural tangent kernel framework that accounts for layer-dependent learning rates. Experimental results on MNIST and CIFAR-10 datasets demonstrate that layer-wise step sizes can provide modest improvements over standard gradient descent, reducing training time by 10-20% while achieving comparable test accuracy. However, the benefits diminish as network depth increases beyond 10 layers. While our theoretical results provide some insight, the assumptions regarding layer-wise smoothness and initialization may be too restrictive for practical settings. Our work suggests that layer-wise step sizes can be a useful heuristic for training shallow networks, but further investigation is needed to understand their role in deeper architectures.",
    "id": 1381
  },
  {
    "title": "Memory-Efficient Batch Normalization Through Kernel Gradient Approximation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Batch normalization remains memory-intensive for training large neural networks due to the need to store intermediate activations for gradient computation. We propose Approximate Batch Normalization (ABN), a simple modification that trades a controlled amount of precision for up to 40% reduction in activation memory. ABN approximates the gradient kernel using a low-rank approximation computed on-the-fly during the backward pass, avoiding the need to store full activation tensors. Our theoretical analysis shows that the approximation error remains bounded when batch sizes exceed 256, and we provide convergence guarantees for convex objectives. We evaluate ABN on ImageNet classification and COCO object detection with ResNet-50, EfficientNet-B0, and Mask R-CNN architectures. Results show that ABN achieves within 0.3% accuracy of standard batch norm while enabling training with 35% larger batch sizes on a single GPU. However, we find that the approximation degrades performance for very small batches (\u2264 64) and networks with extreme depth (> 200 layers). While ABN provides practical memory savings, the limitations for small-batch and ultra-deep architectures suggest careful application is necessary. Code is available at anonymized-url.",
    "id": 1382
  },
  {
    "title": "Revisiting Feature Dropout: A Simple Baseline for Improving Transformer Calibration",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Jones, D."
    ],
    "abstract": "While transformer architectures have achieved remarkable success across domains, their calibration properties remain suboptimal \u2014 often exhibiting overconfident predictions that limit deployment in high-stakes applications. We investigate whether simple architectural modifications can improve calibration without sacrificing accuracy. Specifically, we revisit feature dropout techniques (dropout applied to hidden representations rather than attention weights) and demonstrate that carefully tuned dropout rates at multiple transformer layers can significantly improve expected calibration error (ECE) across vision and language tasks. Our method, Multi-Layer Feature Dropout (MLFD), adds minimal computational overhead and can be implemented in <20 lines of code. On ImageNet and GLUE benchmarks, MLFD reduces ECE by 15-25% compared to standard transformers while maintaining competitive accuracy. However, we find the calibration improvements are most pronounced on larger models (>100M parameters) and diminish on smaller architectures. This suggests the benefits may be attributed to implicit regularization effects rather than fundamental mechanisms. While MLFD provides a strong baseline for calibrated transformers, our theoretical analysis remains limited \u2014 we provide partial insights through the lens of noise injection in Gaussian processes, but a complete theoretical understanding remains elusive. Our implementation and pretrained models are available at [anonymous link].",
    "id": 1383
  },
  {
    "title": "Don't Blame the Learning Rate: Revisiting Step Size Scheduling in AdamW Through the Lens of Batch Statistics",
    "authors": [
      "Liu, S.",
      "Garcia, M.",
      "Thompson, J."
    ],
    "abstract": "We investigate the interaction between adaptive optimizers and step size scheduling in large-scale neural network training, focusing on why cosine annealing with AdamW often outperforms more sophisticated schedules. Through careful analysis of batch gradient statistics across ImageNet and CIFAR-10 training runs, we find that the apparent effectiveness of cosine schedules correlates with implicit gradient norm regularization rather than classical learning rate effects. Our key observation is that cosine schedules produce gradient distributions with lower kurtosis, leading to improved generalization. Surprisingly, we find that a fixed learning rate combined with explicit gradient clipping achieves comparable performance to cosine annealing in 7/12 experimental settings, with minor modifications to the clipping threshold. While our empirical findings provide valuable insights for practitioners choosing optimizers and schedules, our theoretical framework remains incomplete. We conjecture that the kurtosis-regularization effect extends beyond cosine schedules to other monotonically decreasing schedules, though verifying this claim requires further investigation. Our code and training logs are available at anonymized-url.",
    "id": 1384
  },
  {
    "title": "Towards Practical Robustness Certificates via Interval Bound Tightening for Small ReLU Networks",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We present a new method for computing tighter robustness certificates for small ReLU networks by refining interval bounds using learned linear relaxations. While existing complete verifiers provide tight bounds, their computational cost scales exponentially with network size. Our approach achieves a middle ground by augmenting efficient incomplete verifiers with learnable linear bound propagation rules trained to minimize bound looseness on representative inputs. We evaluate our method on MNIST and CIFAR-10 networks with fewer than 3 hidden layers. Across all tested architectures, our certificates improve certified accuracy by 8-15% over standard interval bound propagation, while maintaining verification times under 2 seconds per example. However, we observe diminishing returns on larger networks (>100k parameters), suggesting our method is primarily suitable for resource-constrained applications like embedded systems. Theoretically, we prove that our learned relaxations preserve soundness while providing monotonically improving bounds with increased training. Our code will be available upon publication.",
    "id": 1385
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Head Dropout Scheduling",
    "authors": [
      "Kumar, P.",
      "Chen, S.",
      "Garcia, M."
    ],
    "abstract": "Transformer attention mechanisms are computationally expensive, but existing pruning methods require full training before compression. We propose Attention Head Dropout Scheduling (AHDS), a training procedure that gradually deactivates attention heads according to a learned dropout schedule that balances model capacity and computational efficiency. Our approach tracks per-head contribution variance during training and dynamically adjusts dropout rates based on a novel entropy-based stability metric. For ViT-B on ImageNet, AHDS reduces inference FLOPs by 22% while maintaining 98.7% of original accuracy, outperforming static dropout baselines. We also demonstrate 15-18% speedup on language modeling tasks across Wikitext-103 and C4 datasets. However, our method shows diminishing returns on smaller models (<100M parameters) and requires hyperparameter tuning for each architecture. Theoretical analysis reveals AHDS optimizes a PAC-Bayesian bound, though our generalization results provide only marginal improvement over existing regularization techniques. While promising for resource-constrained deployment, further investigation is needed to extend these gains to more diverse architectures and tasks.",
    "id": 1386
  },
  {
    "title": "Reparameterizing Adam: A Cheap Heuristic for Sparse Model Training",
    "authors": [
      "Liu, H.",
      "Johnson, T.",
      "Kumar, S."
    ],
    "abstract": "We propose Sparse Reparameterized Adam (SRA), a simple modification to Adam that improves training efficiency for large, sparse neural networks. Our approach reparameterizes the optimizer state by maintaining an importance-weighted exponential moving average of gradients, where weights are computed from activation sparsity patterns. This allows significant reduction in memory usage (up to 3.2\u00d7) while preserving convergence guarantees for convex objectives. We evaluate SRA on language modeling and vision transformer tasks, achieving competitive perplexity and accuracy with 30-50% fewer optimizer state updates compared to standard AdamW. While the method shows consistent gains across all tested architectures, theoretical analysis reveals the convergence bound depends critically on the sparsity ratio, limiting applicability to moderately sparse networks (sparsity \u2265 40%). Notably, SRA can be implemented in 15 lines of PyTorch code, making it easily adoptable. Our results suggest the technique works best when combined with existing sparsity regularizers, though we observe some instabilities in the presence of large learning rate schedules. Code is available at anonymous-link.",
    "id": 1387
  },
  {
    "title": "Gradient Surgery Revisited: Do Adaptive Methods Need Sharper Boundaries?",
    "authors": [
      "Liu, J.",
      "Krishnan, S.",
      "Foster, D."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradient signals that prevent simultaneous optimization of all objectives. Recent work in gradient surgery methods like PCGrad and GradDrop has shown promising results, but relies on hand-tuned thresholds to determine when and how to modify gradients. We propose Adaptive Gradient Surgery (AGS), which automatically learns these decision boundaries during training using a lightweight meta-learning procedure. Our method introduces a learnable gating function that outputs soft masks for gradient components based on their alignment with task-specific objectives. On standard multi-task benchmarks including NYUv2 and Cityscapes, AGS achieves modest improvements (+1.3% mIoU, +0.8% accuracy) over strong baselines while eliminating threshold tuning. However, we find that AGS introduces additional hyperparameters related to the meta-learning rate and can be unstable when tasks have vastly different gradient magnitudes. Analysis reveals that AGS primarily benefits from reducing negative gradient interference rather than discovering novel gradient combinations. While the automatic threshold learning is conceptually appealing, our empirical results suggest the gains may not justify the increased complexity for most applications.",
    "id": 1388
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Merging with Dynamic Thresholds",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Kim, J."
    ],
    "abstract": "Self-attention mechanisms in Transformers suffer from quadratic complexity with respect to sequence length, limiting their applicability to long sequences. We propose Dynamic Threshold Token Merging (DTTM), a method that adaptively reduces computational cost by merging redundant tokens during inference. Our approach learns sequence-dependent thresholds using a lightweight auxiliary network that predicts merge boundaries based on attention patterns. Unlike previous static compression methods, DTTM allows for flexible trade-offs between speed and accuracy at runtime. We evaluate our method on language modeling and image classification tasks, achieving 1.3-1.8\u00d7 speedup with less than 2% performance degradation on standard benchmarks. While our empirical results show consistent improvements over uniform compression baselines, we observe that the effectiveness varies significantly across domains and model scales. Theoretical analysis reveals that our merging strategy approximately preserves attention entropy under mild assumptions, though we acknowledge this analysis has limitations for deeply layered models. Code and pre-trained models will be released upon acceptance.",
    "id": 1389
  },
  {
    "title": "Gradient Compression with Learned Threshold Schedules: A Variational Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Communication bottlenecks remain a critical challenge for distributed training of large neural networks. While gradient compression techniques like top-k sparsification show promise, their fixed thresholds often lead to suboptimal trade-offs between compression and convergence. We propose VARSAG (Variational Adaptive Rate for Sparsified Accelerated Gradients), a method that learns threshold schedules by framing compression as a variational inference problem. Our key insight is to model the threshold as a latent variable whose posterior distribution can be optimized alongside model parameters. We derive an evidence lower bound that balances compression-induced bias against communication savings, enabling adaptive threshold adjustment based on gradient statistics. Experiments on CIFAR-100 and ImageNet using ResNet-50 and Vision Transformer architectures show 2-4\u00d7 communication reduction compared to fixed-threshold baselines, with \u22641.5% accuracy loss. However, theoretical convergence guarantees only hold under restrictive assumptions about gradient noise, and benefits diminish when batch sizes exceed 1024. While promising for resource-constrained environments, our method requires hyperparameter tuning and adds computational overhead that may offset advantages in certain regimes.",
    "id": 1390
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Simple Combination of SAM and Gradient Dropout for Improved Generalization",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising approach for improving generalization by seeking flat minima, but its effectiveness remains inconsistent across architectures and datasets. We observe that SAM's gradient conflicts with standard gradient descent directions, potentially limiting its benefits. Motivated by this, we propose DropSAM, a simple modification that combines SAM with targeted gradient dropout during the ascent step. Our method randomly drops gradients with highest curvature in selected layers, reducing conflicting updates while maintaining SAM's sharpness regularization. Through extensive experiments on CIFAR-10/100 and ImageNet, we show DropSAM achieves 0.3-0.7% accuracy improvements over SAM with minimal computational overhead, particularly effective for ResNet architectures. While the improvements are modest, we provide theoretical insights suggesting our approach bounds generalization error through reduced gradient interference. Code will be made available upon acceptance.",
    "id": 1391
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simpler Alternative with Provable Guarantees",
    "authors": [
      "Liu, Q.",
      "Chen, B.",
      "Johnson, K."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance. While recent gradient surgery methods like PCGrad and GradDrop show empirical success, they lack theoretical justification and introduce sensitive hyperparameters. We propose Gradient Harmonization, a simple approach that reweights gradients based on their alignment with the average gradient direction. Our method requires only a single scalar parameter and admits a clean theoretical analysis: we prove convergence under mild assumptions by viewing it as approximate projected gradient descent on the Pareto front. Experiments on standard benchmarks (NYUv2, CIFAR-100) show modest improvements over PCGrad (+1.2% average accuracy) with 30% less compute, but gains are inconsistent across task combinations. Ablation studies reveal performance degrades as task dissimilarity increases. While our theory provides insight into why gradient surgery works, empirical benefits are incremental and confined to specific settings. Code is available.",
    "id": 1392
  },
  {
    "title": "Adaptive Regularization via Gradient History: A Practical Approach for Training Deep Neural Networks",
    "authors": [
      "Li, S.",
      "Garcia, M.",
      "Kim, J."
    ],
    "abstract": "While adaptive optimizers like Adam and AdaGrad have demonstrated strong empirical performance, they often struggle with achieving both fast convergence and good generalization in deep networks. We propose GRADHIST, a novel regularization technique that leverages gradient history to dynamically adjust L2 penalty strengths during training. Our method maintains a running summary of gradient directions across parameter blocks, adaptively increasing regularization for weights with consistently low-gradient variance while relaxing penalties for rapidly changing parameters. We analyze convergence properties under convex assumptions and conduct extensive experiments on CIFAR-10/100 and ImageNet with ResNet and Vision Transformer architectures. GRADHIST achieves modest improvements over standard baselines (0.3-0.8% accuracy gains) while reducing training time by 10-15%. However, ablation studies reveal performance degrades when applied to small networks (ResNet-18) and some NLP tasks. Though our theoretical analysis makes strong convexity assumptions that may not hold in practice, GRADHIST provides a lightweight alternative to manual hyperparameter tuning with minimal additional computational cost.",
    "id": 1393
  },
  {
    "title": "Improving Generalization in Reinforcement Learning with Adaptive Experience Replay Scheduling",
    "authors": [
      "Chen, Y.",
      "Rodriguez, L.",
      "Kim, J."
    ],
    "abstract": "Experience replay has become a cornerstone technique in deep reinforcement learning, yet its impact on out-of-distribution generalization remains poorly understood. We investigate whether dynamically scheduling replay buffer sampling rates can improve zero-shot transfer performance across environments with varying dynamics. Our method, AdaptiveReplay, adjusts the replay ratio based on estimated policy improvement uncertainty using an ensemble of value functions. On 8 continuous control tasks from MuJoCo and 4 procedurally generated environments, AdaptiveReplay achieves a 12% relative improvement in transfer performance compared to fixed replay schedules, though gains are inconsistent across domains. While the approach introduces minimal computational overhead, we find that benefits diminish when source and target domains differ substantially, suggesting limitations in our uncertainty-based scheduling mechanism. These results indicate that replay scheduling offers modest but measurable generalization improvements, particularly when domain shifts are moderate.",
    "id": 1394
  },
  {
    "title": "Gradient Amplification as Stabilization for Transformer Training with Low-Precision Activations",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Training transformers with reduced numerical precision offers compelling speed and memory advantages, but often leads to unstable optimization and degraded performance. We propose Gradient Amplification (GradAmp), a simple technique that selectively amplifies gradient norms in low-precision regimes to counteract vanishing update magnitudes during backpropagation. Our method operates by scaling gradients with an adaptive coefficient derived from the ratio of full-precision to low-precision parameter updates, computed efficiently without full precision computation. While conceptually straightforward, GradAmp demonstrates consistent improvements across standard transformer architectures (BERT-base and GPT-2 medium) when training with 8-bit activations, achieving 0.5-1.2 BLEU score gains over naive low-precision baselines on WMT14 and GLUE tasks. Theoretical justification remains partial\u2014we establish convergence under simplified convex assumptions, but our analysis does not tightly capture the empirical behavior in non-convex transformer training. Preliminary ablations suggest GradAmp primarily stabilizes early training dynamics rather than correcting fundamental precision-related representation degradation. Though performance gains are meaningful, they come with increased memory overhead from gradient statistics buffering, and we observe diminishing returns beyond 8-bit quantization. Our results suggest GradAmp as a practical but incremental advance toward stable low-precision transformer training.",
    "id": 1395
  },
  {
    "title": "Lipschitz-Regularized Gradient Descent: A Simple Yet Effective Approach to Improving Adversarial Robustness",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Johnson, A."
    ],
    "abstract": "We propose Lipschitz-Regularized Gradient Descent (LRGD), a lightweight modification to standard training that improves adversarial robustness without computationally expensive adversarial training. Our approach adds a simple penalty term to the loss function that constrains the Lipschitz constant of the network during training. While the theoretical motivation for regularizing Lipschitz constants is well-established, we show that existing practical implementations often over-regularize, leading to accuracy degradation. LRGD addresses this by adaptively adjusting the regularization strength based on layer-wise gradient statistics. On CIFAR-10, LRGD achieves 47.3% robust accuracy under PGD attacks (\u03f5=8/255), matching adversarial training while requiring only 1.2\u00d7 the compute of standard training. However, we find that LRGD performs comparably to simpler baseline approaches like gradient clipping combined with weight decay, raising questions about the source of observed improvements. Additionally, while LRGD shows consistent gains on small datasets (CIFAR-10/100), its benefits diminish on ImageNet-scale tasks. Code is available at [link].",
    "id": 1396
  },
  {
    "title": "Momentum With Adaptive Restart: A Simple Variant That Sometimes Works",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose an embarrassingly simple modification to standard momentum-based optimization that adaptively restarts the momentum buffer when the loss increases. While restart schemes have been explored in convex optimization, we empirically demonstrate that our approach provides modest improvements over SGD with momentum on a subset of ImageNet training runs, particularly for ResNet-50 architectures trained with aggressive learning rates. Our method adds only 3 lines of Python code and requires no additional hyperparameters beyond standard momentum. Through extensive experiments on CIFAR-10, CIFAR-100, and 4 GLUE tasks, we show that our restart condition improves final validation accuracy by 0.3-0.7% in 40% of experimental settings, while degrading performance by similar margins in 25% of cases. We provide convergence analysis under restrictive assumptions that partially explain these mixed empirical results. Our implementation is available at anonymous-github-link.",
    "id": 1397
  },
  {
    "title": "Gradient Descent with Memory-Efficient Curvature Approximation via Rank-1 Sketches",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Subramanian, K."
    ],
    "abstract": "We propose a memory-efficient variant of second-order optimization that approximates curvature information using rank-1 sketches of the Hessian matrix. Our method, called Sketched Curvature Descent (SCD), stores only O(d) parameters compared to O(d\u00b2) for standard quasi-Newton methods, while still capturing local curvature beyond what first-order methods provide. The key insight is that carefully constructed random rank-1 measurements of the Hessian can provide sufficient information for effective preconditioning when combined with momentum-like updates. We evaluate SCD on training ResNet-18 on CIFAR-10 and CIFAR-100, as well as transformer language models on WikiText-103. Results show modest improvements over Adam and SGD+Momentum on some tasks (0.5-1.2% accuracy gains, 10-15% faster convergence), but not consistently across all settings. Analysis reveals that the approximation quality degrades for ill-conditioned problems, suggesting the method is best suited for moderately conditioned objectives. While SCD provides a practical trade-off between first and second-order methods, it does not fundamentally resolve the challenges of scaling curvature-based optimization to modern deep architectures. Code is available at [anonymous link].",
    "id": 1398
  },
  {
    "title": "Towards More Robust Few-Shot Learning via Cyclic Feature Augmentation",
    "authors": [
      "Liu, J.",
      "Kumar, P.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Few-shot learning remains challenging due to the limited diversity of support examples. We propose CycleAug, a simple but effective augmentation strategy that leverages learned cyclic transformations to hallucinate additional training examples. Our method learns differentiable augmentations that map instances through feature space and back, implicitly regularizing the hallucinated features to preserve semantic structure. Unlike complex meta-learning frameworks, CycleAug acts as a plug-and-play module compatible with standard prototypical networks and can be trained end-to-end with minimal overhead. Experiments on mini-ImageNet and tiered-ImageNet show 2-4% improvements over strong baselines, with particularly large gains in 1-shot scenarios. While our ablations reveal the cyclic constraint is necessary, we find performance saturates quickly with more than three transformation steps. The approach struggles on cross-domain tasks and shows sensitivity to the choice of augmentation strength. Nevertheless, CycleAug represents a lightweight alternative to memory-intensive meta-learning methods, requiring no additional parameters at inference. Code and pre-trained models are available at [anonymous link].",
    "id": 1399
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Probabilistic Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a parameter-efficient method for fine-tuning large language models, but its effectiveness heavily depends on the choice of rank hyperparameters which are typically set heuristically. We propose LoRA-Drop, a simple yet effective method that automatically determines low-rank dimensions during training through probabilistic rank dropout. Our approach employs a learnable gating mechanism that progressively prunes ranks based on gradient-based importance scores, eliminating the need for manual rank selection. We evaluate LoRA-Drop on instruction tuning tasks across three model families (GPT-2, Llama-2-7B, and RoBERTa-large), demonstrating 12-18% fewer trainable parameters compared to standard LoRA while maintaining comparable downstream performance on GLUE and Vicuna benchmarks. However, we observe that LoRA-Drop's benefits diminish on larger models (>10B parameters), where our method exhibits higher variance in convergence behavior. While our theoretical analysis connecting rank selection to implicit regularization provides partial justification, the assumptions may not hold for all architectures. Code and pre-trained models will be released upon acceptance.",
    "id": 1400
  },
  {
    "title": "Gradient Magnitude Asymmetry: A Simple Indicator for Mode Collapse in GANs",
    "authors": [
      "Lee, D.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We investigate whether the magnitude of discriminator gradients can serve as an early warning signal for mode collapse in Generative Adversarial Networks. Through empirical analysis across 8 datasets and 4 architectural variants, we find that the ratio of gradient norms between real and fake samples exhibits a consistent asymmetry pattern preceding collapse events. Building on this observation, we propose Gradient Averaging Regularization (GAR), which penalizes this asymmetry during training. While our method shows modest improvements on established metrics (FID improves by 8-12% on average), the primary contribution lies in providing practitioners with a computationally lightweight diagnostic tool requiring only minor code modifications to existing pipelines. We conduct ablations demonstrating correlation with mode drop in synthetic mixture experiments, though generalization to more complex settings remains partial. Code will be released upon acceptance.",
    "id": 1401
  },
  {
    "title": "Revisiting Scheduled Sampling in Transformer Decoders with Adaptive Noise Injection",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "Scheduled sampling has long been proposed as a remedy for exposure bias in sequence-to-sequence models, yet its benefits for modern Transformer architectures remain inconclusive. We re-examine scheduled sampling for Transformer language generation through a unifying lens that connects it to a family of adaptive noise injection techniques. Our theoretical analysis reveals that while exposure bias exists, scheduled sampling only helps under restrictive conditions on the data distribution and model mismatch. Building on these insights, we propose Adaptive Scheduled Noise (ASN), which modulates the noise injection schedule based on decoder uncertainty estimates. Experiments on machine translation and summarization benchmarks show modest improvements of 0.4-0.7 BLEU/ROUGE over standard training when carefully tuned. However, we find these gains are inconsistent across datasets and sensitive to hyperparameter choices. Further analysis suggests the benefits may stem from increased robustness to distributional shift rather than solving exposure bias per se. While our findings clarify when scheduled sampling can help Transformers, they also highlight fundamental limitations that suggest complementary approaches may be necessary.",
    "id": 1402
  },
  {
    "title": "Token-Level Contrastive Learning for Enhancing Pre-trained Language Models via Neighborhood Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose NeighborCLR, a simple token-level contrastive learning framework that augments pre-trained language models without architectural changes. Motivated by observations that nearby tokens in contextualized representations often exhibit semantic similarity, we construct positive pairs from neighboring tokens within the same sequence and negatives via in-batch sampling. Our method adds a lightweight contrastive objective during fine-tuning, encouraging local smoothness in representation space while preserving task-specific performance. We evaluate on GLUE and SuperGLUE benchmarks using BERT-base and RoBERTa-base, achieving 1-2% improvements over standard fine-tuning across most tasks, with particularly strong gains on sentence-pair tasks like MNLI (+3.2%). However, we find limited benefits on tasks requiring fine-grained reasoning (e.g., ReCoRD). Analysis reveals the effectiveness depends heavily on neighborhood radius selection and batch size. While our approach is simple to implement and requires no additional data, the improvements are modest and come with 1.3x training time overhead. Our code and checkpoints are released for reproducibility.",
    "id": 1403
  },
  {
    "title": "Improving Gradient Descent Convergence via Adaptive Learning Rate Sharding",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, A."
    ],
    "abstract": "We propose a simple modification to standard gradient descent that adaptively splits the learning rate across parameter blocks based on historical gradient norms. Our method, called Learning Rate Sharding (LRS), maintains the computational efficiency of vanilla SGD while providing faster convergence on certain loss landscapes. The key insight is that different parameter blocks often exhibit varying gradient magnitudes, suggesting non-uniform learning rates could be beneficial. We derive a lightweight update rule that re-allocates learning rates every k steps using only gradient statistics already computed during training. Experiments on ResNet-50 and Vision Transformer show 8-12% faster convergence on CIFAR-100 and ImageNet-subset, though gains diminish on well-tuned baselines with proper learning rate schedules. While our theoretical analysis only establishes convergence for strongly convex quadratic objectives, empirical results suggest the method works more broadly. However, we find LRS provides limited benefits over carefully tuned baselines on standard benchmarks, raising questions about its practical utility. Nevertheless, the simplicity and negligible computational overhead may make it appealing for practitioners seeking marginally faster training without extensive hyperparameter tuning.",
    "id": 1404
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Reexamination with Adaptive Routing",
    "authors": [
      "Liu, K.",
      "Chen, M.",
      "Rodriguez, J."
    ],
    "abstract": "Gradient surgery methods like PCGrad have shown promise for mitigating gradient conflicts in multi-task learning, but their effectiveness varies significantly across task combinations. We propose Adaptive Gradient Routing (AGR), a simple modification that dynamically decides whether to apply gradient surgery based on the cosine similarity between task gradients. Our approach uses a learned gating mechanism trained with reinforcement learning to determine when surgical intervention is beneficial. On a diverse suite of 12 multi-task benchmarks spanning computer vision and NLP tasks, AGR achieves an average 2.3% improvement in aggregate performance metrics over PCGrad, while reducing computational overhead by 18%. However, we find that AGR's benefits are concentrated in specific task combinations, particularly those with high gradient conflict. Through extensive ablation studies, we demonstrate that the learned routing policy often defaults to simple heuristics, raising questions about the necessity of the reinforcement learning component. While our results suggest gradient surgery may be over-applied in current practice, the method's inconsistent gains across tasks limit its practical applicability. Code will be made available upon acceptance.",
    "id": 1405
  },
  {
    "title": "Layer-wise Learning Rates for Transformer Fine-tuning: A Simple Yet Surprisingly Effective Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose a simple modification to the Adam optimizer that assigns different learning rates to each transformer layer during fine-tuning, inspired by the observation that lower layers typically require smaller updates than upper layers. Our method requires only a single hyperparameter controlling the decay rate across layers and can be implemented in fewer than 15 lines of code. Experiments on GLUE and SuperGLUE benchmarks show consistent improvements over standard Adam fine-tuning, with an average gain of 0.8-1.2% across tasks, particularly on data-efficient regimes with limited training examples. While the gains are modest in absolute terms, our approach is computationally free and robust across different pretrained models (BERT, RoBERTa, DeBERTa) without additional hyperparameter tuning. We provide theoretical motivation through a layer-wise noise analysis of the Adam updates, though our formal results require limiting assumptions. The simplicity and reproducibility of our method make it a practical drop-in replacement for standard fine-tuning, especially when computational resources are constrained for extensive hyperparameter search.",
    "id": 1406
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Rank Scheduling",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, A."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto standard for parameter-efficient fine-tuning, yet existing methods use fixed rank allocations that may under- or over-utilize model capacity. We propose LoRA-Drop, a simple technique that dynamically adjusts LoRA ranks during training using gradient-based importance scores. Our method begins with a high-rank initialization and iteratively drops the least important components according to a novel criterion combining gradient norm and singular value magnitude. On the GLUE benchmark with RoBERTa-base, LoRA-Drop achieves 89.2% average score while using 35% fewer parameters than standard LoRA with full rank 16, and matches full fine-tuning performance with only 0.6M trainable parameters. However, on larger models like GPT-2 medium, the gains diminish to 1-2% with comparable parameter counts. Our analysis reveals that LoRA-Drop primarily benefits tasks with strong pretrained representations, suggesting limited utility when source and target domains diverge significantly. While our approach provides marginal improvements on established benchmarks, we believe it offers a valuable sanity check for practitioners concerned about rank selection in resource-constrained settings.",
    "id": 1407
  },
  {
    "title": "Improving Transformer Efficiency via Learned Sparse Attention Patterns with Fixed Memory Budgets",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose SparseFlex, a method for reducing transformer memory usage by learning sparse attention patterns under strict memory constraints. While existing sparse attention mechanisms use predefined patterns or require expensive re-training, our approach learns sparsity masks via auxiliary loss functions that enforce fixed memory budgets during both training and inference. Our key insight is to optimize attention sparsity jointly with task objectives using a two-stage training procedure: first pre-training with full attention, then fine-tuning with learned sparsity masks constrained by memory targets. Experiments on WikiText-103 and C4 datasets show 2.1\u00d7 memory reduction compared to dense attention with only 3.2% perplexity increase. However, we observe the method proves brittle when targeting aggressive sparsity (>90%), and fails to transfer well across domains without re-training. Ablations reveal the importance of temperature-annealing schedules for mask learning, though optimal hyperparameters vary significantly across tasks. While our results suggest learned sparse attention can achieve practical memory savings, the approach offers limited theoretical guarantees and depends critically on careful hyperparameter tuning. Code and pre-trained masks are available at anonymized-url.",
    "id": 1408
  },
  {
    "title": "Gradient Surgery in Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization. While existing gradient surgery methods like PCGrad and GradDrop address this by dropping or projecting conflicting components, we show that these aggressive interventions can eliminate useful signal. We propose Conservative Gradient Surgery (CGS), which only modifies gradients when the cosine similarity between task gradients falls below a learned threshold. Our method uses a lightweight hypernetwork that adapts the threshold online based on validation loss trends. On three standard benchmarks (CIFAR-100, NYUv2, and QM9), CGS achieves small but consistent improvements over PCGrad (+0.8% average accuracy, +1.2% mIoU, +0.5% MAE). However, ablation studies reveal that gains primarily emerge in high-conflict regimes, and gains vanish with careful hyperparameter tuning of baseline methods. While CGS offers a principled alternative to hard-coded gradient surgery rules, its complexity may not justify the modest improvements. Code is available at anonymous.url/CGS.",
    "id": 1409
  },
  {
    "title": "Gradient Norm Regularization Improves Robustness but Only for Nearly Convex Models",
    "authors": [
      "Chen, L.",
      "Ramaswamy, V.",
      "Obermayer, K."
    ],
    "abstract": "We propose Gradient Norm Regularization (GNR), a simple weight decay variant that penalizes the L2 norm of gradients during training. While designed to improve adversarial robustness, our extensive experiments reveal a more nuanced picture: GNR yields consistent robustness improvements on ResNet-18 and WRN-34, but provides no benefit for Vision Transformers and even degrades performance on larger ResNet-50 models. Through theoretical analysis in the convex setting, we show that GNR effectively regularizes weights towards flatter minima, establishing a connection between gradient penalties and margin-based generalization bounds. However, these guarantees fail in non-convex settings, which we empirically validate through sharpness-loss correlation analysis across model architectures. Our results suggest that GNR's effectiveness is limited to models whose training dynamics remain near-convex, raising questions about its broad applicability. We release code and pre-trained models at anonymous-url.",
    "id": 1411
  },
  {
    "title": "Efficient Gradient Compression via Adaptive Tensor Sketching for Distributed Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.K.",
      "Johnson, M.",
      "Kim, S."
    ],
    "abstract": "Communication bottlenecks in distributed training remain a critical challenge for large-scale machine learning systems. We propose Adaptive Tensor Sketching (ATS), a compression scheme that dynamically selects between different sketching matrices based on gradient statistics. Unlike fixed compression methods, ATS uses a lightweight online learning approach to choose between count-sketch, Gaussian, and sign-stable sketches without requiring hyperparameter tuning. Our method achieves up to 4.8\u00d7 compression on ResNet-50/ImageNet training while maintaining 98.3% of baseline accuracy, improving upon static compression baselines by 2-7% in final accuracy across computer vision and language modeling tasks. While our theoretical analysis provides regret bounds for the adaptive selection process, it relies on bounded gradient assumptions that may not hold in practice. Experiments on up to 32 GPUs demonstrate wall-clock speedups of 1.3-1.6\u00d7, though communication-bound speedups diminish as bandwidth increases. The method offers a practical middle ground between aggressive compression and convergence guarantees, making it suitable for bandwidth-constrained training scenarios where exact preservation of gradient information is not critical.",
    "id": 1412
  },
  {
    "title": "On the Effectiveness of Weight-Sharing in Neural Architecture Search via Gradient Descent",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Neural Architecture Search (NAS) has emerged as a promising approach for automating neural network design, but remains computationally expensive. Weight-sharing NAS methods have gained popularity by training a single super-network whose weights are shared across candidate architectures, claiming to reduce search time from GPU-days to GPU-hours. We provide a systematic study of weight-sharing effectiveness under gradient-based optimization. Through theoretical analysis of simplified linear models, we show that weight-sharing introduces systematic biases favoring architectures that maintain gradient flow, which may not correlate with final performance after standalone training. Empirically, we evaluate three popular weight-sharing NAS algorithms on CIFAR-10/100 and ImageNet-subsets. We find that while weight-sharing achieves competitive accuracy compared to random search (\u00b10.5% on average), it underperforms more expensive non-sharing methods by 1-3%. Our key finding is that the ranking correlation between shared-weight validation accuracy and final standalone accuracy deteriorates significantly (\u03c1 < 0.3) when architectures differ substantially in depth or width. These results suggest moderate effectiveness of weight-sharing in constrained search spaces, but raise concerns about its reliability in more general settings.",
    "id": 1413
  },
  {
    "title": "Layer-wise Learning Rate Warmup: An Empirical Study on Transformer Training Dynamics",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "While learning rate warmup is standard practice for Transformer training, little is known about how different layers respond to warmup schedules. We conduct a comprehensive empirical investigation of layer-wise learning rate warmup, where different layers follow distinct warmup patterns. Through systematic experiments on BERT and GPT-style models, we find that early layers (embedding and lower Transformer blocks) benefit from slower warmup rates compared to deeper layers. Conversely, the output layer and final layer norms converge faster with aggressive warmup. Based on these observations, we propose a layer-wise warmup schedule that assigns linearly increasing warmup lengths from shallow to deep layers. Our method achieves 0.3-0.7% improvement on GLUE tasks and 0.2 perplexity reduction on Wikitext-103 compared to standard global warmup, with minimal computational overhead. However, gains diminish with larger models (\u22651.3B parameters) and show high variance across downstream tasks. While our findings provide insights into Transformer optimization dynamics, theoretical justification and broader applicability across architectures remain open questions.",
    "id": 1414
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning Through Selective Forgetting",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Thompson, A."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent gradient surgery methods attempt to resolve conflicts through gradient projection, they can still propagate detrimental gradient components that degrade performance on individual tasks. We propose Adaptive Memory Gradient Surgery (AMGS), which maintains a memory bank of historical gradient directions and selectively forgets directions that consistently produce high training loss. Our method combines gradient projection with an exponential moving average of gradient conflict scores, allowing dynamic adjustment of the gradient surgery threshold during training. On three standard multi-task vision benchmarks (NYUv2, CityScapes, and CelebA), AMGS achieves an average 2.3% improvement over baseline gradient surgery methods, with particularly strong gains on underperforming tasks. While our results demonstrate consistent improvements, the computational overhead of maintaining gradient statistics increases training time by 15-25%. Furthermore, our theoretical analysis assumes Lipschitz smooth objectives, limiting generalization to non-smooth loss landscapes. Our code and pretrained models are available at [anonymized link].",
    "id": 1415
  },
  {
    "title": "Improved Generalization Bounds for Noisy Gradient Descent via Noise-Augmented PAC-Bayes",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We present new generalization bounds for models trained with noisy gradient descent algorithms, including SGD and gradient perturbation methods. While existing PAC-Bayesian bounds often yield vacuous guarantees for deep networks, we show that injecting additional isotropic Gaussian noise during training tightens these bounds significantly. Our key insight is that the noise acts as an implicit regularizer, enabling tighter KL divergence terms in the PAC-Bayesian framework. We derive bounds that scale as O(\u221a(L/n)), where L is the noise variance and n is the dataset size, improving upon previous O(1/\u221an) rates by leveraging data-dependent priors. Experimental validation on CIFAR-10 and ImageNet subsets demonstrates our bounds are non-vacuous for small networks (\u22645M parameters), providing meaningful guarantees when noise variance exceeds 0.01. However, the bounds remain loose for standard architectures like ResNet-50. While our theoretical contribution is novel, practical applicability is limited to moderate-scale settings.",
    "id": 1416
  },
  {
    "title": "Gradient Surgery in Distributed Training: When Pruning Helps More Than Averaging",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Liu, H."
    ],
    "abstract": "Federated learning faces a fundamental tension between minimizing client drift and preserving local accuracy. While methods like FedAvg effectively average gradient updates, we observe that noisy or conflicting gradients from straggler clients can significantly degrade global model performance. We propose Gradient Conflict Pruning (GCP), a simple technique that identifies and removes the bottom-k gradient components with highest cosine disagreement before averaging. Surprisingly, we find that discarding up to 20% of gradient information often improves final accuracy by 1-3% across CIFAR-10, FEMNIST, and Shakespeare benchmarks. Our theoretical analysis shows that GCP approximately solves a bias-variance tradeoff, where the variance reduction from pruning outweighs the induced bias under realistic assumptions of gradient heterogeneity. Experiments on 100-client setups demonstrate consistent improvements over FedAvg, particularly when data distributions are non-IID and communication rounds are limited. While GCP adds minimal computational overhead compared to standard averaging, we acknowledge its sensitivity to the pruning threshold and limited benefits when clients have highly correlated updates. Our work suggests that aggressive gradient averaging may be unnecessary in heterogeneous federated settings, opening questions about optimal compression strategies in distributed optimization.",
    "id": 1417
  },
  {
    "title": "Adaptive Gradient Clipping with Learnable Thresholds via Meta-Gradient Descent",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient clipping is widely used in training neural networks, yet the clipping threshold is typically chosen heuristically or set to a large constant. We propose Learning to Clip (L2C), a simple meta-learning approach that adaptively adjusts clipping thresholds during training via meta-gradient descent. Our method augments any optimizer with minimal overhead, introducing only two additional hyperparameters that control the meta-learning rate and window size for threshold updates. We evaluate L2C on transformer language modeling tasks and reinforcement learning benchmarks, showing modest improvements over standard clipping baselines. Specifically, L2C achieves 1-3% better perplexity on Wikitext-103 and 5-8% faster convergence in several MuJoCo control tasks. While we establish convergence guarantees under simplified assumptions, our theoretical analysis requires stronger smoothness conditions than typically satisfied in practice. Ablation studies reveal that L2C's benefits diminish with careful manual tuning of clipping thresholds, suggesting the approach primarily automates hyperparameter search rather than discovering fundamentally new clipping strategies. Code and hyperparameters are provided for reproducibility.",
    "id": 1418
  },
  {
    "title": "LoRA-MoE: Efficient Mixture-of-Experts via Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J.",
      "Johnson, S."
    ],
    "abstract": "We propose LoRA-MoE, a method that combines Mixture-of-Experts (MoE) architectures with Low-Rank Adaptation (LoRA) to achieve parameter-efficient scaling of large language models. While MoE models enable computational efficiency through conditional computation, they require full model parameter updates during training. LoRA-MoE addresses this limitation by applying low-rank decomposition to expert weights, reducing the number of trainable parameters from billions to millions. Our approach introduces a routing mechanism that operates on the low-rank matrices rather than the full expert weights, maintaining the sparsity benefits of MoE while enabling efficient fine-tuning. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-MoE achieves 95% of full-parameter MoE performance while training only 5% of the parameters. However, we observe a consistent 2-3% performance drop compared to standard MoE across larger model scales (\u22657B parameters), suggesting the low-rank constraint may limit expert capacity. Analysis reveals that our method particularly struggles on tasks requiring fine-grained reasoning, where the reduced parameter count appears to constrain the expressivity of individual experts. While LoRA-MoE provides a practical solution for resource-constrained fine-tuning of MoE models, the trade-offs between efficiency and task performance warrant further investigation for downstream applications with stringent accuracy requirements.",
    "id": 1419
  },
  {
    "title": "Gradient Compression via Learned Quantization Tables: When Simple Heuristics Outperform Learned Methods",
    "authors": [
      "Chen, L.",
      "Okafor, K.",
      "Singh, P."
    ],
    "abstract": "Communication overhead remains a critical bottleneck in distributed training, with various learned compression schemes proposed to reduce gradient transmission costs. We revisit the problem of gradient compression through the lens of learned quantization tables, but with a deliberately constrained approach that limits model complexity to maintain interpretability. Our method learns separate quantization tables for different gradient magnitude ranges via small auxiliary networks (\u22643 layers), then freezes these tables for the remainder of training. Surprisingly, on standard benchmarks (CIFAR-10/100, ImageNet), our approach achieves compression ratios within 5-10% of recent state-of-the-art learned methods while requiring 50-70% fewer parameters in the compression model itself. However, we also demonstrate that carefully tuned uniform quantizers with simple threshold-based sparsity patterns can achieve comparable performance in many settings, suggesting that the gains from learned tables may be incremental rather than transformative. Theoretical analysis shows our method preserves convergence under standard assumptions, though with looser bounds than compression-agnostic methods. While our contributions are incremental, they provide practical insights into when learned compression schemes justify their complexity overhead.",
    "id": 1420
  },
  {
    "title": "Improving Convergence of AdamW with Gradient Norm Clipping for Transformer-Based Language Models",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Okafor, N.",
      "Mueller, J."
    ],
    "abstract": "We investigate whether simple modifications to AdamW can match the convergence speed of more sophisticated optimizers in transformer training. Building on recent observations that gradient magnitudes exhibit heavy-tailed distributions in language models, we propose AdamW-GNC, which applies coordinate-wise gradient norm clipping before the Adam update. While our theoretical analysis only guarantees the same convergence rate as standard AdamW under standard assumptions, we demonstrate empirically that AdamW-GNC achieves comparable performance to Lion and Sophia on GPT-style models with up to 1.3B parameters across three language modeling benchmarks. Our method introduces only two additional hyperparameters (clipping thresholds) which we found to be relatively insensitive to tuning. Experiments on the Pile dataset show 1.2\u00d7 speedup in training iterations to reach baseline perplexity, though the wall-clock improvement is only 1.05\u00d7 due to the clipping overhead. Our PyTorch implementation adds 20 lines of code, making it easy to integrate into existing training pipelines. While these gains are modest compared to recent second-order optimizers, our work suggests that tuning simple first-order methods may be more practical than adopting complex alternatives.",
    "id": 1421
  },
  {
    "title": "Improving Transformer Training Stability Through Adaptive Attention Dropout",
    "authors": [
      "Chen, L.",
      "Johnson, M.",
      "Rodriguez, A."
    ],
    "abstract": "Transformer models often exhibit unstable training dynamics, particularly when scaling to deeper architectures. While attention dropout has been shown to improve generalization, fixed dropout rates may be suboptimal for different training phases and model components. We propose Adaptive Attention Dropout (AAD), a simple method that adjusts dropout rates for attention heads based on gradient norms and attention entropy during training. Our approach uses a lightweight controller network that learns to modulate dropout probabilities every k steps using only local statistics. On standard NLP benchmarks (IWSLT'14, WMT'14, GLUE), AAD achieves modest improvements over baseline transformers (0.3-0.7 BLEU, 0.5-1.2 GLUE points) while reducing training instability by 15-20% as measured by loss variance. The computational overhead is <3% in wall-clock time. However, our method shows limited effectiveness on very large models (>1B parameters) and requires careful hyperparameter tuning. We provide extensive ablations but note that improvements are not consistent across all tasks. Code and pretrained weights are available at [anonymous URL].",
    "id": 1422
  },
  {
    "title": "Frequency-Aware Training: A Simple Regularization Strategy for Improving Out-of-Distribution Robustness",
    "authors": [
      "Chen, L.",
      "Garcia, J.",
      "Thompson, K."
    ],
    "abstract": "While deep neural networks achieve impressive performance on i.i.d. test sets, their performance degrades significantly under distribution shift. We propose Frequency-Aware Training (FAT), a simple regularization technique that encourages models to rely less on high-frequency image features that are brittle to common corruptions. FAT adds a lightweight frequency decomposition module during training, penalizing the model when predictions differ substantially between original and low-pass filtered inputs. Our method requires only a single hyperparameter and minimal computational overhead. Experiments on CIFAR-10-C and ImageNet-C show 12-15% relative improvement in corruption robustness compared to standard training, with minimal impact on clean accuracy. While these gains are notable, we find that FAT provides complementary benefits to existing augmentation-based approaches rather than achieving state-of-the-art robustness alone. Analysis reveals FAT primarily improves robustness to blur and noise corruptions, with limited benefits for weather-related shifts. Additionally, scaling to larger architectures shows diminishing returns. Though simple and broadly applicable, our work highlights the challenges of addressing diverse distribution shifts through a single training modification. Code is available at [url].",
    "id": 1423
  },
  {
    "title": "Gradient Surgery Revisited: An Empirical Analysis of Multi-Task Optimization in Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning promises to improve efficiency by sharing representations across tasks, but gradient conflicts between tasks often lead to suboptimal performance. While recent approaches like PCGrad and GradNorm have proposed gradient manipulation techniques to address these conflicts, the conditions under which these methods provide meaningful benefits remain poorly understood. We conduct a large-scale empirical study examining these gradient surgery methods across 8 datasets and 3 model architectures. We find that while some methods reduce gradient interference, these reductions do not consistently translate to improved final performance. Surprisingly, simple weighted averaging with properly tuned task weights achieves comparable or better results in 65% of our experimental settings. Through careful analysis of loss landscapes and task similarity metrics, we identify two key factors - task signal-to-noise ratio and gradient alignment angles - that predict when gradient surgery will be beneficial. Our results suggest that the benefits of sophisticated gradient manipulation methods are more limited than previously claimed, and provide practical guidance for practitioners choosing multi-task optimization strategies. While our findings primarily offer empirical insights rather than theoretical novelty, they highlight important gaps between theoretical motivation and practical performance in multi-task learning.",
    "id": 1424
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Selective Gradient Accumulation",
    "authors": [
      "Liu, K.",
      "Gonzalez, M.",
      "Johnson, T."
    ],
    "abstract": "Multi-task learning in deep neural networks often suffers from conflicting gradient directions between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradNorm provide simple heuristics for resolving gradient conflicts, they rely on instantaneous gradient estimates and lack mechanisms for long-term gradient history. We propose Adagradient Memory (AGM), a lightweight framework that maintains an adaptive memory of past gradients to guide conflict resolution. Our method selectively accumulates gradient components across training steps, using a novel similarity-based gating mechanism to determine which past gradients influence current updates. Experiments on standard multi-task benchmarks (NYUv2, CityScapes) show 1.2-2.1% improvement over PCGrad on average, with minimal computational overhead (<3% additional memory). While our empirical gains are modest, we provide theoretical analysis showing AGM converges under certain regularity conditions. However, we find performance is sensitive to hyperparameter choices and degrades on highly imbalanced task distributions. Code is available at [anonymized].",
    "id": 1425
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Continuous Relaxation with Learned Temperature",
    "authors": [
      "Chen, Z.",
      "Kumar, A.",
      "Liu, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. While continuous relaxations like the Gumbel-Softmax trick provide differentiable approximations, they rely on fixed temperature parameters that must be tuned heuristically. We propose TempVAE, a simple extension that learns temperature parameters jointly with model parameters using a variational lower bound. Our key insight is that allowing layer-wise, learnable temperatures reduces gradient variance while maintaining the discrete structure in the limit. Experiments on text generation and image VAE benchmarks show modest improvements over baseline methods (1-2 BLEU points, 0.5-1.0 nats improvement), though gains diminish at larger scales. Ablations reveal learned temperatures vary significantly across layers, suggesting uniform temperature may be suboptimal. While our method is easy to implement and generally improves training stability, the benefits are most pronounced in low-data regimes. The approach adds minimal computational overhead but requires careful scheduling of temperature annealing. Our results indicate that learned temperature scheduling represents a small but consistent improvement over fixed schedules, though the fundamental challenges of discrete optimization remain unresolved.",
    "id": 1426
  },
  {
    "title": "Revisiting Gradient Norm Regularization in Transformer Training Through a Low-Rank Lens",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "While gradient clipping is standard for stabilizing transformer training, the role of gradient norm regularization (GNR) remains poorly understood. We investigate whether low-rank approximations of attention matrices can reduce the need for aggressive clipping while maintaining stability. Our key observation is that attention matrices in early training exhibit rapidly decaying spectra, suggesting that gradient noise may be concentrated in low-rank subspaces. Building on this, we propose GNR-LR: a training method that applies gradient norm penalties only to the top-k singular components of attention matrices. Through experiments on GLUE tasks and small-scale language modeling (GPT-2 124M), we find that GNR-LR achieves 4-6% faster convergence under constant learning rates compared to standard clipping baselines. However, these gains diminish when robust hyperparameter tuning is applied to baselines. We provide theoretical motivation via a simplified linear attention model, though the connection to full transformers involves limiting assumptions. While our results suggest targeted regularization can improve training dynamics, the practical benefits over well-tuned baselines are modest. Code will be released upon acceptance.",
    "id": 1427
  },
  {
    "title": "Momentum without Momentum: An Adaptive Learning Rate Schedule Inspired by Classical Mechanics",
    "authors": [
      "Liu, C.",
      "Thompson, J.",
      "Choudhary, A."
    ],
    "abstract": "We present a learning rate scheduling technique that achieves similar convergence benefits to momentum-based optimizers without explicitly storing momentum terms. Our method, inspired by principles from classical mechanics, interprets gradient descent as a particle moving in a potential energy field. By adaptively adjusting the learning rate based on the gradient's local curvature and alignment with past updates, we recover much of momentum's acceleration behavior while reducing memory overhead by 25-40%. We derive theoretical convergence guarantees for convex quadratic objectives and demonstrate competitive performance on standard benchmarks, matching Adam's accuracy on CIFAR-10 and ImageNet while using fewer hyperparameters. However, we find our approach less robust to extreme learning rates and sensitive to the curvature estimation window. Our method is particularly effective for memory-constrained environments and transformer fine-tuning, though gains diminish on extremely deep networks (>100 layers). The simplicity of implementation and reduced memory footprint make this a practical alternative to traditional momentum, despite modest performance trade-offs.",
    "id": 1428
  },
  {
    "title": "LoRA-Connect: Memory-Efficient Adapter Fusion for Multi-Task Pretrained Models via Sparse Routing",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "We propose LoRA-Connect, a parameter-efficient method for combining multiple task-specific LoRA adapters in pretrained language models without catastrophic interference. Our approach learns a sparse routing function that dynamically selects adapter combinations at inference time, achieving better task performance than na\u00efve averaging while using 40% fewer parameters than storing separate adapters. The key insight is to formulate adapter fusion as a constrained optimization problem over top-k sparse combinations, regularized by a novel inter-task similarity metric computed from Fisher information matrices. Experiments on GLUE and Super-NaturalInstructions show 3-5% improvements over single-adapter baselines and competitive results with larger, task-specific fine-tuned models. However, we find the method shows diminishing returns beyond 10 tasks and struggles with highly conflicting objectives (r=0.68 correlation with task similarity). While our theoretical analysis proves convergence under restricted assumptions, the general case remains an open problem. Code and pre-trained adapters will be released upon publication.",
    "id": 1429
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less Intervention Yields More",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning faces a fundamental challenge when gradients from different tasks conflict, often leading to degraded performance. While recent gradient surgery methods like PCGrad and GradDrop propose sophisticated interventions to resolve these conflicts, we find that overly aggressive gradient modification can harm learning dynamics. We propose Conservative Gradient Surgery (CGS), a simple approach that only applies gradient surgery when the cosine similarity between task gradients falls below a learned threshold. Our method uses a lightweight auxiliary network to predict whether intervention is beneficial, reducing computational overhead by 40% compared to existing methods. Experiments on three standard multi-task benchmarks (NYUv2, CelebA, and QM9) show modest but consistent improvements of 1-3% over baseline methods, particularly when tasks have moderate gradient conflicts. However, we observe limited benefits when tasks are either highly aligned or severely conflicting. Our analysis reveals that the effectiveness of gradient surgery depends heavily on the underlying task relationships, which are often more nuanced than previously assumed. While CGS provides a practical improvement for certain multi-task scenarios, our findings suggest the need for more adaptive methods that can better characterize task interactions.",
    "id": 1430
  },
  {
    "title": "Recursive Knowledge Distillation with Temperature Annealing for Lightweight Language Models",
    "authors": [
      "Kumar, V.",
      "Chen, L.",
      "Santos, J."
    ],
    "abstract": "Knowledge distillation has proven effective for compressing large language models, but existing approaches primarily use one-way transfer from teacher to student. We propose Recursive Knowledge Distillation (RKD), where student checkpoints are iteratively recast as teachers for even smaller models while dynamically adjusting the softmax temperature. Our method uses a novel temperature annealing schedule that starts high for early distillation steps and decays to facilitate sharper distributions as models shrink. Experiments on BERT-base show RKD achieves 94.2% of teacher performance at 35% parameters, comparable to static distillation baselines while requiring 2.3x training time. Analysis reveals diminishing returns beyond three recursive steps, with fifth-generation students degrading to 87.1% performance. Though RKD provides marginal gains over standard distillation for moderate compression ratios, we establish theoretical conditions under which recursive teaching provably cannot improve distillation quality, highlighting fundamental limitations of our approach.",
    "id": 1431
  },
  {
    "title": "Memory-Efficient Training of Large Language Models via Iterative Gradient Compression with Error Feedback",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "We present a simple approach for reducing the memory footprint during transformer training by compressing gradients using quantization and sparsification in an iterative fashion. Our method builds upon existing gradient compression techniques by introducing an error feedback mechanism that accumulates compression errors across training steps and periodically applies corrections. The approach requires only minor modifications to standard training pipelines and can be implemented in ~100 lines of PyTorch. We evaluate our method on GPT-2 (125M) and LLaMA-7B using the C4 dataset, achieving 60-80% memory reduction compared to full-precision training while maintaining perplexity within 5% of baseline. However, training wall-clock increases by 2-3\u00d7 due to compression overhead. While our results are consistent across multiple seeds, we observe degradation for larger models (\u226513B). Theoretical analysis shows convergence guarantees under bounded compression error assumptions, though these may not hold for aggressive compression rates. Our work provides a practical trade-off for researchers with memory constraints, though we acknowledge limitations in scaling to state-of-the-art models and datasets.",
    "id": 1432
  },
  {
    "title": "On the Implicit Bias of Adam with Weight Decoupling in Small-Scale Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate the convergence behavior and implicit bias of AdamW in small-scale transformer architectures (\u2264 50M parameters). While previous work has established theoretical guarantees for Adam variants, empirical observations suggest the behavior differs significantly in practice, particularly when weight decay is decoupled from the optimization step. We propose a refined update rule that incorporates layer-wise adaptive moments and demonstrate improvements in downstream fine-tuning tasks. Our theoretical analysis, while limited to simplified linear cases, provides partial justification for our empirical findings. Experiments on GPT-2 small and BERT-base models show 2-3% improvements on GLUE benchmarks and modest gains in language modeling perplexity. The proposed method requires minimal hyperparameter tuning and incurs less than 2% computational overhead compared to standard AdamW. While our contributions are incremental, we believe the insights into adaptive optimization dynamics in transformer architectures may guide future algorithmic improvements. Limitations include the restricted theoretical setting and modest scale of experimental validation.",
    "id": 1433
  },
  {
    "title": "Learning with Reusable Task Embeddings via Self-Supervised Curriculum Generation",
    "authors": [
      "Kumar, V.",
      "Liang, J.",
      "Obermeyer, C."
    ],
    "abstract": "We propose a method for improving sample efficiency in multi-task reinforcement learning by learning reusable task embeddings from automatically generated curricula. Our approach trains a hypernetwork to produce task-specific policies conditioned on learned embeddings, while simultaneously learning a curriculum generator using self-supervised objectives based on policy improvement and task similarity. Unlike prior work relying on human-designed curricula or expensive meta-optimization, we use contrastive learning to discover task structures without supervision. Experiments on Meta-World and procedurally generated navigation tasks show 15-25% relative improvement in sample efficiency over standard fine-tuning baselines. While our method improves performance on similar tasks, we observe limited generalization to significantly out-of-distribution tasks. The learned embeddings appear to capture low-level motion primitives rather than abstract task structure, suggesting room for improvement in representation learning. Our ablation studies indicate that both curriculum generation and hypernetwork conditioning contribute to performance gains, though the benefits diminish when task distributions become highly diverse. We provide open-source code and pretrained models to facilitate reproducibility.",
    "id": 1434
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum for Non-Stationary Online Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is widely used in neural network training to prevent exploding gradients, but fixed clipping thresholds can be suboptimal in non-stationary environments where gradient distributions change over time. We propose Adaptive Gradient Clipping with Momentum (AGCM), a simple modification that combines exponential moving averages of gradient norms with adaptive clipping bounds. AGCM uses two momentum terms to dynamically adjust clipping thresholds based on recent gradient behavior, eliminating the need for manual tuning. We provide theoretical analysis showing convergence rates within a constant factor of vanilla SGD in convex settings. Empirically, AGCM achieves 2-4% improvements over standard clipping on CIFAR-10/100 when training ResNet-18 under simulated distribution shift, and shows modest gains in online learning tasks with concept drift. While the improvement is not dramatic, AGCM requires no additional hyperparameters beyond standard clipping and adds minimal computational overhead. Limitations include lack of improvement on stable datasets and sensitivity to the initial clipping threshold when momentum terms are poorly initialized.",
    "id": 1435
  },
  {
    "title": "Improving Transformer Training with Layer-Dependent Dropout Schedules: A Systematic Study",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Anderson, J."
    ],
    "abstract": "While dropout is widely used in transformer training, the conventional wisdom of using a fixed dropout rate throughout all layers may be suboptimal. We investigate whether layer-dependent dropout schedules can improve transformer performance across various NLP tasks. Through extensive experiments on BERT and T5 architectures, we propose a family of layer-wise dropout schedules that decrease monotonically from lower to higher layers, motivated by the observation that earlier layers typically learn more robust features while later layers specialize for specific tasks. Our method, DropFlow, introduces a simple parameterization controlled by two hyperparameters: maximum dropout rate and decay exponent. Empirical evaluation on GLUE, SuperGLUE, and three additional benchmarks shows modest improvements of 0.3-1.2% over vanilla dropout baselines. Notably, our approach yields larger gains (up to 2.1%) on low-resource settings with less than 10k training examples. While the improvements are consistent across tasks, the magnitude remains relatively small compared to architectural innovations. The method requires minimal implementation changes and maintains computational efficiency. Code and pre-trained models are available at [URL].",
    "id": 1436
  },
  {
    "title": "Learning to Prune Neural Networks via Graph Neural Network Surrogates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Anderson, J."
    ],
    "abstract": "Neural network pruning typically requires extensive retraining or iterative magnitude-based scoring, both of which become computationally prohibitive for large models. We propose GNN-Prune, a framework that uses graph neural networks to predict which neurons should be pruned based on local connectivity patterns and activation statistics. Our key insight is that neurons' structural roles within the computational graph, rather than their individual magnitudes, better predict their importance. We train a small GNN on subgraphs extracted from pretrained networks to output pruning probabilities, which are then thresholded to create sparse architectures. Across ResNet50, ViT-B/16, and GPT-2 on ImageNet, CIFAR-100, and WikiText-103, GNN-Prune achieves competitive sparsity-accuracy trade-offs (within 0.5% of magnitude pruning) while reducing pruning time by 10\u00d7 on ImageNet. However, our approach struggles with out-of-distribution architectures and requires architecture-specific training. While not achieving state-of-the-art sparsity, GNN-Prune offers a practical middle ground between costly retraining-based methods and simple magnitude pruning, particularly benefiting practitioners seeking faster pruning in deployment scenarios.",
    "id": 1437
  },
  {
    "title": "Gradient Surgery Meets Transformer Attention: An Empirical Study of Task Arithmetic in Multi-Task Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "Pre-trained transformers are increasingly fine-tuned on multiple tasks, yet we lack principled methods for composing learned behaviors. While task arithmetic\u2014adding and subtracting gradient directions\u2014has shown promise in vision models, its effectiveness remains unclear for the attention mechanisms central to NLP. We systematically investigate gradient surgery techniques for multi-task transformer fine-tuning across 12 GLUE tasks. Our approach modifies standard gradient surgery by incorporating attention head importance scores as weighting factors during gradient aggregation. Experiments on BERT-Large and T5-Base reveal modest improvements over baseline fine-tuning (average 1.2% GLUE score increase), with consistent gains on tasks with high semantic similarity. However, performance degrades on dissimilar task pairs, and we observe a trade-off: improved multi-task accuracy comes at the cost of increased computational overhead (1.8\u00d7 training time). Analysis of attention patterns suggests our method succeeds by attenuating conflicting gradients in lower layers while preserving task-specific representations in higher layers. While not achieving state-of-the-art results, our findings indicate that careful gradient manipulation can yield practical benefits for multi-task NLP systems, particularly when tasks share underlying structure. Code and trained models will be released upon acceptance.",
    "id": 1438
  },
  {
    "title": "Improving Transformer Training Stability through Layer-wise Gradient Shaping with Learnable Curvature Constraints",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chen, L."
    ],
    "abstract": "Training instability remains a persistent challenge for deep transformers, often manifesting as gradient explosion or vanishing despite careful initialization and normalization. We propose Gradient Shape Regularization (GSR), a simple technique that applies layer-specific curvature constraints to stabilize training without architectural modifications. GSR introduces learnable scaling factors that adaptively bound the spectral norm of each layer's Jacobian, effectively creating soft trust regions during optimization. Our method requires only minimal computational overhead (3% increase in training time) and can be implemented with 12 lines of PyTorch code. Experiments across 4 vision and 3 language benchmarks show consistent but modest improvements: we achieve 0.7-2.3% better final accuracy and reduce training variance by 15-40% over 5 seeds. While our theoretical analysis suggests GSR approximately preserves Lipschitz continuity, our bounds are loose and rely on assumptions that may not hold in practice. Ablation studies reveal performance degrades significantly when curvature constraints are too aggressive, highlighting the delicate balance our method must strike. Though GSR provides a practical training tool, its impact appears limited to specific training regimes where instability is the primary bottleneck.",
    "id": 1439
  },
  {
    "title": "Mixture of Low-Rank Experts: Efficient Fine-Tuning for Large Language Models Without Parameter Sharding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Large language model fine-tuning remains computationally prohibitive for practitioners with limited resources. We propose Mixture of Low-Rank Experts (MoLRE), a parameter-efficient fine-tuning method that learns a routing network to dynamically compose low-rank adaptations for each input. Unlike LoRA which trains static low-rank matrices, MoLRE maintains a bank of rank-4 expert matrices and learns input-dependent routing weights through a lightweight gating mechanism. This allows selective activation of expertise while keeping total parameters per example below 0.1% of the base model. We evaluate MoLRE on instruction tuning and domain adaptation tasks across Llama-2-7B and Mistral-7B, achieving comparable performance to full fine-tuning (+2.1 ROUGE on summarization) with 87% reduction in trainable parameters. However, we observe increased inference latency (6-18%) due to routing computation and some tasks show minimal gains over standard LoRA. Our results suggest that while mixture-based approaches can provide marginal improvements in specific settings, the overhead may not justify the complexity for most applications. Code is available at anonymous-github.com/molre2024.",
    "id": 1440
  },
  {
    "title": "Fixing BatchNorm Statistics in Few-Shot Learning: A Lightweight Approach via Meta-Adaptive Running Estimates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Few-shot learning models often struggle with BatchNorm layers when training and test data come from different distributions. While recent work has proposed various normalization schemes and feature adaptation techniques, these typically require expensive meta-learning or introduce complex architectural modifications. We present a lightweight alternative: Meta-Adaptive Running Estimates (MARE), which learns to adjust BatchNorm statistics via a small set of meta-parameters that modulate the exponential moving average update rule. Our approach adds only 0.3K parameters per layer and can be integrated into existing architectures with minimal changes. We evaluate MARE on standard few-shot benchmarks (mini-ImageNet, CUB-200) across multiple backbone architectures, achieving 2-3% improvements over standard BatchNorm baselines and modest gains over recent normalization alternatives. While our method provides consistent benefits with low computational cost, we find performance saturates on more challenging cross-domain settings and larger backbones. The simplicity of MARE makes it a practical plug-in for few-shot systems, though we acknowledge the improvements are incremental and the theoretical analysis remains preliminary. Code will be released upon acceptance.",
    "id": 1441
  },
  {
    "title": "Gradient Surgery for Stabilizing Transformer Training via Layer-wise Learning Rate Modulation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models often exhibit unstable training dynamics, particularly when scaling to deeper architectures. We propose a simple yet effective modification to standard gradient descent that applies layer-specific learning rate scaling based on gradient norm ratios. Our method, termed Gradient Surgery, computes adaptive per-layer learning rates by comparing each layer's gradient magnitude to the median across all layers, scaling down updates for layers with disproportionately large gradients. This approach requires no additional hyperparameters beyond the base learning rate and adds minimal computational overhead. We evaluate our method on standard language modeling benchmarks (WikiText-103, C4) and machine translation tasks (WMT'14 En-De, En-Fr). Experiments show consistent training stability improvements, reducing loss spikes by 34% on average across tasks while maintaining comparable final performance to baseline transformers (\u00b10.2 BLEU scores). While our method provides consistent stabilization benefits, the impact on final model quality remains modest, and we observe diminishing returns as model sizes exceed 1B parameters. The simplicity of our approach makes it readily implementable, though further theoretical justification for the gradient norm heuristic is needed.",
    "id": 1442
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Johnson, M.K.",
      "Rodriguez, A."
    ],
    "abstract": "While gradient noise injection has shown promise for escaping sharp minima and improving generalization in deep networks, existing approaches use fixed noise levels that may not adapt to the optimization landscape. We propose Adaptive Gradient Noise Injection (AGNI), a simple modification that adjusts noise magnitude based on gradient history and batch statistics. Our method adds scaled isotropic Gaussian noise to gradients during training, where the noise scale is modulated by the ratio of recent gradient norm to historical gradient norm. We evaluate AGNI on CIFAR-10/100 and ImageNet, achieving 0.3-0.7% improvements over baselines across ResNet-50, Vision Transformer, and BERT architectures. Theoretical analysis suggests AGNI encourages exploration in regions of high curvature while maintaining stability in flat minima. However, we observe strong sensitivity to the hyperparameter controlling noise decay rate, and gains diminish with larger batch sizes or extensive hyperparameter tuning. Our method adds minimal computational overhead (\u22483% training time) and integrates easily with existing optimizers. While the improvements are modest and not consistent across all settings, AGNI provides a lightweight alternative to complex regularization techniques.",
    "id": 1443
  },
  {
    "title": "Momentum Residual Connections for Improved Optimization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "While residual connections have enabled training of very deep networks, we argue that their additive nature creates optimization dynamics that can slow convergence in later training stages. We propose Momentum Residual Connections (MoRes), which replace the standard skip addition with a momentum-weighted combination of the identity and transformation branches. Our key insight is that this modification allows the network to gradually shift from primarily learning residuals to learning full transformations, providing smoother optimization than vanilla residual blocks. We demonstrate the effectiveness of MoRes on ImageNet and CIFAR-10, achieving 0.8% and 1.2% accuracy improvements over baseline ResNet architectures respectively. Through empirical analysis, we show that MoRes networks exhibit lower gradient variance and faster convergence in later training phases. However, we also identify sensitivity to the momentum hyperparameter across datasets, suggesting the need for careful tuning. While our results are modest, they suggest that reconsidering the residual connection mechanism may offer practical benefits for training deep architectures. Code is available at: anonymous-url.github.io/mores.",
    "id": 1444
  },
  {
    "title": "Improving Transformer Efficiency through Learnable Sparse Attention with Entropy-Guided Dropout",
    "authors": [
      "Chen, L.",
      "Srinivasan, K.",
      "M\u00fcller, H.",
      "Jones, T."
    ],
    "abstract": "Transformer models struggle with quadratic complexity in self-attention as sequence lengths grow. While prior work has explored sparse attention patterns using fixed sparsity masks or learned patterns with reinforcement learning, these approaches either require manual design or suffer from unstable training dynamics. We propose Entropy-Guided Sparse Attention (EGSA), which learns attention sparsity using a differentiable entropy regularizer that encourages structured sparsity during training. Our method combines a learned gating mechanism with dropout probability conditioned on attention entropy, allowing dynamic sparsity adaptation across layers and heads. On WikiText-103 and Long Range Arena benchmarks, EGSA achieves 1.3\u00d7 speedup over standard attention with <2% perplexity increase on language modeling tasks. However, performance degrades on tasks requiring dense interactions between distant positions. Ablation studies reveal most gains come from early layers where attention patterns are naturally more sparse. While EGSA shows promise for efficient transformers, current limitations include difficulty scaling to extremely long contexts (>8k tokens) and sensitivity to the entropy regularization weight. Code and models will be released upon acceptance.",
    "id": 1445
  },
  {
    "title": "Gradient Surgery Revisited: When Gradient Conflict Removal Hurts More Than It Helps",
    "authors": [
      "Chen, M.",
      "Rodriguez, L.",
      "Kim, J.H.",
      "Kumar, A."
    ],
    "abstract": "Multi-task learning increasingly relies on gradient surgery techniques to resolve conflicting gradients from different objectives. While methods like PCGrad and GradDrop demonstrate strong empirical performance, we reveal that aggressive gradient conflict removal can inadvertently eliminate gradient signal that encodes task relationships. Through theoretical analysis of two-layer linear models, we establish that perfect gradient conflict removal leads to task interference that grows unbounded with the number of tasks. Motivated by this finding, we propose soft gradient surgery (SGS), which selectively removes only gradient components with magnitude-normalized cosine similarity below a learned threshold. On MultiMNIST and CityScapes, SGS achieves comparable or superior performance to baselines while requiring minimal hyperparameter tuning. However, our exhaustive experiments on CIFAR-100 Superclasses show that even SGS provides limited gains over naive parameter sharing once batch normalization is properly initialized. While our theoretical insights apply only to simplified settings and our empirical gains are modest, this work challenges the prevailing assumption that aggressive gradient conflict removal always benefits multi-task learning. Code and hyperparameters will be provided upon acceptance.",
    "id": 1446
  },
  {
    "title": "Gradient Descent with Memory-Augmented Momentum: A Simple Trick for Better Optimization",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "We propose MementoSGD, a simple modification to standard momentum-based gradient descent that maintains a small memory buffer of past gradients to adaptively reweight the momentum term. Unlike adaptive methods that require second-order statistics, MementoSGD adds minimal computational overhead while demonstrating improved convergence on non-convex optimization problems. Our method uses a learned gating mechanism based on similarity between current and stored gradients to dynamically adjust the momentum coefficient. We evaluate MementoSGD on image classification tasks using ResNet-50 and Vision Transformers across CIFAR-10/100 and ImageNet-1k. Experiments show 2-4% relative improvement over vanilla SGD with momentum after 100 epochs, with particularly strong results in low-data regimes. However, gains diminish with careful hyperparameter tuning and data augmentation. Theoretical analysis reveals connections to quasi-Newton methods, though our convergence bounds are weaker than state-of-the-art adaptive optimizers. While the improvements are modest, MementoSGD offers a practical compromise between the simplicity of SGD and performance of adaptive methods, making it potentially useful for practitioners seeking better optimization without the complexity of fully adaptive algorithms.",
    "id": 1447
  },
  {
    "title": "Looped Transformers Can Learn to Reason About Algorithms, But Only Sometimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, K."
    ],
    "abstract": "We investigate whether transformer language models can learn algorithmic reasoning tasks when their layers are applied recurrently (looped transformers), a setting that reduces parameter count while potentially increasing computational depth. Through extensive experiments on algorithmic tasks including sorting, graph traversal, and dynamic programming, we find that looped transformers exhibit a phase transition: they succeed when the target algorithm's complexity falls below a threshold that scales linearly with the number of loop iterations, but fail otherwise. We provide theoretical analysis showing this threshold emerges from the transformer's inability to maintain precise intermediate state representations across many loops, supported by empirical evidence from attention pattern analysis. Our results suggest that while architectural modifications like looped transformers can solve some algorithmic tasks efficiently, they face fundamental limitations that may require additional inductive biases. The work provides insight into the capabilities and limitations of reasoning within end-to-end neural architectures.",
    "id": 1448
  },
  {
    "title": "Rethinking Early Stopping Through the Lens of Sharpness-Aware Minimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Early stopping is ubiquitously used to prevent overfitting in machine learning models, yet its theoretical understanding remains limited. We propose Sharpness-Aware Early Stopping (SAES), a principled approach that incorporates the loss landscape's sharpness into the stopping criterion. By analyzing the Hessian trace as a measure of solution sharpness, we establish a connection between early stopping and sharpness-aware minimization. Our method introduces a simple modification to the standard early stopping protocol: we stop training when (1) validation loss plateaus and (2) the sharpness measure exceeds a predefined threshold. We evaluate SAES on ResNet-18 and Vision Transformer architectures across CIFAR-10, CIFAR-100, and ImageNet subsets. Results show modest improvements of 0.8-1.2% accuracy over standard early stopping baselines, with the benefit being most pronounced on smaller datasets. While our theoretical analysis provides insights into the implicit bias of early stopping, the practical gains are incremental and the computational overhead non-negligible. Nevertheless, SAES offers a principled perspective on an under-explored connection between generalization and the loss landscape geometry. Code and pre-trained models will be made available.",
    "id": 1449
  },
  {
    "title": "Gradient Descent with Adaptive Learning Rates via Online Bin Packing",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Foster, J."
    ],
    "abstract": "We propose a novel adaptive learning rate method for gradient descent that draws inspiration from online bin packing algorithms. Our key insight is to view the learning rate selection problem as a resource allocation task, where we pack gradient updates into bins representing learning rate values. Using the Harmonic+ algorithm from the bin packing literature, we partition parameter updates into groups with similar gradient magnitudes, assigning appropriate learning rates to each group. Theoretically, we prove O(log T) regret bounds for convex Lipschitz functions, matching standard adaptive methods. Empirically, we evaluate on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures, achieving 0.3-0.7% accuracy improvements over AdamW and 1.2-1.8% over SGD with momentum. While our method shows consistent gains on small models and datasets, we observe the improvements diminish with larger models (ViT-B/16). Additionally, our approach introduces non-negligible computational overhead (15-20% increase in training time) due to the bin packing subroutine. We release PyTorch code and configurations for reproducibility.",
    "id": 1450
  },
  {
    "title": "Improving Transformer Fine-tuning with Adaptive Weight Re-initialization",
    "authors": [
      "Liu, K.",
      "Chen, M.",
      "Rodriguez, J."
    ],
    "abstract": "While pre-trained transformers achieve remarkable performance on downstream tasks through fine-tuning, we observe that the standard practice of using small random initialization for task-specific heads often leads to slow convergence and suboptimal local minima. We propose Adaptive Weight Re-initialization (AWR), a simple technique that dynamically adjusts initialization scales based on layer-wise gradient statistics during the first few training steps. Our method requires no additional hyperparameters beyond standard fine-tuning and adds minimal computational overhead (<5% training time). We evaluate AWR on GLUE benchmark tasks using BERT-base and RoBERTa-base models, achieving modest but consistent improvements (avg. 0.8% GLUE score increase) over standard fine-tuning. Analysis reveals AWR particularly benefits tasks with limited training data, suggesting improved optimization in low-data regimes. However, we find diminishing returns on larger models (\u2265350M parameters) and minimal impact when using strong regularization techniques. While our contributions are incremental, AWR provides insights into how initialization affects fine-tuning dynamics and offers a practical enhancement for small-to-medium scale applications.",
    "id": 1451
  },
  {
    "title": "Neural Network Weight Clustering for Improved Generalization via Determinantal Point Processes",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Johnson, M."
    ],
    "abstract": "Weight redundancy in deep neural networks has been observed across many architectures, yet principled approaches to reduce redundancy while maintaining generalization remain underexplored. We propose WeightDrop, a training method that induces structured sparsity in neural networks by modeling neuron activations as Determinantal Point Processes (DPPs), promoting diverse weight representations without hard pruning. Our approach encourages weight clustering during training, naturally identifying redundant parameters that can be subsequently removed. We demonstrate that WeightDrop achieves competitive performance on CIFAR-10 and ImageNet, reducing model size by 30-40% while maintaining accuracy within 0.5% of dense baselines. Theoretically, we establish generalization bounds showing that clustered weights enjoy improved margin distributions, though our analysis relies on strong distributional assumptions. Extensive ablations reveal that performance gains are most pronounced in architectures with residual connections, and that the method's effectiveness correlates with initial overparameterization levels. While our empirical results are promising, we acknowledge that the computational overhead during training may limit scalability to larger models, and that improvements over standard pruning baselines are modest.",
    "id": 1452
  },
  {
    "title": "Meta-Learning Data Augmentation Schedules through Gradient-Based Hyperparameter Optimization",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, M.",
      "Johnson, E."
    ],
    "abstract": "Data augmentation has become a cornerstone of modern deep learning, yet the scheduling of augmentation strength during training remains largely heuristic. We propose MASDA, a meta-learning framework that learns optimal augmentation schedules by treating schedule parameters as hyperparameters optimized through implicit differentiation. Our method jointly trains a base model and a lightweight hypernetwork that outputs per-epoch augmentation intensities, guiding models toward better convergence. Experiments on CIFAR-10/100 and ImageNet subsets show 1.2-2.1% improvements over hand-tuned baselines with comparable compute budgets. However, we find performance gains diminish with larger models and longer training, suggesting limited scalability. Our ablation reveals that schedule learning primarily helps in early training phases, with diminishing returns after epoch 50. While MASDA provides marginal improvements in specific scenarios, the computational overhead makes it less practical for resource-constrained settings. Code and pretrained models are available at anonymous-url.github.io/masda.",
    "id": 1453
  },
  {
    "title": "Attention Mixup: Improving Generalization Through Token-Level Interpolation",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose Attention Mixup, a simple augmentation technique that interpolates attention-weighted token representations for transformer-based models. Unlike traditional Mixup which operates on raw inputs or final representations, our method selectively interpolates tokens based on their attention scores, theoretically providing better regularization by focusing on salient content. We evaluate Attention Mixup on text classification, sentiment analysis, and natural language inference tasks across BERT, RoBERTa, and DistilBERT architectures. Experiments show consistent improvements of 0.5-1.2% accuracy over standard Mixup and 1.3-2.1% over baseline models. While our approach is computationally efficient with minimal overhead, we observe diminishing returns on larger models and tasks with synthetic data. Theoretical analysis reveals that Attention Mixup acts as a form of adaptive ridge regression, though the connection to model generalization remains heuristic. Code and pre-trained models are available at anonymized.github.io.",
    "id": 1454
  },
  {
    "title": "Learning to Bridge: Interpolating Between Fine-Tuned and Zero-Shot Capabilities via Task-Adaptive Linear Mappings",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Joshi, K."
    ],
    "abstract": "Large language models exhibit two distinct operating modes: zero-shot behaviors inherited from pre-training and specialized capabilities acquired through fine-tuning. However, practitioners lack principled approaches for navigating the trade-offs between these extremes. We propose Task-Adaptive Linear Mappings (TALM), a lightweight intervention that learns to interpolate between frozen zero-shot and fine-tuned checkpoints through learnable linear combinations of their parameter differences. Our method requires only 0.1% additional parameters and can be trained in under 5 GPU-hours. Across 12 datasets spanning sentiment analysis, question answering, and mathematical reasoning, TALM achieves modest but consistent gains over either extreme (average +1.8% zero-shot, +2.1% fine-tuned), with particularly strong performance on distribution shifts (+5.3% relative improvement). While our theoretical analysis connects TALM to gradient descent in function space, the bounds remain loose and empirical validation is limited to encoder-decoder architectures. These results suggest linear interpolations in parameter space can be surprisingly effective, though the method's applicability appears constrained to tasks where zero-shot and fine-tuned solutions share significant overlap in their learned representations.",
    "id": 1455
  },
  {
    "title": "Gradient Surgery with Momentum: A Simple Fix for Multi-Task Learning Instabilities",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Rodriguez, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradient directions between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradDrop modify gradients to mitigate interference, these approaches ignore the critical role of momentum in optimization dynamics. We propose Momentum-Aware Gradient Surgery (MAGS), a lightweight modification that incorporates momentum history into gradient conflict resolution. Our method computes gradient projections not only in the current gradient space but also in the momentum-smoothed direction, reducing oscillations that plague existing techniques. We evaluate MAGS on standard multi-task benchmarks including Multi-MNIST, NYUv2 depth estimation, and Amazon review classification. Results show consistent improvements over baselines, with a 3.2% average gain on held-out tasks compared to PCGrad. However, the benefits diminish when task complementarity is already high, suggesting our method primarily helps in high-conflict regimes. While MAGS introduces minimal computational overhead, we acknowledge its reliance on hyperparameter sensitivity around the momentum coefficient and the lack of theoretical convergence guarantees. Code and experiments are available at [anonymous link omitted].",
    "id": 1456
  },
  {
    "title": "Adaptive Gradient Clipping with Moving Quantile Estimates for Transformer Training",
    "authors": [
      "Liu, K.",
      "Chen, B.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient clipping is widely used to stabilize transformer training, but existing threshold selection methods rely on heuristics or require expensive hyperparameter tuning. We propose Adaptive Quantile Clipping (AQC), which dynamically sets clipping thresholds using moving quantile estimates of gradient norms. AQC tracks the 90th percentile of gradient norms over fixed-size windows, adjusting the clipping threshold to maintain stable training while preserving convergence. Our method introduces two new hyperparameters: window size and target percentile, but we show these are robust across tasks. Experiments on language modeling (WikiText-103) and machine translation (WMT'14 EN-DE) demonstrate AQC matches or slightly improves upon tuned static clipping baselines without requiring manual threshold selection. However, gains are modest (0.5-1.2 BLEU/0.02-0.05 perplexity) and primarily observed in training stability rather than final performance. While AQC reduces hyperparameter sensitivity, its computational overhead (5-8% increase in training time) may limit its applicability. Code is available at github.com/anonymous/aqc-icml2024.",
    "id": 1457
  },
  {
    "title": "LoRA\u00b2: Low-Rank Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, A."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular method for parameter-efficient fine-tuning of large language models, but its effectiveness diminishes for complex downstream tasks. We propose LoRA\u00b2, a simple extension that applies LoRA recursively: instead of learning a single low-rank decomposition of weight updates, we decompose each rank-1 component into an additional low-rank structure. This yields a factorization that maintains the same parameter count as standard LoRA for ranks \u2264 2, while potentially capturing more complex patterns. Our theoretical analysis shows that LoRA\u00b2 can represent a strictly larger class of transformations under mild rank conditions. Experiments on GLUE and SuperGLUE benchmarks show modest improvements (average +0.8 points) over LoRA across 8 tasks, with gains concentrated on natural language inference problems. However, we find that LoRA\u00b2 provides no benefit over standard LoRA when the base rank exceeds 4, suggesting the approach's limitations scale with model complexity. While our method offers an interesting perspective on hierarchical low-rank structures, the practical improvements are task-specific and do not justify the additional complexity in most settings. Code and pre-trained adapters will be released upon publication.",
    "id": 1458
  },
  {
    "title": "Revisiting Curriculum Learning as Adaptive Policy Optimization",
    "authors": [
      "Liu, C.",
      "Kumar, P.",
      "Zhao, T."
    ],
    "abstract": "Curriculum learning has traditionally been viewed as a scheduling problem for training data orderings. We propose reinterpreting curriculum design as a meta-learning problem, where an auxiliary policy network learns to dynamically adjust training difficulty for individual samples. Our method, AdCur, trains a lightweight policy alongside the main model to modulate loss weights based on validation performance gradients. We evaluate AdCur on image classification (CIFAR-10/100, ImageNet subsets) and language modeling (WikiText-103), showing 2-5% improvements in final accuracy compared to uniform training baselines. While the approach yields consistent gains, we observe diminishing returns as model capacity increases\u2014suggesting that adaptive curricula may primarily benefit smaller architectures. Theoretical analysis reveals our method approximates a trust region optimization in curriculum space, though we note that training instability can occur in high-noise regimes. Compared to recent work on data pruning and teacher-student curriculum methods, AdCur achieves comparable results without requiring pre-training or human-designed curricula. Code and hyperparameter configurations are provided for reproducibility.",
    "id": 1459
  },
  {
    "title": "Gradient Norm Regularization Improves Adversarial Robustness by Balancing Margin and Lipschitzness",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, D."
    ],
    "abstract": "Adversarial training remains the dominant approach for achieving robustness against adversarial attacks, yet it suffers from high computational cost and unreliable generalization to unseen threat models. We propose a simple alternative: adding gradient norm regularization to standard training as a lightweight defense mechanism. Our method constrains the gradient norm of the loss with respect to inputs, leading to models with improved margin and bounded Lipschitz constants. Through experiments on CIFAR-10 and ImageNet, we show that gradient norm regularization achieves robust accuracies of 42.3% and 30.1% against PGD attacks, respectively, outperforming vanilla training (12.1% and 8.7%) while requiring only 15% additional training time compared to adversarial training's 8-10x overhead. We provide theoretical analysis connecting gradient regularization to margin maximization under the assumption of data concentration around low-dimensional manifolds. However, our defense shows limited effectiveness against adaptive attacks that incorporate the regularization term into the optimization objective. While not achieving state-of-the-art robustness, our work highlights an unexplored trade-off between computational efficiency and adversarial robustness, offering practitioners a computationally feasible alternative when adversarial training is prohibitive.",
    "id": 1460
  },
  {
    "title": "Gradient Norm Regularization Improves Transformer Training Stability: An Empirical Study",
    "authors": [
      "Liu, Q.",
      "Rodriguez, C.",
      "Kim, J."
    ],
    "abstract": "We investigate the effect of gradient norm regularization on transformer model training stability across various natural language processing tasks. While gradient clipping is widely used to prevent gradient explosion, we propose a simple alternative: adding an L2 penalty on gradient norms during backpropagation. Through extensive experiments on language modeling, machine translation, and GLUE tasks, we find that this regularization reduces training instability and improves convergence on 7 out of 12 benchmark datasets, particularly for deeper transformer variants (48+ layers). Our analysis reveals that the regularization primarily affects the early training phase, where it smooths the loss landscape without significantly affecting final model capacity. However, we find the benefits diminish with careful hyperparameter tuning of standard training procedures, raising questions about whether our approach provides advantages beyond existing optimization techniques. Our code will be released upon acceptance.",
    "id": 1461
  },
  {
    "title": "Gradient Surgery Without Scalpels: Momentum-Based Gradient Interpolation for Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Multi-task learning in deep neural networks often suffers from conflicting gradients that prevent simultaneous optimization of multiple objectives. While existing gradient surgery methods project gradients onto conflict-free subspaces, we propose a simpler interpolation approach based on gradient momentum. Our method (MomentumGrad) computes task-specific momentum buffers and blends gradients using learned interpolation coefficients derived from the cosine similarity between momentum directions. This eliminates the need for expensive gradient decompositions while maintaining improved stability over standard multi-task training. We evaluate MomentumGrad on three vision-language benchmarks and two multi-objective RL environments. Results show modest improvements over baselines (0.5-2.3% average performance gains) at the cost of increased memory complexity. Theoretical analysis reveals that momentum directions tend to align with lower-curvature directions, suggesting partial conflict resolution. However, we find the method exhibits instability on tasks with highly dissimilar objectives, sometimes degrading performance relative to single-task training. While MomentumGrad offers a lightweight alternative to sophisticated gradient surgery methods, its benefits appear limited to specific regime of moderately-coupled tasks. Code is available at [repository].",
    "id": 1463
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Structured Sparsity in Low-Rank Adapters",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its memory footprint during training remains problematic for large models. We propose LoRA-Lite, which applies structured sparsity patterns to LoRA adapters to reduce memory usage by 35-50% without modifying the base model. Our method learns to prune adapter weights using a simple magnitude-based criterion combined with a novel block-wise reparameterization trick that maintains gradient flow during training. On GLUE tasks with GPT-2 Large, LoRA-Lite achieves 98% of full LoRA performance while using 42% less memory. Results on Stable Diffusion fine-tuning show similar trends, though we observe some degradation on out-of-domain prompts. Theoretically, we provide convergence bounds showing the method preserves the original LoRA training dynamics under mild assumptions. While LoRA-Lite offers clear practical benefits for memory-constrained settings, our analysis reveals trade-offs between sparsity level and task performance that may limit applicability in some scenarios. Code will be released upon acceptance.",
    "id": 1464
  },
  {
    "title": "Rethinking Curriculum Learning: A Weak Supervision Approach via Pre-trained Model Priors",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "Curriculum learning has shown promise for training neural networks on hard tasks by gradually increasing difficulty, but designing effective curricula remains challenging. We propose Curriculum by Weak Supervision (CWS), a simple method that leverages pre-trained language models to automatically generate weak supervision signals for curriculum design. Our approach uses masked language models to identify 'easier' examples through token-level confidence scores, creating an implicit curriculum without human-designed rules. We validate CWS on three classification benchmarks (AG News, DBPedia, and ChemProt) and a synthetic reasoning task. Results show 2-4% improvement over standard training in low-data regimes (\u22641k examples), but minimal gains with abundant data. While CWS provides consistent improvements over random ordering baselines, it underperforms compared to hand-crafted curricula in domains where expertise is available. We demonstrate our method is particularly effective when task-specific curricula are expensive to design. However, our approach inherits limitations from the pre-trained model's biases and struggles with out-of-domain transfer. Code and data are available at [anonymous-link].",
    "id": 1465
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique for Stabilizing Deep Network Training",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Gradient Dropout, a lightweight regularization method that randomly masks gradient components during backpropagation to improve training stability in deep neural networks. While dropout is traditionally applied to activations, we show that applying stochastic masking to gradient values can effectively prevent co-adaptation of weight updates without introducing additional computational overhead during inference. Our method adds a single hyperparameter p_g, the probability of dropping gradient components, which we typically set between 0.1 and 0.3 based on validation performance. Empirical evaluation on CIFAR-10/100 and ImageNet shows modest improvements over standard dropout, with our best results achieving 1.2% and 0.8% accuracy gains on CIFAR-100 and ImageNet respectively when applied to ResNet-50 architectures. We provide convergence analysis under standard Lipschitz assumptions and demonstrate that Gradient Dropout can be interpreted as implicit gradient noise injection. However, we find limited benefits on smaller architectures like LeNet and smaller datasets, suggesting the technique may be most useful for very deep networks. Code will be made available upon publication.",
    "id": 1466
  },
  {
    "title": "Gradient Flows in Neural Networks: A Simplified Approach to Mean-Field Analysis",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "We present a simplified framework for analyzing gradient flows in two-layer neural networks using mean-field theory. While existing approaches rely on complex measure-theoretic arguments, we propose an alternative formulation based on smoothed particle approximations that leverages classical ODE techniques. Our key insight is that controlling the Wasserstein-1 distance between empirical measures and their continuous limits can be achieved through tighter regularization of the activation function. We establish non-asymptotic bounds on the approximation error and prove convergence rates of O(1/\u221an) under mild regularity assumptions. Empirically, we demonstrate improved stability in training shallow networks on synthetic datasets, with convergence behavior matching theoretical predictions. While our framework currently handles only bounded Lipschitz activations and requires careful tuning of the regularization parameter, our simplified analysis may provide more interpretable insights into the gradient dynamics of overparameterized networks. Code and experiments are available at [url].",
    "id": 1467
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Simple Regularization Technique for Improving Generalization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "While sharpness-aware minimization (SAM) has emerged as a promising approach for improving generalization in deep neural networks, its computational overhead and hyperparameter sensitivity limit practical adoption. We propose a lightweight alternative that combines gradient surgery techniques with implicit sharpness regularization. Our method, termed Sharpness-aware Gradient Surgery (SGS), identifies conflicting gradient directions during optimization and selectively attenuates updates that increase sharpness, while maintaining computational efficiency comparable to standard SGD. We provide theoretical analysis showing SGS converges to flatter minima under mild assumptions, and demonstrate competitive performance relative to SAM on CIFAR-10/100 and ImageNet with 40-50% less computational cost. However, we observe performance improvements diminish on larger models and certain architectures (e.g., ViTs), suggesting limitations in our sharpness estimation approach. Our empirical evaluation across various architectures and datasets reveals consistent but modest improvements (0.5-1.2% accuracy gains) over baselines, particularly beneficial for smaller datasets. While SGS offers a practical compromise between performance and efficiency, we acknowledge that the gains are incremental and may not justify the added complexity in all applications.",
    "id": 1468
  },
  {
    "title": "Gradient Clipping Improves Training Only When Learning Rates Are Poorly Tuned: A Large-Scale Empirical Analysis",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "While gradient clipping is widely used to stabilize neural network training, its benefits are typically attributed to preventing exploding gradients. We conduct a systematic empirical study across 500 training runs spanning 12 diverse architectures (CNNs, RNNs, Transformers) on 8 datasets. Surprisingly, we find that gradient clipping primarily helps when initial learning rates are either too high or too low relative to the optimal schedule; with well-tuned learning rates, clipping provides minimal or negative impact on final performance. Our analysis reveals that clipping acts as a crude form of learning rate adaptation, with clipping thresholds correlating strongly (\u03c1 = 0.87) with the ratio between actual and optimal learning rates. We propose a lightweight diagnostic to predict when clipping will help based on gradient norm statistics during early training, achieving 82% accuracy in hold-out experiments. While our findings are robust across architectures, they raise questions about whether clipping should remain a default practice in modern training pipelines with extensive hyperparameter tuning.",
    "id": 1469
  },
  {
    "title": "Gradient Descent with Iterative Label Smoothing for Noisy Supervision",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Nakamura, T."
    ],
    "abstract": "We propose an iterative label smoothing approach to improve robustness against noisy labels in deep learning. Our method alternates between two phases: (1) training a neural network on the current labels using standard cross-entropy loss, and (2) updating the labels by blending them with the network's softmax predictions. This creates a feedback loop where the model gradually learns to down-weight suspicious labels while converging to more reliable decision boundaries. We analyze convergence properties on linear models and demonstrate empirical improvements on CIFAR-10 and CIFAR-100 with synthetic noise patterns. Our method achieves 1-2% accuracy gains over confidence-based baselines when 40-60% of labels are corrupted, with minimal additional computational cost. While the theoretical guarantees are limited to simple settings, ablations suggest the iterative correction mechanism is key to improved performance, particularly when noise rates are moderate. Code and experiments are available at [link].",
    "id": 1470
  },
  {
    "title": "Momentum Without Velocity: A Memory-Efficient Variant for Large-Scale Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, D."
    ],
    "abstract": "We propose Compressed Momentum (CM), a variant of stochastic gradient descent that reduces memory usage by storing gradient history in a compressed form while preserving convergence guarantees. Our method applies quantization and sketching techniques to momentum buffers, achieving up to 16\u00d7 memory reduction compared to standard momentum SGD. We prove O(1/T) convergence for convex objectives and O(1/\u221aT) for non-convex settings, matching standard momentum rates up to logarithmic factors. Experiments on CIFAR-10/100 with ResNet-18 and ImageNet with ViT-small show 0.3-1.2% accuracy degradation compared to uncompressed baselines, while enabling training with 50% larger batch sizes within the same memory budget. Additionally, CM reduces communication costs in distributed settings by 65% without additional hyperparameter tuning. While our theoretical analysis assumes smoothness and bounded gradients, empirical results on language modeling (GPT-2 small) suggest broader applicability, though we observe training instability with extremely aggressive compression (\u226532\u00d7). Our method provides a practical trade-off between memory efficiency and model performance, particularly valuable for training on resource-constrained devices.",
    "id": 1471
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Pruning for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) have enabled effective adaptation of large language models while updating only a small subset of parameters. However, LoRA applies uniform rank across all layers regardless of their importance, potentially leading to computational waste. We propose LoRA-Drop, a simple yet effective approach that adaptively assigns ranks based on layer-wise sensitivity analysis. Our method employs a bilevel optimization framework that simultaneously trains LoRA adapters and learns to prune unnecessary ranks through differentiable gating mechanisms. Experiments on NLP benchmarks (GLUE, SuperGLUE) and vision-language tasks show LoRA-Drop achieves 20-30% parameter reduction compared to standard LoRA while maintaining comparable performance (within 1% accuracy). While our method provides modest efficiency gains, we observe the pruning strategy sometimes removes useful rank components, particularly in lower layers. Ablations reveal the bilevel optimization is sensitive to hyperparameter choices, sometimes leading to unstable training. Our results suggest rank allocation heuristics could be more effective than learned pruning for certain architectures. Code will be released upon acceptance.",
    "id": 1472
  },
  {
    "title": "Gradient Surgery Meets Sharpness Awareness: A Unified Framework for Improving Generalization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Despite significant advances in optimizing neural networks for better generalization, gradient surgery-based methods and sharpness-aware minimization (SAM) have evolved as largely separate approaches. We propose Sharpness-Aware Gradient Surgery (SAGS), a novel framework that unifies these perspectives by performing targeted gradient manipulation based on local sharpness estimates. Our method identifies directions in parameter space with high sharpness and applies gradient surgery only along these directions, enabling simultaneous optimization of task performance and flatness. Experiments on CIFAR-100 and ImageNet show modest improvements (0.2-0.5% accuracy) over SAM alone, but our ablations reveal the gains depend heavily on model size and dataset characteristics. While SAGS provides theoretical guarantees for convergence under restricted assumptions, these assumptions may not hold in practical settings. Additionally, the computational overhead (2.3x standard training time) limits scalability. Our work suggests that combining sharpness awareness with gradient surgery is possible, but whether the complexity is justified remains questionable. Code will be released upon acceptance.",
    "id": 1473
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: A Simple Fix for Catastrophic Forgetting in Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts that lead to catastrophic forgetting, particularly when tasks have competing objectives. While recent approaches like gradient surgery show promise, they require careful tuning of task-specific hyperparameters and scale poorly with the number of tasks. We propose a simple extension that augments gradient surgery with an adaptive memory mechanism to automatically balance task gradients. Our method maintains running estimates of gradient norms across tasks and dynamically adjusts the surgery threshold based on observed conflicts. On standard multi-task benchmarks including split CIFAR-100 and Meta-World, our approach achieves modest improvements over vanilla gradient surgery (2-3% average accuracy) while reducing the need for task-specific hyperparameter tuning. However, we observe that these gains diminish when scaling beyond 10 tasks, and our method occasionally introduces instability during early training. Our analysis reveals that gradient conflicts are better resolved through explicit architectural modifications rather than gradient-level interventions. While our approach provides a lightweight drop-in replacement for existing gradient surgery methods, we acknowledge its limitations in scenarios with extreme task dissimilarity.",
    "id": 1474
  },
  {
    "title": "Gradient Amplification: A Simple but Effective Training Strategy for Small Batch Deep Learning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Gonzalez, M."
    ],
    "abstract": "Training deep neural networks with small batch sizes remains challenging due to gradient noise and instability, particularly when learning rates are scaled proportionally. While techniques like gradient accumulation and batch normalization help, they often require careful tuning or architectural changes. We propose Gradient Amplification (GradAmp), a surprisingly simple method that multiplies gradients by a constant factor \u03b1 > 1 during backpropagation. We show both empirically and through analysis of a simplified quadratic model that this amplifies the signal-to-noise ratio of gradient estimates without changing the optimization trajectory in expectation. On ImageNet with ResNet-50, GradAmp achieves 76.8% top-1 accuracy at batch size 32 compared to 75.1% for standard training, with similar gains on CIFAR-10. However, we find GradAmp benefits diminish with larger batch sizes (>256) and can cause instability on certain architectures. Our theoretical analysis provides convergence guarantees for convex problems but does not extend to the non-convex case. While GradAmp offers a practical improvement for constrained memory scenarios, its limitations suggest it may serve best as a simple baseline rather than a general solution to small-batch training.",
    "id": 1475
  },
  {
    "title": "Efficient Feature-Dropout via Correlation-Based Pruning at Initialization",
    "authors": [
      "Jones, K.",
      "Li, S.",
      "Abdurahman, M.",
      "Thompson, E."
    ],
    "abstract": "Recent work has explored pruning neural networks at initialization, achieving comparable performance to training while reducing computational overhead. However, most approaches focus on weight pruning rather than feature-level pruning, limiting their effectiveness for modern architectures with feature reuse. We propose Correlation-Based Feature Pruning (CBFP), a simple method that prunes entire input features before training based on their correlation with the target variable. CBFP computes feature-target correlations in linear time and efficiently masks out low-correlation features, achieving 25-40% FLOP reduction across standard benchmarks while maintaining accuracy within 2% of full models. Our theoretical analysis shows CBFP is equivalent to importance sampling of the input space, with convergence guarantees under mild assumptions. While less accurate than iterative pruning methods that use multiple rounds of training, CBFP provides fast, one-shot compression with minimal preprocessing. Limitations include reduced effectiveness on datasets with weak feature-target correlations and applicability only to architectures without skip connections. Despite these constraints, ongoing experiments suggest CBFP may be particularly useful for edge deployment scenarios where preprocessing costs are negligible but inference time is critical.",
    "id": 1476
  },
  {
    "title": "LoRA-Scale: Measuring the Effective Rank of Low-Rank Adaptation in Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, C.",
      "Kumar, A."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method for large language models, but little is understood about how the choice of rank affects downstream performance. We introduce LoRA-Scale, a theoretically-motivated framework for estimating the 'effective rank' needed for task adaptation. Using tools from matrix perturbation theory, we derive a data-dependent bound on the minimum rank required to maintain 95% of full fine-tuning performance. We validate LoRA-Scale across 12 GLUE tasks and 3 model sizes (350M to 7B parameters), showing our estimates reduce computational cost by 2-4x compared to standard LoRA configurations while maintaining within 1.2% of full fine-tuning accuracy. However, we find our bounds become increasingly loose for tasks requiring significant behavioral shifts from pre-training. Our results suggest LoRA-Scale provides practical guidance for rank selection in resource-constrained scenarios, though theoretical improvements are needed for more complex adaptation scenarios.",
    "id": 1477
  },
  {
    "title": "ReLoRA: Low-Rank Adaptation with Relative Gradient Perturbations for Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its performance degrades on tasks requiring substantial distribution shifts. We identify that LoRA's fixed low-rank structure limits its capacity to adapt to new domains. To address this, we propose ReLoRA, which augments LoRA with relative gradient perturbations computed from the difference between pre-trained and task-specific gradients. Our method introduces an additional low-rank perturbation matrix that is updated during training while keeping the original LoRA weights fixed. On GLUE and SuperGLUE benchmarks, ReLoRA achieves modest improvements over LoRA (average +0.8% accuracy) with only 12% additional parameters. However, we observe that gains are task-dependent, with negligible improvements on sentiment analysis but stronger results on reasoning tasks. Analysis reveals that ReLoRA's effectiveness correlates with the magnitude of distribution shift from pre-training, suggesting the method is most beneficial when fine-tuning data diverges significantly from pre-training distributions. While ReLoRA demonstrates consistent but small improvements over LoRA, the computational overhead and additional hyperparameters may limit practical adoption for many applications.",
    "id": 1478
  },
  {
    "title": "Gradient Alignment Improves Sharpness-Aware Minimization by Implicit Curvature Matching",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, A."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising approach for improving generalization by seeking flat minima, but its understanding remains limited. We investigate the role of gradient alignment between the base and perturbed optimization steps in SAM, revealing that traditional SAM implementations suffer from gradient misalignment that degrades stability. Motivated by this observation, we propose Gradient-Aligned SAM (GA-SAM), which introduces a lightweight curvature matching mechanism that adaptively aligns the gradient directions across optimization steps. Our method adds only 3% computational overhead compared to vanilla SAM while providing theoretical guarantees on the alignment quality under mild assumptions. On CIFAR-10/100 and ImageNet, GA-SAM achieves 0.3-0.7% and 0.1% accuracy improvements over SAM respectively, though gains diminish with increased augmentation. While our method provides modest improvements and demonstrates interesting connections between gradient alignment and sharpness, we acknowledge limitations including failure to consistently improve upon strong baselines on larger models (>100M parameters) and lack of compelling ablations for specific design choices.",
    "id": 1479
  },
  {
    "title": "Gradient Surgery for Training Transformers with Layer-wise Adaptive Dropout Rates",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "Fujimoto, Y."
    ],
    "abstract": "We propose a simple method to automatically adjust dropout rates in transformer networks during training by analyzing gradient statistics at each layer. Our approach computes layer-wise gradient norms and surgically modifies dropout rates inversely proportional to gradient stability, theoretically motivated by noise injection in overparameterized regimes. While similar to existing adaptive regularization techniques, our method introduces minimal computational overhead and can be implemented in under 20 lines of code. We evaluate on standard NLP benchmarks including GLUE, WMT'14, and WikiText-103, achieving modest improvements over baseline transformers (1.2% average GLUE score increase, 0.5 BLEU improvement on WMT). However, performance gains are inconsistent across tasks, disappearing entirely for smaller models and datasets. Our ablations reveal the method primarily helps for models with >300M parameters and >10B training tokens. Although the technique is straightforward to implement, we find it sensitive to hyperparameter choices and architectural variations, limiting broader applicability.",
    "id": 1480
  },
  {
    "title": "Improved Conditioned Gradient Methods for Neural Network Training through Adaptive Basis Updates",
    "authors": [
      "Liu, K.",
      "Chen, F.",
      "Rodriguez, M."
    ],
    "abstract": "We propose AdaptiveCG, a modification of the Frank-Wolfe method for neural network training that dynamically updates the constraint set basis during optimization. While Frank-Wolfe methods offer projection-free updates and implicit regularization, their convergence rate remains suboptimal for overparameterized networks due to fixed constraint sets. Our key insight is that basis updates using second-order information from the loss surface can better align the constraint set with the optimization trajectory. We derive convergence guarantees under standard assumptions, showing O(1/T) rates similar to standard Frank-Wolfe while empirically achieving faster practical convergence. Experiments on CIFAR-10 with ResNet-18 architectures demonstrate 15-20% reduction in training epochs compared to vanilla Frank-Wolfe, though results vary significantly across architectures. On ImageNet, we observe 8% improvement over baseline but underperform Adam by 12%. Our method adds minimal computational overhead (<5%) but requires tuning of basis update frequency. While theoretical contributions are limited (similar rates to prior work), AdaptiveCG provides a practical compromise between projection-free updates and modern optimizers. Code is available at anonymous-url.github.io/AdaptiveCG.",
    "id": 1481
  },
  {
    "title": "Gradient Descent with Momentum is Not Always Optimal: A Large-Scale Empirical Analysis",
    "authors": [
      "Chen, J.",
      "Liu, B.",
      "Thompson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We present the largest systematic comparison of gradient-based optimizers to date, evaluating 12 methods across 8 datasets and 3 neural network architectures. While momentum-based methods show consistent improvements over vanilla SGD in convex settings, our experiments reveal that adaptive methods like Adam achieve only marginal gains over carefully-tuned SGD+momentum in modern deep learning scenarios. Surprisingly, across 2,400 training runs, we find no single optimizer consistently outperforms others when hyperparameters are optimized via random search. We introduce a simple scheduling scheme that adapts the momentum parameter during training, yielding modest (1-3%) improvements over fixed schedules in 60% of tested configurations. These findings suggest that optimizer choice may be less critical than previously assumed when sufficient hyperparameter tuning is performed. Our code and 500GB of training logs are publicly available, though we note several training runs failed to converge, particularly with extreme learning rates.",
    "id": 1482
  },
  {
    "title": "Towards More Robust Few-Shot Learning via Adaptive Instance Normalization and Meta-Feature Augmentation",
    "authors": [
      "Chen, L.",
      "Johnson, K.A.",
      "Garcia, M."
    ],
    "abstract": "Few-shot learning methods struggle with distribution shift between support and query sets in realistic deployment scenarios. We propose a simple yet effective approach combining adaptive instance normalization (AdaIN) with meta-feature augmentation to improve robustness. Our method first applies channel-wise AdaIN to align support set features with the query set distribution during meta-testing, followed by a novel feature augmentation strategy that generates synthetic support examples by interpolating between learned class prototypes. We evaluate our approach on standard benchmarks including mini-ImageNet, tiered-ImageNet, and CUB-200-2011. Results show 2-4% improvements in accuracy over strong baselines like MAML and ProtoNets, with particularly gains under mild distribution shift (0.2-0.5 FID differences). However, performance degrades under severe distribution shift, and our method adds 15-20% computational overhead during meta-testing. While the specific contribution may appear incremental, our systematic analysis reveals that careful normalization choices can provide consistent benefits across diverse few-shot scenarios. Code will be released upon acceptance.",
    "id": 1483
  },
  {
    "title": "Gradient Descent with Memory-Efficient Lookahead: A Simple Approach to Stable Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Training large neural networks remains computationally expensive, particularly when using adaptive optimizers like Adam. We propose Memory-Efficient Lookahead (MEL), a modification to standard gradient descent that approximates the benefits of optimizer lookahead at reduced memory cost. MEL maintains two weight copies: the current parameters and a 'fast' buffer moved N steps ahead using standard SGD, then periodically anchors the slow weights to the fast buffer. A simple exponential smoothing decays the buffer after each anchor, improving stability. We theoretically show that MEL converges at the same rate as vanilla SGD for smooth convex objectives, while providing implicit regularization benefits similar to lookahead optimizers. Experiments on CIFAR-10, ImageNet, and WikiText-103 demonstrate 0.5-1.2% accuracy/rouge improvements over SGD with momentum while using 40% less memory than Lookahead optimizer. However, gains diminish on larger batch sizes and highly tuned baselines. Our method achieves these results with only 6 lines of code changes, making it easy to integrate into existing training pipelines.",
    "id": 1484
  },
  {
    "title": "Gradient Surgery Doesn't Always Help: An Empirical Study of Multi-Task Optimization in Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "Multi-task learning in transformers often uses gradient surgery techniques like PCGrad and GradNorm to handle conflicting gradients between tasks. We conduct a large-scale empirical study across 15 NLP benchmarks and 8 vision-language tasks, examining when these methods actually improve performance. Our experiments reveal that gradient surgery provides modest gains (0.8-1.2% average improvement) only in specific regimes: when tasks have high gradient cosine similarity (>0.6) and balanced loss scales. Surprisingly, in 34% of configurations, gradient surgery actually hurts performance by up to 3.5% compared to vanilla multi-task training. We show these failures correlate with tasks exhibiting phase transitions in their loss landscapes. While we provide a simple heuristic for predicting when gradient surgery will help based on gradient statistics collected during early training, our theoretical analysis remains incomplete. Our results suggest practitioners should be cautious about applying gradient surgery as a default, challenging common assumptions in the multi-task optimization literature.",
    "id": 1485
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Dynamic Subspace Projection",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a dominant approach for parameter-efficient fine-tuning of large language models, but its fixed-rank structure may underutilize model capacity. We propose LoRA-Lite, a simple extension that dynamically adjusts the rank during fine-tuning through iterative subspace projection. Our method begins with an extremely low-rank decomposition (rank 1-2) and gradually increases the rank only for layers showing high gradient coherence with the current subspace. This approach achieves 25-40% reduction in memory usage compared to standard LoRA on LLaMA-7B fine-tuning, while maintaining comparable performance across GLUE and SuperGLUE benchmarks (within 0.8% average accuracy). However, we observe that LoRA-Lite's benefits diminish on larger models (30B+ parameters), where the dynamic rank allocation introduces minimal gains. Our theoretical analysis provides convergence guarantees under mild assumptions but yields bounds that are looser than prior work. While LoRA-Lite offers practical speedups for practitioners with memory constraints, its primary contribution is incremental rather than foundational, particularly relevant for resource-limited fine-tuning scenarios.",
    "id": 1486
  },
  {
    "title": "Gradient Descent with Momentum Works Even When Momentum is Negative",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We investigate the behavior of gradient descent when the momentum parameter is set to negative values. While negative momentum is known to be unstable in convex settings, we show that in overparameterized neural networks, negative momentum can provide measurable improvements in training speed compared to standard (positive) momentum. Our theoretical analysis focuses on quadratic loss functions, where we prove that negative momentum can escape saddle points faster by approximately 30% under certain curvature conditions. We validate these findings empirically on small-scale vision and language models, achieving 2-5% faster convergence on CIFAR-10 and a subset of GLUE tasks. However, the benefits diminish as model size increases, and we observe instabilities with batch normalization layers. Our results suggest that negative momentum represents a minor but reliable optimization trick for specific architectures, though it does not fundamentally change the optimization landscape. Code is available at: [github.com/negative-momentum/icml2024]",
    "id": 1487
  },
  {
    "title": "LoRA^2: Recursive Low-Rank Adaptation for Multi-Task Learning with Limited Parameters",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models, but its effectiveness diminishes when adapting to multiple tasks simultaneously. We propose LoRA\u00b2, a recursive application of low-rank decomposition that hierarchically decomposes adaptation weights across tasks. Our method applies low-rank adaptation to the low-rank matrices themselves, creating a nested structure that enables more efficient parameter sharing. We evaluate LoRA\u00b2 on 8 diverse NLP tasks using the Pythia-6.9B model, achieving comparable performance to full fine-tuning while using only 0.8% of base model parameters. Specifically, LoRA\u00b2 improves average accuracy by 2.3% over standard LoRA when jointly trained on 4 tasks, though we observe diminishing returns beyond this. Our analysis reveals that the recursive structure primarily benefits tasks with similar characteristics, while potentially harming performance on dissimilar tasks. Despite promising results on intermediate model sizes, the computational overhead and implementation complexity may limit practical adoption. Code will be made available upon acceptance.",
    "id": 1488
  },
  {
    "title": "LoRA-Miner: Efficient Task-Adaptive Subnetworks via Low-Rank Lottery Tickets",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M.",
      "Singh, P."
    ],
    "abstract": "We propose LoRA-Miner, a simple method for identifying task-specific subnetworks within large pre-trained models using low-rank adaptation. Building on the lottery ticket hypothesis, our approach trains low-rank matrices to identify sparse subnetworks that achieve comparable performance to full fine-tuning while using 40% fewer parameters. Our method combines iterative magnitude pruning with low-rank factorization, automatically discovering which layers can be approximated by low-rank structures versus those requiring full weights. Experiments on GLUE and SuperGLUE benchmarks show LoRA-Miner achieves 96-98% of full fine-tuning performance across classification tasks, with 2.3\u00d7 compression over standard LoRA while maintaining interpretable sparsity patterns. However, we find the method's effectiveness diminishes on generative tasks (e.g., summarization) and larger models (\u22657B parameters), where identified subnetworks converge to dense patterns. Analysis reveals our lottery tickets primarily emerge in middle Transformer layers, suggesting architectural biases in sparsity. While not universally effective, LoRA-Miner provides a practical compromise between parameter efficiency and task performance for moderate-scale applications.",
    "id": 1489
  },
  {
    "title": "LoRA-Pro: A Slightly Better Way to Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, Y.",
      "Kumar, S.",
      "Johnson, L."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models, but its theoretical properties remain poorly understood. We present LoRA-Pro, a simple modification that adds a learnable scaling parameter to the low-rank decomposition and incorporates a novel initialization scheme based on singular value analysis. While the improvement appears modest\u20140.8-1.2% average gains on GLUE benchmarks compared to standard LoRA\u2014the modification requires no additional hyperparameter tuning and adds minimal computational overhead. We provide a theoretical analysis showing that LoRA-Pro converges faster under certain assumptions about the eigenspectrum of the downstream task gradients, though these assumptions may be difficult to verify in practice. Experiments on 7B and 13B parameter models across 12 diverse tasks demonstrate consistent but small improvements over baselines. Ablation studies suggest the initialization scheme accounts for approximately 60% of the observed gains. Our PyTorch implementation requires only 12 lines of code to modify existing LoRA implementations. While the contribution is incremental, LoRA-Pro offers practitioners a drop-in replacement that yields consistent improvements with no additional cost.",
    "id": 1490
  },
  {
    "title": "Gradient Surgery Doesn't Always Scale: On the Limitations of Gradient Conflict Resolution in Federated Learning with Heterogeneous Data",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient-based conflict resolution methods, particularly gradient surgery techniques, have emerged as popular solutions for handling client drift in federated learning with non-IID data. While these methods demonstrate effectiveness in small-scale settings with carefully curated benchmarks, their behavior under realistic federated deployment conditions remains poorly understood. We empirically investigate the scaling properties of three state-of-the-art gradient surgery methods across a range of federated settings with increasing client heterogeneity and communication constraints. Our experiments reveal that while these methods maintain strong performance on existing benchmarks, they exhibit unexpected degradation when deployed on more realistic data distributions, with accuracy drops of up to 15% compared to simple baselines. We identify that gradient surgery techniques suffer from accumulating approximation errors when local updates become increasingly dissimilar, leading to suboptimal global updates. Our analysis suggests that the current generation of gradient conflict resolution methods may be fundamentally limited by their local-global update paradigm. We provide an open-source benchmark suite to reproduce our findings and facilitate future work in this direction.",
    "id": 1491
  },
  {
    "title": "Gradient Surgery for Transformer Attention: A Memory-Efficient Approach via Low-Rank Factorization",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Self-attention mechanisms in transformers suffer from quadratic memory complexity with sequence length, limiting applications to long contexts. While numerous efficient attention variants have been proposed, most sacrifice model quality or require specialized hardware. We propose Gradient Surgery for Attention (GSA), a training-time method that reduces memory usage by factorizing attention gradients into low-rank components. Our approach applies singular value decomposition to the gradient flow during backpropagation, retaining only the top-k singular values and vectors. This yields O(nk) memory complexity where k << n for sequence length n. We evaluate GSA on language modeling benchmarks including WikiText-103 and OpenWebText, achieving 1.4\u00d7 memory reduction compared to standard attention with <1% perplexity degradation. However, we observe training instability in certain configurations, particularly when k < 32. The method shows promise for modest sequence lengths (2K-4K tokens) but exhibits diminishing returns beyond 8K tokens due to gradient approximation errors. While our empirical results are encouraging, we acknowledge theoretical limitations: our analysis assumes gradient independence that may not hold in practice, and our convergence guarantees require bounded gradient noise. Code and pretrained models are available at [url].",
    "id": 1492
  },
  {
    "title": "Gradient Amplification Makes Transformers Work Better, Sometimes",
    "authors": [
      "Liang, J.",
      "Okafor, K.",
      "Mikhailov, D."
    ],
    "abstract": "Transformer architectures suffer from gradient vanishing in their self-attention layers, particularly when processing long sequences. We propose Gradient Amplification via Attention Restoration (GAAR), a simple technique that selectively scales attention gradients during backpropagation to improve training dynamics. Our method operates by multiplying gradient flows through attention matrices by a learned factor \u03b1, which is computed from a lightweight gating network conditioned on layer statistics. Experiments on Wikitext-103 language modeling show GAAR reduces validation perplexity by 8.2% compared to standard training, with consistent gains across model sizes (125M-3B parameters). However, our analysis reveals GAAR's effectiveness is highly sensitive to initialization seeds and appears to primarily help models that are already underperforming baseline perplexities. On downstream tasks like GLUE and SuperGLUE, improvements are limited to a subset of datasets (4/9 showing statistically significant gains). We further investigate potential failure modes, finding GAAR can destabilize training when \u03b1>5, leading to diverging loss curves in 12% of runs. While our theoretical analysis provides some insights into why gradient scaling helps attention mechanisms, our derivations rely on simplifying assumptions that may not hold in practice. Code and checkpoints are available with minor modifications needed to reproduce the exact initialization conditions.",
    "id": 1493
  },
  {
    "title": "Variance-Reduced Bandits: A Practical Approach to Exploration in Large-Scale Systems",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We present a simple variance-reduction technique for stochastic multi-armed bandits that achieves near-optimal regret bounds while maintaining computational efficiency comparable to \u03b5-greedy methods. Our approach combines adaptive exploration bonuses with a control variate estimator, yielding theoretical improvements over UCB for sub-Gaussian rewards. On standard benchmarks including MovieLens and Ads CTR prediction datasets, our method improves cumulative regret by 12-18% over strong baselines with negligible computational overhead. While our contribution is primarily algorithmic rather than conceptual, we provide the first empirical evaluation of this technique at web-scale, demonstrating practical deployment in a production recommendation system serving 100M+ users. Analytical results are limited to simplified i.i.d. settings that exclude contextual information, and our regret bounds contain logarithmic factors that may not be tight. We release code and an accompanying small-scale experimental pipeline to facilitate reproduction.",
    "id": 1494
  },
  {
    "title": "Practical Few-Shot Learning via Iterative Data Augmentation with Diffusion Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Few-shot learning remains challenging due to the limited diversity of training examples. While data augmentation can help, traditional transformations often provide marginal gains. We propose a novel approach that leverages pre-trained diffusion models to iteratively generate synthetic examples that complement the original data. Our method alternates between training the classifier and updating the diffusion model to focus on regions where the classifier is uncertain. On five standard few-shot benchmarks, we achieve 3-5% absolute improvements over strong baselines with minimal computational overhead. However, we observe that gains diminish as the base dataset size increases beyond 20 examples per class, and performance varies significantly across datasets. Our extensive ablations reveal that the quality of the pre-trained diffusion model is critical\u2014using lower-quality models can hurt performance. While our approach provides a practical way to boost few-shot performance without architectural changes, the reliance on pre-trained diffusion models may limit applicability in domains where such models are unavailable. Code and augmented datasets will be released upon acceptance.",
    "id": 1495
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Making Gradient Interference Work in Multi-Task Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task reinforcement learning faces a fundamental challenge: gradient interference between tasks leads to destructive updates and degraded performance. While recent gradient surgery methods like PCGrad mitigate this, we find they discard up to 40% of gradient information, limiting sample efficiency. We propose Gradient Surgery with Adaptive Memory (GSAM), a method that retains conflicting gradients in a learned memory bank and reincorporates them when the gradient direction changes. Our approach uses a lightweight attention mechanism to select relevant stored gradients based on task similarity. We evaluate GSAM on Meta-World and ContinualWorld benchmarks with 10-50 tasks. Our method achieves 7-12% higher success rates than PCGrad while using 50% fewer environment steps. However, our theoretical analysis reveals GSAM introduces a bounded bias that may affect convergence in adversarial environments. Ablations show the memory bank size strongly influences performance, and naive extensions to larger networks increase compute overhead substantially. While GSAM provides practical improvements over existing methods, its benefits diminish with expert demonstrations and when tasks have vastly different reward scales.",
    "id": 1496
  },
  {
    "title": "Learning with Cross-Domain Label Structure via Regularized Partial Optimal Transport",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We propose a framework for semi-supervised learning that leverages structural relationships between labels across different domains through partial optimal transport. Our method couples the label distributions of source and target domains while accounting for varying label granularities, regularizing the transport plan via an entropy-based term that encourages sparse mappings. The resulting optimization problem is solved efficiently using a modified Sinkhorn algorithm with early stopping. We evaluate our approach on text classification and image recognition tasks, achieving 2-3% improvements over baselines on 3 out of 7 datasets when labels are scarce. However, performance degrades when domain shifts are minimal or when label structures are poorly aligned. Our contributions include: (1) a novel objective that integrates partial optimal transport with structural label constraints, (2) convergence guarantees for the proposed algorithm, and (3) empirical validation on cross-domain tasks. Code is available at anonymous.url.",
    "id": 1497
  },
  {
    "title": "Gradient Surgery on Attention Scores: Improving Transformer Training Stability with Minimal Changes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer training instability remains a significant challenge, particularly when scaling to larger models. While numerous techniques have been proposed to stabilize training, many introduce substantial computational overhead or model modifications. We present a simple gradient-based intervention that operates directly on attention scores during training. Our approach identifies and surgically removes gradient components that contribute to attention entropy collapse, using a lightweight projection technique that requires no architectural changes. We evaluate our method on language modeling and image classification tasks across various model sizes (125M-1.5B parameters). Results show modest but consistent improvements in training stability, reducing catastrophic failure rates by 23% on average without impacting convergence speed. However, the method shows diminishing returns beyond 750M parameters and offers limited benefits on tasks with short input sequences. While our approach provides a theoretically motivated solution to a well-defined problem, its practical impact relative to simpler baselines remains questionable in many settings. Code will be released upon acceptance.",
    "id": 1498
  },
  {
    "title": "Mixture of Projections: Lightweight Experts via Random Feature Sharing",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H.",
      "Johnson, S."
    ],
    "abstract": "Scaling mixture-of-experts (MoE) models faces fundamental memory bottlenecks as expert count grows. We propose Mixture of Projections (MoP), a parameter-efficient alternative that replaces full expert networks with lightweight random projections. Each expert learns to apply a small set of task-specific linear transformations over shared random features drawn from a frozen Gaussian distribution. Our theoretical analysis shows that these shared bases preserve expressivity while reducing parameters by 95% compared to standard MoE. Empirically, MoP achieves comparable performance to vanilla MoE on language modeling (0.8 perplexity gap) and vision tasks (0.7% accuracy loss on ImageNet) with 4x fewer parameters. While our method reduces training memory, we observe increased sensitivity to initialization and slower convergence during early training. Surprisingly, we find that learned projections often cluster into redundant patterns, suggesting potential over-parameterization despite efficiency gains. These trade-offs make MoP particularly attractive for resource-constrained deployment, though we acknowledge limitations in extremely low-parameter regimes where random features may underfit.",
    "id": 1499
  },
  {
    "title": "Gradient Alignment Improves Few-Shot Domain Adaptation via Meta-Learned Feature Spaces",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Alignment Meta-Learning (GAML), a simple regularization technique for few-shot domain adaptation that encourages better alignment between gradient directions from source and target domains. While meta-learning has shown promise in few-shot settings, existing approaches typically assume similar feature distributions across domains\u2014a assumption that often fails in practice. GAML adds an auxiliary objective that maximizes cosine similarity between gradients computed on source and target data during meta-training. Our experiments on 4 benchmark datasets show 2-5% improvements over MAML and ProtoNets in standard 5-way 5-shot scenarios, with more consistent gains when source-target discrepancy increases. However, we observe diminishing returns as shot count increases, and our method performs comparably to simple fine-tuning baselines when domain shift is small. We provide theoretical analysis showing GAML optimizes an upper bound on target domain loss under certain smoothness assumptions, though we acknowledge these assumptions may not hold for highly non-stationary distributions. The method requires minimal additional compute (~15% overhead) and can be integrated into existing meta-learning pipelines. Code is available at anonymous-url.",
    "id": 1500
  },
  {
    "title": "LoRA-ICL: Improving In-Context Learning via Low-Rank Adaptation of Language Model Representations",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "In-context learning (ICL) enables large language models to adapt to new tasks with just a few demonstrations, but the underlying mechanisms remain poorly understood. We propose LoRA-ICL, a method that applies low-rank adaptation to intermediate representations within transformer layers during ICL. Our approach identifies task-relevant subspaces by computing gradients on demonstration examples and then applying low-rank updates to steer model outputs. We evaluate LoRA-ICL on 12 classification and generation tasks across different model scales (7B-70B parameters). Results show modest improvements of 2-4% over standard ICL on 8/12 tasks when demonstrations are scarce (k \u2264 4), though gains diminish with more examples. Analysis reveals that learned adaptations primarily affect attention patterns in middle layers, suggesting ICL operates through selective feature amplification rather than fundamental task learning. While our method provides minimal computational overhead (<0.1% parameter increase), performance gains may not justify implementation complexity in practice. Code and experiments are reproducible, though results show high variance across prompt formats and model checkpoints.",
    "id": 1501
  },
  {
    "title": "Accelerated Gradient Descent with Cyclical Step-Size Scheduling: A Simple Extension with Marginal Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez-Diaz, A.",
      "Kim, S."
    ],
    "abstract": "We present a simple modification to accelerated gradient descent that employs cyclical step-size scheduling, inspired by recent work on cyclical learning rates in neural network training. Our method alternates between two predetermined step-sizes in a fixed pattern, requiring no additional hyperparameter tuning beyond standard accelerated methods. We theoretically analyze convergence rates for smooth convex objectives, proving an O(1/T^1.12) convergence rate \u2014 a modest improvement over the standard O(1/T) for gradient descent but matching Nesterov acceleration. On a standard set of logistic regression and matrix completion benchmarks, we achieve 2-8% reductions in iteration counts compared to standard accelerated methods, though these improvements diminish as problem conditioning worsens. While the theoretical novelty is limited to an elementary extension of existing Lyapunov analysis, we believe the simplicity and occasional practical benefits warrant consideration. Code is provided for reproducibility, though extensive hyperparameter sensitivity analysis suggests the gains are inconsistent across problem instances. This work sits at the intersection of optimization theory and practical machine learning, offering a potentially useful but not revolutionary contribution.",
    "id": 1502
  },
  {
    "title": "Gradient Noise Injection as Implicit Regularization in Overparameterized Models",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Training large neural networks with stochastic noise has been empirically observed to improve generalization, yet theoretical understanding remains limited. We show that adding carefully scaled Gaussian noise to gradients during training acts as an implicit regularizer equivalent to Tikhonov regularization on the network's Jacobian. Our analysis reveals that the effective regularization strength depends not just on the noise variance, but on the interaction between noise scale and batch size in non-intuitive ways. Experiments on CIFAR-10 and ImageNet show consistent improvements of 2-5% over standard SGD with momentum when hyperparameters are tuned for each architecture, though we find the benefit vanishes under certain learning rate schedules. While our theory applies to linear networks, we provide empirical evidence suggesting similar behavior in non-linear models, though the gap between theory and practice remains significant. Our results suggest gradient noise injection could serve as a lightweight alternative to explicit regularization techniques, though its effectiveness depends heavily on problem-specific hyperparameter tuning.",
    "id": 1503
  },
  {
    "title": "Variance-Reduced Temporal Difference Learning with Adaptive Step Sizes for Off-Policy Evaluation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We present an improved temporal difference (TD) learning algorithm for off-policy evaluation that combines variance reduction techniques with adaptive step size selection. While existing variance-reduced TD methods like VRTD achieve faster convergence in theory, their practical performance often degrades due to sensitivity to step size choices and poor empirical variance estimates. Our method introduces an adaptive scheme that adjusts step sizes based on a running estimate of the stochastic variance, eliminating the need for manual tuning. We provide convergence guarantees showing O(1/T) rate with potentially better constants than vanilla TD, though our analysis requires stronger assumptions on the behavior policy. Experiments on standard RL benchmarks show 10-30% improvement in mean squared error over TD and VRTD when hyperparameters are well-tuned, but the gains diminish under model misspecification. The method is particularly effective in low-data regimes, though computational overhead increases by roughly 2x due to variance estimation. Our main contribution is practical stabilization of variance-reduced TD rather than novel theoretical insights.",
    "id": 1504
  },
  {
    "title": "Residual Layer Path Analysis: A Frequency Domain Perspective on Skip Connections",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate the role of skip connections in deep neural networks through the lens of frequency domain analysis. By decomposing the forward pass using Fourier transforms, we demonstrate that residual connections selectively amplify specific frequency bands during training. Our theoretical analysis shows that skip connections create an implicit low-pass filter that prevents high-frequency noise from dominating gradient flow. We propose a simple modification to ResNet blocks that introduces learnable frequency-dependent scaling factors, allowing dynamic adjustment of the filter characteristics. Experiments on CIFAR-10 and ImageNet show modest improvements over baseline ResNets (0.3-0.5% accuracy gain), with the most pronounced benefits observed on noisy datasets. However, our method incurs 15-20% additional computation cost and shows diminishing returns as network depth increases beyond 50 layers. While our frequency-domain perspective provides interesting insights into skip connection behavior, the practical benefits are limited outside specific noisy data regimes. Code and pre-trained models are available at anonymous-url.",
    "id": 1505
  },
  {
    "title": "Revisiting Adam with Layer-wise Learning Rates via Gradient Flow Analysis",
    "authors": [
      "Chen, J.",
      "Bhattacharya, S.",
      "Liu, K."
    ],
    "abstract": "Adaptive optimization methods like Adam have become standard for training deep neural networks, yet their theoretical understanding remains incomplete. We observe that standard Adam exhibits non-uniform gradient norms across layers, leading to inconsistent update magnitudes. Building on recent work connecting optimization algorithms to continuous-time dynamical systems, we propose LayerFlow-Adam, a modified Adam variant that applies layer-specific learning rates derived from analyzing the gradient flow of each layer independently. Our method computes adaptive rates based on the Frobenius norm of layer-wise Jacobians, requiring minimal additional computational overhead. Experiments on ResNet architectures for CIFAR-10 and ImageNet show modest improvements (0.3-0.8% accuracy gains) over standard Adam, particularly in low-data regimes. While the theoretical analysis provides intuition for layer-wise adaptation, our convergence guarantees are limited to simplified settings. The method shows sensitivity to hyperparameters and yields similar performance to AdamW with careful tuning. Nevertheless, LayerFlow-Adam offers a transparent approach to addressing layer-wise update imbalance without introducing additional optimizer states, making it a lightweight alternative for practitioners facing training instabilities.",
    "id": 1506
  },
  {
    "title": "Improved Gradient Estimation for Meta-Learning with Hessian-Free Approximations",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Agrawal, P."
    ],
    "abstract": "Meta-learning methods often rely on accurate gradient estimates of the validation loss with respect to hyperparameters, but computing these via implicit differentiation requires expensive Hessian operations. We propose an approximate gradient estimator that uses a Neumann series truncated after just 3 iterations combined with random projection to reduce computational cost from O(d\u00b3) to O(d\u00b2) per task. Our method introduces a novel damping scheme based on validation loss curvature that automatically adjusts the approximation quality during training. Experiments on few-shot classification benchmarks (Mini-ImageNet, CIFAR-FS) show that our approach achieves comparable accuracy to exact implicit differentiation while reducing training time by 40-60%. On the tiered-ImageNet benchmark, we obtain 72.1% accuracy compared to 73.2% with exact gradients. However, we find the method is sensitive to the projection dimension and performs poorly when tasks have high intrinsic dimensionality. While our approximation introduces theoretical bias, empirical results suggest this trade-off between accuracy and efficiency may be acceptable for many practical applications. Code is available at [URL omitted for review].",
    "id": 1507
  },
  {
    "title": "Improving Transformer Efficiency Through Adaptive Attention Sparsification During Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "We present a method for improving the computational efficiency of transformer models by learning sparse attention patterns during training. While prior work has focused on post-training pruning or pre-defined sparsity masks, our approach learns to predict which attention heads and positions are unnecessary for each input sequence using a lightweight gating network. Specifically, we augment each multi-head attention layer with trainable binary gates that determine whether to compute full attention or use a low-rank approximation. The gating network is trained with an auxiliary loss that balances predictive accuracy against computational cost. On language modeling tasks (WikiText-103 and OpenWebText), our method reduces FLOPs by 25-40% while maintaining within 0.8 perplexity points of the dense baseline. However, we observe that these gains diminish on code generation tasks or when combining with existing optimization techniques like mixed-precision training and gradient checkpointing. Our experiments are conducted on models up to 1.3B parameters, and we provide implementation details and hyperparameters for reproducibility.",
    "id": 1508
  },
  {
    "title": "Learning to Weight: Efficient Gradient Descent through Adaptive Sample Weighting",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose an adaptive reweighting scheme for gradient-based optimization that dynamically adjusts per-sample contributions during training. Our method, called Learned Sample Weighting (LSW), trains a lightweight meta-network to predict sample weights based on gradient information, theoretically justifying the approach through a PAC-Bayesian bound on generalization error. We evaluate LSW across vision and NLP benchmarks, observing 2-5% accuracy improvements over standard SGD on CIFAR-10 and moderate gains on text classification tasks. However, experiments reveal consistent computational overhead of 20-30% and diminished benefits on larger datasets like ImageNet. While our best results match or slightly exceed recent methods like SAM and sharpness-aware training, the improvements are task-dependent and sometimes within statistical noise. Our analysis suggests LSW may provide most benefit for small-to-medium datasets with noisy labels, though the computational cost may limit practical adoption. Code will be released upon acceptance.",
    "id": 1509
  }
]