[
  {
    "title": "A Margin-Based Perspective on Gradient Descent with Label Noise",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "Jones, M."
    ],
    "abstract": "Label noise is ubiquitous in large-scale datasets, yet its effect on the implicit bias of gradient-based optimization remains poorly understood. We study how label noise interacts with the margin dynamics of gradient descent on linearly separable data. By characterizing the limiting distribution of parameter iterates under symmetric label-noise perturbations, we show that the noise magnitude effectively controls the trade-off between margin maximization and memorization of corrupted labels. Our analysis reveals a phase-transition phenomenon: for noise levels below an explicit dataset-dependent threshold, gradient descent remains robust and converges to the max-margin classifier; above the threshold, the algorithm fits noise and generalization degrades. We validate our theoretical predictions on both synthetic and small-scale benchmark datasets, observing qualitative agreement between empirical margins and our closed-form bounds. While our results are currently limited to linear models under simplified noise models, we believe they offer a useful conceptual lens for understanding when and how label noise can be tolerated during training. Experiments on modern CNNs show similar margin-noise trade-offs, suggesting potential broader applicability.",
    "id": 1
  },
  {
    "title": "Improved Gradient Bounds for Stochastic Optimization via Iterated Logarithm Averaging",
    "authors": [
      "Chen, B.",
      "Kumar, V.",
      "Rodriguez, S."
    ],
    "abstract": "We propose a variant of stochastic gradient descent that incorporates an iterated logarithm weighting scheme to improve convergence guarantees for non-convex optimization. While standard SGD achieves optimal O(1/t) rates for convex objectives, our method achieves a logarithmic factor improvement for functions satisfying the Polyak-\u0141ojasiewicz inequality. The key insight is to weight recent gradients more heavily using a carefully constructed sequence that depends on both the iteration count and the empirical variance of gradients. On several benchmark tasks including CIFAR-10 classification and matrix factorization, our method shows modest improvements (2-3%) over standard baselines. Theoretical analysis establishes convergence rates within a logarithmic factor of known lower bounds. While the practical gains are limited to specific problem settings, our approach provides new theoretical insights into the role of adaptive weighting in stochastic optimization.",
    "id": 5
  },
  {
    "title": "Revisiting Gradient Clipping in Private Optimization with Layer-wise Clipping Norms",
    "authors": [
      "Krishnan, V.",
      "Liu, S.",
      "Ortega, J."
    ],
    "abstract": "Gradient clipping has become a standard ingredient in differentially private optimization, yet its interaction with adaptive optimizers and parameter groups remains understudied. We propose Layer-Adaptive Private Optimization (LAPO), a lightweight modification that sets the clipping threshold for each layer independently based on an exponential moving average of past gradient norms. On CIFAR-10, ImageNet, and Penn Treebank language modeling tasks, LAPO improves accuracy by 0.2\u20131.1 pp relative to constant-threshold DP-SGD at the same privacy budget, while maintaining the same theoretical privacy complexity. We give a near\u2013dimension-independent excess risk bound that depends linearly on the maximum layer width rather than the full parameter count, yielding tighter guarantees when layers are unbalanced. Ablation shows that 70 % of the gain comes from the first two epochs, suggesting fast threshold adaptation is key. While our theory assumes Lipschitz losses and bounded inputs, empirical results hold for transformer architectures. Code is provided for reproducibility.",
    "id": 12
  },
  {
    "title": "Revisiting Regularization Paths with Adaptive Early Stopping",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Liu, T."
    ],
    "abstract": "We study the interplay between regularization and early stopping in gradient-based optimization, proposing a simple heuristic that adapts the stopping epoch to the regularization strength. Motivated by empirical observations on over-parameterized linear models, we derive approximate bounds suggesting an optimal stopping schedule that scales smoothly with the L2 penalty coefficient. Across a suite of small-scale vision and NLP benchmarks, adaptive early stopping yields test accuracy within 0.2% of exhaustive grid search while requiring roughly half the training time. Although the theoretical guarantees rely on restrictive assumptions (realizability, isotropic features) that rarely hold in practice, the method remains competitive with more sophisticated hyper-gradient approaches at a fraction of the implementation cost. We release a lightweight PyTorch wrapper to encourage adoption. Our findings highlight the continued practical relevance of simple regularization heuristics, but they also underscore the gap between linearized theory and deep-network behavior; future work includes extending the analysis to stochastic settings and more complex regularizers.",
    "id": 19
  },
  {
    "title": "Gradient Boosting with Adaptive Sampling for Large-Scale Weakly Supervised Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "Weak supervision offers an economical route to large labeled datasets, but existing gradient-boosting implementations treat label noise as uniform and consequently suffer from accumulated errors across successive boosting rounds. We introduce AdaBoost-WS, a gradient-boosting variant that dynamically re-weights instances according to estimated label reliability rather than the traditional loss-based focus. AdaBoost-WS interleaves each boosting iteration with a small validation sweep that approximates per-example reliability via agreement between lightweight auxiliary models, eliminating the need for a separate cleaned subset. Across three public weakly supervised tabular benchmarks totaling 2.4M noisy instances, AdaBoost-WS improves test AUC by 2.1\u20133.4% over CatBoost and XGBoost while using 30% fewer trees, and scales linearly on 16-core Spark clusters. Ablations show that removing either the adaptive sampling or the auxiliary agreement module degrades performance to within 0.5% of baselines, suggesting both components contribute modestly. While effective on noisy tabular data, preliminary ImageNet experiments with gradient-boosted CNN features yield only marginal gains, highlighting limits to high-dimensional regimes. Theoretically we provide a loose generalization bound that assumes bounded noise but relies on a restrictive data-purity condition unlikely to hold in practice. Code and scripts are publicly available.",
    "id": 20
  },
  {
    "title": "Revisiting Weight Averaging for Better Flat-Minima Selection",
    "authors": [
      "Nguyen, T.",
      "Klein, J.",
      "Singh, V."
    ],
    "abstract": "Recent empirical work suggests that wide, flat minima generalize better than sharp ones, prompting interest in optimizers that bias trajectories toward flat regions. We revisit the classical idea of exponentially smoothing the iterate (EMA) and uncover a surprising bias: when the damping coefficient is tuned just below the divergent limit, successive averages accumulate along an approximate eigenvector of the Hessian at the optimum, yielding a flat-minimum solution without modifying the underlying optimizer. Building on this, we propose Flat-Seeking Iterate Averaging (FSIA), a post-hoc wrapper that performs only three extra full-backward passes at the end of training to choose the flattest among the averaged candidates. On CIFAR-10/100 and ImageNet with ResNet-18/50, FSIA lowers the sharpness by 15\u201322% and improves top-1 accuracy by 0.4\u20130.7pp over vanilla SGD momentum baselines, while incurring <0.01% overhead. Theoretical analysis in a quadratic toy model characterizes when the bias emerges, but the story is less complete for deep nets. Code, code-anonymized, and checkpoints are provided.",
    "id": 28
  },
  {
    "title": "Residual Temperature Scaling: Post-Hoc Confidence Calibration for Black-Box Neural Networks",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "Well-calibrated predictive are essential for trustworthy deployment, yet modern deep networks tend to output over-confident probabilities. We revisit temperature scaling\u2014the simplest post-hoc calibration rule\u2014and reveal a systemic under-correction when the network\u2019s own embeddings drift from the target domain. Motivated by this observation, we propose Residual Temperature Scaling (RTS), a lightweight wrapper that refits a scalar temperature on held-out data, then adds a small residual correction obtained from a linear model on penultimate-layer features. On standard image and text benchmarks, RTS yields average ECE reductions of 5.3% over plain temperature scaling with no extra cost at test time, and requires less than 0.6% additional parameters. We accompany RTS with a finite-sample generalization bound that predicts calibration error within \u00b10.6% on CIFAR-100. Code and trained models are released to facilitate reproduction. While empirical gains are incremental, our analysis sheds new light on the role of feature drift in calibration and offers practitioners a drop-in upgrade that is compatible with existing checkpoints.",
    "id": 32
  },
  {
    "title": "Accelerating Transformer Training with Progressive Gradient Dropout",
    "authors": [
      "Liu, J.",
      "Banerjee, S.",
      "Nguyen, T."
    ],
    "abstract": "Large transformer models require extensive compute budgets, motivating methods that speed training without hurting final accuracy. We introduce Progressive Gradient Dropout (PGD), a simple modification to standard stochastic gradient optimizers that probabilistically drops gradient coordinates during back-propagation. PGD gradually increases the dropout rate according to a scheduled cooling function, letting early updates use dense signals while later updates focus on salient directions. Across 6 GLUE tasks, PGD speeds pre-training by 9\u201314% (wall-clock) and downstream fine-tuning by 6\u201310% relative to AdamW with no hyper-parameter retuning. Ablation experiments show the schedule shape matters more than the maximal rate. Theoretical analysis in a stylized quadratic setting shows PGD implicitly performs coordinate-wise adaptive regularization, offering partial explanation for its empirical stability. While gains are consistent, they remain modest on larger architectures (\u22651.1B parameters) and do not transfer to black-box optimizers such as LAMB. Code and configurations are released to ensure reproducibility.",
    "id": 39
  },
  {
    "title": "Revisiting Reset Schedules for Policy Optimization: When Warm Starts Meet Periodic Restarts",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Policy restart strategies are commonly used in deep reinforcement learning to escape local optima, yet the theoretical underpinnings of when and how to restart remain poorly understood. We revisit the classic optimization idea of warm-started periodic restarts and adapt it to policy gradient methods. Our method, WR-PO, periodically resets the policy to a geometric mixture of the current iterate and an earlier checkpoint, with restart frequency and mixing coefficient chosen via a simple grid-search heuristic. On a suite of nine continuous-control tasks, WR-PO improves average return over the vanilla PPO baseline by 4.7% with similar sample complexity, and matches SAC on four of nine tasks. Ablation studies indicate that the benefit is largest in environments with sparse rewards, suggesting that controlled resets help exploration. While our approach is easy to implement and yields consistent gains, the improvements are incremental and the heuristic nature of the schedule limits generality. We provide partial convergence guarantees under strong convexity assumptions, but the general case remains open. Code and 50 random seeds are provided to ensure reproducibility.",
    "id": 48
  },
  {
    "title": "Improving Transformer Generalization with Curriculum-Based Positional Encoding",
    "authors": [
      "Liu, K.",
      "Nguyen, T.",
      "Kowalski, M."
    ],
    "abstract": "Positional encodings are crucial for Transformer architectures, yet their role in generalization remains poorly understood. We hypothesize that abrupt exposure to long sequences hampers extrapolation to longer contexts. To address this, we introduce Curriculum Positional Encoding (CPE), a simple schedule that progressively increases the maximum sequence length during training. CPE first trains on short subsequences with truncated absolute sinusoidal encodings, then gradually lengthens them until the target length is reached. Our experiments on language modeling, image completion, and symbolic music generation show that CPE yields perplexity reductions of 1\u20132 % over standard fixed encodings on sequences up to 4 k tokens, and improves zero-shot extrapolation by \u2248 5 % when tested on 8 k tokens. Ablations reveal that gradual length progression matters more than the specific encoding variant. While our improvements are modest and diminish on very long contexts (>16 k), CPE adds no inference cost and can be implemented in < 20 lines of code. We provide PyTorch code and trained checkpoints to support reproducibility.",
    "id": 54
  },
  {
    "title": "Gradient Amplification for Stabilizing GAN Training with Uneven Learning Rates",
    "authors": [
      "Garcia, M.",
      "Kumar, S.",
      "Thompson, L."
    ],
    "abstract": "We propose gradient amplification, a simple plug-in technique that re-weights generator and discriminator updates in GANs when the two networks are trained with different learning rates\u2014a common practical heuristic that often leads to unstable oscillations. Starting from a local bilinear game approximation, we derive a closed-form coefficient that amplifies generator gradients when the discriminator learns faster and shrinks them in the opposite regime. On CIFAR-10, gradient amplification improves Inception score from 6.18 \u00b1 0.12 to 6.43 \u00b1 0.07 without architectural changes, while on the more challenging 128\u00d7128 ImageNet subset it reduces training FID by 8.2%. Ab but only 56% of runs still diverge, suggesting that higher-order dynamics remain unaccounted for. The method introduces one extra hyper-parameter \u03b2; while \u03b2 = 1/2 works well in all experiments, we lack a principled automatic tuning rule. Theoretical analysis is limited to a two-parameter quadratic game and does not extend to general non-convex settings. Despite these limitations, gradient amplification can be implemented in 5 lines of code and integrates seamlessly with existing optimizers, offering practitioners a lightweight stabilizer for large-scale GAN training.",
    "id": 56
  },
  {
    "title": "Improved Sharpness-Aware Minimization with Momentum-Enhanced Gradient Averaging",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Momentum-Enhanced Sharpness-Aware Minimization (ME-SAM), a drop-in replacement for stochastic optimizers that encourages convergence to flat minima by periodically averaging stochastic gradients over small parameter neighborhoods. ME-SAM builds on Sharpness-Aware Minimization (SAM) but replaces its two-backward-pass strategy with an efficient momentum-based gradient buffer that reuses past gradients. We derive a convergence bound for \u00b5-nonconvex objectives that improves the leading constant of \u03a9(1/\u221a) dependence in prior work to \u00d5(1/\u03b5^2) under a bounded-intermediate-gradient assumption. Empirically, ME-SAM matches SAM\u2019s test accuracy on CIFAR-10/100 and ImageNet while cutting wall-clock training time by 14\u201322%. We also show that ME-SAM provides a 0.7\u20131.1% boost over vanilla SGD on small-scale text classification with BERT-Base, suggesting robustness to domain shift. Although the theoretical analysis hinges on a locally-Lipschitz Hessian restriction that may not hold universally, extensive ablations demonstrate consistent empirical gains across vision and NLP tasks. Code and checkpoints are provided for reproduction. Future work will explore adaptive neighborhood sizes and extension to federated settings.",
    "id": 60
  },
  {
    "title": "Rethinking Batch Normalization: A Gradient-Norm Perspective for Improved Optimization",
    "authors": [
      "Kumar, A.",
      "Jiang, S.",
      "Bennett, K."
    ],
    "abstract": "Batch Normalization (BN) has become a staple in deep learning, yet its precise mechanism for accelerating optimization remains debated. We propose a new perspective that explains BN's benefits through gradient norm equalization across layers. By analyzing the spectral properties of the Jacobian, we show that BN implicitly balances gradient magnitudes without requiring careful initialization. We introduce Layer-Adaptive Batch Normalization (LABN), a lightweight modification that adapts the normalization strength based on gradient statistics. On CIFAR-100 and ImageNet, LABN achieves modest improvements of 0.3-0.7% over standard BN while reducing training time by 5-10%. However, the gains diminish on very deep architectures like ResNet-152, suggesting our method works best for moderately deep networks. We also explore LABN's interaction with different optimizers, finding it particularly effective with SGD but showing limited benefits with Adam. While our theoretical analysis provides new insights into BN's role in optimization, it relies on simplifying assumptions that may not hold in practice. Code and pretrained models are available.",
    "id": 63
  },
  {
    "title": "Improving Few-Shot Learning with Class-Aware Mixup and Adaptive Margin Loss",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "M\u00fcller, H."
    ],
    "abstract": "Few-shot classifiers struggle when base and novel classes have overlapping feature distributions. We study this problem in the 5-way, 5-shot mini-ImageNet setting and observe that cross-entropy with standard data augmentation can assign nearly identical scores to visually similar classes, leading to frequent confusions. Motivated by this observation, we propose CAMA, a lightweight training recipe that couples Class-Aware MixUp with an Adaptive Margin loss. During meta-training, CAMA interpolates features only between classes whose prototypes are closer than a learned threshold, while the margin term dynamically expands the decision boundary of each class proportionally to its observed confusion rate. On mini-ImageNet, CAMA improves the 1-shot accuracy of a ResNet-12 backbone by 2.3% (65.7% \u2192 68.0%) over a competitive baseline, and by 1.1% over the previous best result that used a deeper network and more parameters. Ablation studies indicate that each component contributes, although gains saturate when the backbone is wider or when more shots are available. Analysis of learned embeddings shows tighter intra-class clusters but also reveals increased sensitivity to the choice of mixup coefficient. Code and pretrained weights are provided to ensure reproducibility.",
    "id": 66
  },
  {
    "title": "Adaptive Gradient Clipping with Scheduled Update Frequency for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient clipping is widely used to stabilize transformer training, but fixed thresholds often require extensive tuning across tasks and model sizes. We propose Adaptive Gradient Clipping with Scheduled Update Frequency (AGC-SUF), a method that automatically adjusts clipping thresholds based on gradient statistics while periodically updating the clipping bound to balance stability and convergence speed. Our approach computes a moving average of gradient norms and scales the clipping threshold proportionally, with an update frequency that decreases during training. We evaluate AGC-SUF on language modeling tasks using GPT-2 architectures ranging from 124M to 1.5B parameters. Experiments show that AGC-SUF reduces the need for hyperparameter tuning by 60% compared to standard gradient clipping while achieving perplexity within 2% of finely-tuned baselines on WikiText-103 and OpenWebText. Additionally, we observe improved training stability in 15% of configurations where standard clipping fails. However, computational overhead increases training time by approximately 8%. While our method provides practical benefits for practitioners by reducing tuning effort, we acknowledge that the theoretical motivation remains heuristic and the improvements, though consistent, are modest in magnitude.",
    "id": 67
  },
  {
    "title": "Gradient Clipping with Adaptive Momentum: A Minor Tweak or a Meaningful Fix?",
    "authors": [
      "Nguyen, T.",
      "Chen, L.",
      "Garcia, M."
    ],
    "abstract": "Gradient clipping is routinely used to stabilize training of large language models, yet its interaction with adaptive optimizers remains poorly understood. We introduce CAB-Clip, a lightweight modification that scales momentum updates by a running estimate of the clipped gradient variance. On three medium-scale language modeling tasks (up to 770 M parameters) CAB-Clip reduces perplexity by 0.4\u20130.9 points versus standard clipping, while halving the number of divergent training runs. Theoretically, we prove that CAB-Clip converges in the convex setting at the same O(1/\u221aT) rate as vanilla SGD, up to a constant factor that depends on the clipping threshold. Ablation studies show that 70 % of the gain vanishes when momentum is frozen, suggesting the key effect is a second-order bias correction rather than mere rescaling. While the method adds only four lines of code, benefits appear task-specific: gains are negligible on image classification and reinforcement-learning benchmarks. Our results indicate that gradient clipping can be slightly improved, but the improvement is incremental and may not justify a new hyper-parameter in every pipeline.",
    "id": 71
  },
  {
    "title": "Revisiting Gradient Clipping for Private-Label Training with Mixed-Batch Normalization",
    "authors": [
      "Kim, J.",
      "Singh, V.",
      "Liu, H."
    ],
    "abstract": "Modern deep nets are trained with increasingly aggressive augmentations whose gradients can explode, prompting widespread use of per-sample clipping. Prior work attributes clipping\u2019s success solely to noise stable optimization, overlooking its implicit regularization of batch-normalization (BN) statistics. We formalize this interaction and propose Mixed-Batch Normalization (MBN), a simple modification that keeps clean and augmented samples in separate BN statistics while sharing all other parameters. On ImageNet and CIFAR-10, MBN improves top-1 accuracy by 0.4\u20130.9 pp over vanilla clipping at the same compute budget; conversely, removing clipping hurts MBN by 1.1 pp, confirming a complementary regularization effect. Theoretically, we bound the Wasserstein distance between the clean and augmented feature distributions in terms of the clipping threshold, yielding a principled schedule that eliminates manual tuning on half of our tasks. Although the gains are incremental and limited to private-label setups, MBN costs one line of code and no extra parameters, making clipping-aware normalization a drop-in replacement for standard BN. Code and trained weights are provided.",
    "id": 72
  },
  {
    "title": "LoRa-GD: Low-Rank Gradient Descent for Parameter-Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Fine-tuning large language models (LLMs) remains computationally prohibitive for most practitioners. While Low-Rank Adaptation (LoRA) offers parameter efficiency, we observe that its training dynamics often diverge from full fine-tuning, particularly in downstream tasks requiring subtle semantic understanding. We propose LoRa-GD, which injects carefully constructed low-rank gradient corrections into LoRA updates to better approximate full fine-tuning trajectories. Our method computes these corrections using an efficient online PCA of the full gradient, requiring only 3% additional memory overhead. On GLUE tasks, LoRa-GD achieves 96.2% of full fine-tuning performance compared to 93.8% for standard LoRA, while using comparable parameters. However, we find these gains diminish on larger models (>7B parameters) and tasks with limited training data. Theoretical analysis shows LoRa-GD converges under similar conditions to standard gradient descent, though with a worse dependence on condition number. While our method provides modest improvements over LoRA with minimal overhead, we acknowledge the performance gap to full fine-tuning remains significant on complex reasoning tasks. Code and pretrained adapters will be made available.",
    "id": 77
  },
  {
    "title": "Momentum-Aided Gradient Descent with Iterate Averaging for Over-Parameterized Networks",
    "authors": [
      "Chen, Y.",
      "Rangan, V.",
      "Johnson, K."
    ],
    "abstract": "We revisit iterate averaging in stochastic gradient descent (SGD) for modern over-parameterized models. While classical theory suggests averaging can improve convergence rates, its benefits in deep learning remain unclear. We propose MAGNet (Momentum-Aided Gradient averaging), a simple modification that combines heavy-ball momentum with iterates averaged using an exponential window. On linear regression and two-layer networks with large widths, MAGNet achieves a 10-15% speedup in wall-clock time over tuned SGD. On CIFAR-10/100 and ImageNet, MAGNet matches baseline performance but shows more stable training curves with reduced variance in the final epoch test accuracy (\u00b10.2% vs. \u00b10.5%). Theoretically, we prove that for quadratic objectives MAGNet achieves the same O(1/t) rate as SGD but with a smaller leading constant when the mini-batch noise is high. Although the gains are incremental and limited to specific regimes, our work suggests that iterate averaging\u2014when properly combined with momentum\u2014can still confer modest practical benefits in large-scale training. Extensive ablations and open-source code are provided.",
    "id": 78
  },
  {
    "title": "Adaptive Gradient Clipping with Layer-wise Curvature Estimates for Faster Transformer Training",
    "authors": [
      "Kumar, S.",
      "Lee, J.",
      "Zhou, H."
    ],
    "abstract": "Gradient clipping is routinely used when training large Transformer models, yet its global threshold is typically set via hand-tuned constants that ignore the varying curvature across layers. We propose a per-layer adaptive clipping strategy that re-scales gradients according to an online estimate of the local Lipschitz constant derived from the Hutchinson estimator. On three medium-scale language-modeling benchmarks (\u2264 1.3 B parameters) our method reduces optimizer steps to target validation loss by 8\u201314 % versus standard clipping, at the cost of a 5 % per-step overhead for the curvature estimator. Ablation studies show that most gains come from the two deepest blocks, corroborating the hypothesis that gradient magnitudes are heteroscedastic across depth. While the speed-up is consistent, we observe no improvement in final perplexity after sufficient training, and wall-clock benefits shrink on systems with stronger compute budgets. Theoretically, we prove that the clipping operation is equivalent to an implicit trust-region step only when the curvature bound holds uniformly; our estimator meets this assumption with high probability for sufficiently large batch sizes (\u2265 2k). Code is provided for reproduction.",
    "id": 113
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Inference via Quadratic Extrapolation",
    "authors": [
      "Kim, S.",
      "Rodriguez, L.",
      "Chen, T."
    ],
    "abstract": "Gradient-based optimization of discrete latent variable models remains challenging due to the high variance of score-function estimators. We propose Quadratic Extrapolation for Gradient Estimation (QEGA), a control-variate technique that constructs quadratic baselines from previous iterations to reduce variance. Unlike moving-average baselines, QEGA adaptively extrapolates parameters based on the optimization trajectory, yielding lower variance when the loss landscape is locally smooth. Across five benchmark tasks\u2014including discrete VAE training and structured prediction\u2014QEGA achieves 5-12 % lower gradient variance and marginal improvements in final ELBO compared to REINFORCE with optimized moving-average baselines. Although our theoretical analysis is limited to strongly log-concave exponential families, empirical results suggest broader applicability. Code and hyper-parameters are provided for reproducibility.",
    "id": 114
  },
  {
    "title": "Revisiting Dropout for Uncertainty Estimation in Small-Scale Neural Networks",
    "authors": [
      "Chen, L.",
      "Vasquez, J.",
      "Kumar, A."
    ],
    "abstract": "Bayesian neural networks promise calibrated uncertainty but remain prohibitively expensive for modest-sized applications. We reconsider standard dropout training as a lightweight alternative and ask whether simple post-hoc corrections can turn arbitrary networks into reliable uncertainty estimators. Starting with the observation that usual Monte-Carlo dropout severely under-estimates epistemic variance, we derive a data-dependent scaling term that re-calibrates predictive variance without model changes. On UCI regression benchmarks our procedure yields 5-15% lower NLL than vanilla dropout and is competitive with Deep Ensembles while using a single model. However, calibration gains vanish on high-dimensional inputs (CIFAR-10), and the method still lags behind gold-standard Hamiltonian Monte-Carlo. Theoretical analysis shows the proposed adjustment is equivalent to a misspecified prior, explaining limited robustness under covariate shift. Code and trained weights are provided, but the calibration procedure requires held-out data, raising fairness concerns in production regimes. Our results suggest semi-Bayesian tricks can trade a small accuracy drop for moderate uncertainty gains, yet fall short of full Bayesian validity.",
    "id": 115
  },
  {
    "title": "RevisitingMomentum: A Lightweight Momentum Variant for Faster Stochastic Optimization",
    "authors": [
      "Kumar, A.",
      "Liu, S.",
      "Roberts, J."
    ],
    "abstract": "We present RevisitingMomentum (RM), a simple modification to classical momentum that adapts the momentum coefficient using a quadratic surrogate of the local loss landscape. Unlike adaptive methods such as Adam or RMSprop, RM retains the low memory footprint of vanilla SGD while offering modest speed-ups on deep network training. Our key insight is that the optimal momentum coefficient can be approximated by the relative change in gradient norm over a short horizon, yielding an update rule with negligible overhead. On CIFAR-10/100 and ImageNet, RM improves convergence by 5\u201312 % over tuned SGD+momentum at the same learning rate schedule, but gains diminish when the baseline is heavily optimized. Theoretically, we prove O(1/T) convergence for convex smooth objectives, matching standard momentum. Ablation studies show that the surrogate horizon hyperparameter is stable across architectures, although extreme batch sizes require retuning. Code is provided for reproducibility. While RM does not surpass carefully tuned schedules or second-order methods, its simplicity and consistent small gains may benefit practitioners with limited compute budgets.",
    "id": 121
  },
  {
    "title": "LoRA-NTK: A Parameter-Efficient Initialization for Low-Rank Adaptation of Large Language Models",
    "authors": [
      "Chen, L.",
      "Krishnan, A.",
      "Roberts, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its performance depends heavily on the random initialization of the low-rank matrices. We propose LoRA-NTK, a simple initialization scheme inspired by the Neural Tangent Kernel (NTK) theory that sets the initial scale of adaptation matrices based on the spectrum of pretrained weights. Our method requires no additional hyperparameters beyond standard LoRA and adds negligible computational overhead. We evaluate LoRA-NTK on instruction tuning and domain adaptation tasks using LLaMA-7B and OPT-13B models. Experiments across 8 datasets show modest but consistent improvements over standard LoRA initialization, with average gains of 1.2% on downstream tasks. Ablation studies reveal that our initialization particularly helps in low-data regimes (\u22641K examples), suggesting better utilization of the pretrained representation. While the improvements are incremental and task-dependent, our work provides theoretical insights into the role of initialization in parameter-efficient fine-tuning and offers practitioners a drop-in replacement that costs nothing to implement. Code is available at anonymous-github.url.",
    "id": 122
  },
  {
    "title": "Randomized Block-Coordinate Adam: Improving Convergence via Subspace Momentum",
    "authors": [
      "Navarro, E.",
      "Zhao, H.",
      "Chen, T."
    ],
    "abstract": "Momentum-based adaptive optimizers such as Adam accelerate training in many deep-learning tasks but suffer from slower final-phase convergence relative to SGD. We present Randomized Block-Coordinate Adam (rBC-A), a simple modification that decouples the moment estimates across randomly selected parameter blocks. By intermittently freezing subsets of the model, rBC-A reduces gradient noise variance while preserving second-order moment adaptation in the active block, yielding better iterate stability without computing full-batch statistics. On convex toy problems, rBC-A is provably within a constant factor of the optimal convergence rate established for SGD with momentum and matches the worst-case bound of Adam. In empirical evaluations on CIFAR-10/100 and WikiText-2 with ResNet-20 and a 6-layer Transformer, rBC-A achieves comparable or slightly better final accuracy (+0.3 % avg) and 6-12 % faster wall-clock time than vanilla Adam, at the cost of two tunable hyper-parameters (block size p and freeze probability \u03b1). Ablation studies suggest that gains diminish as model width grows beyond 50 M parameters, indicating that the method is most useful for moderate-scale regimes. Code and 20 random seeds are available in the supplementary ZIP.",
    "id": 130
  },
  {
    "title": "Combining Momentum with Adam: A Gentle Push for Better Generalization",
    "authors": [
      "Kumar, V.",
      "Li, S.",
      "Oliveira, T."
    ],
    "abstract": "Adaptive optimizers such as Adam are widely adopted for training deep networks, yet they sometimes lag behind SGD+momentum in final test accuracy. We revisit the role of classical momentum in adaptive schemes and propose \u201cPAdam\u201d, a lightweight modification that injects a momentum buffer into Adam's update only when the gradient signal-to-noise ratio falls below a learned threshold. The key idea is to let the optimizer borrow SGD-like stability in low-curvature regions while preserving Adam's fast early progress. On CIFAR-10/100 and ImageNet, PAdam yields 0.3\u20130.7% accuracy gains over vanilla Adam at no extra cost, and matches or slightly outperforms finely-tuned SGD schedules. We further show that PAdam adapts more smoothly to aggressive learning-rate warmup, leading to a 5-8% reduction in validation loss variance across five runs. A regret bound is provided for convex Lipschitz losses, although our analysis relies on a bounded-gradient assumption that may not hold in practice. Code is available at anonymized-url.",
    "id": 131
  },
  {
    "title": "Residual-Mixup: A Lightweight Data Augmentation Baseline for Vision Transformers",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Liu, J."
    ],
    "abstract": "Data augmentation improves generalization, yet the community has not converged on an augmentation recipe for Vision Transformers (ViTs) comparable to the established \"medium-strength\" policy used for CNNs. We propose Residual-Mixup, a trivial modification that linearly interpolates image patches after the patch-projection layer while simultaneously blending labels. By mixing in patch space rather than pixel space, the augmentation is applied at the same granularity as the transformer\u2019s internal tokens, inducing smoother attention landscapes without extra parameters. On ImageNet-1k with four common ViT variants (Ti/16, S/16, B/16, L/16) trained from scratch for 300 epochs, Residual-Mixup yields consistent +0.6-0.9 % top-1 gains over the standard augmentation baseline, and is complementary to RandAugment. Ablations show the effect persists across different patch sizes and training schedules, and that it marginally reduces attention entropy. The method is easy to implement (five lines of code), introduces no measurable overhead, and supports arbitrary \u03b1\u2208[0,1] without tuning. Code is provided. While the contribution is incremental and limited to supervised image classification, the technique offers a practical, reproducible baseline that may ease future ViT research.",
    "id": 133
  },
  {
    "title": "Scheduled Temperature Averaging for Improved Semi-Supervised Learning",
    "authors": [
      "Garcia, L.",
      "Nguyen, P.",
      "Dubois, M."
    ],
    "abstract": "Self-training and consistency regularization have proven effective for semi-supervised learning yet remain sensitive to the quality of the teacher model. While prior work relies on exponential moving average (EMA) of weights with a fixed decay, we observe that a carefully scheduled temperature parameter integrated into the averaging process improves pseudo-label accuracy, especially when labeled examples are extremely scarce. We introduce Scheduled Temperature Averaging (STA), a simple wrapper that modulates temporal ensembling through a cyclical temperature schedule during training. On CIFAR-10 with 250 labels, STA boosts the baseline by 1.8% accuracy; similar gains appear on ImageNet-1k with 10% labels. Theoretical insights demonstrate that STA implicitly performs moment-matched distillation, which stabilizes early training iterations. Although gains diminish when label noise or domain shift is large, STA incurs minimal overhead and integrates seamlessly with existing frameworks. Code is available at anonymous-link.",
    "id": 139
  },
  {
    "title": "AdaMix: Adaptive Initialization for Mixup Training via Reinforcement Learning",
    "authors": [
      "Liu, C.",
      "Kumar, S.",
      "Zhao, J."
    ],
    "abstract": "Mixup, a simple data augmentation technique that trains on convex combinations of training examples, has demonstrated empirical success across vision and language tasks. However, its reliance on uniform sampling of interpolation weights has been noted to under-utilize the geometric structure learned by the model during training. We present AdaMix, a method that uses reinforcement learning (RL) to dynamically schedule the mixup coefficient at every iteration. A lightweight policy network, trained on meta-features extracted from the current mini-batch, predicts the optimal mixing weight without additional forward passes on held-out data. We evaluate AdaMix on ResNet-18/50 and ViT-B/16 classifiers trained on CIFAR-10, CIFAR-100 and ImageNet-1k, observing average test accuracy gains of 1.2\u00b10.2% across datasets and models. Notably, AdaMix yields larger improvements (+2.1%) when training from a poor random initialization produced by reduced hyper-parameter tuning budgets. Ablation studies indicate that the RL component contributes roughly 60% of the gain, while schedule annealing supplies the remainder. Although gains are dataset-dependent and the policy network adds 1.4% FLOPs per iteration, AdaMix introduces no overhead at inference and can be integrated into existing pipelines via three lines of code. Our findings suggest that RL-guided augmentation policies can provide modest but consistent improvements for mixup-based training with minimal disruption to existing workflows. Code and checkpoints are released.",
    "id": 141
  },
  {
    "title": "Improved Gradient Penalties for Wasserstein GANs via Adaptive Sampling",
    "authors": [
      "Chen, J.",
      "Nair, A.",
      "Schmidt, L."
    ],
    "abstract": "Training Generative Adversarial Networks (GANs) with Wasserstein distances has shown promise in producing stable and high-quality samples, yet selecting appropriate gradient-penalty weights remains empirically intensive. We propose an adaptive sampling strategy that adjusts the weight of the gradient penalty on a per-sample basis according to local discrepancies between the generator and discriminator distributions. Our method introduces a lightweight controller network trained online to predict penalty weights that keep gradient norms close to the theoretical constraint without heavy tuning. On CIFAR-10 and CelebA, the proposed technique achieves an FID of 15.3 and 8.7 respectively\u2014comparable to standard baselines with carefully tuned fixed weights\u2014while reducing sensitivity to initial learning rates by 40% across three optimizer choices. Theoretically, we provide a partial showing that the controller\u2019s objective upper-bounds local Wasserstein distances, although the global guarantee holds only under restrictive smoothness assumptions which may not apply to practical architectures. Experiments demonstrate moderate improvements in sample diversity metrics, yet the method incurs a 14% computational overhead and still exhibits mode collapse in limited-data regimes. Code and trained models are available to reproduce all experiments.",
    "id": 143
  },
  {
    "title": "Revisiting Gradient Clipping Thresholds for Transformer Pre-Training with Layer-Wise Learning Rate Tuning",
    "authors": [
      "Kovacs, B.",
      "Das, P.",
      "Nguyen, T."
    ],
    "abstract": "Gradient clipping is routinely applied to stabilize large-scale transformer training, yet the clipping threshold is usually chosen by ad-hoc rules. We introduce a simple re-scaling rule that sets layer-specific clipping bounds proportional to the square root of their initialization variances, combined with a schedule that anneals the global factor by 0.1% each update. On 117M-parameter language models trained with Adam we obtain 1\u20132% lower validation perplexity versus the standard global threshold of 1.0. The same rule accelerates convergence in masked-language-modeling by \u223c8% wall-clock time on the One Billion Word benchmark. Theoretical analysis for two-layer linear networks shows clipped SGD with our bound yields an O(1\u221aT) convergence rate, identical to unclipped SGD but with tighter constant factors when gradients are heavy-tailed. Ablation studies indicate the schedule contributes about two-thirds of the gain, while the layer-wise component adds the remainder. Code is provided for fair comparison.",
    "id": 145
  },
  {
    "title": "Lookahead-Q: Improving Deep Q-Learning with One-Step Rollouts",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "Deep Q-Networks (DQN) remain a foundational algorithm for reinforcement learning, yet sample efficiency remains a challenge in complex environments. We present Lookahead-Q, a simple extension to DQN that incorporates one-step model rollouts to improve value estimation. Our method trains a learned dynamics model alongside the Q-network and uses it to simulate the immediate next state for each transition, incorporating the resulting Q-value estimates through a convex combination with the standard temporal difference target. This approach requires minimal additional compute and no architectural changes to the Q-network. Across 15 Atari games, Lookahead-Q achieves a 12% median improvement in sample efficiency compared to Double DQN, with particularly strong gains on games requiring planning-like behavior. Theoretical analysis shows our update rule reduces variance under mild assumptions about model accuracy. While the improvements are consistent, they are incremental and the method adds hyperparameter sensitivity. Our code and trained models are available online.",
    "id": 146
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum Compensation for Improved Transformer Training",
    "authors": [
      "Liu, K.",
      "Chen, T.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient clipping is widely used to stabilize transformer training, but standard clipping thresholds are often set heuristically and can impede convergence. We propose Adaptive Gradient Clipping with Momentum Compensation (AGCM), a method that dynamically adjusts clipping thresholds based on gradient history while compensating for the bias introduced to momentum-based optimizers. Our approach tracks the ratio of gradient norm to parameter norm across mini-batches and adapts clipping thresholds using a exponential moving average with learnable decay. We theoretically show that AGCM reduces the upper bound on convergence rate by O(1/T) compared to fixed clipping under standard smoothness assumptions. On the WMT'14 English-German translation task, AGCM achieves 0.4 BLEU improvement over standard clipping with AdamW, while requiring minimal hyperparameter tuning. Experiments on vision transformers (ViT-B/16) demonstrate 1.2% accuracy gain on ImageNet with 15% reduction in training time. Though improvements are consistent, they are modest compared to recent architectural innovations. Our method adds computational overhead of ~5% during training and introduces one additional hyperparameter. Code is available at anonymous-url.github.io/agcm.",
    "id": 147
  },
  {
    "title": "LoRA-Flow: Memory-Efficient Adaptation of Transformer Language Models via Dynamic Low-Rank Paths",
    "authors": [
      "Nguyen, T.",
      "Kovacs, D.",
      "Zhao, L."
    ],
    "abstract": "We present LoRA-Flow, a simple extension of Low-Rank Adaptation (LoRA) that conditions rank-specific matrices on a lightweight routing network instead of learning a single static adapter. Motivated by empirical evidence that optimal intrinsic dimension varies across layers, we allocate a bank of rank-1 to rank-16 matrices per layer and learn a sparse mixture of them. On eight GLUE tasks and two domain-shift datasets, LoRA-Flow matches full fine-tuning within 0.3 F1 of LoRA while reducing trainable parameters by 35-50%. Furthermore, our ablations show that routing stabilizes optimization when LoRA rank is misspecified. Although we observe consistent gains on moderate-sized models (\u2264 1.3 B parameters), improvements vanish on larger 7 B checkpoints checkpoints, suggesting scalability limits. Theoretically, we bound the excess risk introduced by dynamic rank selection under Gaussian data assumptions; however, the bound scales unfavorably with model width. Code and hyper-parameters are provided to ensure reproducibility. While LoRA-Flow offers practical memory savings for downstream practitioners, its contribution is largely incremental and the scalability challenges raise open questions about rank-adaptive fine-tuning at scale.",
    "id": 155
  },
  {
    "title": "Towards Faster Neural Network Training via Cyclical Block-Diagonal Approximations",
    "authors": [
      "Kovacs, P.",
      "Wang, L.",
      "Garcia, J."
    ],
    "abstract": "Second-order optimization methods can accelerate neural network training, but their high per-iteration cost and memory complexity limit practical adoption. We propose Cyclical Block-Diagonal Adam (CBDA), a hybrid optimizer that periodically approximates the Fisher information matrix with a dynamically sized block-diagonal structure updated only every m steps. On CIFAR-10/100 and ImageNet subsets, CBDA reaches target accuracies 11\u201317% sooner in wall-clock time than AdamW on ResNet-18 and Vision Transformer models, while using 38% less memory than K-FAC. Theoretically, we show that CBDA converges at aO(1/\u221aT) rate under standard smoothness assumptions and that the block-diagonal approximation error remains bounded by a quadratic function of the learning rate. Although our ImageNet gains diminish on larger models (EfficientNet-B3), extensive ablations indicate that the procedure is most effective when combined with small weight decay and cosine annealing. Code and hyper-parameter configs are made publicly available for straightforward reproduction. The method\u2019s simplicity and drop-in PyTorch compatibility make it attractive for practitioners, yet the approximation\u2019s reliance on layer ordering and heuristic block schedules raises questions about broader applicability.",
    "id": 158
  },
  {
    "title": "Momentum Rescaling: A Lightweight Alternative to Adaptive Optimizers",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Ortiz, J."
    ],
    "abstract": "Adaptive optimizers such as Adam and RMSProp accelerate training by per-parameter learning-rate adjustment, but they double memory use and can impair generalization. We propose Momentum Rescaling (MoRe), a drop-in modification to vanilla stochastic momentum SGD that re-normalizes each parameter\u2019s update by the inverse of its recent gradient variance estimated from a short, fixed-length history. MoRe retains the single-state memory footprint of SGD while adaptively rescaling updates similar to second-order moment methods. On CIFAR-10/100 and ImageNet, MoRe matches the convergence speed of Adam on ResNet and Transformer models, but achieves 0.2-0.6% higher test accuracy. We prove that MoRe converges at rate O(1/T) for smooth, non-convex objectives, matching standard SGD guarantees. Ablation studies show that the rescaling horizon (8-16 steps) and exponential decay factor (0.95) are robust across tasks. The method is implemented in <10 lines of PyTorch code and incurs <1% overhead compared to SGD. While MoRe does not surpass tuned SGD+momentum on every benchmark, it consistently outperforms adaptive optimizers in generalization, offering a middle ground between simplicity and speed. Code is available at anonymous URL.",
    "id": 161
  },
  {
    "title": "Improving Transformer Generalization with Gradient-Based Context Compression",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Transformer models often struggle with long-context tasks due to quadratic attention complexity and difficulty attending to relevant information. We propose Gradient-based Context Compression (GCC), a training-free method that compresses input sequences by pruning tokens with minimal gradient contributions. Our approach computes token importance through gradients of a proxy loss, then iteratively removes up to 50% of tokens while maintaining task performance. We extend this idea in a learned variant that trains a lightweight compression network to predict gradient-based importance scores, achieving 2.3\u00d7 speedup during inference. Experiments on document classification, question answering, and summarization show GCC maintains 94-98% of full-context performance while reducing computational cost by 30-60%. While our method demonstrates practical efficiency gains, we observe performance degradation on tasks requiring fine-grained reasoning over long documents. GCC provides a simple plug-and-play technique for accelerating transformer inference, though careful hyperparameter tuning is required for optimal compression ratios.",
    "id": 163
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Small-Batch Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose AdaNoise, a simple modification to standard stochastic gradient descent that adaptively injects Gaussian noise during optimization. While previous work has shown noise injection can improve generalization, existing methods rely on fixed schedules or require additional hyper-parameters. AdaNoise estimates the gradient noise scale online and adjusts the injection variance proportionally. Our theoretical analysis shows this approach approximately preserves the stationary distribution of SGD while encouraging exploration in sharp minima. On CIFAR-10 and ImageNet, AdaNoise improves test accuracy by 0.5-1.2% compared to strong baselines when training ResNet-18 with batch sizes 32-128. However, benefits diminish with larger batches and deeper networks. Ablations reveal the method is sensitive to the noise estimation window size, and performance varies significantly across architectures. While AdaNoise provides consistent improvements for small-batch scenarios common in resource-constrained settings, its impact appears limited for standard large-batch training regimes. Code is available at anonymized-url.",
    "id": 165
  },
  {
    "title": "LoRa-SGD: Locally-Randomized SGD with Theoretical Guarantees for Heterogeneous Federated Learning",
    "authors": [
      "Kim, J.",
      "Rodriguez, C.",
      "Singh, A."
    ],
    "abstract": "Federated learning (FL) struggles when client data distributions are highly heterogeneous, even with popular personalization techniques. We propose LoRa-SGD, a lightweight modification that injects client-specific random projections into local SGD updates. Using a new matrix-Bernstein concentration bound, we prove that LoRa-SGD converges at a rate of O(1/\u221aT) under (\u03b1,\u03b2)-dissimilarity, matching the best-known rates for homogeneous FL while requiring only projection matrices to be shared. On standard benchmarks (CIFAR-10 non-IID, Shakespeare) LoRa-SGD improves test accuracy by 2.3% over FedAvg and 1.1% over FedProx while adding <0.5% communication overhead. However, gains diminish on larger-scale datasets (ImageNet), and hyper-parameter sensitivity increases with client drift. Our code is available at anonymized-url.",
    "id": 169
  },
  {
    "title": "LoRa-Flow: Lightweight Low-Rank Normalizing Flows for Fast Density Estimation",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Liu, Y."
    ],
    "abstract": "Normalizing flows provide flexible probabilistic models by learning invertible transformations, but their computational cost limits deployment on edge devices. We propose LoRa-Flow, which injects low-rank constraints into the weight matrices of coupling layers to reduce parameters without redesigning the network. On five UCI benchmarks, LoRa-Flow achieves 1.5\u00d7 speed-up and 60 % fewer parameters at the cost of <2 % bits-per-dimension increase versus unconstrained RealNVP. Theoretical analysis shows the low-rank bottleneck still preserves universal approximation in the affine coupling class. However, ablations reveal degradation on high-dimensional images, suggesting the low-rank prior is too restrictive for modalities with complex spatial correlations. Code is available.",
    "id": 173
  },
  {
    "title": "Gradient-free Optimization of Attention Temperatures via Quotient Manifold Search",
    "authors": [
      "Kumar, S.",
      "Okafor, C.",
      "Hu, Q."
    ],
    "abstract": "Transformer self-attention distributions are controlled by a single temperature parameter that is typically fixed or set via heuristics. We propose a gradient-free optimization scheme that tunes layer-wise temperatures by treating the attention cone as a point on a quotient manifold of positive-definite matrices. Our algorithm performs zeroth-order search along geodesics identified through Grassmannian embeddings, requiring only black-box access to downstream validation loss. On WMT'14 En-De and ImageNet-1k we obtain consistent BLEU/Top-1 gains of 0.3-0.5 points over learned-by-default baselines at comparable compute budgets. Theoretically, we bound the regret of the search procedure under bounded variance of the stochastic ruler estimator, yielding a convergence rate of \u00d5(T^{-2/3}). While the improvements are reliable, they are modest; ablation shows 60% of the gain can be recovered by a learnable scalar bias. Code will be made available.",
    "id": 174
  },
  {
    "title": "Block-Diagonal Adaptation: A Structured Approach to Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Dubois, Y."
    ],
    "abstract": "We present Block-Diagonal Adaptation (BDA), a lightweight fine-tuning method that learns task-specific block-diagonal perturbations of pre-trained weights. Unlike prior adapters that introduce bottlenecks or sparse masks that can fragment gradient flow, BDA factorizes the adaptation matrix into a small set of dense, non-overlapping blocks, balancing expressivity and parameter efficiency. On GLUE and three vision datasets, BDA matches full fine-tuning with 14\u201322% of the parameters, outperforming LoRA and (IA)$^3$ by 0.8\u20131.3 pp average accuracy while using comparable FLOPs. Ablations indicate that block size acts as an implicit regularizer: larger blocks improve transfer on high-resource tasks but may over-fit on low-resource ones. Theoretically, we bound the generalization error under sub-Gaussian covariates, showing a favorable trade-off that depends on the total number of blocks rather than the ambient dimension. Code is provided, and full results can be reproduced in <24 GPU-hours on RTX-4090 hardware. Though competitive, gains over strong baselines are modest, and extending BDA to billion-scale models remains future work.",
    "id": 178
  },
  {
    "title": "Improving Few-Shot Classification via Iterative Meta-Augmentation with Confidence-Based Filtering",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Johnson, L."
    ],
    "abstract": "Few-shot learning remains challenging due to the limited availability of labeled examples. While data augmentation techniques have shown promise in this setting, existing approaches often generate noisy samples that degrade performance. We propose Iterative Meta-Augmentation (IMA), a method that progressively augments the support set by leveraging model confidence to filter synthetic examples. Our approach alternates between training a classifier on the current support set and generating new samples using a conditional generative model. Generated samples are retained only if the classifier's prediction confidence exceeds an adaptive threshold. Through experiments on miniImageNet and tieredImageNet, IMA achieves 62.3% and 70.1% 5-way 1-shot accuracy respectively, representing a 2-3% improvement over strong baselines. We provide theoretical analysis showing that our confidence-based filtering reduces label noise in the augmented set. While our results are positive, we acknowledge several limitations: the method requires careful hyperparameter tuning for each dataset, and the computational cost is higher than simpler augmentation strategies. Additionally, we observe that performance gains diminish when moving from 1-shot to 5-shot scenarios. Our code is available at [anonymous repository].",
    "id": 180
  },
  {
    "title": "Gradient Alignment Improves Transfer Learning in Limited-Data Regimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Fine-tuning large pre-trained models has become the dominant approach for transfer learning, yet its performance degrades sharply when target data are scarce. We propose Gradient Alignment Transfer (GAT), a simple regularizer that encourages the fine-tuning gradients to align with the pre-trained feature directions. By penalizing gradient components orthogonal to the principal subspace of pre-training updates, GAT constrains optimization to parameters that matter most for the source domain. On 8 few-shot vision and NLP benchmarks, GAT yields 2\u20134% absolute gains over standard fine-tuning while adding <1% overhead. Theoretical analysis shows GAT is equivalent to a soft constraint on the distance between source and target feature covariances, clarifying when the method helps. Although the improvements are consistent, they are modest and diminish as data increase; above 5k examples per class, GAT matches vanilla fine-tuning. Our code, tuned for stability rather than speed, requires 30% more memory because of second-order moment tracking. Overall, GAT offers a lightweight, interpretable way to nudge transfer learning, but its narrow scope and incremental gains may limit broader impact.",
    "id": 182
  },
  {
    "title": "Improving Gradient Flow in Residual Networks with Learnable Skip-Scaling",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rodriguez, A."
    ],
    "abstract": "Deep residual networks have shown remarkable success in training very deep architectures, yet the choice of how skip connections are weighted remains largely heuristic. We propose a simple method for learning per-layer, data-independent scalar coefficients that dynamically rescale the residual branch during training. Our approach introduces only one additional parameter per residual block and is optimized jointly with the network weights via standard back-propagation. We demonstrate that learnable rescaling can accelerate convergence on CIFAR-10 and ImageNet by up to 18% in wall-clock time and yield modest accuracy gains of 0.3\u20130.5% over the original ResNet baseline. Although the technique generalizes across common vision benchmarks, gains diminish when strong regularization or modern architectural refinements are present. Extensive ablations indicate that improvements are most pronounced in mid-depth networks (20\u201350 layers), while deeper models benefit less. The method requires no architectural redesign and is easily integrated into existing frameworks. Our code and trained models are publicly available.",
    "id": 199
  },
  {
    "title": "Revisiting Mixup for Semi-Supervised Learning: When Do Interpolated Labels Improve Generalization?",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "While Mixup has been widely adopted as a data-augmentation technique in supervised learning, its application to semi-supervised learning (SSL) remains under-explored. We empirically investigate whether Mixup improves SSL performance when combined with consistency-regularization methods such as FixMatch and FlexMatch. Across CIFAR-10/100 and Mini-ImageNet, we observe that na\u00efve Mixup can actually degrade accuracy by up to 2.3% when the labeled set is extremely small (<5%), but yields marginal gains (+0.5%) when more labels are available. Motivated by this non-monotonic trend, we propose Curriculum-Mixup (CM), a simple schedule that gradually increases both Mixup strength and interpolation probability as the model's prediction confidence rises. CM recovers the lost performance at low-label regimes and achieves state-of-the-art results on CIFAR-10 with only 40 labels (89.1%). Ablation studies reveal that CM's benefit is primarily due to calibrated pseudo-labels rather than input diversity. Although our method is intuitive and implementation requires only 20 additional lines of code, our theoretical analysis is limited to linear models and does not explain non-monotonicity in deep nets. Code and checkpoints are publicly available.",
    "id": 201
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Autoencoders via Continuous Relaxations with Learned Temperature",
    "authors": [
      "Klein, S.",
      "Murali, V.",
      "Andersson, J."
    ],
    "abstract": "Discrete latent variables offer interpretable representations but complicate training of variational autoencoders (VAEs) because reparameterization gradients are unavailable. While continuous relaxations such as the Gumbel-softmax trick yield low-bias gradient estimates, they require a fixed temperature schedule that can lead to high variance or over-regularized latent codes. We propose L-Temp-VAE, a simple extension that learns a free parameter per latent dimension to control relaxation temperature jointly with the generative and inference networks. In experiments on text generation, molecule generation, and semi-supvervised MNIST classification, L-Temp-VAE lowers gradient variance by 15\u201330% compared with annealed baselines and improves ELBO by 0.5\u20131.6 nats without additional computational cost. Ablation studies reveal that learned temperatures automatically soften during early training and stiffen toward convergence, behaviour difficult to replicate with hand-designed schedules. Although the idea is conceptually incremental, our results suggest that learnable relaxation parameters can be integrated into existing discrete-VAE pipelines with minimal effort. Code is available at anonymous.url.",
    "id": 202
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Inference via Continuous Relaxations with Control Variates",
    "authors": [
      "Liu, M.",
      "Johnson, K.",
      "Chen, S."
    ],
    "abstract": "Variational inference for discrete latent variables remains challenging due to the high variance of gradient estimators. While continuous relaxations such as the Gumbel-Softmax enable reparameterization, they introduce bias that can degrade performance. We propose a simple modification that incorporates a Taylor-based control variate into the relaxed gradient estimator, yielding lower variance without additional model evaluations. Applied to variational autoencoders with categorical latents, our method reduces gradient variance by 35% on average across three benchmark datasets, translating to modest improvements in held-out likelihood. Theoretically, we bound the bias introduced by the relaxation and show that the control variate does not asymptotically increase it. Empirically, we observe faster convergence in training and slightly better sample quality. While the gains are incremental and the approach is limited to moderate-sized vocabularies, the technique is easy to implement and may complement existing variance-reduction strategies. Code is available at anonymized-url.",
    "id": 205
  },
  {
    "title": "Improved Generalization via Periodic Weight Averaging with Cyclical Learning Rates",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Cyclical Weight Averaging (CWA), a simple extension to Stochastic Weight Averaging (SWA) that leverages cyclical learning rate schedules to improve generalization in deep neural networks. While SWA averages weights along the optimization trajectory after the learning rate has converged, CWA performs averaging throughout training by identifying stable points during cyclical learning rate annealing. Our method requires minimal hyperparameter tuning and adds negligible computational overhead compared to standard training. We evaluate CWA on CIFAR-10/100 and ImageNet with ResNet and Vision Transformer architectures, achieving 0.3-0.8% accuracy improvements over SWA and 1.2-1.9% over standard training. Theoretical analysis suggests CWA converges to a wider optimum by exploring multiple modes during the cyclical schedule. However, we find that the benefits diminish with stronger data augmentation and regularization techniques. While our empirical results are consistent across architectures and datasets, the theoretical justification remains incomplete, and computational overhead increases for very large models. Code and pre-trained models are available at anonymous-url.",
    "id": 207
  },
  {
    "title": "Self-Attention with Learnable Temperature for Improved Generalization in Low-Data Regimes",
    "authors": [
      "Chandran, K.",
      "Osei, E.",
      "Li, M."
    ],
    "abstract": "We introduce learnable temperature scaling into the self-attention mechanism of Transformer architectures, aiming to improve generalization when training data are scarce. By re-parameterizing the softmax temperature as a light-weight, input-dependent function, we obtain a flexible attention module that can interpolate between peaky and diffuse attention patterns. On standard NLP and vision benchmarks, our method yields 1.2\u20132.4 % accuracy gains over baseline Transformers when fewer than 5 k labeled examples are available, while matching performance in the large-data regime. A Bayesian interpretation reveals that the learned temperature performs an implicit form of posterior tempering, trading off data likelihood against model complexity. Extensive ablations show that gains diminish when pre-training is provided and that results are sensitive to initialization. Code and pre-trained weights are provided.",
    "id": 210
  },
  {
    "title": "Improved Confidence Bounds for Linear Bandits via Root-Penalized Least Squares",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "We revisit the standard confidence ellipsoid used in linear stochastic bandits and propose a simple modification to the least-squares estimator. By adding a square-root regularization term to the ridge penalty, we obtain tighter confidence widths that scale with the empirical standard deviation of the observed rewards rather than the worst-case range. The resulting algorithm, RootLinUCB, achieves a gap-dependent regret bound of  \u00d5(d\u221a(Sn) + d\u00b2) where S is the empirical reward variance, improving on the classical  \u00d5(d\u221an) when rewards exhibit low variance. Experiments on synthetic data with heteroscedastic noise show 12\u201318% regret reduction compared to OFUL, while performance on standard benchmarks (e.g., Jester dataset) is comparable. Although the regret improvement is incremental and the analysis relies on sub-Gaussian noise, the approach is easy to implement and requires no additional hyper-parameters beyond standard ridge regression. Our results suggest that data-dependent confidence widths can yield modest practical gains without sacrificing computational efficiency.",
    "id": 217
  },
  {
    "title": "Improved Gradient Norm Estimation for Adaptive Learning Rates in Deep Networks",
    "authors": [
      "Chen, L.",
      "Kothari, P.",
      "Nguyen, T."
    ],
    "abstract": "Adaptive learning rates are crucial for training deep neural networks efficiently. While Adam and its variants dominate practice, they rely on biased gradient moment estimates, potentially hindering convergence. We propose GN-Adam, a lightweight modification that re-scales Adam's update by an online estimate of the true gradient L2-norm obtained via a momentum-based EMA of per-sample gradients. Theoretically, we show GN-Adam achieves a  \u00d5(1/\u221aT) regret bound in the convex setting, matching Adam but under weaker assumptions. Empirically, GN-Adam yields 1\u20133% top-1 accuracy gains over AdamW on CIFAR-10/100 and ImageNet when training ResNet-18/50 and Vision-Transformers, with comparable wall-clock time. Ablation studies indicate the improvement comes chiefly from better handling of gradient noise early in training. While our bound does not extend to the non-convex case and the additional memory footprint equals the size of one gradient tensor, GN-Adam is easy to implement and may offer practical benefits for large-scale vision and language tasks.",
    "id": 226
  },
  {
    "title": "LoRA-Lite: Structured Low-Rank Adaptation with Gradient-Free Hyper-Network Tuning",
    "authors": [
      "Kim, J.",
      "Liu, S.",
      "Martinez, C."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method, yet its reliance on back-propagation through the full network still incurs non-negligible memory and compute overhead. We propose LoRA-Lite, a plug-in extension that eliminates gradient flow through frozen weights by learning a compact hyper-network to generate the low-rank adapters. A factorized tensor-train compression of the hyper-network reduces trainable parameters by 38% on 7B-scale language models while maintaining the same inference path as standard LoRA. Empirically, LoRA-Lite recovers 96\u201398% of full fine-tune performance on GLUE when the base model is held fixed, and matches baseline LoRA scores on five downstream summarization tasks. Ablations show that the choice of rank scheduling and the hyper-network bottleneck width are critical; removing either component drops average scores by 2\u20133 points. Although our method introduces extra hyper-parameters and a small pre-training cost for the meta-weights, it enables true weight-free optimization and opens the door to on-device adapter personalization. Code and adapters are provided for reproducibility.",
    "id": 229
  },
  {
    "title": "On the Importance of Normalization in Transductive Few-Shot Learning",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transductive few-shot learning (TFSL) leverages test-time query statistics to adapt class prototypes, yet performance varies dramatically across benchmark splits. We hypothesize that batch normalization layers, fixed after meta-training, distort feature scales under domain shift. We propose Adaptive Transductive Normalization (ATN), a lightweight wrapper that re-estimates batch statistics using support and query features, coupled with a temperature-scaled cross-entropy loss. Extensive experiments on mini-ImageNet, tiered-ImageNet and CUB-200 show gains of 1.3\u20132.7% over state-of-the-art TFSL methods, while adding only 0.4 ms per task. Ablation reveals that half of the improvement comes from temperature scaling alone, and benefits shrink when pre-training on larger corpora. Analysis indicates that ATN chiefly corrects over-confident logits in low-shot regimes, leaving inter-class margins largely unchanged. Although our contribution is modular and easy to plug into existing pipelines, its theoretical justification is limited: we provide only intuitive arguments and empirical correlation rather than generalization bounds. Code and trained checkpoints are publicly available.",
    "id": 230
  },
  {
    "title": "Gradient Norm Annealing: A Simple Trick for Better Generalization in Stochastic Optimization",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Roberts, C."
    ],
    "abstract": "We propose Gradient Norm Annealing (GNA), a lightweight modification to standard stochastic optimizers that periodically rescales gradients based on their historical norms. Motivated by the observation that flat minima often correlate with improved generalization, GNA adjusts the effective learning rate for each parameter proportionally to the exponential moving average of past gradient norms. Unlike adaptive methods such as Adam or RMSProp, GNA retains the sign and direction of stochastic gradients while only modulating their magnitude, leading to minimal overhead and compatibility with existing architectures. On CIFAR-10/100 and ImageNet, GNA improves test accuracy by 0.4\u20130.9 pp over vanilla SGD with momentum at no additional compute cost, and matches or slightly outperforms strong baselines including SAM and SWA. Averaged across eight diverse tasks\u2014from language modeling on WikiText-2 to reinforcement learning on Atari\u2014the gains are smaller (+0.2 pp) but consistent. Theoretical analysis in a quadratic model suggests GNA encourages convergence to wider valleys, yet the effect vanishes under high label noise. Code is publicly available. While the contribution is incremental, extensive ablations and a simple implementation may prove useful to practitioners.",
    "id": 235
  },
  {
    "title": "LoRa-\u0394: Partial Low-Rank Adaptation for More Parameter-Efficient Fine-Tuning",
    "authors": [
      "Nguyen, T.",
      "Kumar, V.",
      "Chen, S.",
      "Garcia, J."
    ],
    "abstract": "We study whether the rank of LoRA adapters can be reduced further without hurting downstream performance. Motivated by the observation that only a subset of layers undergo large distributional shift during fine-tuning, we propose LoRa-\u0394, a simple variant that applies low-rank updates to a data-dependent fraction of transformer blocks. A lightweight importance score, computed from the Fisher information of each layer on a small validation split, selects the target blocks; the remaining layers are frozen. On GLUE and NLG benchmarks, LoRa-\u0394 retains 96-99 % of full-LoRA quality while training up to 27 % fewer parameters. Ablation shows that layer selection matters more than rank allocation, and that a single shared rank is often sufficient. Although our gains are incremental, the method is implementation-trivial, introduces no hyper-parameters beyond a sparsity threshold, and is orthogonal to other efficiency techniques. We release PyTorch code and adapters to facilitate reproduction.",
    "id": 236
  },
  {
    "title": "Momentum Without the Mass: Memory-Free Weight Interpolation for Online Continual Learning",
    "authors": [
      "Fernandez, L.",
      "Zhao, K.",
      "Nguyen, P."
    ],
    "abstract": "Continual learning methods typically rely on replay buffers or regularization to mitigate catastrophic forgetting, both of which incur memory or compute overheads that scale with task sequence length. We propose Streaming Weight Averaging (SWA), an embarrassingly simple alternative that keeps only a single exponentially-weighted copy of the parameters and uses no replay data. SWA interpolates between current and past weights with a scalar momentum coefficient that is annealed according to a schedule we derive from a stochastic linearization of the loss surface. On standard benchmarks such as Split-CIFAR-100 and CORe50, SWA matches or marginally outperforms rehearsal-based baselines while storing 0\u00d7 examples; on larger-scale ImageNet-1k sequences it lags the best buffer approaches by 2\u20134 % accuracy but still outperforms regularization-only methods. Theoretically, we bound the forgetting gap under convexity assumptions and relate the schedule to the task switch rate. Although our analysis relies on approximate quadratic loss surfaces and does not yet extend to modern deep architectures, SWA offers practitioners a parameter-only baseline that can be implemented in 5 lines of code. Code is available at anonymous URL.",
    "id": 240
  },
  {
    "title": "Improved Convolutional Filters via Learnable Pixel Permutations",
    "authors": [
      "Kumar, S.",
      "Ortega, M.",
      "Zhou, J."
    ],
    "abstract": "Convolutional Neural Networks (CNNs) achieve translation equivariance by sharing filters across spatial locations, yet this very weight sharing prevents them from adapting to non-stationary image statistics such as slowly varying lighting or texture gradients. We propose PermuteConv, a lightweight module that learns a smooth, input-dependent pixel permutation to re-order spatial locations before standard convolution. A small auxiliary network predicts permutation parameters from local neighborhoods; these parameters are constrained to form a diffeomorphic warp so that gradients flow reliably and no costly permutation matrix is stored. On CIFAR-100 and ImageNet our ResNet-50 equipped with PermuteConv yields +0.7% and +0.4% top-1 accuracy with <1% parameter overhead. Ablation shows gains concentrate on images with non-uniform illumination. While the improvement is incremental and wall-clock time rises by 8%, PermuteConv is the first module that allows convolutions to adapt spatial sampling without abandoning weight sharing, providing a plug-and-drop replacement for standard layers. Code is available at anonymous-url.",
    "id": 242
  },
  {
    "title": "Revisiting Gradient Clipping in Private SGD: A Second-Order Perspective",
    "authors": [
      "Koh, J.",
      "Singh, A.",
      "Morrison, D."
    ],
    "abstract": "Gradient clipping is a standard ingredient in differentially-private stochastic optimization, yet its interaction with curvature information remains poorly understood. We introduce Curvature-Aware Clipped SGD (CAC-SGD), a variant that adapts clipping thresholds using local second-order statistics estimated by the Hutchinson method. On logistic regression and small-scale vision tasks, CAC-SGD yields privacy\u2013utility curves within 1.5% of the unclipped baseline while spending 20\u201335% less privacy budget. Theoretically, we bound excess empirical risk as  \u00d5(\u221arank(H)/\u03b5n) under (\u03b5,\u03b4)-DP, where H is the averaged Hessian. Although our bound does not improve the worst-case rate, empirical Hessian spectra on real datasets show that rank(H) can be an order of magnitude smaller than dimension, suggesting tighter instance-dependent guarantees. A distributed implementation in JAX requires only a constant-factor overhead in wall-clock time. Limitations include scalability to very large models (>200M parameters) and reliance on subsampling-based curvature estimates that may amplify privacy noise. We release code and anonymized checkpoints.",
    "id": 248
  },
  {
    "title": "AdaSmooth: Adaptive Label Smoothing with Gradient Feedback for Calibration",
    "authors": [
      "Nguyen, K.",
      "Chen, L.",
      "Garcia, M."
    ],
    "abstract": "Label smoothing improves calibration and generalization by softening one-hot targets, yet the optimal smoothing rate remains data- and model-dependent. We propose AdaSmooth, a plug-in method that adjusts per-sample smoothing on-the-fly using gradient feedback from a held-out calibration mini-batch. During training, a light-weight meta-network predicts sample-wise smoothing magnitudes; the meta-parameters are updated by minimizing the expected calibration error on the calibration set, propagated through the base model via implicit differentiation. Experiments on CIFAR-100 and ImageNet with ResNet-50/101 show that AdaSmooth yields modest gains in top-1 accuracy (+0.3%) and reduces expected calibration error by 5\u20138% relative to uniform label smoothing. Ablations indicate that the calibration set can be as small as 5% of the training data without hurting performance. While the approach is applicable to arbitrary architectures, computational overhead amounts to \u224815% longer training time. Theoretically, we bound the generalization gap under adaptive smoothing by extending uniform-stability arguments; however, the bound scales with the calibration set size and is loose in practice. Code is made available.",
    "id": 252
  },
  {
    "title": "Adaptive Gradient Clipping with Local Lipschitz Estimates Can Improve GAN Training",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "Training GANs remains challenging due to instability from unbounded gradients and rapidly changing discriminators. We propose a simple modification to Adam\u2014termed LiGradClip\u2014that clips gradients using online estimates of the local Lipschitz constant of the loss surface. Empirically, LiGradClip reduces catastrophic gradient spikes by 34% on CIFAR-10 and lowers FID on CelebA-64 from 13.2 to 11.8 versus standard Adam with fixed clipping. Although our method adds two hyper-parameters (window size and percentile), we provide a heuristic grid-search routine that completes in <20% of total training time. Theoretically, we show LiGradClip converges at the same O(1/\u221aT) rate as Adam for smooth non-convex objectives. Ablation studies indicate that local estimation is crucial; fixed clipping actually degrades FID when the discriminator capacity is high. While our gains are incremental and mostly demonstrated on smaller datasets, LiGradClip requires minimal code changes and could be complementary to recent regularization techniques. We release PyTorch code to reproduce all experiments.",
    "id": 259
  },
  {
    "title": "Improved Margin-Based Bounds via Data-Dependent Partitioning",
    "authors": [
      "Nguyen, T.",
      "Kumar, A.",
      "Chen, Y."
    ],
    "abstract": "We revisit margin-based generalization bounds for multiclass classifiers and propose a simple data-dependent partition scheme that tightens classical bounds without introducing extra hyper-parameters. By clustering training features and separately bounding the complexity within each cluster, we obtain a bound that scales with the margin of the worst cluster rather than the entire dataset. The approach applies to any margin-loss and yields closed-form expressions for linear and kernel machines. On twelve UCI benchmarks we achieve $3$\u2013$7\\%$ tighter bounds over the uniform-margin baseline; on CIFAR-10 and ImageNet subsets the improvement reaches $12\\%$ at moderate network width. Although the gain vanishes when the feature space is nearly homogeneous, the method runs in $<5\\%$ of the training time and provides inexpensive side information during architecture search. While our bound is still looser than state-of-the-art PAC-Bayesian bounds, it requires no posterior optimization and may offer practitioners a lightweight sanity check before committing to expensive retraining.",
    "id": 261
  },
  {
    "title": "Improved Gradient Bounds for Noisy Curriculum Learning via Sample-Dependent Smoothing",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Gao, Y."
    ],
    "abstract": "Training deep networks on datasets with gradually increasing difficulty has empirically been shown to improve final accuracy. Despite recent work, theoretical justification is scarce: existing analyses either assume bounded gradient noise\u2014a condition regularly violated in practice\u2014or invoke growth conditions that make generalization bounds vacuous for early curriculum rounds. We propose a new sample-dependent smoothing scheme that adaptively re-weights the contribution of each example based on its current estimated hardness. By coupling this re-weighting with a time-varying step-size schedule, we derive the first convergence guarantee of stochasticastic curriculum learning under a convex relaxation of deep networks. On ImageNette and three synthetic hard-positive tasks, our method reduces training error by 4\u20137% and yields marginally better calibration. While the gradient bounds remain looser than those available for vanilla SGD on clean datasets, our analysis avoids the strong Lipschitz assumptions typically required in curriculum theory. The results suggest that adaptive smoothing can partially explain curriculum gains, yet scalability to deeper architectures and non-convex landscapes requires additional assumptions. Code is available at https://github.com/nguyent/noisy-curriculum.",
    "id": 266
  },
  {
    "title": "Lipschitz-Regularized PCA for Improved Generalization in Noise-Robust Representation Learning",
    "authors": [
      "Chen, Y.",
      "Rao, P.",
      "Nguyen, K."
    ],
    "abstract": "Principal Component Analysis (PCA) remains a cornerstone of unsupervised representation learning, yet its sensitivity to input perturbations and limited robustness to noise are well-documented. In this paper, we revisit classical PCA through the lens of generalization theory and propose a Lipschitz-regularized variant (L-PCA) that explicitly controls the Lipschitz constant of the projection operator. We present an efficient alternating minimization scheme that jointly optimizes the principal subspace and a data-dependent Lipschitz bound, yielding closed-form updates with only 5% overhead over standard PCA. Theoretically, we derive a new generalization bound that trades reconstruction error against expected worst-case perturbation, and demonstrate that L-PCA achieves a tighter trade-off than the vanilla method. Empirically, L-PCA improves test accuracy by 1.2\u20132.1% on downstream linear classification tasks across four vision and tabular benchmarks when 5% label noise is injected. Ablation studies reveal that the regularizer also reduces eigengap overfitting, suggesting improved stability. Although the gains are incremental on clean data, our results indicate that explicit Lipschitz control offers a lightweight, drop-in enhancement for PCA-based pipelines when robustness is desired. Code is available at anonymized-url.",
    "id": 272
  },
  {
    "title": "Revisiting Momentum with Adaptive Restart: A Practical Acceleration Scheme for Stochastic Optimization",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Chen, S."
    ],
    "abstract": "We propose AR-SGD, a simple variant of stochastic gradient descent that adaptively restarts momentum when the loss plateaus. Motivated by the observation that momentum can overshoot near sharp minima in stochastic settings, AR-SGD monitors the angle between consecutive gradient estimates and resets momentum buffers when this angle exceeds a learned threshold. We provide convergence guarantees for quadratic objectives and demonstrate empirically that AR-SGD accelerates training on CIFAR-10 and ImageNet by 1.1-1.3\u00d7 compared to standard momentum SGD with minimal hyperparameter tuning. While our theoretical analysis is limited to convex problems and does not capture the full benefits observed in deep networks, AR-SGD requires only 3 lines of code change to existing optimizers and introduces negligible overhead (<0.5%). Experiments on language modeling show mixed results, suggesting the technique may be less effective when gradient noise is dominated by batch statistics rather than local curvature. Our method offers a practical middle ground between adaptive methods like Adam and classical momentum, though we acknowledge that the improvement margins are modest and may not justify adoption in all settings.",
    "id": 273
  },
  {
    "title": "Improved Gradient Bounds for Unrolling with Learned Optimization Step Sizes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient-based meta-learning algorithms often rely on unrolling optimization procedures to compute higher-order derivatives, but this can lead to exploding or vanishing gradients when many steps are used. We propose a simple modification to existing unrolled optimizers where the step sizes are learned as parameterized functions of the current gradient history, allowing the optimizer to adaptively shrink gradients in unstable regions while preserving informative updates elsewhere. Our approach only adds 5% overhead in compute time compared to standard unrolling and can be implemented in 20 lines of PyTorch code. Experiments on few-shot image classification benchmarks show modest improvements of 1-2% over baseline unrolling methods on 5-way 5-shot tasks, with more pronounced gains in 20-shot settings. We provide theoretical analysis showing that our learned step sizes effectively bound gradient norms under standard smoothness assumptions, though our bounds are looser than recent work that uses carefully engineered clipping schemes. While our method does not achieve state-of-the-art performance compared to more sophisticated meta-learning approaches, it offers a practical drop-in replacement for unrolling that improves stability without requiring extensive hyperparameter tuning. Code is available at anonymized-url.github.io.",
    "id": 284
  },
  {
    "title": "Memory-Efficient Training of Transformers via Gradient Checkpointing with Learned Activation Recomputation",
    "authors": [
      "Chen, K.",
      "Alvarez, M.",
      "Singh, P."
    ],
    "abstract": "We present a method for reducing the memory footprint of transformer training while maintaining computational efficiency. Our approach combines gradient checkpointing with a learned recomputation policy that selectively caches intermediate activations based on their estimated recomputation cost. The key insight is that certain activation tensors in transformer blocks exhibit predictable structure, allowing accurate approximation during the backward pass. We train a lightweight neural network to predict which activations to cache, minimizing a cost function that balances memory savings against extra computation. Experiments on language modeling and machine translation tasks show 25-40% memory reduction with <15% training slowdown compared to standard training. While our method underperforms the strongest baseline system (activation compression) in some settings, it requires no architectural modifications and generalizes across model sizes. Code is provided, though full reproduction of large-scale experiments requires substantial compute resources. Our work suggests that learned policies can improve memory efficiency, though the overhead may limit benefits for smaller models.",
    "id": 285
  },
  {
    "title": "Gradient Descent with Lookahead Meets Online Mirror Descent: A Modular Analysis",
    "authors": [
      "Chen, L.",
      "Krishnan, V.",
      "Gomez, S."
    ],
    "abstract": "We reinterpret the recently proposed Lookahead optimizer as a two-time-scale online learning procedure, and use this lens to derive convergence guarantees for smooth non-convex objectives. By casting the fast inner loop as an online mirror descent player that best-responds to a slow-moving predictor, we obtain a regret bound of order O(\u221aT) that translates, via standard online-to-batch conversion, to a 1/\u221aT stationarity rate. We then show that inserting a fixed, constant-length lookahead window inside Nesterov-type acceleration preserves this rate while reducing the provable dependence on from 1/(1-\u03b2) to 1/(1-\u03b2\u00b2). Experiments on CIFAR-10/100 and small-scale ImageNet reveal speed-ups of 3\u20136% over vanilla momentum and 1.5% over AdamW, with comparable final accuracy. While the gains are modest, our framework unifies several existing acceleration tricks under a single regret bound and offers a simple Python wrapper that works with any base optimizer. We discuss limitations\u2014including increased memory footprint and sensitivity to window size\u2014and provide full source code and hyper-parameter sweeps to encourage reproducibility.",
    "id": 291
  },
  {
    "title": "LoCo-ViT: Low-Rank Compression of Vision Transformers with Learnable Token Routing",
    "authors": [
      "Chen, L.",
      "Vaswani, A.",
      "M\u00fcller, H."
    ],
    "abstract": "Vision Transformers (ViTs) deliver impressive accuracy yet remain bottlenecked by quadratic self-attention complexity and large parameter counts. We propose LoCo-ViT, a two-stage compression pipeline that jointly learns (i) low-rank projections of attention weights and (ii) a lightweight routing module that skips entire blocks for uninformative tokens. Starting from a pre-trained ViT, we interleave each attention layer with rank-r bottleneck projections and train a differentiable gating network to drop up to 30 % of tokens per block while maintaining task-specific rewards. On ImageNet-1k, LoCo-ViT yields 1.8\u00d7 FLOPs reduction and 2.3\u00d7 throughput gain versus the base DeiT-Small with only 0.7 % top-1 accuracy loss. Ablations show that low-rank constraints account for two-thirds of the savings, while learned routing provides the rest. Although our method scales to larger models, gains saturate beyond 224\u00d7224 inputs, and downstream transfer to COCO detection underperforms prior static pruning baselines by 1.1 mAP. Nevertheless, LoCo-ViT offers a simple, training-efficient recipe for on-device deployment of vision transformers when extreme compression is more critical than final accuracy.",
    "id": 296
  },
  {
    "title": "Revisiting Regularization Paths with Adaptive Shrinkage for Over-parameterized Models",
    "authors": [
      "Kim, H.",
      "Rojas, C.",
      "Singh, V."
    ],
    "abstract": "We study the trajectory of iterative weight decay in over-parameterized networks and propose Adaptive Shrinkage (\u03b1-shrink), a simple modification that rescales the penalty by the current parameter norm. Empirically, \u03b1-shrink accelerates early-phase convergence and yields 1\u20132% test-error reductions on CIFAR-10/100 and ImageNet compared to standard weight decay when training ResNet and Vision Transformer families. Theoretically, we show that \u03b1-shrink is equivalent to a time-varying L2 penalty whose limit coincides with ridge regression in a tractable two-layer linear network; the derived closed-form solution predicts when adaptive shrinkage outperforms fixed regularization. Ablation studies reveal most benefits appear when labels are noisy or when data are few, suggesting \u03b1-shrink operates as a principled form of robust regularization. Code is available at anonymized-link.",
    "id": 297
  },
  {
    "title": "Improved Generalization via Gradient Norm Clipping with Scheduled Thresholds",
    "authors": [
      "Liu, J.",
      "Martinez, C.",
      "Vaser, R."
    ],
    "abstract": "We revisit gradient-norm clipping as a simple yet under-explored technique for enhancing generalization in over-parameterized networks. Motivated by recent empirical observations that stochastic-gradient noise correlates with the relative change in test error, we propose an adaptive clipping schedule that monotonically reduces the clipping threshold during training. Using a synthetic noisy-quadratic benchmark, we demonstrate that the schedule can recover the generalization benefits of full-batch gradient descent while retaining the optimization speed of mini-batch SGD. On CIFAR-10/100 and ImageNet-sub, our method matches the accuracy of strong data-augmentation baselines with 20\u201335% fewer parameter updates, ablating competitively against a well-tuned cosine-learning-rate schedule. Theoretical insight is provided via a stability bound that depends on the cumulative clipped gradient norm. Although our bound does not yet explain the full empirical gains, we believe the results highlight scheduled clipping as a lightweight alternative to more complex optimizers. Code is available at anon-not-provailed.github.io/adaclip.",
    "id": 299
  },
  {
    "title": "Gradient Alignment for Improved Transfer Learning in Limited-Label Settings",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rosenfeld, A."
    ],
    "abstract": "Transfer learning has achieved remarkable success across domains, yet its effectiveness diminishes when only a handful of target labels are available. We propose Gradient Alignment Transfer (GAT), a lightweight method that reweights source-domain gradients during fine-tuning to better match the geometry of the target loss landscape. GAT computes gradient similarity scores via an efficient approximation to the Fisher Information Matrix, requiring no additional forward passes. On five standard vision benchmarks with 5\u201320 labels per class, GAT improves average accuracy by 2.3% over standard linear probing and 1.1% over full fine-tuning, while adding under 5% computational overhead. Although the gains are modest and tasks are limited to small-scale vision datasets, GAT introduces no new parameters and can be dropped into existing pipelines. Extensive ablations reveal that alignment toward the final layers matters most, and that the method degrades gracefully when source and target domains are poorly matched. Our code and checkpoints are publicly available.",
    "id": 303
  },
  {
    "title": "Gradient Surgery with Momentum: A Simple Fix for Multi-Task Learning in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that can impede optimization. While recent works have proposed sophisticated gradient manipulation techniques, we show that a simple modification to standard SGD with momentum can achieve comparable performance. Our method, Momentum-Adjusted Gradient Surgery (MAGS), applies a lightweight projection step that leverages momentum history to resolve gradient conflicts without requiring explicit gradient decomposition. We evaluate MAGS on three standard multi-task benchmarks: NYUv2 semantic segmentation/depth estimation, PASCAL face landmark detection/age estimation, and SURREAL human pose estimation. Results show MAGS achieves competitive performance (\u00b11.2% of state-of-the-art) while reducing computational overhead by 37% compared to PCGrad and 52% compared to CAGrad. However, we observe that MAGS struggles in settings with high task imbalance and provides diminishing returns for tasks with aligned gradients. Our extensive ablations reveal that momentum history primarily helps in early training, with benefits tapering off after 25-30 epochs. While our solution is simple and effective for moderate-scale multi-task problems, we acknowledge it may not address fundamental limitations of gradient-based multi-task optimization. We provide PyTorch code for reproducibility.",
    "id": 308
  },
  {
    "title": "Gradient Surgery for Stale Momentum: A Lightweight Framework for Asynchronous Federated Optimization",
    "authors": [
      "Chen, L.",
      "Gonzalez, J.",
      "Krishnan, S."
    ],
    "abstract": "Federated learning faces significant performance degradation under asynchronous updates when client updates arrive with varying staleness. We propose StaleMomentum, a gradient clipping technique that reweights momentum terms based on staleness intervals without requiring synchronized clocks or additional communication rounds. Our method introduces an inexpensive correction term derived from a second-order Taylor approximation of the staleness error, allowing clients to locally compensate for outdated momentum states. On benchmark datasets (CIFAR-10, FEMNIST), StaleMomentum improves convergence by 12-18% over standard asynchronous FedAvg while adding negligible computational overhead (<3%). However, gains diminish under extremely heterogeneous data partitions (>80% non-IID), suggesting the technique is most beneficial in moderate staleness regimes. Theoretical analysis provides convergence guarantees with weakened assumptions compared to prior work, though our bounds remain looser than synchronous counterparts. Code is available at [link].",
    "id": 311
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient compression is essential for distributed training at scale, but existing methods rely on hand-crafted schedules that may not adapt to varying training dynamics. We propose Adaptive Quantized SGD (AQ-SGD), which learns compression parameters jointly with model parameters using a meta-learning approach. By framing quantization levels as continuous variables that are periodically projected onto discrete values, our method discovers instance-specific compression schedules without communicating additional metadata. Experiments on CIFAR-10 and ImageNet with ResNet-50 show AQ-SGD achieves 40\u00d7 gradient compression while maintaining within 1.2% of baseline accuracy, matching state-of-the-art static methods on most tasks. However, we observe instabilities on Transformer architectures and convergence issues when batch sizes are small, suggesting our current formulation may not capture all necessary dynamics. Theoretical analysis proves convergence to stationary points under bounded gradient assumptions, but requires stronger conditions than uncompressed SGD. Our method shows promise for adaptive distributed training, though practical deployment may require additional stabilization techniques for certain architectures.",
    "id": 322
  },
  {
    "title": "Gradient Descent with Memory-Efficient Momentum via Taylor Approximation",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Liu, S."
    ],
    "abstract": "We propose a memory-efficient variant of momentum-based gradient descent that reduces storage requirements by approximately 50% for large neural networks without significant performance degradation. Our key insight is to approximate the historical gradient information using a second-order Taylor expansion around an exponentially-decaying basis, eliminating the need to store full gradient histories. We theoretically analyze the convergence properties under smoothness assumptions similar to standard momentum, showing an O(1/\u221aT) convergence rate. Experiments on ResNet-50 and ViT-B/16 across CIFAR-10, ImageNet-1k, and GLUE benchmarks demonstrate comparable accuracy to traditional momentum (within 0.5-1.2% on average) while halving memory usage. However, we observe increased sensitivity to hyperparameter selection and occasional instability on deeper networks. While the method provides practical memory benefits for resource-constrained training, particularly for larger batch sizes, the theoretical approximation introduces limitations on general convergence guarantees. Code will be made available.",
    "id": 329
  },
  {
    "title": "Improved Convergence Rates for SGD with Time-Varying Step Sizes via Loss-Dependent Bounds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We provide tighter convergence guarantees for stochastic gradient descent (SGD) when the loss function exhibits certain regularity properties. Motivated by the empirical success of cosine annealing schedules in deep learning, we derive non-asymptotic bounds for general time-varying step sizes that depend on the accumulated gradient noise rather than worst-case quantities. Our analysis combines traditional martingale techniques with a novel loss-dependent decomposition of the update rule, yielding rates that can be significantly better than the standard O(1/\u221aT) when the training loss decreases quickly. For overparameterized linear regression, we obtain an explicit O(1/T\u00b2) rate under mild assumptions on the data. While our theoretical results are restricted to convex losses, experiments on ResNet-18 training with CIFAR-10 demonstrate 5-10% faster convergence compared to standard schedules when using our theoretically-motivated decay parameters. However, our bounds become vacuous for highly non-convex settings typical in modern deep learning, and our theoretical contributions are incremental rather than transformative.",
    "id": 338
  },
  {
    "title": "Improved Convergence Rates for Stochastic Gradient Descent with Adaptive Polyak Step-Size",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We revisit the classical Polyak step-size for stochastic gradient descent (SGD) and propose a simple adaptive variant that achieves improved convergence rates under standard assumptions. While the original Polyak step-size requires knowledge of the optimal function value, our method estimates this quantity online using a running average of past losses. We establish O(1/T) convergence for convex Lipschitz functions and O(log T/T) for strongly convex cases, improving upon prior adaptive methods by logarithmic factors. Experimental results on logistic regression and neural network training demonstrate modest but consistent improvements over Adam and SGD with cosine annealing. However, empirical gains diminish in overparameterized settings, suggesting the theoretical analysis may not fully capture the practical benefits. The proposed method introduces negligible computational overhead and is particularly effective for ill-conditioned problems. While our contribution is largely incremental, it bridges an important gap between theoretical guarantees and practical step-size selection. Code is available at anonymous-url.",
    "id": 340
  },
  {
    "title": "Efficient Low-Rank Adaptation via Structured Kronecker Products",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a popular approach for parameter-efficient fine-tuning of large language models. We propose Kronecker-LoRA, which replaces the standard low-rank decomposition with a structured Kronecker product to reduce memory footprint and speed up training. Our method decomposes the adaptation matrix into a product of smaller Kronecker factors, achieving a compression ratio of up to 5\u00d7 compared to standard LoRA while maintaining competitive performance. On a benchmark of 5 diverse NLP tasks, Kronecker-LoRA achieves within 1.2% of LoRA's downstream performance while using 40% fewer parameters during training. However, we find the approach is sensitive to the choice of Kronecker factor shapes and exhibits instability on smaller models when the intrinsic dimension becomes too low. Theoretical analysis shows our decomposition reduces the effective rank bound, which may explain the observed limitations. Despite these challenges, Kronecker-LoRA offers a practical alternative for deployment scenarios where memory constraints are severe.",
    "id": 347
  },
  {
    "title": "Self-Refining without Self-Critique: Improving Language Model Alignment through Iterative Constrained Decoding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Thompson, K."
    ],
    "abstract": "We propose an alternative to reinforcement learning from human feedback (RLHF) for aligning large language models that leverages constrained decoding during inference-time refinement. Our method, Iterative Constrained Alignment (ICA), generates candidate responses through beam search with dynamically adjusted constrained sampling, eliminating the need for reward model training or rejection sampling. We demonstrate that ICA achieves comparable performance to PPO-based RLHF on helpfulness benchmarks while requiring only 15% of the computational resources during training. Across three domains (instruction following, summarization, and mathematical reasoning), ICA matches or slightly improves upon RLHF baselines on automated metrics (MT-Bench: +0.1, GSM8K: +2.3%), though we observe higher variance in human evaluations and reduced performance on longer-context tasks. Analysis reveals ICA's effectiveness stems from constraining the initial generation space rather than learning policy updates, suggesting trade-offs between training efficiency and response diversity. While our method reduces computational overhead and sidesteps reward hacking commonly associated with RLHF, we acknowledge limitations including increased inference latency and sensitivity to constraint hyperparameters. Code and models will be released upon publication.",
    "id": 349
  },
  {
    "title": "Gradient Surgery Revisited: Do Adaptive Optimizers Really Benefit from Task-Aware Gradient Modification?",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task optimization often suffers from conflicting gradient directions across tasks, leading to suboptimal performance. Recent work has proposed various gradient surgery techniques that modify gradients to resolve conflicts, but these methods are typically evaluated with standard SGD. We investigate whether these techniques remain beneficial when combined with adaptive optimizers like Adam, which already perform implicit gradient rescaling. Through extensive experiments on standard multi-task benchmarks including CIFAR-MTL and NYUv2, we find that gradient surgery provides modest improvements over tuned Adam baselines (2.3% average gain), but often underperforms compared to the same surgery applied to SGD (6.8% gain). Our analysis reveals that adaptive optimizers partially mitigate conflicting gradients through their per-parameter learning rates, reducing the efficacy of explicit gradient modification. While we confirm previous findings that gradient surgery helps SGD navigate multi-task loss landscapes, our results suggest practitioners using modern adaptive optimizers may not need these additional modifications, calling into question the widespread recommendation to apply gradient surgery techniques regardless of optimizer choice.",
    "id": 351
  },
  {
    "title": "Gradient Surgery Without the Surgery: A Lightweight Approach to Multi-Task Gradient Conflict Resolution",
    "authors": [
      "Liu, Q.",
      "Andersson, K.",
      "Chen, Z."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts where tasks compete for shared representations, leading to degraded performance. Existing methods like PCGrad and GradNorm resolve conflicts through computationally expensive gradient projections or complex reweighting schemes. We propose a surprisingly simple alternative: instead of manipulating gradients directly, we identify conflicting directions via cosine similarity and apply learned scaling factors to the loss terms themselves. This approach requires only O(n) additional parameters for n tasks and avoids gradient projection computations entirely. Extensive experiments on three computer vision benchmarks (NYUv2, CityScapes, and CelebA) demonstrate our method achieves competitive performance (within 2% of PCGrad) while reducing training time by 15-30%. While our theoretical analysis is limited to the two-task case and we find the method struggles when task correlations are extremely negative, empirical results suggest this lightweight approach provides a practical middle ground between naive multi-task learning and more sophisticated gradient surgery techniques. Code and pre-trained models will be made available.",
    "id": 355
  },
  {
    "title": "Gradient Surgery for Stabilizing Auxiliary Loss Learning in Sparse Reward Environments",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning with auxiliary objectives has shown promise for improving sample efficiency in reinforcement learning with sparse rewards. However, we observe that naively combining policy gradient updates with auxiliary losses often leads to destructive interference, particularly in high-dimensional action spaces. We propose Gradient Surgery for Auxiliary Loss stabilization (GSAL), a simple method that projects auxiliary gradients onto the null space of the policy gradient before applying updates. Our theoretical analysis shows that GSAL preserves the stationary points of the primary objective while incorporating useful curvature information from auxiliary tasks. We evaluate GSAL on a suite of continuous control tasks with sparse rewards, achieving 12-15% improvement over baseline methods on standard benchmarks. While the improvements are consistent across environments, we find that GSAL's effectiveness diminishes when auxiliary tasks are poorly aligned with the primary objective. The method requires minimal hyperparameter tuning and adds less than 5% computational overhead compared to standard policy gradient methods. Our results suggest that careful gradient alignment can provide modest but reliable gains in challenging RL settings, though the approach may be constrained by the quality of available auxiliary objectives.",
    "id": 369
  },
  {
    "title": "Attention Bottlenecks in Transformer Memorization: A Low-Rank Analysis of Inductive Biases",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Thompson, E."
    ],
    "abstract": "We investigate how transformers memorize training sequences through analysis of attention patterns in overparameterized language models. Our theoretical analysis shows that memorization occurs through rank collapse in attention matrices, with rank dropping from O(d) to O(k) where k is the number of memorized patterns. We propose a low-rank regularization scheme that encourages rank collapse during training while maintaining downstream performance. Experiments on synthetic memotasks and WikiText-103 show our method reduces memorization by 15-20% without hurting perplexity, suggesting some memorization may be beneficial for generalization. However, our regularization does not improve performance on established benchmarks like GLUE. While our theoretical analysis provides insight into transformer memorization dynamics, our empirical evaluation is limited to language modeling and may not extend to other modalities. Code and experiments are available at our anonymized repository.",
    "id": 373
  },
  {
    "title": "Adaptive Curriculum Learning for Few-Shot Classification via Task Similarity Clustering",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Curriculum learning has shown promise in few-shot classification by ordering tasks from simple to complex. However, existing approaches rely on heuristic difficulty metrics that remain fixed across datasets. We propose a novel adaptive curriculum learning method that clusters tasks based on their similarity in prototype space and dynamically adjusts the learning schedule. Our approach first learns task embeddings using a siamese network trained on query-support similarities, then applies spectral clustering to group related tasks. During meta-training, we gradually introduce harder clusters based on inter-cluster distance and intra-cluster variance. Experiments on mini-ImageNet, tiered-ImageNet, and CUB-200 demonstrate 2-3% improvements over baseline MAML and ProtoNets. While the method shows consistent gains across datasets, the clustering approach sometimes over-segments natural task groupings, and performance gains diminish when the number of classes per task increases beyond 5-way. Our findings suggest that adaptive scheduling via task similarity is beneficial but may be fundamentally limited by the quality of task representations learned during few-shot training.",
    "id": 375
  },
  {
    "title": "Reparameterized Momentum: Adapting Polyak's Heavy Ball to Modern Deep Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We revisit Polyak's classical Heavy Ball momentum method and propose Reparameterized Momentum (RepMom), a simple modification that adapts the momentum coefficient based on the relative scale of gradient norms across layers. Our key insight is that the optimal momentum parameter varies significantly across different magnitude scales in deep networks, leading to suboptimal convergence when using a global value. RepMom employs a lightweight heuristic that adjusts momentum per-parameter using running estimates of gradient statistics, without introducing additional hyper-parameters beyond those in standard SGD with momentum. We evaluate RepMom on image classification benchmarks (CIFAR-10/100, ImageNet) and language modeling tasks, showing 3-8% improvement in final accuracy over tuned baselines, particularly for deeper architectures. While we provide convergence guarantees for quadratic objectives under simplified assumptions, we acknowledge these results do not extend to the general non-convex case used in our experiments. Our method adds minimal computational overhead (5-10% increase in iteration time) and can be implemented in 15 lines of PyTorch code. Despite improvements over baselines, the gains are moderate and may be partially attributable to improved hyper-parameter sensitivity rather than fundamental algorithmic innovation.",
    "id": 376
  },
  {
    "title": "Improved Gradient Variance Bounds for Distributed Stochastic Optimization via Adaptive Batch Synchronization",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "We present a distributed optimization framework that adaptively adjusts batch synchronization frequency to minimize gradient variance in stochastic gradient descent. While existing distributed methods typically use either synchronous or asynchronous updates, we propose a hybrid approach that dynamically switches between these modes based on local gradient statistics. Our method maintains a running estimate of gradient variance at each worker and triggers synchronization only when local estimates exceed a data-dependent threshold. This yields improved convergence rates compared to standard synchronous SGD while avoiding the instability issues common in fully asynchronous methods. On CIFAR-10 and ImageNet training tasks, our approach achieves 1.2-1.7\u00d7 speedup over baseline synchronous methods with comparable generalization. Theoretical analysis shows our variance bounds degrade gracefully with network delays, though we rely on assumptions about gradient Lipschitz continuity that may not hold in practice. Our results suggest adaptive synchronization can reduce communication overhead in moderately-sized clusters (\u226432 workers), though benefits diminish as network heterogeneity increases.",
    "id": 387
  },
  {
    "title": "Improving Transformer Efficiency through Selective Layer Dropping during Training",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, M.",
      "Johnson, A."
    ],
    "abstract": "We propose LayerDrop-Train, a simple method to reduce computational costs when training large transformers by dynamically dropping intermediate layers based on learned gating parameters. Unlike existing pruning approaches that operate post-training, our method progressively learns which layers to skip during the forward pass, effectively creating shorter paths through the network. We introduce a soft gating mechanism trained with straight-through gradient estimation that determines layer usage per sequence. Experiments on standard NLP benchmarks (GLUE, WMT) show 15-25% training time reduction with minimal performance degradation (within 1-2% of baseline). Analysis reveals our method primarily drops middle layers, suggesting redundancy in standard transformer architectures. While our approach provides practical training speedups for modest model sizes (\u2264350M parameters), we observe training instability for larger models and find the benefits diminish with increased parallelization. Code and pre-trained models will be available upon acceptance.",
    "id": 397
  },
  {
    "title": "Gradient Surgery in Federated Learning: When Do Top Clients Really Help?",
    "authors": [
      "Liu, Q.",
      "Garcia, M.",
      "Kim, J."
    ],
    "abstract": "Federated learning systems traditionally aggregate client gradients uniformly, but emerging work suggests that prioritizing updates from high-performing clients might accelerate convergence. We investigate this phenomenon through the lens of gradient interference, proposing a simple client selection mechanism that weights updates by their historical training loss reduction. Our method requires no additional communication rounds and introduces minimal computational overhead compared to standard FedAvg. We evaluate our approach across vision and language tasks on heterogeneous data partitions, observing 5-12% faster convergence in early training phases compared to baselines. However, we find these gains diminish as training progresses, with final test accuracy differences becoming statistically insignificant on 7 out of 9 datasets. Our theoretical analysis reveals that the benefit of client selection is fundamentally limited by the alignment of local and global objectives\u2014a condition that becomes less favorable with increased data heterogeneity. While our method provides practical benefits in specific settings (particularly when client participation rates are low), our results caution against the universal applicability of client prioritization strategies. Code will be provided for reproducibility.",
    "id": 402
  },
  {
    "title": "LoRA-FA: Low-Rank Adaptation with Feature Alignment for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Singh, A."
    ],
    "abstract": "We propose LoRA-FA, a simple modification to Low-Rank Adaptation (LoRA) that improves performance on downstream tasks by aligning low-rank adapter features with pre-trained representations. While LoRA has become popular for parameter-efficient fine-tuning, we observe that learned adapter directions often diverge from semantically meaningful directions in the pre-trained model. LoRA-FA addresses this by adding a lightweight feature alignment loss that encourages adapter features to align with the span of top singular vectors from pre-trained layers. Our method introduces only 3% additional parameters compared to standard LoRA and requires minimal computational overhead. We evaluate LoRA-FA on GLUE, SuperGLUE, and vision-language tasks, showing average improvements of 1.2% over standard LoRA, with particularly strong gains on low-resource datasets. However, we find the benefits diminish for larger models (>30B parameters) and saturate quickly with increased adapter rank. While the feature alignment constraint slightly reduces training efficiency (10% slower convergence), it provides more stable fine-tuning across hyperparameters. Our experiments suggest LoRA-FA offers modest but consistent improvements for practical deployment scenarios, though we acknowledge the gains are incremental rather than transformative.",
    "id": 406
  },
  {
    "title": "Rethinking Dropout in Transformers: A Frequency-Domain Perspective",
    "authors": [
      "Liu, K.",
      "Brown, S.",
      "Kumar, V."
    ],
    "abstract": "While dropout is widely used to regularize transformer networks, its interaction with multi-head attention mechanisms remains poorly understood. We propose FreqDrop, a dropout variant that operates in the frequency domain of attention maps rather than the standard element-wise masking. Our key insight is that traditional dropout disrupts low-frequency attention patterns that are crucial for long-range dependencies, while preserving noisy high-frequency components. FreqDrop addresses this by learning to selectively mask frequency bands based on their spectral energy, effectively providing adaptive regularization that varies across layers. Experiments on standard NLP benchmarks (GLUE, SQuAD) show modest improvements of 0.3-0.7% over standard dropout when using large pretrained models, with particularly strong gains on longer sequences (>512 tokens). However, improvements shrink considerably when training from scratch, and computational overhead increases by 15-20% due to FFT operations. While our approach offers a principled view of attention regularization, the practical benefits appear limited to specific regimes. Code and pretrained models are available at anonymized-url.",
    "id": 412
  },
  {
    "title": "Momentum-Scheduled SAM: Improving Sharpness-Aware Minimization with Curriculum-Based Update Magnitudes",
    "authors": [
      "Kim, S.",
      "Rodriguez, A.",
      "Liu, J."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising optimizer for improving generalization, but its computational cost and sensitivity to the perturbation radius \u03c1 remain practical limitations. We propose Momentum-Scheduled SAM (MS-SAM), a lightweight modification that dynamically adjusts \u03c1 using a momentum-based curriculum derived from gradient statistics. Unlike previous work that requires computing additional Hessian information, our scheduler estimates local sharpness through exponential moving averages of gradient norms, eliminating extra backward passes. We evaluate MS-SAM on CIFAR-10/100 and ImageNet, achieving competitive accuracy improvements over vanilla SAM (up to 0.8% on CIFAR-100) while reducing training time by 15-20%. However, we observe diminishing benefits on larger architectures like ViT-L/16 and mixed results on out-of-distribution robustness benchmarks. Our theoretical analysis reveals that MS-SAM converges under assumptions slightly stronger than standard SAM, though these assumptions are verified empirically on image classification tasks. While not a universal improvement, our method provides a computationally efficient alternative for medium-scale vision tasks where training budget is constrained.",
    "id": 434
  },
  {
    "title": "Revisiting Momentum in Stochastic Optimization: A Probabilistic Interpretation with Improved Adaptive Bounds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Momentum-based optimizers remain central to modern deep learning, yet theoretical understanding of their stochastic behavior lags behind empirical success. We propose a probabilistic re-interpretation of Polyak momentum as a Bayesian filtering problem, yielding novel adaptive hyperparameter schedules. Our key insight connects momentum coefficient \u03b2 to the signal-to-noise ratio of mini-batch gradients, enabling dynamic adjustment without additional hyperparameters. Through a second-moment analysis of the filtering equations, we derive improved convergence bounds for non-convex objectives that tighten existing O(1/\u221aT) rates to O(log T/T) under the PL inequality. Experiments on ResNet training demonstrate consistent but modest improvements (0.5-1.2% accuracy) over AdamW and SGD+Momentum across CIFAR-10/100 and ImageNet subsets, with particular gains in low-data regimes. While our theoretical contributions provide new perspective on momentum dynamics, empirical improvements are incremental and sensitive to architecture choices. We opensource our PyTorch implementation for reproducibility.",
    "id": 437
  },
  {
    "title": "Don't Throw Away Your Model Just Yet: Improving Transfer Efficiency with Selective Fine-tuning in Low-Resource Settings",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Pre-trained models offer promise for low-resource tasks, but naive fine-tuning often leads to overfitting and catastrophic forgetting when data is extremely limited. We propose Selective Layer Transfer (SLT), a simple method that identifies and updates only the most relevant layers for a target task while keeping others frozen. Using attention scores from a held-out validation set, SLT adaptively determines which layers contribute most to task performance. We evaluate SLT on 15 NLP benchmarks with 50-500 training examples, comparing against full fine-tuning, adapters, and linear probing. SLT achieves 2-4% improvements over baselines on 8 datasets while training 35% fewer parameters. However, benefits diminish as data increases, and performance plateaus on 3 tasks where full fine-tuning remains superior. Analysis reveals SLT particularly helps when source and target domains differ moderately\u2014not when they are nearly identical or completely distinct. While our method provides practical gains in data-scarce scenarios, theoretical understanding remains limited. Code and hyperparameters are provided for reproducibility.",
    "id": 440
  },
  {
    "title": "LoRA-Prop: Low-Rank Adaptation for Efficient Gradient Propagation in Large Language Models",
    "authors": [
      "Chen, L.",
      "Singh, P.",
      "Garcia, M."
    ],
    "abstract": "While low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, understanding how these low-rank updates affect gradient propagation remains poorly understood. We introduce LoRA-Prop, a theoretical framework that analyzes gradient flow through low-rank adaptation layers using tools from matrix perturbation theory. Our key insight is that LoRA updates create implicit regularization effects that can either help or hinder learning depending on the conditioning of the pre-trained weights. We derive bounds on the gradient norms and provide empirical evidence that standard LoRA initialization can lead to gradient vanishing in deeper layers. To address this, we propose a simple modification to LoRA initialization based on singular value scaling. Experiments on GLUE and SuperGLUE benchmarks show our initialization scheme achieves comparable performance to standard LoRA (average 1.2% improvement) while requiring 15% fewer training steps. However, we find these improvements are inconsistent across tasks and model sizes. Our work provides theoretical grounding for understanding LoRA's training dynamics but leaves open questions about the interplay between rank selection and downstream task complexity.",
    "id": 444
  },
  {
    "title": "Gradient Noise Injection as Implicit Regularization in Overparameterized Neural Networks",
    "authors": [
      "Lee, S.",
      "Kumar, A.",
      "Chen, J."
    ],
    "abstract": "We investigate the connection between noise added during gradient descent and the implicit regularization effects observed in deep neural networks. While previous work has focused on noise added to inputs or labels, we systematically study controlled gradient perturbations introduced during optimization. Our theoretical analysis characterizes the regularization properties for two-layer ReLU networks under Gaussian gradient noise, showing that noise variance is related to an effective L2 regularization term. However, our experiments on CIFAR-10 and ImageNet demonstrate that the benefits diminish for deeper architectures (ResNet50+) and larger datasets. We further propose an adaptive noise scheduling scheme based on gradient norm, which provides modest improvements over SGD with fixed noise levels but underperforms compared to standard regularization techniques like dropout and weight decay. Our results suggest that while gradient noise injection offers theoretical insights into implicit regularization, its practical utility appears limited for modern deep learning applications. Code is available at [URL].",
    "id": 459
  },
  {
    "title": "Temporal Ensembling with Cyclical Confidence Thresholding for Semi-Supervised Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, M.",
      "Johnson, A."
    ],
    "abstract": "Semi-supervised learning has achieved impressive results by leveraging large amounts of unlabeled data, but current approaches often struggle with confirmation bias when pseudo-labels are noisy. We propose a simple yet effective modification to temporal ensembling that introduces cyclical confidence thresholds to selectively incorporate pseudo-labels during training. Our method maintains an exponential moving average of network predictions while dynamically adjusting the confidence threshold based on training progress and prediction entropy. This cyclical schedule allows for more aggressive pseudo-labeling early in training when the model is rapidly improving, followed by conservative refinement stages. We evaluate our approach on CIFAR-10/100 and ImageNet benchmarks, achieving 5-8% improvements over standard temporal ensembling baselines. While our method shows consistent gains on these datasets, theoretical analysis reveals the approach works best when label distributions are balanced and may degrade when class imbalance is severe. Our empirical results suggest the cyclical strategy is more robust to hyperparameter choices than fixed threshold approaches, though we acknowledge computational overhead and memory requirements remain similar to standard ensembling methods. Code is available at [link].",
    "id": 467
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Selective Forgetting",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Gonzalez, A."
    ],
    "abstract": "Multi-task learning faces the fundamental challenge of conflicting gradients between tasks, leading to suboptimal shared representations. While recent gradient surgery methods like PCGrad and GradDrop effectively zero out conflicting components, they discard potentially useful information. We propose AdaSurgery, a memory-augmented approach that retains conflicting gradients in differentiable memory banks, selectively reintroducing them based on task similarity scores computed using small meta-networks. Our method combines sparse gradient projection with adaptive memory recall, weighing previously discarded gradients against current task objectives. Experiments on 3 standard multi-task vision benchmarks (Cityscapes, NYUv2, CelebA) show 2-4% improvements over gradient surgery baselines, with particular gains in data-scarce regimes. However, we observe increased sensitivity to hyperparameter choices and memory bank size. While our adaptive memory mechanism offers a new perspective on gradient conflicts, we acknowledge that the computational overhead (15% slower training) and reliance on additional hyperparameters limit immediate practical adoption. The theoretical analysis of convergence properties remains incomplete for non-convex objectives, though empirical convergence holds across tested scenarios. Code and pretrained models will be released upon acceptance.",
    "id": 471
  },
  {
    "title": "Memory-Efficient Training of Large Language Models Through Selective Weight Freezing",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training large language models (LLMs) remains computationally prohibitive due to memory requirements for gradient storage and optimizer states. We propose Selective Weight Freezing (SWF), a simple yet effective approach that dynamically identifies and freezes weights requiring minimal updates during training, reducing memory overhead without sacrificing convergence. Our method computes Fisher information for parameter importance every k steps and freezes the bottom p% of parameters, allocating resources to more critical weights. Experiments on GPT-2 (1.5B) and LLaMA (7B) models demonstrate 23-31% memory reduction with <1% perplexity degradation on standard NLP benchmarks. While SWF achieves comparable performance to full training on most tasks, we observe increased variance on smaller datasets (WikiText-103) and minor degradation in few-shot learning scenarios. Theoretical analysis shows convergence under mild assumptions, though tighter bounds remain elusive. Our PyTorch implementation requires minimal code changes and integrates with existing training pipelines. While SWF demonstrates practical benefits for memory-constrained training, we acknowledge limitations in extreme fine-grained selection (p<5%) and potential brittleness under aggressive learning rates. Code and hyperparameters are provided for reproducibility.",
    "id": 480
  },
  {
    "title": "ReLoRA: Recursive Low-Rank Adaptation for Multi-Task Learning with Limited Data",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "We present ReLoRA, a parameter-efficient method for multi-task learning that combines recursive low-rank decomposition with gradient accumulation to improve transfer learning when target tasks have limited data. Our approach builds upon LoRA by recursively decomposing the low-rank updates into even smaller matrices, enabling fine-grained sharing across tasks while maintaining computational efficiency. We demonstrate that ReLoRA achieves comparable or superior performance to standard LoRA on 8 diverse NLP benchmarks (GLUE, SuperGLUE), with up to 2.4\u00d7 fewer trainable parameters. Experiments show consistent improvements on low-resource scenarios (\u22641k examples), with average gains of 3.2% over baseline methods. However, we find diminishing returns on high-resource tasks and computational overhead in recursive decomposition stages. While our theoretical analysis provides convergence guarantees under reasonable assumptions, our empirical evaluation is limited to encoder models and English tasks. Our code is available at anonymized-link.",
    "id": 483
  },
  {
    "title": "Feature Recycling: Improving Neural Network Efficiency Through Selective Layer Reinitialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We propose a simple yet surprisingly effective method for improving computational efficiency in training deep neural networks by selectively recycling features from early training stages. Our approach identifies 'stagnant' neurons whose activations change minimally over training epochs, then reinitializes their weights while preserving valuable learned features through a teacher-student distillation process. Experiments on CIFAR-10/100 and ImageNet show 15-25% reduction in training time without significant accuracy loss (\u22640.3% for ResNet-18/50, \u22641.2% for Vision Transformers). While our method works particularly well for vision tasks, we observe limited benefits on language modeling benchmarks - possibly due to the sequential nature of text data. Theoretical analysis proves convergence under simplified convex assumptions, though the general non-convex case remains challenging. Our implementation requires only 50 lines of PyTorch code and introduces minimal hyperparameters, making it practical for practitioners. However, we acknowledge that the accuracy-time trade-offs may not justify deployment in all scenarios, and the technique's benefits diminish with more advanced optimizers like AdamW with cosine scheduling. Code will be released upon acceptance.",
    "id": 487
  },
  {
    "title": "Learning to Prune Adapters: A Gradient-Free Approach for Efficient Multi-Task Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "The proliferation of adapter-based fine-tuning has made it possible to share pretrained models across multiple downstream tasks, but at the cost of increased memory and inference time. We propose Prune-Ada, a simple yet effective method that eliminates adapter parameters without retraining or gradient computation. Our approach uses a novel perturbation-based sensitivity analysis to identify task-specific adapter weights that contribute minimally to the loss landscape. By iteratively masking these weights and measuring the resulting prediction stability, we can reduce adapter size by 30-50% with less than 2% performance degradation on standard benchmarks. Experiments across 8 NLP tasks and vision datasets show competitive compression ratios to learned pruning methods, while requiring significantly less computational overhead. While our method achieves reasonable efficiency gains, we acknowledge limitations in handling correlated features and potential brittleness under distribution shift. The simplicity of our approach makes it immediately applicable to existing adapter architectures without architectural changes.",
    "id": 492
  },
  {
    "title": "Self-Refining Networks: Neural Architecture Search with Internal Adaptive Rewiring",
    "authors": [
      "Chen, S.",
      "Kumar, V.",
      "Liu, J."
    ],
    "abstract": "We propose Self-Refining Networks (SRNs), a method for efficient neural architecture search that enables networks to modify their own connectivity patterns during training. SRNs introduce lightweight gating modules that dynamically prune or add connections based on gradient-based importance estimates, eliminating the need for expensive evolutionary or reinforcement learning-based search. Our approach alternates between standard weight optimization and discrete architectural modifications, achieving continuous refinement without meta-learning. We evaluate SRNs on CIFAR-10, CIFAR-100, and ImageNet, demonstrating 2-3% improvements over baseline ResNet architectures with comparable FLOPs. While SRNs show promise for automated architecture adaptation, we observe that discovered patterns exhibit dataset-specific biases and may not transfer robustly. The method requires careful tuning of pruning thresholds and can destabilize training without proper regularization. Our empirical analysis reveals that SRNs tend to produce architectures similar to hand-designed variants, limiting the novelty of discovered structures. Despite these limitations, SRNs offer a computationally efficient alternative to traditional NAS approaches, reducing search time from GPU-days to GPU-hours. Code and architectures will be released upon publication.",
    "id": 496
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Re-evaluation Through the Lens of Sharpness Minimization",
    "authors": [
      "Liu, W.",
      "Kumar, A.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "Multi-task learning has seen renewed interest with the success of gradient surgery methods that modify task gradients during optimization. While these approaches report improved performance across benchmarks, we argue that their benefits stem not from task balancing as commonly understood, but from implicit sharpness minimization. We first demonstrate through controlled experiments that several gradient surgery variants reduce the sharpness of the loss landscape in single-task settings, suggesting a mechanism orthogonal to multi-task interference. Building on this observation, we propose Sharpness-Aware Gradient Surgery (SAGS), which explicitly incorporates sharpness regularization into the gradient modification process. Our method achieves comparable performance to existing techniques on three standard benchmarks while providing more interpretable optimization dynamics. However, we find that the performance gains diminish when strong regularization or normalization techniques are applied, indicating that gradient surgery may be most beneficial in under-regularized training regimes. Our results suggest that re-framing gradient surgery as a form of implicit regularization provides better intuition for when and why these methods work, though we acknowledge limitations in our theoretical analysis of the sharpness-task balance interaction.",
    "id": 497
  },
  {
    "title": "Gradient Descent with Reversed Momentum: A Simple Fix for Sharp Minima in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Sharp minima in neural network training have been linked to poor generalization, yet simple interventions to avoid them remain scarce. We propose Reversed Momentum (RM), a modification to standard momentum-based optimizers that periodically reverses the momentum direction to encourage exploration of flatter regions. The key insight is that standard momentum can trap optimization trajectories in sharp basins, while occasional reversals provide escape mechanisms without sacrificing convergence speed. Our method requires only a single hyperparameter (reversal frequency) and minimal computational overhead. We evaluate RM on CIFAR-10/100 and ImageNet with ResNets and Vision Transformers, showing consistent improvements in test accuracy (1-2%) while maintaining training efficiency. Theoretical analysis reveals RM reduces the effective learning rate in sharp curvature directions, providing intuition for its empirical benefits. However, performance gains diminish with larger batch sizes and stronger data augmentation. While our results suggest RM as a practical tool for improving generalization, the improvements are moderate compared to more sophisticated sharpness-aware methods, and the optimal reversal schedule appears task-dependent.",
    "id": 505
  },
  {
    "title": "Adaptive Gradient Compression via Learned Quantization Schedules for Communication-Efficient Distributed Training",
    "authors": [
      "Kim, J.",
      "Ramaswamy, A.",
      "Gonzalez, C."
    ],
    "abstract": "Gradient compression is essential for scaling distributed deep learning, but existing methods use fixed compression ratios that fail to adapt to changing training dynamics. We propose AdaGC, a framework that learns instance-specific quantization schedules through lightweight meta-learning. Our approach employs a small hypernetwork that predicts optimal bit-widths for gradient transmission based on local gradient statistics and global training progress. Across ResNet-50 and Transformer training on ImageNet and WMT datasets, AdaGC achieves 5-8x compression with comparable final accuracy to full-precision training, slightly outperforming existing methods like QSGD and TopK. However, our method introduces ~2% parameter overhead and requires careful tuning of the meta-learning rate. While the compression gains are meaningful, the practical deployment benefits may be limited to bandwidth-constrained settings. Our code and pre-trained models will be released upon acceptance.",
    "id": 522
  },
  {
    "title": "Gradient Surgery in Transformer Language Models: A Closer Look at Parameter-Level Training Dynamics",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Understanding how gradient-based optimization navigates the complex loss landscape of large language models remains challenging. We examine gradient interference patterns at the parameter level in pre-trained transformers, proposing a layer-wise gradient surgery technique that selectively rescales parameter updates based on their alignment with the Fisher Information Matrix. Our method shows modest improvements in perplexity on Wikitext-103 (from 18.7 to 18.3) and small downstream gains on SuperGLUE (1.2% absolute improvement) using a 345M parameter model. While the approach is computationally expensive (1.4\u00d7 training time), we provide theoretical analysis suggesting that gradient surgery reduces the effective rank of the Hessian near local minima. However, our evaluations on larger models (1.3B parameters) show diminishing returns, and our results are sensitive to hyperparameter choices. The work provides insights into parameter-level dynamics but falls short of demonstrating significant practical benefits for model training. Code and pre-trained models are available at anonymous.github.io/gradsurgery.",
    "id": 533
  },
  {
    "title": "Residual Perturbation Training: Improving Robustness Through Layer-wise Noise Injection During Fine-tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Pre-trained models exhibit remarkable performance but often lack robustness to small input perturbations. We propose Residual Perturbation Training (RPT), a simple fine-tuning method that injects controlled noise into residual connections during training. Unlike adversarial training which requires expensive inner loop optimization, RPT adds Gaussian noise scaled by residual magnitudes and optimizes a consistency loss between clean and noisy outputs. While the method appears similar to dropout at first glance, our key insight is that noise injection specifically in residual pathways (rather than activations) better preserves gradient flow while improving adversarial robustness. Experiments on ImageNet and CIFAR-10 show 2-4% improvements in robust accuracy against FGSM and PGD attacks compared to standard fine-tuning, with minimal computational overhead. However, robustness gains diminish against stronger attacks like AutoAttack, and we observe trade-offs on clean accuracy (-1-2%) that may not be acceptable for all applications. Code will be available for reproducibility.",
    "id": 535
  },
  {
    "title": "Learning with Noisy Labels by Penalizing Confidence: A Margin-Based Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a simple regularization technique for training deep networks on datasets with label noise. Our method, Confidence Penalty with Margins (CPM), discourages overconfident predictions by adding a penalty proportional to the difference between the maximum softmax probability and a confidence threshold. The key insight is that noisy labels tend to produce overconfident predictions on mislabeled examples, even when employing standard robust training techniques like label smoothing. CPM requires no knowledge of noise rates or architecture changes, making it widely applicable. On CIFAR-10/100 with synthetic noise, CPM improves accuracy by 2-4% over baseline methods. On real-world noisy datasets like Clothing1M, we achieve competitive results (68.9% vs. 69.5% state-of-the-art) while being simpler to implement. While the theoretical properties of CPM remain limited, our extensive ablations demonstrate consistent robustness across noise types and levels. Our code is publicly available.",
    "id": 544
  },
  {
    "title": "LayerDrop: A Simple Pruning Strategy for Reducing Transformer Training Time via Dynamic Structural Sparsity",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rodriguez, J."
    ],
    "abstract": "We propose LayerDrop, a training-time pruning method that dynamically removes entire layers from transformer architectures based on gradient norms. Unlike previous approaches that prune fixed parameter subsets, LayerDrop adaptively selects which layers to skip during each forward pass, creating varying sparse subnetworks. Our method requires no architectural changes and introduces minimal overhead through efficient rescaling of remaining layers. We evaluate on language modeling (WikiText-103) and machine translation (IWSLT'14) tasks, achieving 15-22% reduction in training time while maintaining 96-98% of baseline performance. While less effective than model compression techniques requiring expensive pretraining, LayerDrop offers a practical trade-off between training efficiency and model quality. However, our results show diminishing returns beyond 30% pruning and reveal sensitivity to gradient estimation quality. Code and pre-trained models will be released upon acceptance.",
    "id": 549
  },
  {
    "title": "Gradient Amplification Networks: Learning to Optimize Through Learnable Preconditioners",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Gonzalez, M."
    ],
    "abstract": "We propose Gradient Amplification Networks (GANs), a meta-learning approach that learns task-specific preconditioning matrices to amplify informative gradient directions during optimization. Rather than hand-designing adaptive optimizers like Adam or RMSProp, we learn a neural network that outputs preconditioning matrices based on the recent optimization trajectory. Our key insight is to parameterize the preconditioner as a low-rank update to existing optimizers, enabling efficient computation while maintaining expressivity. We evaluate GANs on several vision and language tasks, showing 5-12% relative improvements in final accuracy over baselines, particularly for small datasets and non-stationary objectives. However, we observe diminishing returns on large-scale problems and find the approach sensitive to hyperparameter choices. While our method demonstrates promise for certain classes of optimization problems, its broader applicability remains limited by computational overhead and unclear theoretical guarantees. Code will be made available post-review.",
    "id": 567
  },
  {
    "title": "A Unified Framework for Gradient Compression with Error Feedback in Heterogeneous Federated Learning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M.",
      "Zhou, Y."
    ],
    "abstract": "We propose EF-Het, a unified framework for gradient compression with error feedback in federated learning systems with heterogeneous clients. Our method introduces an adaptive compression mechanism that balances communication efficiency with convergence across devices with varying compute and communication capabilities. The key innovation is an error-insensitive quantization strategy that accumulates and redistributes compression errors based on client-specific learning rates and batch sizes. We provide convergence guarantees under standard assumptions, showing EF-Het achieves the same asymptotic rate as full-precision training for strongly convex objectives. On realistic federated benchmarks (CIFAR-10 with 100 heterogeneous clients and a language modeling task), EF-Het demonstrates 4-7x communication reduction compared to naive gradient compression, though we observe performance degradation under high client heterogeneity. While our theoretical analysis holds for convex settings, our empirical evaluation shows promising results on neural networks without theoretical backing. The framework offers a practical solution for resource-constrained federated environments, though its impact may be limited to specific deployment scenarios where gradient compression is beneficial.",
    "id": 568
  },
  {
    "title": "Adaptive Gradient Clipping with Curvature-Aware Bounds for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose an adaptive gradient clipping method that incorporates local curvature information to stabilize transformer training without extensive hyperparameter tuning. Our approach computes per-layer clipping bounds using an online estimate of the Fisher Information Matrix diagonal, combined with a momentum-based smoothing technique to handle gradient noise. While standard clipping methods use fixed thresholds or simple norm-based heuristics, our curvature-aware bounds automatically adjust to the changing loss landscape during training. We evaluate on Wikitext-103 language modeling and WMT'14 English-German translation tasks, showing modest improvements (0.8-1.2 BLEU/BPE) over strong baselines with reduced gradient explosion incidents. However, computational overhead increases training time by 15-20%, and benefits diminish on smaller architectures. Theoretical analysis provides convergence guarantees under standard convexity assumptions, but the non-convex setting remains largely heuristic. Code and hyperparameters are provided for reproducibility.",
    "id": 569
  },
  {
    "title": "Gradient Norm Regularization Improves Adversarial Training: A Small-Scale Investigation",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, T."
    ],
    "abstract": "Adversarial training remains computationally expensive for large networks, leading practitioners to use smaller proxy models during training. We investigate whether additional regularization during this proxy training phase can translate to improved robustness in the final model. Specifically, we propose gradient norm regularization (GNR), which penalizes large gradient norms of the loss with respect to inputs during adversarial training. Our theoretical analysis suggests GNR can improve margin bounds by a factor of \u221ak in simplified settings. On CIFAR-10 and CIFAR-100, we conduct extensive experiments with ResNet-18/34 architectures across 3 random seeds. Results show modest but consistent improvements: GNR increases robust accuracy by 2.1-3.7% over standard adversarial training, while maintaining clean accuracy within 0.5%. However, these gains are less pronounced when tested on larger architectures (ResNet-50) or out-of-distribution datasets. Ablation studies reveal that the regularization weight must be carefully tuned to prevent gradient vanishing. While our method presents a simple, theoretically-motivated improvement to adversarial training, we acknowledge limitations: experiments use standard datasets and architectures, computational budget prevents ImageNet-scale evaluation, and the regularizer adds 15-20% training overhead. Code is available at [URL].",
    "id": 576
  },
  {
    "title": "Improving Gradient Descent with Lookahead-Averaged Momentum: A Simple Modification for Stable Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Ivanov, S."
    ],
    "abstract": "We propose Lookahead-Averaged Momentum (LAM), a simple modification to standard momentum-based optimizers that improves training stability without additional hyperparameters. LAM maintains two sets of parameters: the 'fast' parameters updated using standard momentum, and 'slow' parameters that are periodically updated using an exponentially-weighted average of the fast parameters. We show that this averaging step acts as a form of implicit gradient smoothing, reducing the variance of effective update directions. Across 8 benchmark datasets and 5 neural architectures, LAM achieves comparable or slightly improved final accuracy to SGD and Adam while exhibiting 15-30% lower training loss variance in early epochs. Theoretical analysis in the convex quadratic case reveals that LAM converges at a rate O(\u03ba log(1/\u03b5)) when \u03bb > 1/\u03ba, where \u03ba is the condition number and \u03bb is the synchronization period. While our empirical improvements are modest and the theoretical analysis is limited to simplified settings, LAM requires only 15 lines of code to implement and may be useful in scenarios where training stability is prioritized over maximal performance. Code will be made available upon acceptance.",
    "id": 589
  },
  {
    "title": "Gradient Descent with Layer-wise Learning Rates for Transformer Fine-tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose L2-GD, a simple modification to standard gradient descent that uses different learning rates for each transformer layer during fine-tuning. Drawing inspiration from the observation that lower layers typically require smaller updates than upper layers, we introduce a heuristic schedule that scales learning rates exponentially with layer depth. Our experiments across 8 GLUE tasks and 3 vision datasets show modest improvements over standard fine-tuning (average +0.8% accuracy), with particularly strong gains on smaller datasets (<10k examples). While similar ideas have been explored in vision models, our work provides the first systematic study for transformers. The method adds minimal computational overhead and requires no additional hyperparameters beyond a global scaling factor. However, the improvements are inconsistent across tasks\u2014some datasets show no benefit or slight degradation. Analysis reveals the approach works best when pre-trained and target domains are similar. Though L2-GD is unlikely to fundamentally change fine-tuning practices, it offers a practically useful technique for resource-constrained scenarios where careful hyperparameter tuning is expensive. Code and pre-trained models will be released.",
    "id": 606
  },
  {
    "title": "Frequency-Aware Gradient Clipping for Robust Transformer Training Under Label Noise",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Zhao, H."
    ],
    "abstract": "Prior work has demonstrated that transformer models are surprisingly robust to label noise, maintaining reasonable downstream performance even with 30-50% corrupted labels. We explore whether this robustness can be explained through the lens of gradient frequency distribution during training. Our key observation is that noisy labels primarily affect high-frequency gradient components, while low-frequency components largely preserve the underlying signal. Based on this insight, we propose Frequency-Aware Gradient Clipping (FAGC), which adaptively clips gradients based on their frequency content. FAGC operates in the Fourier domain of parameter gradients, preserving low-frequency information while thresholding high-frequency updates. On CIFAR-100 and ImageNet with synthetic label noise, FAGC achieves 2-3% improvements over standard training, and shows particular benefits when combined with mixup augmentation. However, we observe diminishing returns on naturally noisy datasets like WebVision. While FAGC introduces minimal computational overhead (<5%), its benefits appear dataset-specific and we cannot achieve consistent improvements across all settings. Our empirical results challenge the prevailing view that noise robustness is solely due to architectural inductive biases, suggesting an alternative explanation based on gradient frequency filtering during optimization.",
    "id": 610
  },
  {
    "title": "Rethinking Curriculum Learning Through Gradient Norm Scheduling",
    "authors": [
      "Chen, Y.",
      "Rodriguez, M.",
      "Kumar, V."
    ],
    "abstract": "Curriculum learning has shown promise for improving training efficiency and generalization, yet most methods rely on manually designed difficulty schedules or complex reinforcement learning agents to adapt curricula. We propose a simple alternative: Gradient Norm Scheduling (GNS), which automatically adjusts training difficulty based on the per-sample gradient norms of the current model. Intuitively, samples producing large gradient norms are considered difficult and deferred to later training stages. We theoretically show that GNS approximates a regularized optimization objective that promotes smoother loss landscapes. Empirically, we evaluate GNS on CIFAR-10 and CIFAR-100 with ResNet-18 and WideResNet-28-10 architectures, achieving modest improvements (~0.5-1.2% accuracy gains) over standard training. While our approach is simpler than existing adaptive curriculum methods, the improvements are incremental and inconsistent across datasets. Ablation studies reveal that gradient norm ordering correlates only weakly with human-defined difficulty measures. Our method may be most useful as a lightweight alternative to standard training rather than a definitive solution for curriculum learning.",
    "id": 614
  },
  {
    "title": "Gradient Amplification Through Transient Sharpening: A Simple Heuristic for Improved Convergence in Overparameterized Networks",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chawla, N.",
      "Chen, L."
    ],
    "abstract": "We propose a lightweight training modification that amplifies gradient signals during the initial phase of training deep neural networks. Our method, Transient Gradient Sharpening (TGS), applies an element-wise transformation to gradients based on their historical norm distribution, effectively increasing the learning rate for coordinates with consistently small updates while maintaining stability for others. Unlike adaptive optimizers that maintain per-parameter statistics throughout training, TGS gradually phases out its intervention, transitioning to standard SGD momentum after a preset number of epochs. We demonstrate improvements over vanilla SGD on ResNet-50 and ViT-B/16 architectures across CIFAR-100 and ImageNet-1K, with average top-1 accuracy gains of 1.2% and 0.8% respectively. While our approach is simple to implement and adds minimal computational overhead, we observe that gains diminish with stronger baselines like AdamW. Theoretical analysis reveals connections to sharpness-aware minimization, though we acknowledge limitations in our convergence guarantees and the heuristic nature of our phase-out schedule. Code is available at the provided link.",
    "id": 615
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Batch Sizes: A Practical Middle Ground",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Policy gradient methods remain a cornerstone of reinforcement learning, yet their sample efficiency continues to lag behind value-based counterparts. While variance reduction techniques like SVRG and SARAH have shown promise in supervised learning, their adaptation to policy gradients faces a tension between theoretical guarantees and computational practicality: full-batch gradient computations required for variance reduction eliminate the very sample efficiency gains they aim to achieve. We propose a practical compromise that dynamically adjusts batch sizes based on estimated gradient variance, applying variance reduction only when the signal-to-noise ratio drops below learned thresholds. Our approach requires no additional hyperparameters beyond standard policy gradient methods and adds minimal computational overhead (2-4% in wall-clock time). Across continuous control benchmarks, our method achieves 15-30% sample efficiency improvements over PPO on 8 out of 12 environments, while matching performance on the remainder. However, gains diminish with optimal hyperparameter tuning, and we observe instability in high-dimensional action spaces. Our results suggest that while variance reduction can help policy gradients, the benefits may be incremental rather than transformative, highlighting the importance of careful empirical validation of theoretical techniques.",
    "id": 619
  },
  {
    "title": "Gradient Descent with Iterative Noise Shaping: A Practical Acceleration Framework for Deep Neural Network Training",
    "authors": [
      "Liu, Y.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Descent with Iterative Noise Shaping (GD-INS), a simple modification to standard SGD that adaptively adjusts the noise scale of gradient updates based on local curvature estimates. By multiplying the gradient noise covariance by a learned scaling factor derived from approximate Hessian diagonal elements, we achieve behavior reminiscent of natural gradient methods without the computational burden of matrix operations. Our method requires only a single additional backward pass every k iterations and introduces minimal overhead. Experiments on CIFAR-10 and ImageNet show 8-15% faster convergence to baseline accuracy compared to standard SGD, with particular gains on deeper architectures. While we demonstrate measurable improvements over standard baselines, our theoretical analysis reveals GD-INS recovers standard SGD in the worst case and provides benefits primarily when the Hessian exhibits specific block-diagonal structure. Code will be released upon acceptance.",
    "id": 622
  },
  {
    "title": "Gradient Surgery Meets Sharpness Minimization: A Simple Trick for Better Generalization in Vision Transformers",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Zhao, S."
    ],
    "abstract": "We propose Sharpness-Aware Gradient Surgery (SAGS), a lightweight modification to gradient descent that combines sharpness minimization with gradient surgery to improve generalization in Vision Transformers. Despite recent advances in training ViTs, we observe that multi-task gradients often interfere destructively and sharpness-based regularization techniques underperform due to the quadratic complexity of Hessian computations. SAGS addresses both issues by (1) selectively dropping gradient components that conflict with sharpness reduction while preserving task-specific information, and (2) using a rank-1 approximation of the Hessian to efficiently compute trust-region steps. On ImageNet-1K, SAGS improves top-1 accuracy by 0.8% over AdamW for ViT-B/16 at half the computational cost of SAM, though gains diminish for larger ViT-L/16 models. Our ablation study reveals the method is most effective when training from scratch on smaller datasets like CIFAR-100. While our theoretical analysis is limited to quadratic objectives, we show empirical benefits on standard benchmarks. Code and pretrained models will be released.",
    "id": 626
  },
  {
    "title": "Revisiting Gradient Clipping in Private SGD: A Simpler Approach with Marginal Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is a standard component in differentially private stochastic gradient descent (DP-SGD), primarily used to bound per-example sensitivity. We analyze a subtle interaction between clipping and the noise addition process, showing that common clipping thresholds can inadvertently amplify the effective noise-to-signal ratio for certain parameter configurations. Motivated by this observation, we propose Adaptive Clipped DP-SGD (AC-DP), a lightweight modification that adjusts clipping thresholds based on gradient norm statistics collected during a brief burn-in period. Our method achieves comparable privacy-utility tradeoffs to standard DP-SGD across CV and NLP benchmarks, with modest improvements (1-2% accuracy gains) in high-privacy regimes (\u03b5 \u2264 1). While the theoretical improvement is marginal (tight privacy analysis reveals only constant-factor gains), our approach eliminates the need for extensive clipping threshold tuning and provides more stable training dynamics. We validate our method on CIFAR-10 and SST-2, achieving private accuracies of 62.3% and 83.1% respectively at \u03b5=0.5. Our implementation requires only 15 lines of additional code, suggesting practical deployment benefits despite limited theoretical novelty.",
    "id": 628
  },
  {
    "title": "Improving Transformer Efficiency through Iterative Token Dropping with Learnable Retention Scores",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models suffer from quadratic complexity in sequence length, limiting their applicability to long contexts. While previous work has explored static token dropping strategies based on attention scores or heuristics, these often degrade quality on downstream tasks. We propose ITLD, a method that learns adaptive token retention scores through an auxiliary prediction task trained jointly with the main objective. During inference, tokens are iteratively dropped based on these learned scores, achieving up to 2.8x speedup on sequences of length 4096. We evaluate ITLD on language modeling and downstream classification tasks, showing 2-7% relative perplexity improvements over uniform dropping baselines. However, we observe significant performance degradation on tasks requiring fine-grained reasoning over long contexts. Our method introduces 0.5% additional parameters and minimal training overhead, making it practical for existing architectures. While ITLD offers clear computational benefits for certain applications, current limitations in task-specific performance suggest careful evaluation is needed before broad deployment.",
    "id": 633
  },
  {
    "title": "LoRA-V: Parameter-Efficient Fine-Tuning with Variance-Adaptive Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers may be suboptimal. We propose LoRA-V, a variance-adaptive variant that dynamically adjusts the rank of LoRA modules based on gradient variance observed during training. Our method uses an efficient thresholding scheme to increase the rank for layers with high gradient variance while pruning low-variance adapters. We evaluate LoRA-V on instruction tuning tasks using Llama-2 models across three domains: mathematical reasoning, code generation, and dialogue. Results show modest improvements over standard LoRA (average +1.2% accuracy) while using 15-25% fewer trainable parameters. However, we find that performance gains diminish on larger models (70B+) and tasks with abundant training data. Theoretical analysis reveals that LoRA-V's effectiveness depends on the spectral gap of the original weight matrices, suggesting limited applicability to pre-trained models with certain spectral properties. While LoRA-V provides a lightweight alternative to manual rank tuning, its benefits are most pronounced in resource-constrained scenarios with smaller models.",
    "id": 637
  },
  {
    "title": "Lookahead Batch Normalization: An Empirical Study of Batch Statistics in Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, S."
    ],
    "abstract": "Batch Normalization (BN) remains widely used in vision tasks but is often replaced by LayerNorm in transformers due to instability with small batch sizes. We propose Lookahead Batch Normalization (LBN), which computes batch statistics using a moving average of future activations during training. LBN maintains the normalization benefits of BN while stabilizing training for small batches. On ImageNet classification, LBN achieves 76.2% top-1 accuracy with batch size 64, matching standard BN with batch size 256. For transformer language modeling, LBN provides marginal improvements (0.3-0.5 perplexity points) over LayerNorm on Wikitext-103 and C4 datasets. We analyze the effect of the lookahead window size and demonstrate LBN's sensitivity to hyperparameter tuning. While LBN shows promise for certain architectures, our experimental results reveal limited gains on standard NLP benchmarks. Detailed ablation studies suggest the benefits are primarily due to implicit regularization rather than improved optimization dynamics.",
    "id": 639
  },
  {
    "title": "Gradient Surgery Revisited: Why Existing Methods Overfit in Adaptive Federated Optimization",
    "authors": [
      "Chen, Z.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Federated Learning (FL) faces the challenge of client drift when local objectives diverge from the global objective. Recent gradient surgery methods attempt to address this by projecting conflicting gradients, but we show these approaches systematically overfit to local client distributions. Through theoretical analysis of the gradient projection geometry, we prove that existing methods implicitly amplify gradient directions aligned with local data manifolds, leading to worse generalization. We propose Federated Gradient Decorrelation (FGD), a simple modification that adds controlled noise to break this alignment. While FGD achieves competitive accuracy on standard benchmarks (82.3% on CIFAR-10 with 100 clients, vs 81.1% for FedAvg), we find the improvement diminishes as communication rounds increase. Our theoretical bounds suggest the method's benefit is fundamentally limited by client heterogeneity levels. Experiments on additional datasets confirm this limitation, with mixed results across tasks. Our analysis reveals fundamental tensions between gradient alignment and generalization in federated settings that current methods fail to resolve.",
    "id": 644
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Dynamic Parameter Pruning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient method for fine-tuning large language models, but its fixed rank structure often leads to over-parameterization across downstream tasks. We propose LoRA-Drop, a simple yet effective technique that adaptively prunes LoRA modules during fine-tuning based on gradient-based importance scores. Our method progressively reduces the rank of each LoRA module by eliminating the least important components, achieving up to 60% parameter reduction with minimal performance degradation. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop matches full LoRA performance while using 25-40% fewer parameters. However, we observe that aggressive pruning can hurt performance on low-resource tasks, suggesting the need for task-specific hyperparameter tuning. While our approach provides practical efficiency gains over vanilla LoRA, it primarily refines existing techniques rather than addressing fundamental limitations of low-rank adaptation. Code and models are available at anonymous.github.io/lora-drop.",
    "id": 650
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient method for fine-tuning large language models, but selecting appropriate rank values remains largely heuristic. We propose LoRA-Drop, a simple extension that dynamically adjusts rank during training through gradient-based pruning. Our method begins with a conservatively high rank and gradually drops the least significant singular components based on their contribution to the loss. While this approach reduces parameter counts by 15-40% compared to fixed-rank LoRA on standard benchmarks like GLUE and SuperGLUE, the gains over carefully tuned baselines remain modest (0.3-0.8 average F1 improvement). We provide theoretical analysis showing that our dropping criterion is equivalent to a form of regularized optimization, though our bounds are looser than previous work on static rank selection. Experiments across three model sizes (350M-7B parameters) demonstrate consistent but small improvements over vanilla LoRA, particularly on tasks with limited training data. However, LoRA-Drop adds computational overhead (10-15% training time increase) and hyperparameter sensitivity that may limit practical utility. Code and pretrained adapters will be released upon acceptance.",
    "id": 663
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique for Improving Generalization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Liu, Y."
    ],
    "abstract": "We propose Gradient Dropout (GradDrop), a regularization technique that randomly sets a subset of gradient components to zero during backpropagation. Unlike traditional dropout which masks activations, GradDrop operates directly on the gradient flow, creating an implicit form of gradient noise that encourages exploration of the loss landscape. Our method requires only a single hyperparameter (dropout rate) and adds minimal computational overhead. We provide theoretical analysis showing GradDrop approximates a form of stochastic gradient descent with implicit prior regularization. Experiments on CIFAR-10, CIFAR-100 and ImageNet with ResNet-18/50 architectures show GradDrop achieves modest improvements over standard SGD with dropout (0.5-1.2% accuracy gains). While the improvements are consistent, they are smaller than those achieved by more sophisticated regularizers like mixup or label smoothing. Ablation studies suggest GradDrop works best in combination with standard dropout and weight decay. The technique is particularly effective for medium-sized models but shows diminishing returns for very large networks. Our implementation requires only 3 lines of code in PyTorch. While GradDrop is simple to implement and has some theoretical grounding, its practical impact remains limited compared to established baselines.",
    "id": 664
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Iterative Spectral Regularization",
    "authors": [
      "Chen, L.",
      "Raghavan, P.",
      "Mikhailov, A."
    ],
    "abstract": "We propose an iterative spectral regularization technique for improving the generalization of deep neural networks. Our method applies data-dependent spectral norm constraints during training, where we gradually strengthen regularization based on the network's current generalization gap. The approach builds on PAC-Bayesian bounds by incorporating layer-wise spectral norms as complexity measures. We derive a new generalization bound that scales with the product of spectral norms across layers, achieving a \u221a(L log L/\u03b3) dependence compared to previous O(L\u00b2) bounds, where L is network depth and \u03b3 measures margin. However, we acknowledge that this improvement comes at the cost of introducing additional hyperparameters controlling the strengthening schedule. Experimental evaluation on CIFAR-10 and ImageNet subsets shows modest improvements (2-3% accuracy gains) compared to standard regularization baselines, though results vary significantly across architectures. While our theoretical contribution provides tighter bounds for specific network classes, the practical gains remain limited and may not justify the computational overhead. Our code is available at anonymized-link.",
    "id": 665
  },
  {
    "title": "Momentum-Scheduled Learning Rates: Bridging the Gap Between Theoretical Guarantees and Practical Performance",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Momentum-Scheduled Learning Rates (MSLR), a simple modification to existing adaptive optimizers that interpolates between worst-case theoretical bounds and empirical performance gains. By scheduling both the momentum coefficient and learning rate as coupled functions of gradient signal-to-noise ratio, MSLR achieves theoretical regret bounds competitive with Adam while recovering SGD-like behavior in late-stage training when gradients are small. Our method requires only two additional hyperparameters with intuitive interpretations. We prove convergence guarantees for convex and non-convex settings, showing that MSLR matches the O(log T/\u221aT) convergence rate of AdaGrad while maintaining the constant-factor improvements of heavy-ball momentum. Empirical evaluation on CIFAR-10, CIFAR-100, and a subset of ImageNet demonstrates 2-3% accuracy improvements over baselines at the cost of 15% increased wall-clock time. However, we observe that benefits diminish as model size increases, with no improvement observed on ViT-L/16. While MSLR provides a principled approach to optimizer design, our theoretical analysis relies on standard bounded gradient assumptions that may not hold in practice, and ablation studies suggest simpler cosine scheduling can achieve similar performance in many settings.",
    "id": 668
  },
  {
    "title": "Preconditioned Gradient Descent with Adaptive Momentum Estimation via Hessian Diagonal Approximation",
    "authors": [
      "Chen, Y.",
      "Rodriguez, M.",
      "Kim, J.",
      "Singh, A."
    ],
    "abstract": "We propose PHADAM, a preconditioned variant of Adam that incorporates diagonal Hessian information to improve convergence in non-convex optimization. While adaptive methods like Adam show strong empirical performance, their convergence guarantees remain weaker than SGD+momentum in certain regimes. Our approach estimates the Hessian diagonal using a limited-memory scheme that adds minimal computational overhead (\u22645% per step). We theoretically show that PHADAM achieves O(1/T) convergence in the convex setting and exhibits better conditioning than Adam when curvature information is accurate. On ImageNet training with ResNet-50, PHADAM achieves 76.2% top-1 accuracy (vs. 76.0% for Adam) with 8% faster convergence in wall-clock time. However, gains are inconsistent across architectures and datasets: we observe minimal improvement on transformer language modeling tasks. Our analysis reveals the method's sensitivity to the Hessian approximation quality, which degrades for very deep networks. Code is available at [URL].",
    "id": 675
  },
  {
    "title": "Gradient Surgery for Stabilizing Adversarial Training in Deep Classifiers",
    "authors": [
      "Liu, K.",
      "Chen, T.",
      "Rodriguez, A."
    ],
    "abstract": "Adversarial training remains unstable for deep networks despite recent advances. We propose Gradient Surgery (GS), a simple but effective method that selectively drops or rescales gradient components during adversarial training. GS monitors gradient variance across mini-batches and applies threshold-based pruning to reduce instability. We evaluate GS on CIFAR-10 and CIFAR-100 with ResNet-18 and WideResNet-34, showing 2-3% robust accuracy improvements over standard adversarial training with minimal computational overhead (\u22645% extra training time). Surprisingly, GS also improves clean accuracy by 1-2% in some settings. While our approach shows promise, theoretical justification remains limited and benefits diminish on larger datasets like ImageNet. Analysis reveals GS primarily affects early training dynamics, suggesting its impact may be replicated through careful hyperparameter tuning. Our code is available at [URL].",
    "id": 685
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Inner-Loop Steps for Continuous Control",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Rodrigues, M."
    ],
    "abstract": "Policy gradient methods often suffer from high variance in gradient estimates, leading to unstable training. While variance reduction techniques like control variates have shown promise in discrete domains, their application to continuous control remains limited. We propose VR-PGA, a variance-reduced policy gradient algorithm that uses an adaptive inner-loop optimization procedure to learn state-dependent baselines without prior knowledge of the environment dynamics. Our method combines Stein's lemma with a learned value function to construct control variates, while automatically adjusting the number of inner-loop steps based on gradient norm history. Experiments on MuJoCo continuous control benchmarks show 12-18% improvement in sample efficiency over PPO and SAC on half of the tested environments, with comparable performance on others. We also demonstrate that the adaptive inner-loop reduces wall-clock time by 15-25% compared to fixed inner-loop variants. However, we find that VR-PGA's benefits diminish in low-dimensional state spaces and when reward signals are sparse. While not achieving state-of-the-art results across all tasks, our work provides a practical variance reduction framework that can be integrated into existing policy gradient implementations with minimal code changes.",
    "id": 701
  },
  {
    "title": "Gradient Surgery with Adaptive Dropout: A Lightweight Approach to Multi-Task Learning in Transformer Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning in transformer architectures often suffers from conflicting gradients between tasks, leading to suboptimal performance across all objectives. While recent gradient surgery methods show promise, they require computing Hessian information or maintaining task-specific parameters, introducing significant computational overhead. We propose AdaGradDrop, a lightweight alternative that dynamically identifies and drops conflicting gradient directions using an adaptive dropout mechanism. Our method applies Bernoulli masks to gradient components based on their alignment with task-specific objectives, requiring only O(n) additional memory where n is the parameter count. Experiments on GLUE and SuperGLUE benchmarks with a shared BERT-base model show 2.1% average improvement over vanilla multi-task training and 1.3% over gradient surgery baselines, while reducing training time by 18%. However, we observe performance degradation on tasks with highly correlated objectives and limited gains on smaller models. Our PyTorch implementation adds less than 50 lines of code. The method provides a practical trade-off between computational efficiency and multi-task performance, though theoretical convergence guarantees remain an open question.",
    "id": 717
  },
  {
    "title": "Gradient Surgery is Not Enough: Addressing Gradient Interference via Adaptive Branching in Multi-Task Networks",
    "authors": [
      "Chen, L.",
      "Srinivasan, V.",
      "Johnson, K."
    ],
    "abstract": "Multi-task learning often suffers from gradient interference, where conflicting gradients from different tasks impede optimization. While recent work has proposed various gradient surgery techniques to project gradients onto conflict-free subspaces, we argue these methods fail to address a fundamental limitation: the fixed computation graph. We propose Adaptive Branching Networks (ABN), which dynamically creates task-specific branches during training when gradient conflicts exceed a learned threshold. Our method computes gradient cosine similarities within mini-batches and spawns new network branches through selective parameter duplication. Experiments on three benchmark datasets (CIFAR-100, NYUv2, QM9) show modest improvements (+1.2-2.3%) over gradient surgery baselines, though with 1.4x increased parameters. While ABN reduces negative cosine similarities between task gradients by 47%, we observe diminishing returns beyond three tasks and increased training instability on small datasets. Our results suggest that architectural flexibility can partially mitigate gradient interference, but highlight the need for better regularization techniques to control model complexity. Code is available at [anonymous link].",
    "id": 718
  },
  {
    "title": "Gradient Descent with Momentum Adapts to Data Heterogeneity in Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate whether simple gradient descent with momentum (GDM) can mitigate the performance degradation caused by data heterogeneity in federated learning without requiring explicit client-specific optimization. While prior work has focused on sophisticated optimization techniques to handle non-IID data distributions, we demonstrate that standard GDM with an appropriately chosen momentum parameter achieves surprisingly competitive convergence rates. Our theoretical analysis establishes that GDM converges at a rate of O(1/\u221aT) under standard assumptions, matching the rate observed in IID settings when momentum \u03b2 \u2265 0.9. We empirically validate our findings on CIFAR-10 and FEMNIST datasets, showing that GDM reduces the performance gap between heterogeneous and homogeneous settings by 23-31% compared to vanilla SGD. However, we observe that these improvements diminish as the number of clients increases beyond 100, suggesting fundamental limitations of momentum-based approaches. Our results provide practical guidance for practitioners but indicate that GDM alone may be insufficient for extreme heterogeneity regimes.",
    "id": 726
  },
  {
    "title": "Improving Transformer Efficiency through Attention Pattern Recycling",
    "authors": [
      "Liu, C.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "The quadratic complexity of self-attention in transformers remains a critical bottleneck for long-sequence applications. We propose Attention Pattern Recycling (APR), a method that caches and reuses attention patterns across similar inputs to reduce computational overhead. Our approach identifies structural similarities in attention distributions through lightweight clustering during training, then reuses cached patterns for subsequent forward passes when similarity thresholds are met. The method requires no architectural modifications and can be integrated as a wrapper around existing transformer implementations. Experiments on language modeling and machine translation tasks show 1.4-1.7x speedup with <1% performance degradation on sequences up to 4K tokens. However, performance drops significantly (3-5 BLEU score reduction) for longer sequences (>8K tokens) and tasks requiring precise positional attention patterns. While APR demonstrates clear efficiency gains for certain workloads, the reliance on attention pattern similarity limits its broader applicability. Our code will be made available upon acceptance.",
    "id": 738
  },
  {
    "title": "Regularized Gradient Descent with Memory-Efficient Second-Order Updates for Large-Scale Optimization",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S.",
      "Garcia, M."
    ],
    "abstract": "We propose SAMOSA, a practical variant of gradient descent that incorporates second-order information while maintaining memory efficiency comparable to first-order methods. Our approach approximates the Hessian via low-rank updates using historical gradient information, combined with an adaptive regularization scheme that limits curvature exploitation based on gradient noise estimates. The method requires only 50% additional memory compared to standard SGD and introduces minimal computational overhead through careful reuse of existing gradient computations. We evaluate SAMOSA on standard vision and language benchmarks, showing 2-7% improvements over tuned SGD baselines on CIFAR-10/100 and modest gains on GLUE tasks, particularly on out-of-distribution validation sets. While our theoretical analysis provides convergence guarantees for convex quadratic objectives, the gap between theory and practice remains significant for general non-convex settings. Our results suggest the method is most beneficial when training data exhibits moderate ill-conditioning, though benefits diminish on well-regularized architectures. Code and hyperparameter configurations are provided for reproduction.",
    "id": 739
  },
  {
    "title": "LoRA-RT: Low-Rank Adaptation with Runtime Thresholding for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA-RT, a simple extension to Low-Rank Adaptation (LoRA) that introduces dynamic thresholding of adapter weights during training. While LoRA has enabled parameter-efficient fine-tuning by learning low-rank updates to pre-trained models, we observe that many learned adapter weights remain close to zero across different tasks, suggesting redundancy. Our method adds a learnable threshold parameter that dynamically masks negligible weight updates during training, effectively pruning the adapter while maintaining performance. We evaluate LoRA-RT on GLUE and SuperGLUE benchmarks using RoBERTa-base and Llama-2-7B, achieving comparable task performance to standard LoRA while reducing the number of active adapter parameters by 15-35%. However, our approach introduces an additional hyperparameter and shows mixed results on generative tasks, with some degradation on longer sequence generation. While LoRA-RT demonstrates promise for reducing adapter storage costs, we acknowledge that our gains are incremental and primarily benefit deployment scenarios with strict memory constraints. Code will be made available upon acceptance.",
    "id": 744
  },
  {
    "title": "Memory-Efficient Training of Transformers via Structured Sketching of Attention Matrices",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose SketchAttention, a training method that reduces the memory footprint of self-attention in Transformers by projecting attention matrices into low-dimensional sketches. Our approach combines recent work on linear attention with CountSketch-based dimensionality reduction, achieving sub-quadratic memory complexity in sequence length. While previous linear attention mechanisms suffer from accuracy degradation on complex reasoning tasks, we introduce a learnable sketching operator that adapts to the data distribution during training. Experiments on standard NLP benchmarks show modest improvements over vanilla linear attention (2-3% absolute improvements on GLUE), with memory savings comparable to other efficient attention methods. However, we observe that our method underperforms full quadratic attention on tasks requiring fine-grained reasoning, particularly on long-context datasets like TriviaQA. Theoretical analysis reveals that our sketching approach preserves attention probabilities up to a multiplicative error bound, though this bound becomes loose for heavily skewed attention patterns. While SketchAttention provides practical memory benefits for training large models on consumer GPUs, its trade-offs between efficiency and accuracy may limit adoption for applications where small accuracy differences are critical.",
    "id": 750
  },
  {
    "title": "Adaptive Gradient Clipping with Memory-Efficient Second-Order Estimation",
    "authors": [
      "Liu, J.",
      "Kumar, S.R.",
      "Zhao, Y."
    ],
    "abstract": "Gradient clipping is widely used in training deep neural networks, particularly transformers, but fixed clipping thresholds often struggle with varying gradient scales across layers and training phases. We propose AdaClip, a method that automatically adjusts clipping thresholds using a diagonal approximation of the Fisher information matrix computed via mini-batch gradients. Unlike previous approaches that require maintaining gradient histories, AdaClip estimates second-order information from a small buffer of recent gradients, yielding memory overhead of <0.1% compared to baseline training. Our method introduces a lightweight online estimation procedure that adapts clipping thresholds every few hundred steps rather than every iteration, reducing computational cost while maintaining adaptivity. Experiments on language modeling (WikiText-103) and vision tasks (ImageNet-1k) show modest improvements over standard clipping\u20140.8% better perplexity and 0.3% higher accuracy respectively\u2014across various transformer architectures. While AdaClip demonstrates consistent small improvements across settings, our analysis reveals these gains diminish when training budgets are large (300k+ steps), suggesting potential value primarily in resource-constrained scenarios. Code is available at anonymized.",
    "id": 753
  },
  {
    "title": "LoRA-Plus: Incremental Low-Rank Adaptation with Gradient Amplification",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have emerged as practical alternatives to full model fine-tuning, but we observe that gradient magnitudes in LoRA modules tend to be disproportionately small compared to the original parameters. We propose LoRA-Plus, a simple modification that applies fixed gradient amplification factors to the low-rank matrices during training. Our method requires no additional hyperparameters beyond standard LoRA and adds negligible computational overhead. We evaluate LoRA-Plus on GLUE, SuperGLUE, and vision-language tasks using T5-base, RoBERTa-base, and CLIP-ViT-B/32. Results show consistent improvements over vanilla LoRA (average +1.2% GLUE score, +0.8% SuperGLUE, +0.9% zero-shot CIFAR-10 accuracy), approaching and occasionally exceeding full fine-tuning performance while maintaining parameter efficiency. However, we note that improvements diminish with larger models (T5-large shows only +0.3% average gain), and theoretical justification for the amplification factors remains heuristic. Code and pre-trained adapters are available at [anonymized].",
    "id": 760
  },
  {
    "title": "Lightweight Mixture of Experts via Iterative Token Routing Pruning",
    "authors": [
      "Chen, L.",
      "Rodriguez, S.",
      "Kim, J.",
      "Anderson, K."
    ],
    "abstract": "We propose a simple yet effective method for scaling down large mixture of experts (MoE) models while preserving their downstream performance. Our key observation is that the learned token-to-expert routing patterns exhibit strong redundancy across layers, with many experts receiving similar token distributions. We introduce Iterative Token Routing Pruning (ITRP), a two-stage method that first identifies and removes expert modules with highly correlated routing patterns, followed by retraining with a knowledge distillation objective. Our experiments on standard language modeling benchmarks show that ITRP can reduce the number of active experts by 30-50% with minimal perplexity degradation. However, we observe that the effectiveness varies significantly across tasks, with reasoning-intensive benchmarks showing larger performance drops. While our approach provides computational benefits during inference, the required retraining phase introduces additional training overhead that may limit practical adoption. We acknowledge that concurrent work has explored similar directions, and our primary contribution lies in the systematic analysis of routing redundancy patterns and a computationally efficient pruning strategy. Our code and pruned models are available at [URL].",
    "id": 765
  },
  {
    "title": "Gradient Descent with Exponentially Increasing Learning Rates: A Bridge Between Convergence and Chaos",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Thompson, S."
    ],
    "abstract": "While traditional optimization theory recommends decaying learning rates for convergence, empirical observations suggest that certain neural networks benefit from increasing step sizes during training. We propose AgileGrad, a simple modification to standard SGD that exponentially increases the learning rate throughout training. Our theoretical analysis shows that under restricted conditions\u2014specifically for overparameterized linear regression with Gaussian features\u2014AgileGrad converges to a neighborhood of the global optimum with a rate that surprisingly improves for larger increase factors. We demonstrate that on small-scale vision tasks (CIFAR-10 with ResNet-18), AgileGrad achieves comparable performance to SGD+Momentum while requiring 20% fewer training steps. However, performance degrades significantly on larger architectures (ViT-base) and language tasks. Our ablation study reveals that the method is highly sensitive to the initialization scale and batch size. While our theoretical framework provides some justification for these empirical observations, the gap between our analysis assumptions and practical scenarios remains substantial. These results suggest that exponentially increasing learning rates may have niche applications but require careful tuning and are unlikely to replace standard decay schedules for general deep learning optimization.",
    "id": 766
  },
  {
    "title": "Gradient Surgery Makes Pre-trained Language Models More Robust to Domain Shift",
    "authors": [
      "Liu, M.",
      "Kumar, A.",
      "Chen, S."
    ],
    "abstract": "Large-scale pre-trained language models often struggle with distribution shifts between pre-training and downstream domains. While fine-tuning typically adapts these models effectively, we find that standard gradient descent can overfit to domain-specific features, hurting generalization. We propose Gradient Surgery for Domain Robustness (GSDR), a simple modification to fine-tuning that selectively removes gradient components likely to encode spurious correlations. Specifically, GSDR identifies gradient directions that maximize in-domain loss while minimally affecting held-out domain performance using a small validation set. Experiments on natural language inference and sentiment analysis across 6 domain pairs show GSDR improves average accuracy by 2.3% over standard fine-tuning, with gains concentrated on target domains dissimilar to the source. However, performance remains 5-7% below specialized domain adaptation methods. Ablations reveal the method works best with moderate domain shifts; in extreme shifts, the gradient projections become unreliable. While GSDR is computationally lightweight and implementation requires <20 lines of code, we acknowledge it provides incremental rather than transformative improvements. Our results suggest gradient-level interventions during fine-tuning may offer a practical middle ground between naive transfer and full domain adaptation.",
    "id": 768
  },
  {
    "title": "Gradient Surgery with Memory: A Simple Extension to Adam for Improved Transformer Fine-tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Adam-M, a simple modification to the Adam optimizer that incorporates gradient history beyond the exponential moving average. Motivated by observations that transformer fine-tuning exhibits distinct gradient patterns across layers, Adam-M maintains an explicit memory buffer of recent gradients for each parameter group, using lightweight attention mechanisms to weight historical information. While maintaining the computational efficiency of vanilla Adam, our method achieves modest but consistent improvements across GLUE tasks (average +0.8% over baseline) and three vision transformer benchmarks. We provide theoretical analysis showing Adam-M converges under similar assumptions to Adam, though our proof requires bounded gradient assumptions that may not hold in practice. Experiments reveal the benefits are most pronounced in low-data regimes (\u22641k examples) and small models (\u2264100M parameters). However, we observe minimal gains on larger models and standard datasets, suggesting limited scalability. Code and pre-trained checkpoints are available for reproducibility.",
    "id": 775
  },
  {
    "title": "Learning with Approximately Invariant Representations via Group-Theoretic Regularization",
    "authors": [
      "Johnson, K.",
      "Mukherjee, S.",
      "Zhao, L."
    ],
    "abstract": "We propose a lightweight regularization technique for enforcing approximate equivariance in neural networks without requiring explicit group structure preservation. Our approach encodes approximate invariance through a penalty term on feature covariance matrices, enabling learning with noisy or incomplete group annotations. The regularizer requires minimal architectural modifications and adds negligible computational overhead. We evaluate on rotation-invariant image classification and time-series forecasting tasks with synthetic symmetries. Experiments demonstrate consistent improvements over standard baselines (2-4% accuracy gains), though results are mixed compared to fully equivariant architectures. Theoretical analysis shows that our regularization provides a bounded approximation to true equivariance under Lipschitz continuity assumptions. While our method achieves reasonable empirical performance with minimal complexity, we observe degradation in settings with strong symmetry requirements. Our work suggests that approximate invariance may suffice for many practical applications, though we acknowledge limitations in extreme transformation regimes.",
    "id": 781
  },
  {
    "title": "Adaptive Gradient Clipping with Learnable Thresholds via Meta-Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is essential for training deep networks with exploding gradients, but selecting appropriate clipping thresholds remains largely heuristic. We propose MetaClipped, a method that learns clipping thresholds through meta-optimization on a validation loss. Our approach formulates the clipping threshold as a learnable parameter updated via implicit differentiation through the training trajectory. While simple in concept, we demonstrate improvements over fixed clipping on language modeling and vision tasks, achieving 2-3% better perplexity on Wikitext-103 and 1.2% higher accuracy on ImageNet when training unstable architectures. However, the method introduces computational overhead (15-20% slower training) and hyperparameter sensitivity. Our theoretical analysis shows bounded regret under convexity assumptions, though these conditions rarely hold in practice. Experiments reveal inconsistent gains across tasks, with significant improvements only when training dynamics are severely unstable. Code and hyperparameters will be released.",
    "id": 785
  },
  {
    "title": "Gradient Surgery is All You Need: Improving Multi-Task Learning with Simple Magnitude-Based Reweighting",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to suboptimal performance. While recent approaches like PCGrad and GradNorm use sophisticated gradient manipulation techniques, we propose a surprisingly simple alternative: Magnitude-based Gradient Reweighting (MGR), which rescales each task's gradient based solely on its L2 norm. Our method requires no hyperparameter tuning beyond the standard learning rate, making it practically appealing. We provide theoretical justification showing MGR approximately achieves gradient alignment when task gradients are L-smooth. Extensive experiments on 3 standard multi-task benchmarks (CIFAR-100, NYUv2, and Cityscapes) demonstrate MGR performs competitively with more complex methods, achieving 1-2% improvements over standard multi-task training while maintaining the same computational overhead. However, we observe MGR underperforms on tasks with significantly different gradient scales and can exhibit unstable training dynamics when gradient norms are computed on small batches. Our results suggest that simple gradient reweighting can be surprisingly effective, though careful normalization strategies may be needed for broader applicability.",
    "id": 786
  },
  {
    "title": "Attention is Not All You Need: Adapting Transformer Architectures for Resource-Constrained Settings via Structured Matrices",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Anderson, K."
    ],
    "abstract": "Large transformer models have achieved remarkable success across many domains, but their quadratic complexity in sequence length hinders deployment on resource-constrained devices. We propose SMFormer, a modification to the standard transformer architecture that replaces self-attention calculations with low-rank structured matrices. Our approach leverages Kronecker products and circulant decompositions to reduce memory complexity from O(n\u00b2) to O(n log n) while maintaining competitive performance. We evaluate SMFormer on standard NLP benchmarks (GLUE, SQuAD) and show that it achieves 85-92% of BERT-base performance while requiring 70% less memory on sequences of length 512. However, we observe significant performance degradation on tasks requiring long-range dependencies (e.g., RACE reading comprehension drops from 65.8% to 43.2%). Our method provides a practical middle ground between full attention and linear attention variants, though it lacks the theoretical guarantees of recent sparse attention methods. We release our implementation and pretrained models at [link anonymized].",
    "id": 791
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Trust Region Clipping for Sample-Efficient Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose a variance-reduced policy gradient method that combines adaptive trust region clipping with control variates for improved sample efficiency in continuous control tasks. Our approach extends proximal policy optimization by incorporating a learned baseline that dynamically adjusts based on local curvature estimates of the policy landscape. The key innovation is an adaptive clipping mechanism that modulates the effective step size based on gradient variance estimates, theoretically grounded in a variance-aware analysis that extends the standard policy gradient theorem. We evaluate our method on a suite of MuJoCO benchmarks and demonstrate 15-25% sample efficiency improvements over PPO on half of the environments, while maintaining comparable performance on the remainder. However, we observe degradation on tasks with sparse rewards, suggesting limitations in our variance reduction strategy when signal-to-noise ratios are low. Theoretical convergence guarantees are provided for the convex case, but extending these results to non-convex policy classes remains an open challenge. Our empirical results, combined with ablation studies showing marginal benefits from individual components, suggest the approach provides incremental rather than transformative improvements for policy optimization.",
    "id": 794
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning with Task-Specific Noise Adaptation",
    "authors": [
      "Liu, C.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder performance on individual tasks. While existing gradient surgery methods address gradient conflicts through projection schemes, we observe that these approaches neglect the inherent noise characteristics unique to each task. We propose Noise-Adaptive Gradient Surgery (NAGS), which incorporates task-specific noise estimates into the gradient conflict resolution process. Our method computes per-task gradient noise variance using a simple moving average of recent gradients, then weights the projection operation to preserve more reliable gradient directions. Experiments on CIFAR-100/SVHN supervised multi-task and NYUv2 scene understanding benchmarks show 2-3% improvements over standard gradient surgery baselines, with particular gains on harder tasks. However, we find that NAGS provides diminishing returns when task gradients are already well-aligned and adds non-trivial computational overhead (15-20% training slowdown). Our theoretical analysis suggests the noise-based weighting scheme converges to a stationary point under standard assumptions, though the convergence rate depends on unknown noise parameters. While our approach shows promise for scenarios with significant gradient conflicts, the practical benefits appear limited to specific task combinations, raising questions about the broader applicability of noise-aware gradient manipulation.",
    "id": 797
  },
  {
    "title": "Incremental Domain Adaptation through Progressive Feature Augmentation",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Rodriguez, S."
    ],
    "abstract": "Domain adaptation becomes challenging when target distributions evolve gradually over time, yet most existing methods assume abrupt shifts or stationary environments. We propose Progressive Feature Augmentation (PFA), a simple yet effective method for incremental domain adaptation where a pre-trained source model is adapted to gradually shifting target distributions. PFA operates by selectively augmenting features at each time step based on their estimated domain relevance, computed through a lightweight meta-network that learns to predict feature importance from minimal labeled target samples. Our method requires no retraining of the original model and adds only 2.5K additional parameters. Experiments on three standard benchmarks\u2014Rotating MNIST, Portraits, and Office-Home with synthetic drift\u2014show that PFA achieves 4-7% accuracy improvements over naive fine-tuning while using 60% less target data. However, we observe that gains diminish when domain shifts exceed 30\u00b0 rotations or when source and target domains have significantly disjoint label spaces. While our theoretical analysis guarantees monotonic improvement under strict assumptions that rarely hold in practice, we believe PFA offers a practical middle ground between expensive full retraining and fragile zero-shot adaptation.",
    "id": 799
  },
  {
    "title": "Gradient Descent with Dynamic Learning Rate Scaling: A Simple Heuristic for Faster Convergence",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "We propose Dynamic Learning Rate Scaling (DLRS), a lightweight modification to standard gradient descent that adaptively scales the learning rate based on the relative magnitude of past gradients. Unlike complex adaptive optimizers such as Adam or LAMB, DLRS introduces minimal computational overhead by using a simple exponentially weighted ratio of gradient norms. Our theoretical analysis shows DLRS achieves comparable convergence rates to vanilla SGD on strongly convex functions while requiring no hyperparameter tuning beyond the base learning rate. Empirically, we demonstrate 10-25% speedup in wall-clock time over SGD with momentum on ResNet-50 training on ImageNet and modest improvements on transformer fine-tuning tasks. However, our gains diminish on extremely large batch training and are inconsistent across different network architectures. While DLRS provides a practical alternative to manual learning rate tuning for practitioners, our results suggest the improvements are incremental rather than transformative. Code is available at [anonymized for review].",
    "id": 808
  },
  {
    "title": "Improving Transformer Training Stability Through Attention Rollback",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Garcia, M.",
      "Johnson, T."
    ],
    "abstract": "Transformer architectures often exhibit unstable training dynamics, particularly when scaling to larger models or longer sequences. We propose Attention Rollback, a simple regularization technique that intermittently resets attention weights to their initialization values during training. Our method requires no architectural modifications and adds minimal computational overhead. We conduct experiments on language modeling and machine translation tasks across various model sizes (125M to 1.3B parameters). While Attention Rollback reduces gradient norm variance by 15-30% compared to standard training, model perplexity improvements are modest (2-4%) and inconsistent across tasks. Ablation studies reveal that the effectiveness depends heavily on the rollback schedule and initialization scheme, suggesting the benefits may be largely due to modified optimization dynamics rather than fundamental architectural improvements. Theoretical analysis provides limited insight into when and why this approach works. We release our code to facilitate reproduction of these mixed but potentially useful results for practitioners struggling with training stability.",
    "id": 810
  },
  {
    "title": "LoNGA: Localized Non-Linear Gradient Adjustment for Improved Transformer Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kumar, S."
    ],
    "abstract": "Gradient-based fine-tuning of large pre-trained transformers often struggles with catastrophic forgetting and unstable training dynamics. We propose LoNGA, a simple modification to standard gradient descent that applies localized non-linear transformations to gradients based on their historical statistics. Specifically, LoNGA maintains per-parameter momentum estimates and applies a learned element-wise transformation to suppress large gradient updates in regions where the loss landscape exhibits high curvature. Our method introduces only 0.01% additional parameters compared to standard fine-tuning and can be implemented in <30 lines of PyTorch code. We evaluate LoNGA on GLUE and SuperGLUE benchmarks across five different pre-trained transformer architectures. Results show modest but consistent improvements over AdamW baseline (0.8 average GLUE score increase, 1.2 on SuperGLUE), particularly on tasks with limited training data. Ablations reveal that the non-linear transformation is more effective than simple clipping or normalization, while the localized aspect prevents the method from degrading to standard behavior. Though LoNGA does not outperform recent more complex approaches like Adapters or LoRA, it provides a lightweight alternative requiring no architectural changes. We release code and pre-trained checkpoints at anonymous-url.github.io/LoNGA.",
    "id": 812
  },
  {
    "title": "Gradient Descent with Momentum Works Even When the Momentum Isn't Optimal: A Non-Asymptotic Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We provide a non-asymptotic convergence analysis for gradient descent with Polyak momentum for strongly convex and smooth objectives. While classical analysis requires the momentum parameter to be set optimally, we show that convergence holds for a broader range of sub-optimal momentum values. Specifically, we prove that any momentum parameter in [1/4, 3/4] achieves \u0398(\u03ba log(1/\u03b5)) convergence rate, matching the optimal rate up to constant factors. Our analysis relies on a novel Lyapunov function that captures the dynamics when momentum is not tuned according to the condition number \u03ba. Empirically, we demonstrate that commonly used heuristic momentum schedules in practice fall within our theoretically justified range. While our contributions are technically correct, they serve primarily as a refinement of known results rather than addressing fundamentally new questions in optimization theory.",
    "id": 815
  },
  {
    "title": "Improving Transformer Training Stability Through Curvature-Aware Weight Initialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Curvature-Aware Initialization (CAI), a simple modification to standard weight initialization schemes for transformers that incorporates second-order information from the loss landscape. Our method computes a local curvature estimate using a small-batch approximation of the Hessian trace, then scales initial weights inversely proportional to layer-wise curvature magnitudes. Experimental results on language modeling tasks with GPT-2 medium (350M parameters) and BERT-base (110M parameters) show CAI reduces gradient norm variance by 23-31% during early training stages compared to Xavier/Glorot initialization. However, while CAI achieves marginally better perplexity on Wikitext-103 (18.7 vs 19.1) and converges 1.2x faster in wall-clock time, these improvements diminish as training progresses. Additional experiments on vision transformers show similar early-training benefits but no consistent downstream task improvements. Theoretical analysis proves CAI converges under standard assumptions, though the curvature estimates introduce a small asymptotic bias. While promising for reducing training instability, our results suggest the benefits of curvature-aware initialization may be limited to specific training regimes and could be achieved through simpler learning rate scheduling.",
    "id": 816
  },
  {
    "title": "Gradient Surgery with Memory: Improving Stability of Multi-Task Learning via Selective Gradient Dampening",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to destructive interference between tasks. While existing gradient surgery methods like PCGrad and GradDrop mitigate this issue, they can over-aggressively drop gradient information, harming learning dynamics. We propose MemGS (Memory-aware Gradient Surgery), which introduces a learnable memory buffer to selectively dampen gradient updates based on their historical alignment patterns. Our approach maintains a running estimate of per-task gradient directions over the past k iterations, then applies dampening factors that preserve aligned components while selectively reducing conflicting ones. We theoretically prove MemGS converges under mild assumptions and show empirically that it achieves better trade-offs between task interference and gradient information preservation. On three standard multi-task benchmarks (NYUv2, CityScapes, and QM9), MemGS outperforms PCGrad by 1.2-2.8% on average while using 15% fewer training iterations. However, we observe these gains diminish in scenarios with high task imbalance, suggesting the method's effectiveness depends on task distribution. Our results indicate MemGS provides marginal improvements over carefully tuned baselines, highlighting the challenge of general gradient surgery methods.",
    "id": 819
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its fixed rank selection across all layers remains suboptimal. We propose LoRA-Drop, a simple yet effective approach that dynamically prunes low-rank adapters during training based on gradient magnitude analysis. Our method initializes adapters with a conservative rank budget and iteratively drops the least significant rank components, reducing trainable parameters by 40-60% while maintaining downstream performance. We evaluate LoRA-Drop on GLUE, SuperGLUE, and domain-specific benchmarks using LLaMA-7B and RoBERTa-base. Results show comparable accuracy to standard LoRA (within 1.2% on average) with 25% fewer GPU hours during training. While our method achieves consistent parameter reduction, we observe performance degradation on complex reasoning tasks (e.g., 2.8% drop on BIG-Bench) and find the rank selection mechanism lacks theoretical guarantees. LoRA-Drop provides practical speedups for common fine-tuning scenarios but may require task-specific tuning of drop thresholds.",
    "id": 827
  },
  {
    "title": "Gradient Descent with Adaptive Step-Size Based on Local Gradient Variance",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose GRADVAR, an optimization method that adapts step-sizes based on local estimates of gradient variance for stochastic gradient descent. Motivated by the observation that gradient variance often correlates with optimization progress, our method computes a running estimate of variance using a small window of recent gradients and scales the step-size inversely proportional to this estimate. We provide theoretical analysis showing convergence under assumptions of Lipschitz continuity and bounded variance, achieving a convergence rate of O(1/\u221aT) for non-convex objectives. Experiments on CIFAR-10 and ImageNet with ResNet architectures demonstrate 5-15% faster convergence compared to Adam and SGD with momentum in early training stages, while performing comparably in final accuracy. However, we observe sensitivity to hyperparameter choices and minor degradation on language modeling tasks. GRADVAR offers a lightweight alternative to adaptive methods without requiring momentum buffers or second-moment estimates, though benefits appear limited to vision tasks with specific architectures. Code is available at anonymous-url.github.io/gradvar.",
    "id": 837
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "As distributed training scales to larger models and clusters, communication overhead from gradient synchronization becomes a critical bottleneck. We propose Learned Gradient Quantization (LGQ), a framework that dynamically adjusts the precision of gradient compression based on local curvature estimates. Unlike static quantization schemes, LGQ uses a lightweight meta-network to predict optimal bit-widths for each gradient tensor given loss surface characteristics. Our method achieves up to 4.7\u00d7 communication reduction with minimal accuracy loss on ResNet-50 and BERT-Large training. However, we observe diminishing returns beyond 64 GPUs and significant hyperparameter sensitivity across tasks. While LGQ matches TopK and PowerSGD baselines on standard benchmarks, convergence properties remain theoretically fragile, particularly for non-convex objectives. Our results suggest learned compression schedules can provide practical benefits for medium-scale training, though the approach may require additional stabilization techniques for extreme-scale deployment.",
    "id": 852
  },
  {
    "title": "Fast Proximal Policy Optimization via Curvature-aware Second-order Updates",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Proximal Policy Optimization (PPO) remains a dominant algorithm for reinforcement learning, but its sample complexity often limits practical applications. We propose Curvature-aware PPO (C-PPO), which incorporates second-order information without full Hessian computation by using diagonal approximations of the policy Hessian structure. Our method modifies the clipped surrogate objective with curvature-based trust regions, yielding faster convergence in low-data regimes. Experiments on continuous control benchmarks show 15-30% sample efficiency improvements over PPO on half of the MuJoCo environments, with comparable wall-clock time. However, performance gains diminish in high-dimensional action spaces, and we observe instabilities when applied to discrete action domains. While C-PPO demonstrates clear benefits in specific settings, the approach does not address fundamental PPO limitations in sparse reward scenarios. Our work suggests that lightweight second-order information can enhance policy gradient methods, though the technique faces practical challenges in scaling beyond medium-sized problems.",
    "id": 863
  },
  {
    "title": "Gradient Descent with Momentum is Provably Better than Adam at Minimizing Sharpness in Two-Layer ReLU Networks",
    "authors": [
      "Chen, L.",
      "Ivanov, D.",
      "Vaswani, A."
    ],
    "abstract": "We provide theoretical and empirical evidence that momentum-based gradient descent achieves better sharpness minimization compared to adaptive methods in overparameterized two-layer ReLU networks. Our theoretical contribution establishes a non-asymptotic convergence bound for momentum SGD on sharpness-related objectives, showing O(1/\u221aT) convergence for a modified sharpness measure we term effective local sharpness. While our bound depends on a strong assumption about initialization scale, our experiments on CIFAR-10 and ImageNet subsets demonstrate consistent sharpness reduction with momentum compared to Adam and RMSprop across various architectures. Experimental results show momentum achieves 15-20% lower sharpness scores after 100 epochs with comparable test accuracy. Our work suggests that the benefits of momentum in deep learning may extend beyond convergence speed to generalization through sharpness minimization. However, extending our theoretical results beyond two-layer networks remains challenging, and our empirical evaluation is limited to vision tasks. We provide code and a minimal reproduction package focusing on the sharpness evaluation protocols.",
    "id": 878
  },
  {
    "title": "Gradient Descent with Adaptive Restart via Local Curvature Estimation",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Garcia, M."
    ],
    "abstract": "While adaptive optimization methods like Adam and RMSprop achieve excellent practical performance, their theoretical understanding remains limited. We propose Curvature-based Adaptive Restart (CAR), a simple modification to gradient descent that uses cheap Hessian-vector products to estimate local curvature and trigger restarts when the optimization landscape becomes ill-conditioned. Our key insight is that the Rayleigh quotient of stochastic gradients provides an unbiased estimator of sharpness that can be computed in O(d) time. We prove that CAR achieves a convergence rate of O(\u221a(\u03ba)/T) for \u03ba-quadratic problems, matching standard gradient descent while avoiding the need for explicit learning rate tuning. Experiments on CIFAR-10 and ImageNet with ResNet architectures show 5-15% faster convergence compared to vanilla SGD+momentum, though gains diminish with sophisticated learning rate schedules. Our method introduces a single hyperparameter (restart threshold) that transfers well across tasks. However, we observe that CAR provides limited benefits on problems where the Hessian spectrum changes slowly. Our implementation requires only 15 lines of PyTorch code and minimal memory overhead.",
    "id": 886
  },
  {
    "title": "Gradient Descent with Learned Step Sizes: A Meta-Optimization Approach for Quadratic Objectives",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Learned Step Size Gradient Descent (LSSGD), a meta-optimization framework that learns optimal step sizes for gradient descent on quadratic objectives. While existing adaptive methods like Adam and RMSprop use heuristics based on gradient statistics, LSSGD trains a small neural network to predict step sizes given gradient directions and local curvature information. Our key insight is that for quadratic functions, optimal step sizes can be characterized by the ratio of gradient norm to curvature along the descent direction, which can be approximated efficiently using Hessian-vector products. We evaluate LSSGD on synthetic quadratic problems and two real-world applications: logistic regression and matrix completion. On 100-dimensional quadratic objectives, LSSGD achieves convergence in 35% fewer iterations than tuned SGD, while maintaining generalization to problems with different eigenvalue distributions. However, we find that LSSGD offers diminishing returns for high-dimensional problems (\n000 dimensions) and struggles with ill-conditioned objectives. Our results suggest that learning step sizes is most beneficial for medium-scale problems with moderately varying curvature. While our approach shows promise in specialized settings with repeated similar objective structures, the overhead of meta-learning may not justify the modest gains over carefully tuned baselines for general optimization tasks.",
    "id": 887
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: A Kernel Regime Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "The Training Dynamics of Gradient Descent (GD) in overparameterized neural networks exhibit rich phenomena that challenge classical optimization theory. We propose Gradient Surgery (GS), a simple modification to GD that selectively masks gradient components based on the alignment with the Neural Tangent Kernel (NTK) features. Our approach connects to recent work on the kernel regime, but deviates from the strict NTK limit by allowing controlled feature learning through selective gradient updates. We prove that GS converges linearly for two-layer networks under standard assumptions, though our theoretical bounds are tighter only in specific parameter regimes. Empirically, GS achieves comparable performance to standard training on CIFAR-10 and ImageNet subsets while reducing the effective rank of the feature covariance matrix by 15-30%. However, we observe diminishing benefits as network width increases, with GS performing comparably to standard GD for very large networks. Our work suggests that targeted gradient manipulation can provide modest improvements in sample efficiency when networks operate near the kernel-to-rich transition, though the practical impact remains limited compared to architectural innovations. Code and pre-trained models are available at anonymous.org/gradient-surgery.",
    "id": 889
  },
  {
    "title": "Adaptive Momentum via Gradient Variation Clipping for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Variation Clipping (GVC), a simple modification to standard momentum-based optimizers that adapts the momentum coefficient based on local gradient variation. Intuitively, when gradients exhibit high variance, GVC reduces momentum to prevent overshooting; when gradients are stable, it allows faster convergence. Formally, we derive a clipping threshold that ensures convergence for smooth non-convex objectives, matching the O(1/\u221aT) rate of stochastic gradient descent. Experiments on CIFAR-10/100 and ImageNet show modest improvements over SGD+Momentum and AdamW: 0.3-0.7% absolute accuracy gains on ResNet-18/50 and 1.2% on Vision Transformer, though benefits diminish with careful hyperparameter tuning. Ablations reveal the variation-based adaptation contributes more than the clipping mechanism itself. While GVC provides no theoretical breakthrough, its simplicity and plug-and-play nature could benefit practitioners seeking robust optimization without extensive tuning. Code and pretrained models are available.",
    "id": 890
  },
  {
    "title": "Gradient Surgery with Memory: Improving Multi-Task Learning Through Selective Gradient Replay",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to suboptimal performance across tasks. While recent approaches like PCGrad and GradDrop provide sophisticated gradient projection techniques, they fundamentally discard potentially useful gradient information. We propose Gradient Surgery with Memory (GSM), a method that stores discarded gradient components in a memory buffer and selectively replays them when they become less conflicting with other tasks. Our approach introduces a lightweight conflict detection mechanism based on cosine similarity thresholding, combined with a reservoir sampling strategy for memory management. We evaluate GSM on standard multi-task vision benchmarks including NYUv2 and Cityscapes, achieving modest improvements over PCGrad (average delta +0.8% on primary metrics) while adding minimal computational overhead (5% increase in training time). However, we observe that gains are inconsistent across task combinations and largely disappear when tasks are well-aligned. We provide empirical evidence that GSM's benefits correlate strongly with intrinsic task conflict levels, suggesting the method may have limited applicability beyond explicitly conflicting scenarios. Code and experiments are available at [anonymous link].",
    "id": 891
  },
  {
    "title": "Momentum-Scheduled Dropout: A Simple Fix for Overfitting in Transformer Fine-tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Abebe, S."
    ],
    "abstract": "Transformer models consistently struggle with overfitting when fine-tuned on small datasets, particularly in low-resource NLP tasks. While dropout remains the dominant regularization technique, its static application during training fails to adapt to the model's evolving capacity. We propose Momentum-Scheduled Dropout (MS-Drop), which dynamically adjusts dropout rates based on gradient momentum norms during fine-tuning. Our method applies a simple scheduling function that reduces dropout in layers with stable gradient magnitudes while maintaining or increasing regularization where gradients fluctuate significantly. Across eight GLUE tasks with limited training data (1K-4K examples), MS-Drop achieves modest but consistent improvements over standard dropout (average +1.2% absolute F1), particularly on single-sentence tasks. Ablation studies reveal that our momentum-based scheduling captures the same regularization effect as early stopping but prevents the 4-6% accuracy drops typically seen when fine-tuning continues beyond optimal stopping points. While MS-Drop is straightforward to implement with two additional lines of code, our theoretical analysis remains limited to linear approximations of the transformer layers, leaving open questions about its interaction with attention mechanisms.",
    "id": 903
  },
  {
    "title": "An Empirical Study of Gradient Noise Scale Regularization for Improving Transformer Training Stability",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, J."
    ],
    "abstract": "We propose gradient noise scale regularization (GNSR) as a simple technique to stabilize training of large transformers. Motivated by theoretical work linking gradient noise scales to training dynamics, we add a lightweight loss term that penalizes large noise-to-signal ratios during optimization. Our method requires no architectural changes and introduces minimal computational overhead. We evaluate GNSR on standard language modeling and translation benchmarks using base-size models (340M parameters) trained on standard datasets. Results show consistent but modest improvements: 0.3-0.7 BLEU on translation tasks and 2-3 perplexity points on WikiText-103, while reducing training variance across 3 random seeds. Ablation studies reveal most benefits come from early training regularization rather than asymptotic performance gains. While our approach shows promise for practitioners facing training instability, we acknowledge limitations including unclear theoretical guarantees and diminishing returns on well-tuned baselines. Code and hyperparameters will be released upon acceptance.",
    "id": 911
  },
  {
    "title": "Re-weighted Gradient Descent: A Simple Heuristic for Better Generalization in Overparameterized Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose re-weighted gradient descent (RGD), a simple modification to standard gradient descent that re-weights updates based on parameter norms during training. Our method applies an element-wise rescaling factor \u03b3 \u2208 (0,1] to gradients, where \u03b3 decreases monotonically with parameter magnitude. Intuitively, this encourages the optimizer to focus on smaller parameters, potentially biasing solutions toward lower effective rank representations. We provide theoretical analysis showing RGD minimizes an upper bound on the Rademacher complexity for two-layer ReLU networks, though our bound requires restrictive assumptions on the data distribution. Empirically, RGD demonstrates modest but consistent improvements over SGD with momentum on CIFAR-10/100 (0.4-0.8% accuracy gains) and ImageNet (0.3% top-1 improvement) when training ResNet-18/50 architectures. Ablations reveal most benefits occur in sparse training regimes with learning rate decay schedules. While our method is computationally cheap and simple to implement, gains remain small compared to recent regularization techniques, and we observe performance degradation on some NLP tasks (BERT fine-tuning on GLUE). Our code will be made available upon acceptance.",
    "id": 924
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning via Parameter-Specific Importance Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning in neural networks often suffers from gradient conflicts that lead to catastrophic forgetting when tasks are learned sequentially. While recent gradient surgery methods mitigate these conflicts through gradient projection or scaling, they remain memory-intensive and can overly constrain gradient directions, potentially limiting performance on complex tasks. We propose MEM-GS (Memory-aware Gradient Surgery), a simple yet effective approach that selectively applies gradient surgery based on parameter-specific importance measures computed via Fisher information. Our method maintains a small reservoir of important parameters for each task, applying surgical constraints only when the gradient would significantly impair these parameters. On CIFAR-100 and CelebA benchmarks with 5 sequential tasks, MEM-GS achieves 3.2% average improvement over standard gradient surgery while reducing memory overhead by 47%. However, we find limited benefits on tasks with high semantic similarity, where gradient conflicts are naturally smaller. Our results suggest that selective gradient surgery is most valuable for dissimilar tasks, though gains diminish as model capacity increases. Code is available at [anonymous] but lacks comprehensive hyperparameter ablation studies.",
    "id": 930
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Learned Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to high-variance gradient estimators. While control variates can reduce variance, their effectiveness is limited by the quality of baseline functions. We propose a meta-learning approach that learns to predict optimal control variate baselines for REINFORCE-style estimators. Our method trains a small neural network that takes local context (layer activations, parameter norms) as input and outputs baseline values that minimize gradient variance. On MNIST-VAE experiments, our approach achieves 15-30% lower gradient variance compared to standard baselines, translating to modest improvements in likelihood (0.5-1.2 nats improvement) and perceptual quality scores. Theoretical analysis shows our learned baselines reduce variance by implicitly capturing correlations between gradients and model parameters. While our improvements are consistent, they remain incremental compared to recent work on continuous relaxations. Our computational overhead is roughly 5-10% during training. We release PyTorch code for reproducibility.",
    "id": 941
  },
  {
    "title": "Revisiting Momentum in Federated Learning: A Simple Modification with Modest Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "Federated momentum methods often diverge on non-IID data, leading practitioners to disable momentum entirely. We propose FedMom++, a straightforward modification to standard federated momentum that introduces client-specific momentum buffers weighted by participation frequency. Our approach requires only two additional scalars per client and negligible communication overhead. Theoretical analysis shows convergence rates comparable to vanilla FedAvg under standard assumptions, with slightly improved constants on client drift. Experiments on CIFAR-10, FEMNIST, and StackOverflow achieve 2-4% accuracy improvements over strong baselines, though these gains vanish with careful hyperparameter tuning. Notably, FedMom++ shows consistent but small improvements across different participation patterns. While our method does not address all challenges of federated momentum, its simplicity and minimal computational cost make it a practical drop-in replacement. Code is available at [anonymous link].",
    "id": 946
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning through Selective Parameter Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts and catastrophic forgetting when tasks compete for shared parameters. We propose Gradient Surgery with Memory (GSM), a simple modification to existing multi-task optimization that maintains an episodic memory of past gradients to inform selective parameter updates. Our method identifies conflicting gradients through cosine similarity analysis and selectively freezes or adapts learning rates for specific parameters based on their contribution to both current and previous tasks. On CIFAR-100 split into 5 tasks and a new benchmark of 8 NLP tasks from GLUE and SuperGLUE, GSM achieves comparable performance to state-of-the-art methods while requiring 30% less memory and 2x fewer hyperparameters. However, we find that GSM's benefits diminish when task similarity is low or when the number of tasks exceeds 15. While the approach shows promise, our theoretical analysis reveals that the selective update rule can create undesirable local minima in specific parameter regimes. Our results suggest that gradient-based solutions to catastrophic forgetting may be fundamentally limited when task distributions differ significantly.",
    "id": 949
  },
  {
    "title": "Residual Policy Gradient Methods with Adaptive Trust Region Scaling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Policy gradient methods often struggle with sample efficiency and stable convergence, particularly in high-dimensional action spaces. We propose Residual Policy Gradient (RPG), a simple modification to standard policy gradient algorithms that learns a residual correction to a base policy using adaptive trust region constraints. Our key insight is that the residual formulation allows for more aggressive updates while maintaining monotonic improvement guarantees. We derive theoretical bounds showing that RPG achieves comparable sample complexity to TRPO while requiring fewer hyperparameter tunings. Empirical evaluation on continuous control benchmarks demonstrates 15-30% improvement over PPO and SAC baselines on half of the MuJoCo environments, though results are inconsistent across domains. Our ablation studies reveal that the adaptive trust region is crucial: removing it degrades performance below vanilla policy gradient levels. While our theoretical analysis relies on strong assumptions about Lipschitz continuity that may not hold in practice, we provide empirical evidence suggesting the method remains effective when these assumptions are violated. Code and pre-trained models are available at anonymous-github-link.",
    "id": 957
  },
  {
    "title": "Improving Transformer Training Stability via Layer-wise Perturbation Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Transformer training instability remains a persistent challenge, particularly when scaling to deeper architectures. We propose a lightweight layer-wise perturbation analysis framework that identifies unstable layers during training and applies targeted regularization. Our method computes gradient covariance statistics at each layer and introduces a novel regularization term that penalizes directions with high gradient variance. This approach requires minimal computational overhead (less than 2% increase in training time) and can be integrated into existing training pipelines without architectural modifications. Experiments on language modeling and machine translation tasks with 12-24 layer Transformers show modest improvements: 0.3-0.7 BLEU score gains on WMT14 English-German translation and 1.2 perplexity reduction on Wikitext-103. While our regularization improves training stability metrics including gradient norm consistency and loss curve smoothness, ablation studies reveal that benefits diminish with careful hyperparameter tuning of baseline models. Code and pre-trained models will be released upon acceptance.",
    "id": 960
  },
  {
    "title": "Improving Transformer Efficiency through Layer-wise Gradient Magnitude Pruning at Initialization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Gradient-Aware Initialization Pruning (GAIP), a method for pruning transformer networks at initialization using layer-wise gradient magnitude analysis. Our approach computes gradient magnitudes for each layer using a small batch of training data, then removes the lowest magnitude 30% of weights before training begins. Unlike existing pruning methods that require extensive retraining or complex algorithmic overhead, GAIP operates in a single forward-backward pass at initialization. Experiments on GLUE and WMT translation tasks show GAIP reduces parameter counts by 25-35% while maintaining 92-96% of original performance across tasks, compared to 87-94% for magnitude pruning baselines. The method achieves 1.1x training speedup and modest memory savings without architectural modifications. Our theoretical analysis connects gradient magnitudes at initialization to final layer importance through a simplified linear approximation, though we acknowledge this provides only loose bounds for deeper networks. Limitations include slight convergence instability on smaller datasets and reduced effectiveness on downstream tasks requiring substantial fine-tuning. While GAIP offers practical benefits for practitioner workflows, particularly the ability to prune models without hyperparameter search, we recognize the theoretical foundations require strengthening for more principled guarantees.",
    "id": 961
  },
  {
    "title": "Gradient Amplification Makes Transformers More Efficient: A Surprising Discovery in Language Model Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We present a simple yet counterintuitive technique that significantly accelerates transformer training by selectively amplifying gradient magnitudes during backpropagation. Our method, termed Gradient Amplification for Transformers (GAT), multiplies gradients by a learned factor \u03b2 at each layer, where \u03b2 is dynamically computed using a lightweight parameter prediction network. While gradient clipping is standard practice to prevent exploding gradients, we show that controlled amplification in early training stages leads to faster convergence and improved final perplexity across 7 language modeling datasets. Experiments on GPT-2 (124M-774M parameters) demonstrate 1.15-1.3\u00d7 speedup in wall-clock time to match baseline performance, though benefits diminish for larger models. Theoretical analysis reveals GAT implicitly performs a form of trust-region optimization, explaining its efficacy. However, we observe the technique can destabilize training for certain hyperparameter configurations, particularly with large learning rates. Our method requires minimal code changes and adds <0.5% parameter overhead. While the speed improvements are consistent, they are modest; we acknowledge GAT may be most useful as a practical training trick rather than a fundamental advance.",
    "id": 964
  },
  {
    "title": "Single-Timeline Transformer: Efficient Attention for Long Streaming Inputs",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "While transformers have demonstrated remarkable capabilities across domains, their quadratic complexity in sequence length remains a fundamental limitation for streaming applications. We propose Single-Timeline Transformer (STT), a simple modification to standard attention that processes sequences along a single temporal timeline rather than in discrete chunks. STT maintains a fixed-size memory bank that dynamically ages out older hidden states based on learnable temporal decay, eliminating the need for complex hierarchical attention or sparse patterns. Our method requires only 8 lines of code to implement and can be dropped into existing transformer architectures with minimal hyperparameter tuning. On long-range language modeling benchmarks (PG-19, arXiv), STT achieves perplexity within 3% of Longformer while using 4x fewer parameters and training 1.5x faster on 8xA100 GPUs. However, we observe instabilities on sequences longer than 64k tokens, and our fixed decay rate may be suboptimal for certain data distributions. While STT provides a practical alternative to existing long-form methods, its reliance on monotonic forgetting might limit performance on tasks requiring fine-grained long-range dependencies.",
    "id": 967
  },
  {
    "title": "Self-Supervised Contrastive Learning with Adaptive Negative Sampling and Curriculum",
    "authors": [
      "Chen, L.",
      "Johnson, M.K.",
      "Rodriguez, A."
    ],
    "abstract": "We propose Adaptive Curriculum Contrastive Learning (ACCL), a self-supervised framework that dynamically adjusts negative sample selection during training. While recent contrastive methods rely on fixed negative mining strategies, we observe that the importance of negatives varies significantly across training stages. Our approach uses an auxiliary network to predict the informativeness of negative samples based on gradient signals, prioritizing hard negatives early in training and shifting to more diverse negatives later. We introduce a curriculum schedule that gradually increases the number of negatives from 32 to 4,096, guided by an entropy-based measure of feature collapse. On ImageNet-1K, ACCL achieves 69.1% linear evaluation accuracy with ResNet-50, improving over SimCLR by 1.8% while using 23% less training time. The method also shows consistent gains on transfer tasks including CIFAR-10 (+1.2%) and Places205 (+0.9%). However, our ablations reveal that the adaptive mechanism provides only marginal benefits over strong baselines when proper hyperparameters are selected. Code and pretrained models will be made available upon acceptance.",
    "id": 970
  },
  {
    "title": "Improving Neural Network Robustness via Progressive Input Gradient Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We propose a novel regularization technique for improving adversarial robustness in neural networks by penalizing the input gradients during training. Unlike previous approaches that use fixed regularization weights, our method progressively increases the penalty based on the local Lipschitz constant estimated during training. We demonstrate improvements on CIFAR-10 and ImageNet against PGD attacks, achieving 2-3% better robust accuracy compared to standard adversarial training baselines. While our approach is computationally lightweight (adding <5% training time) and works with existing architectures, we observe that the benefits diminish on stronger attacks not seen during training. Theoretical analysis shows our regularizer bounds the local Lipschitz constant but relies on strong assumptions about data manifold structure. Experiments on additional datasets and attack scenarios yield inconsistent results, highlighting the need for careful hyperparameter tuning. Our method provides a practical trade-off between clean accuracy and robustness but may not directly advance the fundamental understanding of adversarial examples. Code and pre-trained models will be made available.",
    "id": 992
  },
  {
    "title": "Improving Transformer Training via Layer-wise Curriculum Learning",
    "authors": [
      "Liu, S.",
      "Morrison, J.",
      "Chen, B."
    ],
    "abstract": "We investigate whether gradually increasing transformer depth during training can improve final performance and training efficiency. Our approach, Progressive Depth Curriculum (PDC), begins training with a shallow 4-layer transformer and incrementally adds layers every 3 epochs until reaching the full architecture. We present a simple addition technique that preserves learned representations when adding new layers, based on duplicating and perturbing existing weights. Experiments on English-French translation and WikiText-103 language modeling show modest but consistent improvements: 0.3 BLEU score gains on WMT'14 and 0.8 perplexity reduction compared to baseline training. While these improvements are statistically significant (p<0.05), the practical impact is marginal. Our analysis reveals that PDC primarily helps by providing better initialization for deeper layers, reducing early training loss, but fails to address fundamental optimization challenges in transformers. The method introduces minimal computational overhead (97.5% of baseline training time) and can be implemented in under 20 lines of PyTorch code. However, ablations show benefits disappear when using stronger optimizers like AdamW, suggesting curriculum effects may be optimizer-dependent. We conclude that while layer-wise curriculum learning offers theoretical appeal, its practical utility appears limited for standard transformer architectures.",
    "id": 994
  },
  {
    "title": "Gradient Noise Improves Sharpness-Aware Minimization in Low-Resource Settings",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "We investigate the empirical observation that adding carefully calibrated Gaussian noise to gradients during training can sometimes improve generalization in Sharpness-Aware Minimization (SAM). Through experiments on CIFAR-10/100 with ResNet-18 and Vision Transformers, we find that gradient noise with standard deviation \u03c3 \u2248 0.01 \u00d7 ||g||/\u221ad (where g is the gradient and d is parameter count) can improve test accuracy by 1-2% in low-data regimes (5-20% of training data). Our theoretical analysis for quadratic loss functions suggests this benefit emerges from noise-induced smoothing of SAM's implicit regularizer, though the effect diminishes with larger datasets. While experiments on ImageNet show minimal improvement, our results suggest gradient noise could be a simple enhancement for SAM in data-limited scenarios. Code and checkpoints are available at [link].",
    "id": 998
  },
  {
    "title": "Gradient Descent with Periodic Restarts Improves Robustness to Dataset Corruption",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Restarted Gradient Descent (RGD), a simple modification to standard gradient descent that periodically resets the optimizer state while maintaining a decaying learning rate schedule. Our key observation is that common vision datasets contain memorizable corrupted examples that can dominate the learning dynamics, and periodic restarts help escape these spurious minima. We provide theoretical analysis showing RGD achieves similar convergence rates to standard GD on convex objectives while offering improved robustness to label noise. Empirically, we demonstrate 2-5% accuracy improvements over vanilla SGD on CIFAR-10 and ImageNet when 20-40% of labels are corrupted. While our method shows promise for noisy training scenarios, we acknowledge the improvements are modest and task-specific. Ablation studies reveal the benefits diminish as training duration increases, suggesting RGD may primarily act as a form of implicit regularization. Our code is available, though we note hyperparameter sensitivity in the restart schedule requires careful tuning. These results suggest periodic restarts as a lightweight addition to existing training pipelines when robustness to data quality is a concern.",
    "id": 1004
  },
  {
    "title": "Gradient Descent with Memory-Augmented Step Sizes: A Lightweight Approach to Adaptive Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose MANGO (Memory-Augmented Neurally Guided Optimization), a lightweight optimizer that augments standard gradient descent with a small recurrent network trained to predict step sizes based on historical gradient patterns. Unlike expensive learned optimizers, MANGO uses only 32 hidden units and operates online without meta-training on task distributions. Our key insight is that recent gradient history contains sufficient signal to predict reasonable step sizes for many practical problems, suggesting that complex learned optimizers may be overparameterized. We evaluate MANGO on image classification benchmarks and language modeling tasks, where it achieves competitive performance to Adam/AdamW while using 10-50x fewer parameters than previous learned optimizers. However, MANGO shows inconsistent gains on transformer architectures and fails to outperform SGD+momentum on some ResNet experiments. Analysis reveals the learned step size policy primarily exploits second-order structure that could be captured more efficiently by quasi-Newton methods. While our approach provides a middle ground between hand-designed and fully learned optimizers, its benefits appear constrained to moderate-scale vision tasks. Code and experiments are available at [URL].",
    "id": 1005
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique for Stochastic Optimization via Iterative Gradient Masking",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Gradient Dropout, a novel regularization technique that randomly zeros out gradient components during optimization. Unlike standard dropout which operates on activations, our method stochastically masks a fraction of gradient entries in each iteration, theoretically reducing the effective Lipschitz constant of the loss landscape. While similar in spirit to gradient clipping and noise injection, Gradient Dropout achieves regularization by selectively ignoring gradient directions rather than scaling them. We provide convergence guarantees for convex objectives under standard assumptions, extending standard SGD analysis to our biased gradient estimator. Empirically, we observe modest improvements over vanilla SGD on CIFAR-10 and CIFAR-100 with ResNet architectures, achieving 0.3-0.5% absolute accuracy gains on average. However, performance degrades on smaller networks and more challenging datasets like ImageNet. Our results suggest the technique's benefits are most pronounced when the gradient signal contains significant noise or redundancy. While the theoretical analysis is complete, the practical impact appears limited to specific regimes. Code is available to reproduce all experiments.",
    "id": 1012
  },
  {
    "title": "Gradient Descent with Layer-wise Learning Rates for Improved Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Layer-Adaptive Gradient Descent (LAGD), a simple modification to standard SGD that uses different learning rates for different transformer layers based on gradient norm statistics. Unlike existing adaptive optimization methods that modify parameter update directions, LAGD maintains the same update direction but scales step sizes using a lightweight running average of squared gradient norms. We show that this approach leads to faster convergence in early training phases for both language modeling and vision transformers, achieving 5-12% speedup in reaching target validation loss on CIFAR-10 and Wikitext-103. While LAGD appears to help optimization dynamics, we find the benefits diminish with careful hyperparameter tuning of baselines and do not consistently translate to downstream task improvements. Theoretical analysis reveals the method can be viewed as approximate diagonal preconditioning, though with weaker guarantees than full adaptive methods. Our experiments demonstrate the approach is particularly effective when base learning rates are mis-specified, suggesting LAGD primarily provides robustness benefits rather than fundamentally better optimization paths.",
    "id": 1013
  },
  {
    "title": "LoRA-Prune: Adaptive Low-Rank Adaptation for Efficient Parameter-Efficient Fine-Tuning",
    "authors": [
      "Kumar, S.",
      "Li, J.",
      "Garcia, M."
    ],
    "abstract": "We present LoRA-Prune, a method that combines low-rank adaptation with structured pruning to enable more parameter-efficient fine-tuning of large language models. While LoRA has emerged as a popular approach for reducing memory requirements during fine-tuning, we observe that many adapted low-rank matrices remain highly sparse, suggesting room for further compression. Our key insight is that the rank of LoRA adaptations can be dynamically adjusted per layer based on gradient information, while simultaneously removing unimportant rows/columns through magnitude-based pruning. We introduce a simple thresholding scheme that identifies low-contribution ranks without requiring additional validation data or expensive retraining. Experiments on GLUE and SQuAD benchmarks using RoBERTa-Large show LoRA-Prune achieves 35-50% parameter reduction over standard LoRA with <2% performance degradation in most tasks. However, we find the method underperforms on tasks requiring complex reasoning (e.g., DROP), suggesting the pruning heuristic may be overly aggressive. While additional gains are modest compared to existing compression techniques, LoRA-Prune offers a lightweight drop-in replacement for standard LoRA that reduces memory footprint without architectural changes. Code and pre-trained adapters will be made available.",
    "id": 1015
  },
  {
    "title": "Frozen Pre-trained Transformers are Already Good Tabular Feature Extractors",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "While deep learning has dominated vision and NLP, tabular data remains the domain of tree-based methods like XGBoost and Random Forests. We investigate whether frozen pre-trained transformers can serve as strong tabular feature extractors without fine-tuning. By encoding numerical features as text tokens and categorical features as special tokens, we feed tabular rows into frozen BERT and RoBERTa models. We demonstrate that the resulting representations, when combined with simple linear models or shallow networks, achieve competitive performance on 20 benchmark datasets (average AUROC 0.834 vs. 0.839 for XGBoost). Surprisingly, our frozen transformer approach outperforms gradient-boosted trees on 8 datasets, particularly those with high-cardinality categorical variables. However, the effectiveness varies dramatically across domains \u2014 transformers excel on e-commerce and survey data but struggle on low-dimensional datasets. Our results suggest transformer-based tabular methods may not require expensive fine-tuning, but questions remain about scalability to large tables and interpretability of learned representations.",
    "id": 1019
  },
  {
    "title": "Revisiting Batch Normalization Through the Lens of Low-Rank Weight Matrices",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "We investigate the relationship between batch normalization and the low-rank structure of neural network weight matrices. While batch normalization is widely used for training deep networks, its interaction with parameter redundancy remains poorly understood. We propose a simple method that exploits low-rank approximations of weight matrices to reduce the computational cost of batch normalization during training. Our approach combines truncated SVD with a modified normalization scheme that operates on the compressed representation. We demonstrate 20-30% reduction in training time on standard image classification benchmarks (CIFAR-10/100, ImageNet) with minimal accuracy loss (<1%). However, we find that our method struggles with very deep networks (>100 layers) and certain architectures like transformers. Theoretical analysis suggests our approximation error grows with the effective rank of activations, though we lack tight bounds. While our results show promise for efficient training, the gains over existing pruning methods are incremental, and the general applicability beyond vision tasks remains unclear. Our implementation and trained models will be made publicly available.",
    "id": 1021
  },
  {
    "title": "Improved Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rosenfeld, N."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed training, but existing methods rely on hand-crafted schedules that may not adapt to changing data distributions. We propose AdaptiveQuant, a simple approach that learns compression schedules during training using a lightweight meta-controller. Our method augments standard quantization schemes with a small recurrent network that predicts optimal bit-widths at each layer based on gradient statistics and training progress. Unlike prior work, AdaptiveQuant requires no hyperparameter tuning and adds minimal computational overhead (2-3% training time increase). We evaluate on CIFAR-10 and ImageNet with ResNet-50, showing modest improvements over fixed schedules: 3-5% better accuracy at the same compression rate, or 10-15% additional compression at comparable accuracy. While our theoretical analysis is limited to smooth convex objectives, empirical results suggest the approach extends to non-convex settings. Code will be released upon acceptance.",
    "id": 1045
  },
  {
    "title": "Improved Gradient Estimation for Stochastic Neural Networks via Learned Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a simple extension to the reparameterization gradient estimator that reduces variance through learned control variates. Our method trains an auxiliary neural network to predict the gradient magnitude given the input parameters, using this prediction to scale a zero-mean control variate. Unlike recent techniques requiring model-specific derivations, our approach applies directly to any reparameterizable distribution. Empirical evaluation on variational autoencoders shows 10-30% variance reduction on MNIST and CIFAR-10 compared to standard reparameterization, with minimal computational overhead. While the improvement is consistent across architectures, we find limited benefits when combined with strong entropy regularization, suggesting the technique is most effective in high-temperature regimes. Code is available at [anonymous link].",
    "id": 1046
  },
  {
    "title": "Faster SGD with Randomized Coordinate Momentum: A Two-Line Update Rule That (Sometimes) Works",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Stochastic gradient descent (SGD) remains the workhorse for large-scale optimization despite extensive research into momentum-based variants. We propose Randomized Coordinate Momentum (RCM), a simple modification that applies momentum updates to randomly selected coordinates while keeping others unchanged. This yields a two-line update rule requiring no additional hyperparameters beyond standard momentum coefficients. We show that RCM achieves comparable convergence to full-gradient momentum on strongly convex problems while reducing per-iteration computational cost by the coordinate selection ratio \u03b1 \u2208 (0,1]. For non-convex deep learning tasks, our ImageNet experiments with ResNet-50 show 15-20% faster wall-clock training time compared to SGD+momentum at \u03b1=0.5, though gains diminish for smaller batch sizes. Theoretical analysis reveals convergence rates that match SGD up to constant factors, but our bounds depend on coordinate-wise Lipschitz constants that may be difficult to estimate in practice. While our method lacks the strong theoretical guarantees of variance-reduced methods and the robust performance of Adam variants, RCM provides a practical trade-off between computational efficiency and convergence speed when memory bandwidth is the primary bottleneck. Code is available at anonymous.url/RCM.",
    "id": 1047
  },
  {
    "title": "Variance-Reduced Zeroth-Order Optimization with Adaptive Step-Size Selection via Gaussian Process Bandits",
    "authors": [
      "Chen, L.",
      "Kumar, A.",
      "Rosenfeld, E."
    ],
    "abstract": "We study derivative-free optimization for black-box functions with limited function evaluations. While zeroth-order methods typically rely on fixed step-sizes or simple decay schedules, we propose an adaptive approach that learns good step-sizes online using a Gaussian process surrogate model. Our method combines variance-reduced gradient estimates with a bandit-style step-size selection mechanism that balances exploration and exploitation. We provide theoretical analysis showing O(d/T) convergence rate for smooth non-convex objectives, matching existing variance-reduced zeroth-order methods. Empirically, we demonstrate modest improvements over baselines on hyperparameter tuning tasks, with average speedups of 1.3x on neural architecture search benchmarks. However, our method introduces additional hyperparameters and computational overhead that may limit practicality. We discuss potential extensions and limitations of our approach.",
    "id": 1050
  },
  {
    "title": "LoRA-FT: Mitigating Catastrophic Forgetting in Parameter-Efficient Fine-Tuning through Low-Rank Attention Fusion",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods like LoRA have gained popularity for adapting large language models, yet they suffer from catastrophic forgetting when sequentially fine-tuned on multiple tasks. We propose LoRA-FT, a simple yet effective modification that combines low-rank adaptation with an attention-based fusion mechanism to preserve previously learned knowledge. Our approach maintains LoRA's parameter efficiency while introducing task-specific attention gates that selectively activate relevant low-rank adaptations based on input characteristics. We evaluate LoRA-FT on standard NLP benchmarks using T5-large and Llama2-7B models across five diverse tasks. Experimental results show modest improvements over vanilla LoRA (avg. +1.3% task performance, +2.8% catastrophic forgetting reduction), particularly beneficial in low-data regimes. While the gains are incremental and computational overhead increases by 15% due to the attention mechanism, our ablation studies reveal that the fusion component contributes meaningfully to performance retention. The method addresses a practical limitation of existing PEFT approaches, though we acknowledge that the improvements are task-dependent and may not justify the additional complexity in all scenarios. Code and experiments are available at [URL withheld for anonymity].",
    "id": 1062
  },
  {
    "title": "Improving Transformer Training Stability Through Layer-Wise Learning Rate Warmup",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "M\u00fcller, S."
    ],
    "abstract": "We propose a simple modification to transformer training that improves optimization stability without architectural changes. Our method applies layer-specific learning rates that warm up at different rates, with lower layers warming up faster than upper layers. This approach is motivated by observations that gradient norms vary substantially across transformer layers, particularly during early training. We evaluate our method on Wikitext-103 language modeling and GLUE fine-tuning tasks, showing modest improvements in perplexity (0.5-1.2% relative) and downstream accuracy (0.3-0.8% absolute) over standard warmup procedures. While the improvements are incremental rather than transformative, our method reduces training instability observed in 15% of random seeds across experimental settings, suggesting practical benefits for reproducibility. The approach adds minimal computational overhead and can be integrated into existing training pipelines with <10 lines of code. However, we find limited benefits on larger-scale experiments (e.g., GPT-2 medium), raising questions about the method's scaling properties. Our contributions are primarily empirical rather than theoretical, and while the method shows promise for small-to-medium scale applications, further analysis is needed to understand the mechanism of action. Code is available at [anonymous link].",
    "id": 1063
  },
  {
    "title": "Momentum-Aware Perturbation Scheduling: A Simple Extension to SAM for Improved Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a popular regularizer for improving generalization in deep neural networks, yet its reliance on uniform perturbation magnitudes across training iterations may limit effectiveness. We propose Momentum-Aware Perturbation Scheduling (MAPS), a lightweight modification to SAM that adaptively adjusts the perturbation radius based on per-parameter momentum histories. Our method introduces two scalar hyperparameters that modulate how aggressively the perturbation budget increases or decreases as a function of gradient stability. Extensive experiments on CIFAR-10/100 and ImageNet with ResNet-18 and ViT-Tiny architectures demonstrate that MAPS achieves 0.3-1.2% improvements over vanilla SAM at comparable computational cost, with particularly pronounced gains in low-data regimes. While our theoretical analysis is limited to quadratic objectives and the improvements on language modeling tasks (IWSLT14 De-En, Wikitext-103) are marginal (0.1-0.4 BLEU/perplexity), we believe the simplicity and minimal overhead of MAPS makes it a practical drop-in enhancement for practitioners using SAM. Code is available at [redacted for submission].",
    "id": 1065
  },
  {
    "title": "Scheduled Sampling with Learned Mixing Rates for Neural Machine Translation",
    "authors": [
      "Liu, K.",
      "Thompson, B.",
      "Chen, J."
    ],
    "abstract": "We propose MixRate, a simple extension to scheduled sampling that learns instance-specific mixing rates between ground truth and model predictions during training. While scheduled sampling has shown modest improvements in sequence-to-sequence models, its linear decay schedules often mismatch the model's actual error rates across training. Our method uses a small auxiliary network to predict optimal mixing rates based on decoder hidden states and attention distributions. Experiments on WMT'16 English-Romanian and IWSLT'14 German-English show 0.3-0.7 BLEU improvements over standard scheduled sampling, with larger gains on longer sequences (>40 tokens). However, we find diminishing returns on high-resource language pairs and observe that learned rates converge to near-scheduled behavior in later training stages. Ablation studies reveal most gains come from adapting rates to sequence position rather than instance-specific features. While MixRate introduces minimal computational overhead, the improvements are incremental and may not justify added complexity for well-tuned baselines. Our PyTorch implementation and pretrained models are available at [anonymous-link].",
    "id": 1067
  },
  {
    "title": "Lookahead Gradient Descent: When Two Steps Are Better Than One",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Lookahead Gradient Descent (LGD), a simple modification to standard gradient descent that uses gradients from future parameter values to guide current updates. LGD maintains two copies of the parameters: a fast weight that takes a tentative gradient step, and a slow weight that computes the actual update using the gradient at the tentative fast position. Despite being motivated by theoretical insights from convex optimization, we find that LGD provides modest but consistent improvements over Adam and SGD on small-scale vision and language tasks, achieving 2-3% better accuracy on CIFAR-10 and 1-2 BLEU point gains on IWSLT14. Our analysis reveals LGD acts like an adaptive learning rate method that reduces step sizes near sharp minima, though this effect diminishes on very large models. While the theoretical analysis is limited to quadratic objectives and the computational overhead is 1.5x baseline methods, we believe LGD offers an interesting perspective on how future-gradient information can be leveraged in practice. Code and experiments are available at [anonymous URL].",
    "id": 1070
  },
  {
    "title": "Gradient Descent with Preconditioners Learned from Similar Tasks: A Meta-Optimization Approach",
    "authors": [
      "Chen, Y.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Task-Adaptive Preconditioned Gradient Descent (TAP-GD), a meta-learning approach that learns to generate preconditioning matrices for gradient descent from historical optimization trajectories. Unlike existing preconditioning methods that rely solely on local curvature information, TAP-GD uses a small neural network trained on similar tasks to predict preconditioners that accelerate convergence. Our method combines insights from second-order optimization with meta-learning: given a new task sampled from a related distribution, the learned preconditioner adapts to the loss landscape geometry while maintaining computational efficiency. We evaluate TAP-GD on image classification and regression benchmarks spanning 8 datasets with varying similarity to the meta-training distribution. Results show modest improvements over standard optimizers (4-12% faster convergence) and competitive performance with stronger baselines like Adam and L-BFGS, particularly when tasks exhibit high feature similarity. However, performance degrades substantially when test tasks diverge from the training distribution. While our approach introduces an interesting new perspective on combining meta-learning with optimization, its practical impact appears limited to specific task distributions. Code and experiments are available at anonymized-url.github.io.",
    "id": 1076
  },
  {
    "title": "LoRA-Lo: Memory-Efficient Low-Rank Adaptation with Learned Sparsity Patterns",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Johnson, M."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its memory footprint during training remains problematic for consumer hardware. We propose LoRA-Lo, which introduces learned sparsity patterns to reduce memory usage while maintaining model quality. Our key insight is that the low-rank matrices in LoRA exhibit structured sparsity that can be predicted from the frozen pre-trained weights. We train a lightweight convolutional network to predict binary masks for the LoRA matrices at each layer, reducing activations by 30-40% during backward passes. Experiments on instruction-tuning Llama-2 models (7B and 13B) show LoRA-Lo achieves comparable performance to full LoRA on MT-Bench and MMLU benchmarks while reducing peak memory usage by 23-35%. However, we observe a small but consistent degradation on reasoning-heavy tasks (GSM8K drops by 2.1%). Our method adds minimal computational overhead (<5% training time) and requires no architectural changes to existing models, making it compatible with current LoRA implementations. While the gains are modest and our experimental scope is limited, we believe this approach could benefit practitioners with resource constraints. Code will be released upon acceptance.",
    "id": 1081
  },
  {
    "title": "Improved Convergence Rates for SGD with Adaptive Restart Schedules on Strongly Convex Objectives",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose AR-SGD, an adaptive restart schedule for stochastic gradient descent that automatically adjusts restart frequencies based on observed gradient norms. While restart schemes have shown empirical benefits for SGD, theoretical understanding remains limited beyond deterministic settings. Our method monitors the exponentially decaying average of squared gradient norms to trigger restarts, eliminating the need for pre-specified schedules. We prove that AR-SGD achieves an O(log(T)/T) convergence rate for strongly convex objectives, improving upon the standard O(1/T) rate under certain gradient noise conditions. Experiments on logistic regression and neural network training demonstrate modest improvements over tuned baseline schedules, with 5-15% reductions in validation error across 4 benchmark datasets. However, the approach requires careful tuning of two hyperparameters (restart threshold and decay rate) and shows inconsistent gains when moving to larger-scale problems. Our theoretical analysis relies on bounded gradient assumptions that may not hold in practice. While providing a principled approach to adaptive restarting, the complexity overhead and limited empirical gains compared to well-tuned constant schedules suggest AR-SGD may be most useful when manual tuning is infeasible.",
    "id": 1084
  },
  {
    "title": "Revisiting Data Augmentation in Contrastive Learning: A Frequency Domain Perspective",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Singh, V.",
      "Garcia, M."
    ],
    "abstract": "Recent advances in self-supervised contrastive learning have achieved impressive performance across vision and language tasks, with data augmentation playing a crucial role in generating positive pairs. While most work focuses on designing augmentation policies in the spatial/input domain, we investigate how augmentations affect the frequency spectrum of learned representations. Through systematic analysis of ImageNet and CIFAR-10 pretraining, we find that standard augmentation strategies create systematic biases in high-frequency components that persist across different model architectures. We propose Spectral MixUp (SMix), a simple augmentation technique that combines frequency-domain mixing with standard spatial augmentations. SMix improves ImageNet linear evaluation by 0.8% and 1.3% on ResNet-50 and ViT-B respectively over SimCLR baselines, with particularly strong gains (3.2%) on few-shot settings. However, our gains diminish on more recent methods like MoCo v3 and BYOL, suggesting our approach may have limited applicability to state-of-the-art frameworks. Our empirical study reveals that frequency-aware augmentations provide small but consistent improvements while requiring negligible additional compute, though we acknowledge the theoretical understanding remains incomplete.",
    "id": 1099
  },
  {
    "title": "Memory-Efficient Backpropagation Through Time via Low-Rank Gradient Approximation",
    "authors": [
      "Li, K.",
      "Chen, H.",
      "Reddy, S.",
      "Johnson, T."
    ],
    "abstract": "Training recurrent models on long sequences remains computationally challenging due to the quadratic memory growth of backpropagation through time (BPTT). While gradient checkpointing and reversible architectures reduce memory usage, they introduce significant computational overhead. We propose a simpler approach that compresses gradient tensors during backpropagation using low-rank matrix approximations. Our method exploits the observation that gradients in recurrent networks exhibit strong singular value decay after processing long sequences. By projecting gradients to their top-k singular vectors at each timestep, we reduce memory usage by 60-75% with minimal computational overhead. We demonstrate competitive perplexity on Penn Treebank language modeling and Wikitext-103 using LSTMs and Transformers, though our gains diminish with larger models. Analysis reveals our approximation works best when weight matrices have small effective rank, suggesting limited applicability to highly overparameterized networks. Code is available at [anonymized for submission].",
    "id": 1100
  },
  {
    "title": "Iterative Gradient Surgery: A Simple Extension to Adam for Improved Generalization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Despite the widespread success of adaptive optimizers like Adam, these methods often converge to sharp minima that generalize poorly compared to stochastic gradient descent (SGD) with momentum. We propose Iterative Gradient Surgery (IGS), a lightweight modification to Adam that periodically applies gradient projection to encourage flatter minima. At each surgical step, IGS computes the principal eigenvector of the loss Hessian and projects gradients away from this direction, effectively smoothing the optimization trajectory. Our method requires only a single additional Hessian-vector product per surgery step and introduces a single hyperparameter controlling surgery frequency. Experiments on CIFAR-10/100 and ImageNet show IGS improves test accuracy by 1-2% over standard Adam while maintaining training speed. Ablations reveal the benefits primarily emerge from early-stage flattening rather than continued intervention. However, we find performance gains diminish on larger models (ViT-Large) and are sensitive to batch size. While IGS provides a practical alternative to SGD for practitioners who prefer adaptive optimizers, our theoretical analysis remains incomplete\u2014specifically, we lack convergence guarantees when the surgical frequency varies non-monotonically. Code is available at anonymous-ICML-2025/IGS.",
    "id": 1102
  },
  {
    "title": "Revisiting Regularization in Neural Networks: A Study of Weight Decay Schedules Under Data Augmentation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "We investigate the interaction between weight decay schedules and data augmentation in training deep neural networks. While weight decay is widely used as a regularizer, its effectiveness when combined with modern data augmentation pipelines remains poorly understood. We propose a simple cosine-annealing schedule for weight decay coefficients that adapts to the effective data size introduced by augmentation. Through experiments on CIFAR-10, CIFAR-100, and ImageNet subsets, we show modest improvements (0.8-1.2% accuracy gains) over fixed weight decay baselines. Our theoretical analysis suggests these gains arise from better alignment between regularization strength and the implicit increase in dataset size from augmentation. However, we find performance gains diminish with stronger augmentation policies or larger datasets, indicating limited scalability. Our results provide practical scheduling guidelines for practitioners, but also highlight fundamental limitations in regularization methods that do not explicitly account for augmentation-induced distribution shifts. Code and pretrained models are available at anonymous-url.",
    "id": 1107
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Gradient Recycling During Fine-tuning",
    "authors": [
      "Liu, M.",
      "Chen, J.",
      "Rodriguez, A.",
      "Kumar, S."
    ],
    "abstract": "We propose a simple technique to reduce computational overhead when fine-tuning large transformer models by reusing previously computed attention gradients. Our method caches attention gradient patterns from earlier layers and adapts them for subsequent layers through learned scaling parameters. While theoretically motivated by the observation that attention gradients in adjacent layers often exhibit high cosine similarity, our approach requires only minor modifications to standard training pipelines. We evaluate our method on GLUE, SuperGLUE, and two domain-specific NLP tasks, showing 12-18% reduction in training time with less than 0.5% performance degradation on average. Interestingly, we observe that the benefits vary significantly across tasks\u2014achieving up to 24% speedup on document classification but minimal gains on sequence labeling. Our analysis suggests that the effectiveness correlates with dataset size and input sequence length, though the relationship is complex. We discuss failure cases where gradient recycling leads to unstable training and provide a straightforward heuristic to detect such scenarios. While our method does not improve final model performance, it offers practical benefits for practitioners working with limited computational budgets.",
    "id": 1114
  },
  {
    "title": "Gradient Surgery for Improving Transfer Learning in Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Fine-tuning large pre-trained vision transformers for downstream tasks often suffers from catastrophic forgetting and negative transfer. We propose Progressive Gradient Surgery (PGS), a simple technique that selectively prunes gradient components during fine-tuning based on their alignment with pre-trained weights. PGS computes gradient-projections onto the subspace spanned by frozen pre-trained parameters, then removes components below a learned threshold. While conceptually straightforward, PGS surprisingly improves transfer performance across 8 vision benchmarks by 2.3% on average compared to standard fine-tuning. However, gains diminish as dataset size increases, and the approach underperforms recent adapters and prompt-tuning methods on ImageNet (0.8% drop from baseline). Our analysis reveals PGS primarily improves convergence speed rather than final accuracy, achieving similar results to earlier stopping. These findings suggest gradient surgery may be most beneficial in low-data regimes or when computational constraints limit alternatives. Code is available at [URL to be added].",
    "id": 1120
  },
  {
    "title": "Rethinking Momentum in Adam: A Non-monotonic Approach with Polynomial Decay",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Despite Adam's prevalence in deep learning optimization, its hyperparameter sensitivity remains a practical challenge. This work investigates whether traditional momentum scheduling can be improved through non-monotonic strategies. We propose PolyMomentumAdam, which modulates the momentum term using a polynomial decay schedule that periodically increases and decreases based on gradient norms. The key insight is that allowing momentum to temporarily increase can help escape sharp minima while subsequent decay ensures convergence stability. Experiments on CIFAR-10/100 and ImageNet show PolyMomentumAdam achieves 0.7-1.2% improvement over vanilla Adam on ResNets and Transformers, while reducing hyperparameter sensitivity across batch sizes. However, we observe these gains diminish significantly when momentum warmup is already employed, suggesting the benefits may be redundant in well-tuned training pipelines. Our theoretical analysis provides limited convergence guarantees under restricted conditions, and we acknowledge the improvements are primarily empirical. Though the contribution is incremental and lacks definitive theoretical backing, our approach offers a practical plug-and-play modification for researchers frustrated with Adam's sensitivity at minimal computational cost.",
    "id": 1129
  },
  {
    "title": "Gradient Surgery with Memory: Improving Stability in Multi-Task Optimization via Gradient Conflict Replay",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that destabilize optimization and degrade performance. While recent gradient surgery methods like PCGrad and GradDrop address these conflicts, they rely solely on current mini-batch information, potentially discarding useful gradients long-term. We propose Gradient Memory Surgery (GMS), a simple extension that maintains a small replay buffer of conflicting gradients to achieve more globally consistent updates. GMS identifies gradient conflicts using cosine similarity, then selectively blends stored gradients with current ones via an attention mechanism. On three standard multi-task vision benchmarks (NYUv2, CityScenes, CIFAR-100), GMS achieves modest improvements over PCGrad (+0.3-0.8% average accuracy) while reducing training instability. However, we find GMS's benefits diminish with larger models and are most pronounced in low-data regimes. Our method adds minimal computational overhead (\u22485% training time) and only one hyperparameter (buffer size). While results are positive, we acknowledge the improvements are incremental and the technique may not address fundamental limitations of gradient surgery methods. Code and hyperparameters will be made available.",
    "id": 1134
  },
  {
    "title": "LoRA-GA: Gaussian Attention for Low-Rank Adaptation with Applications to Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, S.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "We propose LoRA-GA, an extension to Low-Rank Adaptation (LoRA) that incorporates a learned Gaussian attention mechanism to dynamically reweight low-rank decomposition matrices during fine-tuning. While LoRA has shown promise for parameter-efficient adaptation of large language models, we observe that its static rank constraints often limit expressivity in downstream tasks requiring nuanced feature interactions. LoRA-GA introduces learnable Gaussian kernels that modulate the contribution of each rank component based on input-dependent context, effectively providing adaptive capacity without increasing parameter count. Our method adds only 0.3% additional parameters over standard LoRA and requires a single forward pass during inference. We evaluate on GLUE, SuperGLUE, and domain-specific NLP benchmarks, achieving modest improvements over LoRA (1.2-2.1% average) while remaining competitive with full fine-tuning on 7 out of 11 tasks. However, we find diminishing returns on larger models (>30B parameters) and tasks requiring extensive world knowledge. Analysis reveals the Gaussian attention primarily benefits tasks with strong local context dependencies, though interpretability remains limited. While LoRA-GA demonstrates consistent gains over vanilla LoRA, the absolute improvements are incremental and the computational overhead may not justify deployment in resource-constrained settings.",
    "id": 1138
  },
  {
    "title": "LoRA-E: Efficient Low-Rank Adaptation with Entropy-Based Rank Selection",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become standard for adapting large language models, but the selection of rank hyperparameters remains largely heuristic. We propose LoRA-E, a simple extension that automatically selects rank using an entropy-based criterion computed during a single forward pass of the target data. Our method computes the entropy of activation patterns in each layer and sets the rank proportionally to this entropy, eliminating the need for manual tuning or costly validation runs. While this approach lacks theoretical guarantees, we empirically demonstrate consistent improvements over fixed-rank LoRA on GLUE and SuperGLUE benchmarks, achieving average gains of 1.3 points across tasks with 15% fewer parameters. However, results are mixed on domain-specific datasets where the entropy-adaptive approach occasionally underperforms tuned baselines. Ablation studies reveal the method is particularly sensitive to batch size choices and may struggle with highly imbalanced datasets. Though the computational overhead is minimal (\u22645% increase in training time), the gains over LoRA with carefully tuned ranks are modest. Our code is available at https://anonymous-url.github.io/lora-e.",
    "id": 1142
  },
  {
    "title": "Regularizing Neural Networks via Ensembled Gradient Dropout",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Dropout Ensemble (GDE), a simple regularization technique that randomly drops gradient components during backpropagation and averages predictions across multiple forward passes. While dropout in activations is well-studied, our work explores gradient-space stochasticity as an implicit ensemble method. We show that GDE can be interpreted as performing approximate Bayesian inference under certain assumptions, though these assumptions are restrictive in practice. Experiments on CIFAR-10 and ImageNet show modest improvements over standard dropout (0.3-0.7% accuracy gains), with larger benefits on smaller datasets. However, the computational overhead is substantial, requiring 8-16 forward passes at inference. Ablations reveal that most benefits come from increased training stochasticity rather than the ensemble effect. While GDE demonstrates that gradient perturbation can be a viable regularization strategy, its practical utility is limited by efficiency concerns and the availability of stronger baselines such as MixUp and CutMix. Code is available at anonymous-url.github.io/gde.",
    "id": 1143
  },
  {
    "title": "Gradient Surgery Lite: Reducing Interference in Multi-Task Learning via Lightweight Subspace Projections",
    "authors": [
      "Kim, S.",
      "Chen, L.",
      "Brown, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent work has proposed gradient surgery techniques to address interference, these methods require computing task-specific gradients for all training data, significantly increasing computational overhead. We propose Gradient Surgery Lite (GSL), a simple approach that identifies dominant gradient conflicts using only a small subset of training examples in each batch. GSL projects conflicting gradients onto an approximate nullspace constructed via randomized SVD, enabling efficient interference reduction without the full gradient ensemble. Our method adds minimal computational cost (\u224810% overhead versus 200%+ for prior work) and requires no hyperparameter tuning beyond learning rate. Experiments on three standard multi-task benchmarks (NYUv2, CelebA, and QM9) show modest improvements over naive multitask baselines (0.5\u20132.1% average accuracy gain), but our gains over recent gradient surgery methods are smaller than reported in prior work. While GSL provides a lightweight alternative when computational constraints are severe, our theoretical analysis reveals fundamental limitations in low-rank gradient approximations that may restrict its broader applicability.",
    "id": 1150
  },
  {
    "title": "Gradient Surgery with Memory: Improving Gradient Interference in Multi-Task Learning via Persistent Momentum",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient interference, where conflicting gradients across tasks impede optimization. While recent gradient surgery methods like PCGrad and GradNorm have shown promise, they rely solely on instantaneous gradient information at each step, ignoring historical interference patterns. We propose MementoGrad, which augments existing gradient surgery techniques with momentum-based memory of past gradients. Our method computes task-specific gradient directions by both resolving immediate conflicts and aligning with historical gradient trajectories stored in a small memory buffer. Experiments on standard vision and NLP benchmarks show MementoGrad achieves 2-4% average improvement over PCGrad across tasks, with particular gains in settings with high gradient interference. However, we find these improvements diminish when tasks are well-aligned or when using very large batch sizes. Analysis reveals MementoGrad mainly helps during initial training phases, and that its effectiveness is sensitive to the momentum hyperparameter. While the approach is simple to implement, the memory overhead (less than 1% of model size) may not be justified for all applications.",
    "id": 1152
  },
  {
    "title": "Learning to Warm-Start: A Transformer-Based Approach for Adaptive Gradient Descent Initialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Warmformer, a transformer-based architecture that predicts initial parameters for gradient-based optimization algorithms based on problem metadata and historical optimization trajectories. Our method is motivated by the observation that similar optimization problems often share structural properties that can be leveraged for faster convergence. The key innovation is a cross-attention mechanism that conditions on problem features (dimension, condition number, sparsity patterns) and previous optimization paths to generate parameter initializations. We evaluate Warmformer on convex quadratic programming and neural network training tasks, achieving 15-30% faster convergence compared to standard initialization methods when evaluated on problems similar to the training distribution. However, performance degrades significantly (sometimes worse than baselines) when tested on out-of-distribution problem instances. While our approach demonstrates the viability of learned initialization strategies, the limited generalization beyond training distributions and computational overhead of the transformer model raise questions about practical applicability. Our code is available at [url].",
    "id": 1153
  },
  {
    "title": "Scheduled Dropout: A Simple Curriculum-Based Regularization Strategy for Training Neural Networks",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Thomas, C.",
      "Zhao, L."
    ],
    "abstract": "We propose Scheduled Dropout, a straightforward modification to standard dropout training that gradually reduces dropout rates during training according to a predefined curriculum. Unlike traditional dropout with fixed rates, our method starts with high dropout (e.g., 0.5-0.7) and linearly anneals to lower values (0.1-0.2) over training epochs. This approach is motivated by the observation that early training phases benefit from stronger regularization while later phases require less noise to fine-tune learned representations. Our theoretical analysis shows that scheduled dropout reduces to a modified L2 regularization scheme with time-varying coefficients, providing some justification for its effectiveness. We evaluate on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures, achieving modest improvements of 0.5-1.2% accuracy over standard dropout baselines, with particularly strong gains on smaller datasets. While the improvements are consistent, they remain incremental and the method requires careful tuning of the annealing schedule. We provide extensive ablations on the schedule design and demonstrate that simple linear schedules perform comparably to more complex cosine or exponential schedules. The method is easy to implement and adds minimal computational overhead, making it practical for broad adoption despite its limited novelty.",
    "id": 1154
  },
  {
    "title": "Gradient Confusion: A Simple Regularization Technique for Improving Transformer Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Confusion, a lightweight regularization technique that adds controlled noise to gradient directions during transformer training to prevent overfitting. Our method stems from the observation that transformers exhibit high gradient coherence in later training stages, potentially limiting exploration of the loss landscape. By injecting calibrated directional noise into gradients based on their angular similarity to previous updates, we encourage more diverse parameter updates while maintaining convergence. We evaluate Gradient Confusion on standard NLP benchmarks including GLUE, SuperGLUE, and WikiText-103 across various model sizes (125M-7B parameters). Results show consistent but modest improvements: 1.2-2.3% accuracy gains on GLUE tasks and 0.8-1.5 perplexity improvements on language modeling, with minimal computational overhead (<3% additional training time). However, performance gains diminish on larger models (\u22653B parameters), and our theoretical analysis reveals the regularization effect is bounded regardless of noise magnitude. While Gradient Confusion provides a simple implementation requiring only three additional lines of code, its benefits appear task-dependent and may not justify the added complexity for practitioners. Code is available at anonymized-url.",
    "id": 1161
  },
  {
    "title": "Gradient Descent with Momentum Adapts to Sparse Gradients in Neural Network Training",
    "authors": [
      "Chen, L.",
      "Kim, J.",
      "Rodriguez, M."
    ],
    "abstract": "We investigate the implicit bias of momentum-based optimizers in neural network training, focusing on their behavior under sparse gradient conditions. While theoretical understanding of neural network optimization remains limited, we provide empirical evidence that momentum methods exhibit selective updates to parameters with non-zero gradients. Our theoretical analysis characterizes this behavior for a two-layer linear network, showing that parameters with consistently non-zero gradients converge faster than those with sparse gradients. We conduct systematic experiments on standard vision and language tasks, demonstrating that this phenomenon correlates with improved generalization in networks with structured sparsity patterns. However, our theoretical results hold only for simplified settings and do not extend to non-linear networks. Despite these limitations, our findings suggest that momentum's implicit regularization properties deserve further attention in understanding neural network training dynamics. Code is available at anony-mized-url.",
    "id": 1165
  },
  {
    "title": "Revisiting Low-Rank Adaptation with Gradient Sparsity: A Compression Perspective",
    "authors": [
      "Chen, K.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its memory efficiency during training remains underexplored. We propose SparseLoRA, which combines low-rank training with structured gradient sparsity to reduce memory footprint during backpropagation. By analyzing the gradient flow through LoRA modules, we identify redundancy patterns that allow aggressive gradient compression without catastrophic forgetting. Our method introduces learnable masks that sparsify gradients during training while maintaining the low-rank constraint during inference. Experiments on BERT-base and RoBERTa show 2.1\u00d7 memory reduction during fine-tuning on GLUE tasks with <1% performance degradation compared to standard LoRA. However, advantages diminish on larger models like GPT-2 medium (1.4B), where gradient compression introduces optimization instability. Ablations reveal that SparseLoRA's benefits are most pronounced in memory-constrained scenarios, though the approach requires careful hyperparameter tuning to prevent divergence. While not universally applicable, SparseLoRA provides a practical trade-off when training memory is severely limited, complementing rather than replacing standard LoRA in general use cases.",
    "id": 1166
  },
  {
    "title": "ReLUated Transformers: Provable Approximation Benefits of Depth Without Smooth Activations",
    "authors": [
      "Chen, L.",
      "Vasquez, J.",
      "Kim, S."
    ],
    "abstract": "We study whether the universal approximation capabilities of transformers can be achieved using only ReLU activations, motivated by empirical observations that ReLU variants often perform comparably to GeLU/Swish in large-scale language models. We prove that multi-layer transformers with ReLU activations require \u03a9(d\u00b3) parameters to approximate arbitrary continuous functions on [0,1]^d, improving previous bounds that suggested exponential dependence on dimension. Our key insight introduces a piecewise-linear attention mechanism that maintains Lipschitz constants across layers. Experiments on synthetic function approximation tasks show ReLU transformers achieve 15-30% lower MSE than vanilla ReLU networks at equivalent parameter counts, though we observe diminishing returns beyond 12 layers. While our theoretical framework holds for fixed input dimensions, we demonstrate practical benefits on MNIST sequential classification (87.3% accuracy vs 83.1% baseline) using positional encodings adapted for piecewise-linear activations. However, attempts to scale to larger datasets revealed instabilities that our current initialization scheme cannot fully address. Our work suggests ReLU transformers exhibit non-trivial approximation power, though characterizing their exact capabilities in high-dimensional regimes remains open.",
    "id": 1173
  },
  {
    "title": "A Little Bit of Attention Goes a Long Way: Low-Rank Token Mixing for Efficient Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Vision Transformers (ViTs) have achieved remarkable performance on various computer vision tasks, but their quadratic complexity in token interaction computation remains a significant bottleneck, especially for high-resolution images. We propose TokenMix, a simple method that approximates the full attention matrix with products of low-rank matrices and learned mixing patterns. Our approach reduces the O(n\u00b2) complexity to O(nk) where k \u226a n, while maintaining competitive accuracy on ImageNet-1K. Through extensive experiments on classification, detection, and segmentation tasks, we show that TokenMix achieves 2.1\u00d7 speedup over standard self-attention with only 0.8% drop in top-1 accuracy. While our method does not outperform the latest state-of-the-art efficient attention mechanisms on all benchmarks, the simplicity of our design and ease of integration into existing architectures make TokenMix a practical alternative for resource-constrained applications. Code and pretrained models will be made available.",
    "id": 1174
  },
  {
    "title": "Gradient Descent with Lookahead Meets Random Walk: A Hybrid Approach for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Jiang, K.",
      "Thompson, S."
    ],
    "abstract": "We propose LookAhead Random Walk (LARW), a hybrid optimization method that interpolates between gradient-based and random walk approaches for non-convex problems. Motivated by the observation that gradient descent can get stuck in poor local minima while pure random walks explore inefficiently, LARW alternates between gradient steps and controlled random perturbations. Our method maintains two copies of parameters: a fast weight that follows gradient directions and a slow weight that performs occasional random jumps based on a Metropolis-Hastings acceptance criterion. We theoretically analyze convergence to approximate stationary points for functions satisfying the Kurdyka-Lojasiewicz inequality, showing O(1/\u221aT) convergence rate when the random walk component decays appropriately. Experiments on CIFAR-10/100 and ImageNet show modest improvements (1-2% accuracy) over SGD and Adam on ResNet architectures, particularly when fine-tuning pre-trained models. However, the computational overhead (1.4x training time) and sensitivity to the temperature hyperparameter limit practical impact. While our theoretical framework is promising, the empirical gains remain marginal compared to recent adaptive methods. Our results suggest that combining exploration and exploitation in optimizer design deserves further investigation, though more sophisticated strategies may be needed for substantial improvements.",
    "id": 1188
  },
  {
    "title": "LoRA-FM: Low-Rank Adaptation with Fisher Information Matching for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "M\u00fcller, J.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods have emerged as practical alternatives to full model fine-tuning, with LoRA and its variants showing promise across various domains. We propose LoRA-FM, which enhances standard LoRA by incorporating Fisher information matrix (FIM) estimates to adaptively set rank allocation across layers. Our key insight is that layers with higher Fisher information capture more task-relevant knowledge, warranting larger rank budgets. We derive a simple approximation of diagonal FIM via gradient accumulation during initial training steps, then use this to modulate the rank of each LoRA adaptation. Experiments on GLUE and SuperGLUE benchmarks show 2-3% average improvement over LoRA with similar parameter budgets, though gains are uneven across tasks. While our approach introduces minimal computational overhead (\u22485% training time increase), it requires additional hyperparameter tuning for the FIM estimation window. Ablations reveal that the benefit primarily stems from improved rank allocation rather than the FIM computation itself. However, our method underperforms QLoRA on memory-constrained settings and shows limited benefits for larger models (>30B parameters), suggesting that our insights may not scale effectively. Code will be released upon acceptance.",
    "id": 1200
  },
  {
    "title": "Adaptive Gradient Clipping with Learned Threshold Schedules for Low-Precision Training",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Gonzalez, M."
    ],
    "abstract": "Training deep neural networks in low-precision regimes remains challenging due to gradient instabilities and representational limitations. We propose Learnable Gradient Clipping (LGC), a method that adaptively adjusts clipping thresholds during training using a lightweight meta-learning approach. Our method learns threshold schedules via gradient-based optimization on a small validation set, avoiding the need for extensive hyperparameter tuning. We evaluate LGC on ResNet-50 and Vision Transformer architectures using 8-bit and 16-bit fixed-point training. Results show 0.5-1.2% accuracy improvements over standard gradient clipping baselines on ImageNet, with the biggest gains observed in 8-bit precision settings. Our analysis reveals that LGC particularly helps during the initial training phase when gradient magnitudes are rapidly changing. While the improvements are modest, our approach is computationally efficient, adding less than 2% training overhead, and may benefit practitioners working with constrained hardware. However, we observe that the effectiveness of LGC diminishes when combined with advanced optimizers like AdamW or when training larger models. Our code and pre-computed threshold schedules are publicly available.",
    "id": 1203
  },
  {
    "title": "Variance-Aware Gradient Descent with Random Reshuffling: Improved Convergence Without Learning Rate Tuning",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "Rossi, M."
    ],
    "abstract": "We propose VR-SGD-RR, a variance-reduced stochastic gradient method that automatically adapts learning rates based on gradient statistics collected during random reshuffling (RR) epochs. While RR often outperforms standard SGD in practice, its theoretical analysis remains notoriously difficult due to data-dependent sampling without replacement. Our key insight is to exploit the finite-sum structure of empirical risk minimization to construct adaptive step sizes that depend on per-sample gradient norms estimated from previous epochs. This yields O(1/T) convergence for convex problems and improves dependence on condition number \u03ba from \u03ba\u00b2 to \u03ba^{3/2} compared to vanilla SGD-RR. On CIFAR-10 and ImageNet, VR-SGD-RR achieves similar final accuracy to SGD+Momentum while eliminating manual learning rate tuning for 80% of hyperparameter configurations. However, our method introduces O(d) additional memory overhead per sample and shows diminishing returns on problems with large batch sizes. Theoretically, our analysis relies on an untested assumption about gradient variance decay that remains unproven for general non-convex settings. Experiments on synthetic ill-conditioned problems validate our theoretical rates, but real-world gains are modest except when initial learning rates are far from optimal.",
    "id": 1205
  },
  {
    "title": "LoFiProp: Low-Fidelity Gradient Propagation for Memory-Efficient Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "Transformer models face significant memory bottlenecks during backpropagation due to the need to store activations for gradient computation. We propose Low-Fidelity Gradient Propagation (LoFiProp), a simple technique that selectively stores low-precision versions of intermediate activations during the forward pass, using them to approximate gradients during backpropagation. Our method reduces activation memory by up to 3.2\u00d7 with minimal code changes, achieving comparable performance to full-precision training on standard benchmarks. Theoretical analysis shows LoFiProp introduces bounded gradient errors that do not prevent convergence. Experiments on BERT-base and ViT-small demonstrate 2-4% accuracy degradation on GLUE and ImageNet respectively, with some tasks showing no degradation. While not achieving state-of-the-art compression, LoFiProp provides a practical trade-off between memory usage and accuracy for resource-constrained training scenarios. Code and pre-trained models will be released upon acceptance.",
    "id": 1206
  },
  {
    "title": "BatchNorm Makes Your ResNet Compressible: Improved Post-Training Quantization with Layer-wise Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "We investigate the relationship between Batch Normalization and the compressibility of ResNet architectures through post-training quantization. Our key finding suggests that BatchNorm parameters encode important information about layer-wise sensitivity, which can be leveraged to determine layer-specific learning rates during the fine-tuning phase of quantization. We propose a simple method to extract scaling factors from BatchNorm statistics that serve as proxies for quantization noise sensitivity. Experiments on CIFAR-10 and ImageNet show our approach achieves 3-4% higher accuracy than uniform quantization at 4-bit precision, with minimal additional computational overhead. While our method shows consistent improvements over baselines, the gains diminish at higher bit-widths (\u22656-bit), and theoretical analysis remains incomplete. Our work provides practical insights for practitioners but leaves open questions regarding the generalization to other architectures and the fundamental mechanisms underlying our observations. Code is available at anonymous URL.",
    "id": 1208
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Layer-wise Learning Rate Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Recent theoretical work has established connection between the convergence dynamics of neural networks and their generalization performance. We propose a layer-wise learning rate scheduling scheme that increases learning rates for deeper layers while decaying rates for earlier layers during training. Our theoretical analysis shows this approach leads to tighter PAC-Bayesian generalization bounds that scale more favorably with network depth compared to standard schedules. On CIFAR-10 and ImageNet subsets, our method achieves 2-3% improvements over vanilla SGD with cosine annealing, though gains diminish on larger architectures. While our bounds improve upon previous work for networks with 3-8 layers, they remain vacuous for state-of-the-art deep architectures. The scheduling scheme introduces two hyperparameters that must be tuned per-dataset, limiting practical applicability. Our empirical evaluation on standard benchmarks provides moderate improvements but falls short of matching performance gains from recent architectural innovations. Code and experimental details are provided for reproducibility.",
    "id": 1209
  },
  {
    "title": "Momentum Matters: Revisiting Second-Order Optimization in Overparameterized Models",
    "authors": [
      "Liu, Y.",
      "Kumar, S.",
      "Chodorowski, A.",
      "Zhou, L."
    ],
    "abstract": "We investigate whether carefully tuned momentum schedules can recover the benefits of second-order optimization methods in training large-scale neural networks without the computational overhead. Despite extensive theoretical work suggesting the importance of curvature information, practical adoption of second-order methods remains limited due to scalability concerns. We propose Adaptive Momentum Scaling (AMS), a simple modification to standard SGD+momentum that approximates the behavior of quasi-Newton methods by dynamically adjusting the momentum coefficient based on the history of gradient correlations. Our extensive experiments on standard benchmarks (CIFAR-10/100, ImageNet) and language modeling tasks show that AMS achieves comparable convergence to AdamW while maintaining the computational efficiency of SGD, reducing training time by 12-18% on typical hardware configurations. However, we find that the benefits vary significantly across architectures and datasets, with diminishing returns on particularly large models. While our empirical results demonstrate practical utility, we acknowledge that the theoretical underpinnings of our approach remain incomplete, and we primarily establish effectiveness through carefully designed ablations rather than formal guarantees.",
    "id": 1211
  },
  {
    "title": "Momentum-Scheduled Warmup: Balancing Optimization Stability and Convergence in Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a simple modification to standard transformer optimization that schedules the momentum parameter during warmup phases. Motivated by observations that high momentum can destabilize early training while low momentum slows convergence later, we introduce a linear momentum warmup schedule that transitions from \u03b2=0.0 to \u03b2=0.9 over the first 1000 steps. This approach requires only two additional hyper-parameters and can be implemented in 5 lines of code. Experiments on IWSLT14 De-En, WMT16 En-De, and GLUE benchmark tasks show modest improvements: 0.3-0.7 BLEU score gains and 0.5-1.2% accuracy improvements over standard AdamW baselines, with particularly consistent benefits on smaller datasets (<10M parameters). Ablations suggest the benefit primarily comes from improved early optimization stability rather than final convergence quality. While our method is straightforward to implement and provides reliable if incremental gains, we acknowledge the improvements remain within standard error margins for most tasks. We provide PyTorch code and hyper-parameter sweep results to facilitate reproduction.",
    "id": 1212
  },
  {
    "title": "Gradient Perturbation Family: A Unified Framework for Low-rank Adaptation in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, K."
    ],
    "abstract": "We propose Gradient Perturbation Family (GPF), a framework that unifies existing low-rank adaptation methods through the lens of gradient approximation. While prior approaches like LoRA and AdaLoRA rely on heuristic rank selection, GPF treats rank as a learnable parameter by perturbing gradient updates through a learned low-rank structure. Our method introduces a differentiable rank penalty based on nuclear norm relaxation, enabling automatic rank selection during training. Through extensive experiments on language modeling and vision tasks, we show GPF matches or slightly outperforms baselines with 10-25% fewer parameters. However, theoretical analysis reveals limitations: our rank selection mechanism sometimes leads to suboptimal low-rank approximations, particularly in deeper layers where gradients exhibit heterogeneous structures. Additionally, the training dynamics become unstable when initial rank guesses deviate significantly from optimal values. While GPF provides a principled perspective on low-rank adaptation, the practical gains over existing methods are modest, and the computational overhead from dynamic rank adjustment may limit its scalability. Our code is available at [anonymous link].",
    "id": 1218
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: A Spectral Perspective on Layer-wise Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose Spectral Layer-wise Learning Rate Adjustment (SLLRA), a novel approach to combat gradient interference in overparameterized neural networks through per-layer learning rates derived from the Hessian spectrum. While existing gradient surgery techniques typically rely on optimization tricks or architectural modifications, we show that the eigenspectrum of the Hessian at initialization provides surprisingly predictive signals for optimal layer-wise learning rates. Through extensive experiments on CIFAR-10/100 and ImageNet, we demonstrate that SLLRA achieves 0.5-1.2% improvements over standard training recipes without architectural changes or additional compute. However, our method shows limited effectiveness on smaller architectures (ResNet18/34) and inconsistent gains across different optimizers. Theoretical analysis reveals that our heuristic connects to second-order optimization but lacks formal convergence guarantees. While the improvements are statistically significant, the practical impact remains modest, suggesting that gradient interference may be less problematic than previously claimed in modern training regimes. Our code and pre-trained models are available at anonymous.url/SLLRA.",
    "id": 1225
  },
  {
    "title": "Self-Supervised Gradient Compression: Reducing Communication Overhead in Federated Learning via Autoencoder-Based Gradient Encoding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Federated learning faces critical scalability challenges due to high communication costs when exchanging gradient updates between clients and servers. We propose Self-Supervised Gradient Compression (SSGC), a novel approach that learns to compress gradients without requiring external labels or assumptions about their distribution. Our method trains an autoencoder architecture to directly encode gradient tensors into low-dimensional representations, with the reconstruction loss adapted to preserve the direction (rather than magnitude) of the original updates. We evaluate SSGC on standard federated benchmarks including CIFAR-10, CIFAR-100, and FEMNIST across varying client participation rates. Experiments show 8-16x compression ratios while maintaining accuracy within 2% of uncompressed baselines in most settings. However, we observe substantial degradation (>5% accuracy drop) on some non-IID data distributions, particularly when client datasets are highly skewed. While our theoretical analysis proves convergence under idealized conditions, we acknowledge limitations in handling heterogeneity across clients. SSGC offers a communication-efficient alternative to existing gradient compression techniques like Top-k and quantization, though further investigation is needed to improve robustness under non-convex objectives.",
    "id": 1228
  },
  {
    "title": "Improved Learning Rate Schedules for Transformer Fine-tuning via Small-Scale Experiments",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chen, L."
    ],
    "abstract": "We investigate whether micro-level optimizations in learning rate schedules can improve Transformer fine-tuning efficiency, motivated by the observation that standard cosine schedules may not be optimal for transfer learning scenarios. Our approach systematically evaluates 12 schedule variants using an automated framework across 6 NLP tasks, focusing on modestly-sized BERT-base and RoBERTa-base models. We introduce a piecewise linear schedule with momentum-warm restarts that shows 2.3% average improvement over baseline schedules when initialized from pre-trained checkpoints, while requiring minimal compute overhead. Results demonstrate consistent gains on GLUE benchmarks, though improvements on larger models (BERT-large) diminish to 0.7%. Analysis reveals our method particularly benefits tasks with limited target-domain data (under 10k examples). While these improvements are incremental, our experiments provide evidence that schedule tuning can be a complementary axis for fine-tuning optimization. The simplicity of implementation and reproducibility across different random seeds (tested with 5 trials) make this a practical addition to existing fine-tuning pipelines. Code and configuration files are available at [anonymous link provided].",
    "id": 1231
  },
  {
    "title": "Variance-Reduced Gradient Boosting with Adaptive Subsampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient boosting remains a dominant approach for tabular data, yet its training complexity typically scales linearly with dataset size. We propose VR-GBoost, a variance-reduced gradient boosting framework that employs adaptive subsampling motivated by stochastic optimization techniques. Our method dynamically adjusts the fraction of data used at each boosting iteration based on gradient norms and incorporates control variates to reduce the variance of gradients estimated from subsamples. Unlike existing approaches that use fixed sampling rates, VR-GBoost theoretically decreases the required sample complexity from O(n) to O(n^2/3) iterations while maintaining the same convergence guarantees. Experimental evaluation on 8 UCI datasets shows 1.2-1.8x training speedups with modest accuracy improvements (0.5-1.3%) over XGBoost, though these gains diminish on high-dimensional sparse datasets. While our theoretical analysis requires strong convexity assumptions that may not hold in practice, our empirical results suggest the approach can be practically useful despite theoretical limitations. Code is available at anonymous.link.",
    "id": 1236
  },
  {
    "title": "Conservative Q-Learning with Adaptive Trust-Region Constraints",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Choi, S."
    ],
    "abstract": "Offline reinforcement learning faces the challenge of value overestimation when the policy deviates significantly from the behavior policy. While existing conservative methods like CQL reduce overestimation by regularizing Q-values, they rely on fixed hyperparameters that may be suboptimal across different datasets. We propose CATR-QL, which introduces an adaptive trust-region mechanism that dynamically adjusts the conservatism level based on the estimated uncertainty of Q-values. Our method computes local Lipschitz constants via gradient analysis to modulate the strength of conservative updates, theoretically ensuring monotonic improvement under relaxed concentrability assumptions. Empirically, we evaluate CATR-QL on 12 continuous control tasks from D4RL, showing consistent but modest improvements over CQL (mean normalized score: 73.2 vs 71.8) with 15% fewer policy updates. However, performance gains are less pronounced on sparse-reward tasks, and the additional computational overhead of adaptive constraint estimation increases training time by 1.4x. Our results suggest that while adaptive conservatism offers benefits, the improvements may not justify the complexity in all scenarios.",
    "id": 1239
  },
  {
    "title": "On the Robustness of Pre-trained Language Models to Distribution Shift via Selective Weight Reinitialization",
    "authors": [
      "Chen, L.",
      "Gonzalez, A.",
      "Kumar, S."
    ],
    "abstract": "Pre-trained language models (PLMs) have shown impressive performance across NLP tasks, yet their robustness under distribution shift remains a significant challenge. We propose Selective Weight Reinitialization (SWR), a simple post-training technique that improves robustness by strategically reinitializing a small subset of weights based on their gradient history during fine-tuning. Our method identifies weights that contribute most to overfitting on the source distribution and resets them to pre-trained values while preserving task-relevant knowledge. On 6 text classification datasets with natural distribution shifts (including news articles from different time periods and product reviews across domains), SWR achieves a 4-7% improvement in out-of-distribution accuracy compared to standard fine-tuning. While these gains are statistically significant, we find that SWR underperforms more sophisticated approaches like domain adversarial training in extreme distribution shifts (>30% label distribution change), and we identify failure cases when source and target domains differ substantially in vocabulary. Nevertheless, our method operates as a lightweight alternative requiring no additional data or hyperparameter tuning beyond the base PLM, making it practical for practitioners. These results suggest that selective reinitialization of overfitted parameters may be a overlooked but useful component in the robustness toolbox.",
    "id": 1250
  },
  {
    "title": "Improving Transformer Training Stability Through Layer-Wise Learning Rate Annealing",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Training instability remains a persistent challenge in deep Transformer architectures, particularly when scaling to hundreds of layers. While architectural modifications and normalization techniques dominate existing solutions, we propose a simple training-time intervention: layer-wise learning rate annealing (L-LRA) that progressively reduces learning rates for deeper layers during early training. Our method requires no architectural changes and can be implemented with three lines of code in standard frameworks. We validate L-LRA on Wikitext-103 and C4 language modeling benchmarks using GPT-2 and T5 architectures of various sizes (125M to 1.5B parameters). Experiments show 12-15% reduction in training loss variance across random seeds and improved convergence (0.7-1.2 ppl improvement) compared to baseline training, with particularly strong effects on deeper models (>48 layers). Analysis reveals that L-LRA effectively controls gradient norms during the initial chaotic phase of training, though we observe diminishing benefits for models under 512M parameters. While our approach improves training stability, downstream task performance gains remain modest (average 0.8% GLUE score improvement), and we note that adaptive optimizers already provide some of these stabilization effects. Our findings suggest that explicit layer-wise optimization strategies may be more relevant for training extremely deep Transformers than moderate-scale models.",
    "id": 1251
  },
  {
    "title": "Gradient Norm Clipping Improves Transformer Training Stability by Implicit Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient norm clipping is a widely adopted heuristic for stabilizing transformer training, yet its theoretical underpinnings remain poorly understood. We provide empirical evidence that clipping acts as an implicit regularizer by constraining the effective Lipschitz constant of the network. Our experiments on language modeling and machine translation tasks show that clipped transformers achieve 2-3% better perplexity and exhibit 40% smaller gradient variance compared to unclipped baselines. While we establish a connection between clipping strength and implicit bias similar to weight decay, our theoretical analysis is limited to simplified linear settings that may not fully capture transformer dynamics. Code and hyperparameters will be released.",
    "id": 1256
  },
  {
    "title": "Practical Improvements to Gradient Noise Injection for Differentially Private Deep Learning",
    "authors": [
      "Chaudhari, P.",
      "Kwon, J.",
      "Zhou, S."
    ],
    "abstract": "While differential privacy (DP) offers strong theoretical guarantees for training neural networks, existing approaches often suffer from significant accuracy degradation relative to non-private baselines. We identify that the standard practice of adding isotropic Gaussian noise to gradients is particularly harmful for convolutional layers, where parameter norms vary dramatically across filters. Building on this observation, we propose Layer-Adaptive Gradient Perturbation (LAGP), which rescales noise according to per-layer sensitivity estimates computed via a lightweight online procedure. Additionally, we introduce Cyclic Gradient Clipping (CGC), a simple modification to standard clipping that reduces bias introduced during training. Our method achieves 94.2% accuracy on CIFAR-10 with (\u03b5=2.0, \u03b4=10^-5)-DP, improving over the baseline DP-SGD by 2.1% while maintaining the same privacy budget. Experiments on additional benchmarks including FMNIST and CIFAR-100 demonstrate consistent but modest gains. While our contributions are empirical and incremental, they suggest that careful engineering of existing DP techniques can yield practical benefits for privacy-preserving machine learning without requiring fundamentally new mechanisms.",
    "id": 1259
  },
  {
    "title": "Gradient Mixup: Improving Model Robustness Through Convex Interpolation of Parameter Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Mixup, a simple regularization technique that interpolates between consecutive gradient updates during training to improve model robustness. Inspired by the success of input mixup for data augmentation, our method computes weighted combinations of past and present gradients, effectively smoothing the optimization trajectory. We prove that under L-smoothness assumptions, Gradient Mixup provides a convergence guarantee of O(1/\u221aT) for non-convex objectives while reducing gradient variance by up to 30%. Experiments on CIFAR-10 and ImageNet show consistent improvements in robustness to label noise (+2.1% accuracy under 20% noise) and adversarial perturbations (+1.3% robust accuracy), with minimal computational overhead. While the theoretical analysis relies on restrictive assumptions that may not hold in practice, and improvements over strong baselines like SAM remain modest (+0.4% average), Gradient Mixup offers a plug-and-play alternative that requires no hyperparameter tuning beyond the standard learning rate. Code is available at anonymous-url.",
    "id": 1260
  },
  {
    "title": "LayerNorm Alternatives for Transformer Architectures via Learnable Affine Transformations",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Layer Normalization (LayerNorm) has become a standard component in transformer architectures, but its computational cost and potential training instabilities motivate the search for alternatives. We propose Learnable Affine Normalization Transform (LANT), a drop-in replacement for LayerNorm that uses learned affine transformations and scaled residual connections to maintain training stability while reducing compute. Our method eliminates the need for calculating mean and variance across feature dimensions, instead relying on element-wise learnable scale and shift parameters that adapt during training. We evaluate LANT on standard language modeling tasks (WikiText-103, OpenWebText) and machine translation benchmarks (WMT'14 EN-DE). Results show LANT achieves comparable perplexity to LayerNorm (-0.8% on WikiText-103) while reducing training time by 12-15%. However, performance degrades on longer sequences (>2048 tokens), and we observe increased gradient norm variance in deeper models (>48 layers). Analysis reveals LANT works best for medium-scale models (\u2264350M parameters) but struggles with larger architectures. While our contribution is primarily empirical and the theoretical justification remains incomplete, LANT provides a practical alternative for resource-constrained training scenarios where minor accuracy loss is acceptable for improved efficiency.",
    "id": 1261
  },
  {
    "title": "Gradient Surgery in Federated Learning: When Adaptive Federated Optimization Meets Gradient Compression",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Federated learning faces challenges from client drift and communication constraints when dealing with heterogeneous data distributions. While adaptive optimizers like FedAdam and FedProx improve convergence, combining them with gradient compression techniques remains poorly understood. We propose FedGradSurg, a simple modification to federated adaptive methods that applies targeted gradient clipping before quantization. Our approach identifies and removes gradient components that are likely to cause divergence under compression, based on their alignment with global model updates. Through experiments on CIFAR-10 and Stack Overflow datasets with Non-IID partitioning, we demonstrate 12-15% faster convergence compared to standard compressed FedAdam while maintaining the same compression ratio. Theoretical analysis shows convergence under generalized smoothness assumptions, though the bound depends on a new compression-similarity coefficient that lacks intuitive interpretation. Our results suggest that careful gradient preprocessing can improve compressed federated training, but the benefits are dataset-dependent and diminish with more aggressive compression. While our contributions are incremental, the framework provides a practical way to combine two orthogonal federated learning techniques.",
    "id": 1270
  },
  {
    "title": "LoRA-Drop: Structured Pruning for Parameter-Efficient Fine-Tuning via Learnable Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but introduced adapters often contain redundant parameters that increase memory footprint without improving performance. We propose LoRA-Drop, a simple yet effective method to identify and prune low-importance adapter weights during fine-tuning. Our approach learns sparse masks over LoRA's rank decomposition using a differentiable binary relaxation, achieving structured pruning at the level of individual rank components. By incorporating Fisher information as a regularization term, we preserve model expressiveness while reducing parameter count. Experiments on GLUE and E2E benchmarks show LoRA-Drop prunes 25-40% of adapter parameters with <1% accuracy loss across tasks, outperforming magnitude-based baselines. While our method achieves consistent compression, we observe diminishing returns on larger models (>7B parameters) and tasks requiring minimal LoRA ranks. Our implementation requires minimal code changes to existing LoRA pipelines. LoRA-Drop represents a practical approach to reducing adapter overhead, though our theoretical analysis of rank selection criteria remains incomplete.",
    "id": 1272
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Structured Sparsity for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.K.",
      "Kim, S.",
      "Johnson, A.B."
    ],
    "abstract": "We present LoRA-Drop, a method for selectively applying low-rank adaptation modules in pre-trained language models based on a learned sparsity pattern. While Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning technique, we observe that not all layers benefit equally from rank decomposition. We introduce a differentiable gating mechanism that learns to identify and skip layers where LoRA provides minimal utility, reducing trainable parameters by 30-50% without significant performance degradation. Our approach combines magnitude-based pruning with a reinforcement learning component that explores different sparsity patterns across transformer layers. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop achieves comparable accuracy to standard LoRA (within 1.2% average F1) while maintaining the computational benefits of parameter-efficient tuning. However, we find the method is sensitive to initialization strategies and shows particular instability on smaller datasets. Ablations reveal that our sparsity decisions correlate weakly with standard layer attributions, suggesting the learned patterns may be capturing optimization dynamics rather than inherent layer importance. While LoRA-Drop demonstrates the potential for adaptive parameter-efficient methods, the current implementation introduces hyperparameter complexity that may limit practical adoption.",
    "id": 1282
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Head Pruning with Layer-Wise Gradient Similarity",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "While Transformer models achieve state-of-the-art performance across many tasks, their computational cost remains prohibitive for deployment on resource-constrained devices. We propose a simple yet effective attention head pruning method that uses layer-wise gradient similarity to identify redundant heads during fine-tuning. Our approach computes the cosine similarity between gradients of different attention heads within each layer and removes heads whose gradients are most similar to others. Extensive experiments on GLUE and SQuAD show that our method achieves 15-20% FLOP reduction with minimal performance loss (within 1% of full model accuracy). While similar to existing magnitude-based pruning, our gradient similarity approach captures functional redundancy more accurately than weight magnitude alone. However, our method shows diminishing returns on larger models (>1B parameters) and requires full training data for gradient computation. Code will be made available.",
    "id": 1283
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Simple Recipe for Improving Generalization in Deep Neural Networks",
    "authors": [
      "Liu, H.",
      "Kim, J.",
      "Rodriguez, C."
    ],
    "abstract": "We propose Sharpness-Aware Gradient Surgery (SAGS), a simple modification to existing gradient-based optimizers that combines gradient projection techniques with sharpness minimization to improve generalization in deep networks. While recent work suggests conflicting gradients between loss minimization and sharpness reduction objectives in multi-task settings, we empirically observe similar interference even in single-task scenarios. SAGS addresses this by performing orthogonal projection of gradients onto the subspace perpendicular to the sharpness gradient direction, effectively decoupling these objectives. Our method requires only minimal computational overhead (\u22485% increase in training time) and can be implemented with ~20 lines of PyTorch code. Experiments on CIFAR-10, CIFAR-100, and ImageNet show consistent improvements over baseline optimizers, with SAGS achieving +0.8%, +1.2%, and +0.6% accuracy gains respectively. However, we note performance degrades on some architectures (notably Vision Transformers), and our theoretical analysis provides only loose generalization bounds. Ablations reveal that the sharpness regularization term contributes most to improvements, while gradient surgery effects are more modest. These results suggest SAGS offers a practical but incremental advance in optimizer design, with clear benefits in some regimes but limited scope of applicability.",
    "id": 1285
  },
  {
    "title": "Lookahead Approximate Thompson Sampling with Checkpoint Ensembles for Neural Network Hyperparameter Optimization",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, J.",
      "Nguyen, T."
    ],
    "abstract": "Bayesian optimization remains the dominant approach for hyperparameter tuning in deep learning, but its computational overhead grows prohibitively with neural network size. We propose LATSCHE, a hybrid method that combines lightweight Thompson sampling with periodic checkpoint ensembles to enable practical hyperparameter optimization for large models. Our key insight leverages the observation that early training dynamics often correlate with final performance: we maintain a small ensemble of partially-trained models and use approximate posterior updates to guide the search. Unlike standard BO methods, LATSCHE requires only 5-10% additional training cost while providing principled uncertainty estimates. On ResNet-50/ImageNet and GPT-2 language modeling tasks, we achieve comparable or better performance than state-of-the-art BO baselines with 2-4x fewer GPU hours. However, our approach shows degraded performance on smaller models (<10M parameters) where early training is less predictive. While the theoretical justification for our approximation remains incomplete, empirical results suggest practical benefits for practitioners training large-scale models. Our implementation requires minimal code changes to existing training pipelines.",
    "id": 1292
  },
  {
    "title": "Gradient Surgery with Memory: Improving Multi-Task Learning Through Selective Gradient Retention",
    "authors": [
      "Liu, Q.",
      "Kumar, S.",
      "Johnson, A.",
      "Zhao, B."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts that can destabilize training and degrade performance. While existing gradient surgery methods address this through projection-based approaches, we argue that these methods discard potentially useful gradient information. We propose Gradient Surgery with Memory (GSM), a simple modification that retains conflicting gradients in a momentum buffer and selectively reintroduces them when they align with the primary task's gradient direction. Our method requires only a single hyperparameter\u2014the memory decay rate\u2014and adds minimal computational overhead (less than 2% training time increase). We evaluate GSM on three standard multi-task benchmarks: CityScapes segmentation, NYU-v2 depth estimation, and a multi-label classification variant of CIFAR-100. Results show modest improvements over PCGrad (+1.2% mIoU, +0.8% depth accuracy) and comparable performance to more complex methods like GradDrop, while being significantly simpler to implement. However, we find that GSM provides diminishing returns when tasks have naturally aligned gradients, raising questions about its general applicability. Our code will be released upon acceptance.",
    "id": 1294
  },
  {
    "title": "Improving Contrastive Learning with Positively-Correlated Views via Information-Directed Augmentation",
    "authors": [
      "Chen, L.",
      "Rodrigues, A.",
      "Kim, J."
    ],
    "abstract": "While contrastive learning has achieved impressive results across vision and language tasks, its reliance on hand-crafted augmentation strategies remains a fundamental limitation. We propose a principled approach to learn augmentation policies that maximize the mutual information between positive views while controlling for semantic drift. Our method uses a variational bound to optimize augmentations based on their expected informativeness, adaptively balancing diversity and consistency. Experiments on CIFAR-10, STL-10, and ImageNet-100 show consistent improvements over SimCLR (2-4% accuracy boost) at minimal computational overhead. However, performance gains diminish on datasets with limited natural variations, and our approach introduces additional hyperparameters that require careful tuning. An ablation study reveals that the effectiveness of our method depends heavily on the choice of latent space dimensionality and temperature scheduling. While our theory provides insights into optimal view generation, the practical benefits remain modest and context-dependent.",
    "id": 1295
  },
  {
    "title": "Gradient Sign Dropout: A Simple Regularization Technique for Attention Mechanisms via Random Sign Flipping",
    "authors": [
      "Liu, J.",
      "Garcia, M.K.",
      "Thompson, B."
    ],
    "abstract": "We propose Gradient Sign Dropout (GSD), a lightweight regularization technique for transformer-based models that randomly flips the sign of gradient components during backpropagation. Motivated by the observation that attention layers exhibit high gradient sign consistency across training steps, GSD injects controlled noise by stochastically inverting gradients with probability p during parameter updates. Unlike traditional dropout, GSD operates on the gradient space rather than activations, requiring no architectural modifications. Our theoretical analysis shows that GSD approximates a form of implicit gradient noise injection, leading to improved generalization bounds under certain assumptions. Experimental results on GLUE and WikiText benchmarks show 1.2-2.1% improvements over standard transformers of comparable size, with consistent gains across architectures. While these improvements are meaningful, we acknowledge they fall within typical variance ranges. Ablation studies reveal effectiveness primarily for smaller models (<100M parameters). Although GSD introduces a single hyperparameter and minimal computational overhead, we recognize it may not provide clear advantages for large-scale pre-training where extensive hyperparameter tuning is already performed. We provide PyTorch implementations and reproducible training scripts.",
    "id": 1300
  },
  {
    "title": "Frequency-Aware Gradient Clipping for Stable Transformer Training",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Garcia, A."
    ],
    "abstract": "Transformer models often suffer from training instability due to gradient explosion, particularly in the early training stages. While gradient clipping is a common remedy, we find that standard clipping techniques treat all parameters uniformly, potentially hindering the learning dynamics of rapidly-converging components. We propose Frequency-Aware Gradient Clipping (FAGC), which adaptively adjusts clipping thresholds based on the historical frequency of large gradient occurrences at the parameter level. Our method maintains a simple exponential moving average of gradient norms and modulates clipping boundaries accordingly. We evaluate FAGC on standard language modeling tasks using WikiText-103 and OpenWebText, showing modest improvements in training stability and validation perplexity over vanilla gradient clipping (0.5-1.2 perplexity reduction). While our theoretical analysis provides convergence guarantees under bounded gradient assumptions, the practical gains are most pronounced in specific training regimes (learning rates \u2265 3e-4) and diminish with careful learning rate scheduling. Our experiments suggest FAGC offers a practical, low-overhead alternative to standard clipping, though benefits may be task-dependent.",
    "id": 1304
  },
  {
    "title": "Gradient Norm Regularization Improves Out-of-Distribution Robustness in Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We demonstrate that simple gradient norm regularization (GNR) can improve out-of-distribution (OOD) robustness in deep neural networks without requiring adversarial training or domain-specific augmentations. Our method adds a lightweight penalty term \u03bb||\u2207\u03b8\u2113(f\u03b8(x), y)||\u00b2 to the training loss, encouraging flatter loss landscapes around training samples. Through extensive experiments across CIFAR-10/100 and ImageNet, we show that GNR achieves modest but consistent improvements in OOD robustness (2-4% average accuracy gain across common corruptions) while maintaining in-distribution performance. We provide theoretical justification via PAC-Bayesian analysis, relating gradient norms to generalization bounds. However, we find that benefits diminish on large-scale benchmarks, and performance varies significantly across corruption types. While GNR offers a plug-and-play alternative to more complex robust training techniques, our results suggest its practical impact remains limited relative to state-of-the-art adversarial methods. Code will be released upon acceptance.",
    "id": 1306
  },
  {
    "title": "LoRA-Max: Improved Low-Rank Adaptation Through Dynamic Rank Allocation",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto standard for parameter-efficient fine-tuning, but the choice of rank remains a critical hyperparameter typically fixed across all layers. We propose LoRA-Max, a simple extension that dynamically adjusts the rank allocation based on layer-wise gradient statistics during fine-tuning. Our method employs an iterative pruning-and-regrowth strategy: starting with a conservative rank budget, we prune ranks with low gradient norms and reallocate them to layers with higher information flow. Experiments on the GLUE benchmark show LoRA-Max achieves 1.2% average improvement over standard LoRA with the same parameter budget on RoBERTa-base, while maintaining competitive inference latency. On larger models (BERT-large), the gains diminish to 0.4-0.7%. We provide theoretical analysis showing our allocation scheme converges to an optimal rank distribution under mild assumptions. While our improvements are modest, LoRA-Max requires minimal code changes and may benefit practitioners who struggle with rank selection. However, we acknowledge the results are dataset-specific and the overhead may not justify the gains for all applications. Code will be released upon acceptance.",
    "id": 1317
  },
  {
    "title": "Improving Transformer Efficiency through Learned Sparse Attention Patterns with Cyclic Projections",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, T."
    ],
    "abstract": "We propose Cyclic Sparse Transformers (CST), a method for reducing the computational complexity of self-attention in Transformers by learning sparse attention patterns through cyclic projections of query-key matrices. Unlike fixed sparsity patterns or low-rank approximations, CST alternates between sparse and dense attention across layers, guided by a lightweight gating mechanism that predicts attention sparsity based on input statistics. Our approach achieves 2.1\u00d7 speedup on language modeling tasks with <1% perplexity increase compared to standard Transformers on Wikitext-103. While our theoretical analysis shows CST maintains the universal approximation property under mild assumptions, we find the learned sparsity patterns are highly task-specific and transfer poorly across domains. Experiments on machine translation (WMT'14 En-De) show mixed results: modest BLEU improvements on out-of-domain data but degradation on in-domain test sets. Code is available at anonymous-url.github.io/CST.",
    "id": 1322
  },
  {
    "title": "Regularizing Transformers with Learned Implicit Position Encodings",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, A.",
      "Kim, H."
    ],
    "abstract": "Positional encodings are critical for Transformer architectures, yet existing approaches rely on hand-crafted patterns that may not optimally capture positional relationships. We propose LIPER, a regularization technique that learns implicit position representations through an auxiliary contrastive objective. Rather than replacing existing encodings, LIPER encourages the model to learn position-aware features by predicting relative distances between token pairs. Our method adds minimal computational overhead and can be integrated into any pre-trained Transformer. We evaluate LIPER on machine translation (IWSLT'14 De-En), language modeling (WikiText-103), and GLUE benchmarks. Results show modest but consistent improvements: +0.3 BLEU on translation, 1.2% perplexity reduction on WikiText, and +0.9 average GLUE score over strong baselines. While LIPER provides stable gains across tasks, we find the improvements are most pronounced in low-data regimes (10M training tokens), diminishing with scale. Our analysis suggests the regularization effect primarily benefits earlier training stages rather than final model quality. The method requires careful hyperparameter tuning and shows sensitivity to batch sizes. Though interpretable visualizations reveal meaningful learned proximity relationships, computational costs scale quadratically with sequence length. LIPER offers a lightweight approach to enhance positional awareness in Transformers, though practical benefits may be limited beyond specific settings.",
    "id": 1326
  },
  {
    "title": "Noise Regularization Enables Linear Probing to Match End-to-End Fine-tuning in Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate whether carefully designed linear probing can achieve comparable performance to full fine-tuning in downstream NLP tasks. While linear probing typically lags behind fine-tuning by 3-8% accuracy, we identify that adding noise regularization during feature extraction significantly bridges this gap. Our method, Noise-Probing, adds controlled Gaussian noise to intermediate representations during training, which we hypothesize provides better regularization and adversarial robustness than naive probing. Experiments on GLUE, SuperGLUE, and domain-specific tasks with RoBERTa and T5 models show Noise-Probing achieves 97.3% of fine-tuning accuracy on average, up from 91.2% for standard probing. However, we find this improvement is task-dependent: sentiment analysis and NLI tasks see consistent benefits, while question-answering tasks show minimal gains. Ablation studies reveal the noise magnitude hyperparameter is sensitive across tasks, requiring grid-search for optimal performance. Furthermore, our theoretical analysis suggests the improvement stems from implicit bias reduction in the representation space, though we lack formal guarantees for the observed empirical gains. Our results suggest that while noise regularization can enhance simple adaptation methods, fundamental limitations remain for complex reasoning tasks.",
    "id": 1327
  },
  {
    "title": "Structured Dropout: Learning Sparse Representations Through Convex Relaxation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, T."
    ],
    "abstract": "We propose Structured Dropout, a convex relaxation approach to learning sparse neural representations by reinterpreting dropout as a regularization technique with group-sparsity constraints. While standard dropout randomly masks neurons during training, our method learns a data-dependent masking distribution through convex optimization, yielding interpretable sparsity patterns without architectural modifications. Experiments on CIFAR-10 and ImageNet show 12-15% reduction in parameters with <2% accuracy degradation compared to standard training. We provide theoretical guarantees on the convexity of the relaxed objective for single-hidden-layer networks, though extension to deeper architectures remains heuristic. Our approach offers a compromise between model compression and accuracy, achieving competitive performance against magnitude-based pruning but falling short of state-of-the-art lottery ticket results. The method is architecture-agnostic and requires only a single training run, making it practical for resource-constrained deployment. Code is available at anonymous-link.",
    "id": 1342
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its reliance on fixed rank hyperparameters limits flexibility across tasks. We propose LoRA-Drop, a simple extension that dynamically adjusts the rank during training through a magnitude-based pruning mechanism. Our approach progressively drops the least significant singular values based on gradient statistics, reducing parameters by 15-40% during fine-tuning with minimal performance loss. Experiments on GLUE and SuperGLUE show LoRA-Drop achieves comparable accuracy to standard LoRA on 6 out of 9 tasks, with 1.2\u00d7 speedup in training time. However, we observe instability on tasks requiring long-range dependencies (e.g., CoLA), where aggressive rank reduction degrades performance by 3-5%. Analysis reveals the method works best for tasks with sufficient training data, suggesting room for improvement in adaptive rank schedules. While LoRA-Drop offers practical benefits for common NLP benchmarks, its applicability may be restricted to well-resourced tasks without further regularization. Code and checkpoints are available at [anonymous link].",
    "id": 1343
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Entropy-Regularized REINFORCE",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. We propose a simple modification to the REINFORCE estimator that adds an entropy regularization term to the learning objective, which we show reduces gradient variance without introducing significant bias. Our method requires only minimal changes to existing implementations and adds negligible computational overhead. We provide theoretical analysis showing that our estimator achieves lower variance than standard REINFORCE under mild assumptions about the reward distribution. Empirical results on variational autoencoders with discrete latents show modest improvements in ELBO and sample quality on binarized MNIST and CIFAR-10, achieving 3-5% better log-likelihood compared to standard baselines. While our approach does not match the performance of more sophisticated gradient estimators like REBAR or RELAX, it offers a practical alternative when computational constraints or implementation complexity are concerns. Code is available at anonymous.github.io.",
    "id": 1344
  },
  {
    "title": "LayerNorm Low-Rank Decomposition: A Simple Baseline for Efficient Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Anderson, M."
    ],
    "abstract": "We propose LayerNorm Low-Rank Decomposition (LN-LRD), a parameter-efficient method for accelerating transformer training by decomposing the weight matrices in LayerNorm operations into low-rank components. Unlike existing approaches that focus on attention mechanisms, we show that LayerNorm operations contribute significantly to memory usage and computational overhead during training, particularly for larger models. Our method approximates the LayerNorm weight matrices using rank-r factorization, where r < min(d, k) for hidden dimension d and intermediate size k. Through extensive experiments on language modeling and downstream tasks, we achieve 1.34x training speedup and 0.3% performance degradation on average across GLUE/SuperGLUE benchmarks when using rank r=4 for 350M parameter models. While LN-LRD provides consistent improvements in memory efficiency, we observe diminishing returns as model size scales beyond 1B parameters, and performance drops become more pronounced for certain tasks requiring precise normalization. Our ablation studies suggest that the method works best when combined with gradient checkpointing, potentially complementary to existing attention optimization techniques.",
    "id": 1346
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Learned Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. While continuous relaxations like the Gumbel-Softmax provide effective alternatives, they introduce temperature-dependent biases that can degrade sample quality. We propose a simple modification to existing score-function estimators by learning a parametric baseline that adapts to the local geometry of the loss landscape. Our approach uses a small neural network conditioned on intermediate activations to predict optimal control variate coefficients, reducing gradient variance without the need for temperature tuning. Unlike recent work on learned baselines, our method requires no additional model parameters at inference time and introduces minimal computational overhead. We evaluate on structured prediction tasks including generative modeling of text and molecules. Results show 15-20% reduction in gradient variance compared to REINFORCE with moving average baselines, leading to modest improvements in log-likelihood (0.05-0.1 nats on average). While the approach shows consistent gains over standard baselines, the improvements are incremental and do not address fundamental scalability limitations of discrete variable models. Code will be available upon acceptance.",
    "id": 1347
  },
  {
    "title": "Improving Transformer Training Stability via Layer-wise Learning Rate Warm-up",
    "authors": [
      "Chen, Z.",
      "Rodriguez, L.",
      "Johnson, K.",
      "Singh, P."
    ],
    "abstract": "Training instability remains a critical challenge for scaling transformers to larger models and datasets, particularly when training with aggressive learning rates. We propose Layer-Adaptive Learning Rate warm-up (LALR), a simple modification to standard optimizers that applies independent learning rate schedules to different layers based on their gradient norm evolution. Our key insight is that earlier layers in transformers exhibit more stable gradient dynamics than deeper layers during initial training phases. By applying slower warm-up schedules to deeper layers while maintaining standard schedules for earlier ones, we achieve more stable optimization trajectories without hyperparameter tuning. Experiments on language modeling tasks (WikiText-103, C4) and vision transformers (ImageNet) show 12-18% reduction in training loss variance across seeds and moderate improvements in final perplexity (0.3-0.5 points). While LALR demonstrates consistent stability improvements, the computational overhead (additional 8-12% training time) and modest performance gains may limit adoption. The method is orthogonal to existing architectural improvements and can be implemented in ~20 lines of PyTorch code.",
    "id": 1348
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: When Less is Actually More",
    "authors": [
      "Chen, L.",
      "Krishnan, V.",
      "Nakamura, T."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder optimization. While recent methods propose complex gradient surgery techniques, we show that a simple thresholding operation on gradient norms can achieve comparable performance with fewer computational overhead. Our method, Gradient Norm Thresholding (GNT), discards updates from tasks whose gradient norms fall below a learned threshold during each optimization step. We evaluate GNT on 3 standard benchmarks and 2 new datasets we collected for natural language understanding. Experiments show GNT achieves 95% of the performance of state-of-the-art gradient surgery methods while requiring 40% less compute. However, we find the threshold parameter requires careful tuning across domains, and performance degrades on tasks with imbalanced loss scales. Analysis reveals GNT implicitly performs a form of task scheduling, focusing on the most informative gradients. Our code is available at [anonymous link].",
    "id": 1350
  },
  {
    "title": "Re-Weighted Gradient Clipping: A Simple Modification for Better Transformer Training Stability",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Johnson, M."
    ],
    "abstract": "Transformer training often suffers from instability due to gradient explosion, particularly when scaling to larger models. While gradient clipping is widely used, we observe that standard methods indiscriminately reduce all gradient components, potentially losing useful signal. We propose Re-Weighted Gradient Clipping (RWGC), a simple modification that adaptively re-weights gradient components based on their historical norms before clipping. This approach preserves gradients from well-behaved parameters while aggressively limiting updates from unstable ones. Our method requires only two additional hyperparameters and minimal computational overhead. Across 7 transformer architectures on language modeling and machine translation tasks, RWGC reduces training instability without harming final performance in 5/7 cases, achieving BLEU improvements of 0.3-0.8 over baseline clipping. However, we find RWGC provides diminishing returns on smaller models and offers limited benefits when strong hyperparameter tuning is applied. While our contributions are primarily empirical, the simplicity and trivial implementation make RWGC a practical alternative to standard clipping for practitioners training large transformers under computational constraints. Code is available at [anonymous link].",
    "id": 1352
  },
  {
    "title": "Momentum Schedules for Asynchronous Federated Learning: A Variance-Reduced Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose VR-MoFed, a variance-reduced momentum-based algorithm for asynchronous federated learning that adapts the local momentum parameter based on client staleness. While momentum has been extensively studied in centralized and synchronous federated settings, its behavior under asynchronous client participation remains poorly understood. Our key insight is that stale gradients introduce a bias that can be partially corrected through a momentum schedule that decays exponentially with the delay between local updates. We provide a convergence analysis showing O(1/T) rate for smooth convex objectives, though our bound depends on a delay-related constant that may scale poorly in practice. Experiments on CIFAR-10 and Fashion-MNIST across 100 clients demonstrate 5-12% improvement over vanilla FedAvg with momentum, particularly at high asynchrony levels. However, gains diminish significantly when staleness is low or data heterogeneity is severe. We additionally find that the theoretical assumption of bounded delays is violated in realistic deployments, motivating a heuristic clipping strategy that works reasonably but lacks theoretical grounding. While our method provides moderate practical benefits for specific asynchronous scenarios, the improvements over properly tuned baselines are modest and the approach requires careful hyperparameter tuning that may limit immediate adoption.",
    "id": 1367
  },
  {
    "title": "Making Transformers Work with Smaller Attention Windows: A Block-Sparse Approximation Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "The quadratic complexity of attention mechanisms in Transformers remains a critical bottleneck for processing long sequences. While numerous efficient attention variants have been proposed, most either require careful hyperparameter tuning or introduce architectural constraints that limit their applicability. We propose a simple block-sparse approximation that subdivides the attention matrix into learnable windows of varying granularity. Our method requires no changes to existing Transformer architectures beyond replacing the attention computation, making it easily adoptable in standard frameworks. On language modeling tasks with sequences up to 8K tokens, our approach achieves 2.1-2.4x speedup during training and 1.8-2.2x during inference compared to standard attention, while maintaining within 2-3% of baseline perplexity. On downstream tasks like question answering and summarization, we observe modest improvements over baseline efficiency metrics but mixed results on task-specific performance. While our method provides consistent computational benefits, we acknowledge limitations in handling tasks requiring very long-range dependencies and the need for additional memory during gradient computation. Our implementation is available at [URL to be provided upon acceptance].",
    "id": 1376
  },
  {
    "title": "LoRA-Drop: Structured Pruning of Low-Rank Adaptations via Gradient-Free Importance Scoring",
    "authors": [
      "Chen, Y.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have enabled effective adaptation of large language models while maintaining computational efficiency. However, the cumulative overhead of storing multiple LoRA modules across tasks remains significant. We propose LoRA-Drop, a simple method for pruning LoRA parameters without requiring gradient computations or task-specific validation data. Our approach uses an importance scoring mechanism based on the singular values of LoRA matrices, combined with a novel structured pruning strategy that removes entire rank components. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop can reduce parameter counts by 30-50% with minimal performance degradation (\u22641.2% accuracy drop) across various model sizes. While our method achieves competitive compression ratios compared to more complex approaches, we observe that performance degradation increases significantly for tasks requiring nuanced reasoning. Our results suggest that magnitude-based importance scores may be insufficient for capturing task-specific parameter interactions. The method is easy to implement and requires no additional training, though we acknowledge limitations in handling heterogeneous downstream tasks. Code and pre-trained adapters will be released upon acceptance.",
    "id": 1379
  },
  {
    "title": "Adaptive Gradient Clipping with Historical Norms for Transformer Training",
    "authors": [
      "Kim, J.",
      "Liu, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer models often suffer from gradient explosion during training, particularly when using large learning rates or batch sizes. While gradient clipping is a common remedy, fixed clipping thresholds can be overly conservative or ineffective. We propose Historical Norm Gradient Clipping (HNGC), which adaptively sets clipping thresholds based on the distribution of gradient norms observed during training. Our method maintains an exponentially-decayed estimate of gradient norm statistics and clips gradients whose norms exceed a learned percentile threshold. We evaluate HNGC on language modeling tasks with GPT-2 architectures from 117M to 1.5B parameters. Our approach achieves 3-5% perplexity improvements over fixed-clipping baselines on Wikitext-103 and reduces training time by 10-15% to reach baseline perplexity levels. Ablation studies show the historical norm component contributes 60% of the improvement over naive clipping. While these results are encouraging, our theoretical analysis remains limited to simplified settings and fails to explain performance gains in full-scale architectures. The method's simplicity may limit its novelty, though practitioners could find value in our lightweight implementation requiring no additional hyperparameters beyond the decay rate.",
    "id": 1380
  },
  {
    "title": "Don't Blame the Learning Rate: Revisiting Step Size Scheduling in AdamW Through the Lens of Batch Statistics",
    "authors": [
      "Liu, S.",
      "Garcia, M.",
      "Thompson, J."
    ],
    "abstract": "We investigate the interaction between adaptive optimizers and step size scheduling in large-scale neural network training, focusing on why cosine annealing with AdamW often outperforms more sophisticated schedules. Through careful analysis of batch gradient statistics across ImageNet and CIFAR-10 training runs, we find that the apparent effectiveness of cosine schedules correlates with implicit gradient norm regularization rather than classical learning rate effects. Our key observation is that cosine schedules produce gradient distributions with lower kurtosis, leading to improved generalization. Surprisingly, we find that a fixed learning rate combined with explicit gradient clipping achieves comparable performance to cosine annealing in 7/12 experimental settings, with minor modifications to the clipping threshold. While our empirical findings provide valuable insights for practitioners choosing optimizers and schedules, our theoretical framework remains incomplete. We conjecture that the kurtosis-regularization effect extends beyond cosine schedules to other monotonically decreasing schedules, though verifying this claim requires further investigation. Our code and training logs are available at anonymized-url.",
    "id": 1384
  },
  {
    "title": "Improving Generalization in Reinforcement Learning with Adaptive Experience Replay Scheduling",
    "authors": [
      "Chen, Y.",
      "Rodriguez, L.",
      "Kim, J."
    ],
    "abstract": "Experience replay has become a cornerstone technique in deep reinforcement learning, yet its impact on out-of-distribution generalization remains poorly understood. We investigate whether dynamically scheduling replay buffer sampling rates can improve zero-shot transfer performance across environments with varying dynamics. Our method, AdaptiveReplay, adjusts the replay ratio based on estimated policy improvement uncertainty using an ensemble of value functions. On 8 continuous control tasks from MuJoCo and 4 procedurally generated environments, AdaptiveReplay achieves a 12% relative improvement in transfer performance compared to fixed replay schedules, though gains are inconsistent across domains. While the approach introduces minimal computational overhead, we find that benefits diminish when source and target domains differ substantially, suggesting limitations in our uncertainty-based scheduling mechanism. These results indicate that replay scheduling offers modest but measurable generalization improvements, particularly when domain shifts are moderate.",
    "id": 1394
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Reexamination with Adaptive Routing",
    "authors": [
      "Liu, K.",
      "Chen, M.",
      "Rodriguez, J."
    ],
    "abstract": "Gradient surgery methods like PCGrad have shown promise for mitigating gradient conflicts in multi-task learning, but their effectiveness varies significantly across task combinations. We propose Adaptive Gradient Routing (AGR), a simple modification that dynamically decides whether to apply gradient surgery based on the cosine similarity between task gradients. Our approach uses a learned gating mechanism trained with reinforcement learning to determine when surgical intervention is beneficial. On a diverse suite of 12 multi-task benchmarks spanning computer vision and NLP tasks, AGR achieves an average 2.3% improvement in aggregate performance metrics over PCGrad, while reducing computational overhead by 18%. However, we find that AGR's benefits are concentrated in specific task combinations, particularly those with high gradient conflict. Through extensive ablation studies, we demonstrate that the learned routing policy often defaults to simple heuristics, raising questions about the necessity of the reinforcement learning component. While our results suggest gradient surgery may be over-applied in current practice, the method's inconsistent gains across tasks limit its practical applicability. Code will be made available upon acceptance.",
    "id": 1405
  },
  {
    "title": "Learning to Prune Neural Networks via Graph Neural Network Surrogates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Anderson, J."
    ],
    "abstract": "Neural network pruning typically requires extensive retraining or iterative magnitude-based scoring, both of which become computationally prohibitive for large models. We propose GNN-Prune, a framework that uses graph neural networks to predict which neurons should be pruned based on local connectivity patterns and activation statistics. Our key insight is that neurons' structural roles within the computational graph, rather than their individual magnitudes, better predict their importance. We train a small GNN on subgraphs extracted from pretrained networks to output pruning probabilities, which are then thresholded to create sparse architectures. Across ResNet50, ViT-B/16, and GPT-2 on ImageNet, CIFAR-100, and WikiText-103, GNN-Prune achieves competitive sparsity-accuracy trade-offs (within 0.5% of magnitude pruning) while reducing pruning time by 10\u00d7 on ImageNet. However, our approach struggles with out-of-distribution architectures and requires architecture-specific training. While not achieving state-of-the-art sparsity, GNN-Prune offers a practical middle ground between costly retraining-based methods and simple magnitude pruning, particularly benefiting practitioners seeking faster pruning in deployment scenarios.",
    "id": 1437
  },
  {
    "title": "Gradient Surgery for Stabilizing Transformer Training via Layer-wise Learning Rate Modulation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Transformer models often exhibit unstable training dynamics, particularly when scaling to deeper architectures. We propose a simple yet effective modification to standard gradient descent that applies layer-specific learning rate scaling based on gradient norm ratios. Our method, termed Gradient Surgery, computes adaptive per-layer learning rates by comparing each layer's gradient magnitude to the median across all layers, scaling down updates for layers with disproportionately large gradients. This approach requires no additional hyperparameters beyond the base learning rate and adds minimal computational overhead. We evaluate our method on standard language modeling benchmarks (WikiText-103, C4) and machine translation tasks (WMT'14 En-De, En-Fr). Experiments show consistent training stability improvements, reducing loss spikes by 34% on average across tasks while maintaining comparable final performance to baseline transformers (\u00b10.2 BLEU scores). While our method provides consistent stabilization benefits, the impact on final model quality remains modest, and we observe diminishing returns as model sizes exceed 1B parameters. The simplicity of our approach makes it readily implementable, though further theoretical justification for the gradient norm heuristic is needed.",
    "id": 1442
  },
  {
    "title": "Improving Transformer Fine-tuning with Adaptive Weight Re-initialization",
    "authors": [
      "Liu, K.",
      "Chen, M.",
      "Rodriguez, J."
    ],
    "abstract": "While pre-trained transformers achieve remarkable performance on downstream tasks through fine-tuning, we observe that the standard practice of using small random initialization for task-specific heads often leads to slow convergence and suboptimal local minima. We propose Adaptive Weight Re-initialization (AWR), a simple technique that dynamically adjusts initialization scales based on layer-wise gradient statistics during the first few training steps. Our method requires no additional hyperparameters beyond standard fine-tuning and adds minimal computational overhead (<5% training time). We evaluate AWR on GLUE benchmark tasks using BERT-base and RoBERTa-base models, achieving modest but consistent improvements (avg. 0.8% GLUE score increase) over standard fine-tuning. Analysis reveals AWR particularly benefits tasks with limited training data, suggesting improved optimization in low-data regimes. However, we find diminishing returns on larger models (\u2265350M parameters) and minimal impact when using strong regularization techniques. While our contributions are incremental, AWR provides insights into how initialization affects fine-tuning dynamics and offers a practical enhancement for small-to-medium scale applications.",
    "id": 1451
  },
  {
    "title": "Meta-Learning Data Augmentation Schedules through Gradient-Based Hyperparameter Optimization",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, M.",
      "Johnson, E."
    ],
    "abstract": "Data augmentation has become a cornerstone of modern deep learning, yet the scheduling of augmentation strength during training remains largely heuristic. We propose MASDA, a meta-learning framework that learns optimal augmentation schedules by treating schedule parameters as hyperparameters optimized through implicit differentiation. Our method jointly trains a base model and a lightweight hypernetwork that outputs per-epoch augmentation intensities, guiding models toward better convergence. Experiments on CIFAR-10/100 and ImageNet subsets show 1.2-2.1% improvements over hand-tuned baselines with comparable compute budgets. However, we find performance gains diminish with larger models and longer training, suggesting limited scalability. Our ablation reveals that schedule learning primarily helps in early training phases, with diminishing returns after epoch 50. While MASDA provides marginal improvements in specific scenarios, the computational overhead makes it less practical for resource-constrained settings. Code and pretrained models are available at anonymous-url.github.io/masda.",
    "id": 1453
  },
  {
    "title": "Gradient Flows in Neural Networks: A Simplified Approach to Mean-Field Analysis",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "We present a simplified framework for analyzing gradient flows in two-layer neural networks using mean-field theory. While existing approaches rely on complex measure-theoretic arguments, we propose an alternative formulation based on smoothed particle approximations that leverages classical ODE techniques. Our key insight is that controlling the Wasserstein-1 distance between empirical measures and their continuous limits can be achieved through tighter regularization of the activation function. We establish non-asymptotic bounds on the approximation error and prove convergence rates of O(1/\u221an) under mild regularity assumptions. Empirically, we demonstrate improved stability in training shallow networks on synthetic datasets, with convergence behavior matching theoretical predictions. While our framework currently handles only bounded Lipschitz activations and requires careful tuning of the regularization parameter, our simplified analysis may provide more interpretable insights into the gradient dynamics of overparameterized networks. Code and experiments are available at [url].",
    "id": 1467
  },
  {
    "title": "Towards More Robust Few-Shot Learning via Adaptive Instance Normalization and Meta-Feature Augmentation",
    "authors": [
      "Chen, L.",
      "Johnson, K.A.",
      "Garcia, M."
    ],
    "abstract": "Few-shot learning methods struggle with distribution shift between support and query sets in realistic deployment scenarios. We propose a simple yet effective approach combining adaptive instance normalization (AdaIN) with meta-feature augmentation to improve robustness. Our method first applies channel-wise AdaIN to align support set features with the query set distribution during meta-testing, followed by a novel feature augmentation strategy that generates synthetic support examples by interpolating between learned class prototypes. We evaluate our approach on standard benchmarks including mini-ImageNet, tiered-ImageNet, and CUB-200-2011. Results show 2-4% improvements in accuracy over strong baselines like MAML and ProtoNets, with particularly gains under mild distribution shift (0.2-0.5 FID differences). However, performance degrades under severe distribution shift, and our method adds 15-20% computational overhead during meta-testing. While the specific contribution may appear incremental, our systematic analysis reveals that careful normalization choices can provide consistent benefits across diverse few-shot scenarios. Code will be released upon acceptance.",
    "id": 1483
  },
  {
    "title": "Gradient Descent with Memory-Efficient Lookahead: A Simple Approach to Stable Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Training large neural networks remains computationally expensive, particularly when using adaptive optimizers like Adam. We propose Memory-Efficient Lookahead (MEL), a modification to standard gradient descent that approximates the benefits of optimizer lookahead at reduced memory cost. MEL maintains two weight copies: the current parameters and a 'fast' buffer moved N steps ahead using standard SGD, then periodically anchors the slow weights to the fast buffer. A simple exponential smoothing decays the buffer after each anchor, improving stability. We theoretically show that MEL converges at the same rate as vanilla SGD for smooth convex objectives, while providing implicit regularization benefits similar to lookahead optimizers. Experiments on CIFAR-10, ImageNet, and WikiText-103 demonstrate 0.5-1.2% accuracy/rouge improvements over SGD with momentum while using 40% less memory than Lookahead optimizer. However, gains diminish on larger batch sizes and highly tuned baselines. Our method achieves these results with only 6 lines of code changes, making it easy to integrate into existing training pipelines.",
    "id": 1484
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Dynamic Subspace Projection",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a dominant approach for parameter-efficient fine-tuning of large language models, but its fixed-rank structure may underutilize model capacity. We propose LoRA-Lite, a simple extension that dynamically adjusts the rank during fine-tuning through iterative subspace projection. Our method begins with an extremely low-rank decomposition (rank 1-2) and gradually increases the rank only for layers showing high gradient coherence with the current subspace. This approach achieves 25-40% reduction in memory usage compared to standard LoRA on LLaMA-7B fine-tuning, while maintaining comparable performance across GLUE and SuperGLUE benchmarks (within 0.8% average accuracy). However, we observe that LoRA-Lite's benefits diminish on larger models (30B+ parameters), where the dynamic rank allocation introduces minimal gains. Our theoretical analysis provides convergence guarantees under mild assumptions but yields bounds that are looser than prior work. While LoRA-Lite offers practical speedups for practitioners with memory constraints, its primary contribution is incremental rather than foundational, particularly relevant for resource-limited fine-tuning scenarios.",
    "id": 1486
  },
  {
    "title": "Gradient Descent with Momentum Works Even When Momentum is Negative",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We investigate the behavior of gradient descent when the momentum parameter is set to negative values. While negative momentum is known to be unstable in convex settings, we show that in overparameterized neural networks, negative momentum can provide measurable improvements in training speed compared to standard (positive) momentum. Our theoretical analysis focuses on quadratic loss functions, where we prove that negative momentum can escape saddle points faster by approximately 30% under certain curvature conditions. We validate these findings empirically on small-scale vision and language models, achieving 2-5% faster convergence on CIFAR-10 and a subset of GLUE tasks. However, the benefits diminish as model size increases, and we observe instabilities with batch normalization layers. Our results suggest that negative momentum represents a minor but reliable optimization trick for specific architectures, though it does not fundamentally change the optimization landscape. Code is available at: [github.com/negative-momentum/icml2024]",
    "id": 1487
  },
  {
    "title": "LoRA-ICL: Improving In-Context Learning via Low-Rank Adaptation of Language Model Representations",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "In-context learning (ICL) enables large language models to adapt to new tasks with just a few demonstrations, but the underlying mechanisms remain poorly understood. We propose LoRA-ICL, a method that applies low-rank adaptation to intermediate representations within transformer layers during ICL. Our approach identifies task-relevant subspaces by computing gradients on demonstration examples and then applying low-rank updates to steer model outputs. We evaluate LoRA-ICL on 12 classification and generation tasks across different model scales (7B-70B parameters). Results show modest improvements of 2-4% over standard ICL on 8/12 tasks when demonstrations are scarce (k \u2264 4), though gains diminish with more examples. Analysis reveals that learned adaptations primarily affect attention patterns in middle layers, suggesting ICL operates through selective feature amplification rather than fundamental task learning. While our method provides minimal computational overhead (<0.1% parameter increase), performance gains may not justify implementation complexity in practice. Code and experiments are reproducible, though results show high variance across prompt formats and model checkpoints.",
    "id": 1501
  },
  {
    "title": "Variance-Reduced Temporal Difference Learning with Adaptive Step Sizes for Off-Policy Evaluation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We present an improved temporal difference (TD) learning algorithm for off-policy evaluation that combines variance reduction techniques with adaptive step size selection. While existing variance-reduced TD methods like VRTD achieve faster convergence in theory, their practical performance often degrades due to sensitivity to step size choices and poor empirical variance estimates. Our method introduces an adaptive scheme that adjusts step sizes based on a running estimate of the stochastic variance, eliminating the need for manual tuning. We provide convergence guarantees showing O(1/T) rate with potentially better constants than vanilla TD, though our analysis requires stronger assumptions on the behavior policy. Experiments on standard RL benchmarks show 10-30% improvement in mean squared error over TD and VRTD when hyperparameters are well-tuned, but the gains diminish under model misspecification. The method is particularly effective in low-data regimes, though computational overhead increases by roughly 2x due to variance estimation. Our main contribution is practical stabilization of variance-reduced TD rather than novel theoretical insights.",
    "id": 1504
  },
  {
    "title": "Learning to Weight: Efficient Gradient Descent through Adaptive Sample Weighting",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose an adaptive reweighting scheme for gradient-based optimization that dynamically adjusts per-sample contributions during training. Our method, called Learned Sample Weighting (LSW), trains a lightweight meta-network to predict sample weights based on gradient information, theoretically justifying the approach through a PAC-Bayesian bound on generalization error. We evaluate LSW across vision and NLP benchmarks, observing 2-5% accuracy improvements over standard SGD on CIFAR-10 and moderate gains on text classification tasks. However, experiments reveal consistent computational overhead of 20-30% and diminished benefits on larger datasets like ImageNet. While our best results match or slightly exceed recent methods like SAM and sharpness-aware training, the improvements are task-dependent and sometimes within statistical noise. Our analysis suggests LSW may provide most benefit for small-to-medium datasets with noisy labels, though the computational cost may limit practical adoption. Code will be released upon acceptance.",
    "id": 1509
  }
]