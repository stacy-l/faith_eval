[
  {
    "title": "Revisiting Empirical Fisher Diagonal Approximations for Modern Neural Networks",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "The Fisher Information Matrix (FIM) underlies many second-order optimization and uncertainty quantification schemes, yet its exact form remains computationally prohibitive for large-scale models. We re-examine the classical diagonal Fisher approximation\u2014long considered too crude for deep networks\u2014by combining it with layer-wise gradient normalization and an online exponential moving average of squared gradients. On CIFAR-10/100 and ImageNet subsets, the resulting optimizer matches or marginally improves upon Adam in final accuracy (+0.3%) while reducing wall-clock time 12%. A Bayesian logistic regression experiment calibrated with our\u8fd1\u4f3cFIM exhibits better sparsity\u2013accuracy trade-offs than recent Kronecker-factored alternatives. However, ablations reveal that most gains stem from the normalization scheme rather than the diagonal structure itself, and performance degrades on transformer architectures. Theoretically, we bound the approximation error under ReLU activations, showing it grows with depth but remains controlled when weight initialization variance is below 1/L. Code and pretrained weights are provided to reproduce all experiments. While the diagonal Fisher is unlikely to supplant structured approximations, our results suggest it warrants renewed consideration for convolutional pipelines when compute budgets are tight.",
    "id": 17
  },
  {
    "title": "Adaptive Gradient Rescaling: A Simple but Modest Improvement for Deep Network Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "We propose Adaptive Gradient Re-scaling (AGR), a lightweight modification to standard optimizers that re-weights parameter updates based on an exponentially smoothed estimate of gradient curvature. While methods like Adam and RMSProp adapt learning-rates coordinate-wise, AGR re-calibrates the global step magnitude by tracking ratio of change in loss to average gradient norm over a sliding window. On CIFAR-10/100 and ImageNet, AGR+SGD yields 0.9\u20131.2% top-1 accuracy gains over tuned SGD+momentum and 0.4% over AdamW at equal wall-clock time, with 5% reduction in validation loss fluctuations. The method adds three lines of code, one extra hyper-parameter (window size w), and negligible memory cost. Ablation shows benefits vanish when curvature signals are noisy. We provide convergence analysis for quadratic objectives, but general theory is limited. Code is included, yet extensive evaluation on larger tasks and downstream transfer is deferred to future work. AGR offers consistent but incremental improvements, making it attractive for practitioners willing to tune an additional hyper-parameter, but its simplicity limits novelty.",
    "id": 25
  },
  {
    "title": "Revisiting Entropy Regularization in Policy Gradient Methods with Adaptive Temperature Schedules",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Entropy regularization is widely used in reinforcement learning to encourage exploration and prevent premature convergence. While fixed temperature parameters are common, we propose an adaptive entropy schedule that dynamically adjusts the regularization strength based on policy performance. Our method tracks the entropy of the policy distribution and modulates the temperature parameter using a simple proportional controller. We evaluate our approach on continuous control tasks from the MuJoCo suite and find modest improvements in sample efficiency, achieving 8-12% faster convergence on half of the environments tested. Theoretical analysis shows that our adaptive schedule preserves the original convergence guarantees under standard assumptions, though we note that the practical benefits appear limited to environments where exploration is particularly challenging. While our contributions are primarily incremental, we believe this work provides a useful ablation study on the role of entropy scheduling in modern policy optimization. Code is available online, though we acknowledge that hyperparameter sensitivity remains a significant limitation that may hinder broader adoption.",
    "id": 47
  },
  {
    "title": "Revisiting Momentum with Look-ahead Model Predictions",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Nguyen, T."
    ],
    "abstract": "Momentum-based optimizers remain the de-facto standard for training deep networks, yet their theoretical benefits often vanish under practical hyper-parameter choices. We propose Look-ahead Momentum (LaM), a simple modification that computes the momentum update using a one-step model prediction rather than the current gradient. By linearly extrapolating the loss landscape, LaM adaptively mixes current and future gradient information, yielding improved convergence on sharp loss surfaces. While similar in spirit to extrapolation methods like Lookahead and extragradient optimizers, LaM requires no additional forward passes or inner-loop updates, making it essentially free at runtime. We prove O(1/T) convergence for LaM under standard convexity assumptions, matching classical momentum rates, and show empirical improvements on CIFAR-10/100 and ImageNet across ResNet and Vision Transformer architectures. However, gains are inconsistent across tasks: LaM marginally improves final accuracy (+0.3-0.7%) on vision transformers but shows negligible benefits on smaller ResNet models. Additionally, LaM introduces two new tunable hyper-parameters whose optimal values vary across datasets, complicating practical adoption. Our findings suggest that while look-ahead gradient information can modestly improve optimization, the benefits are task-dependent and may not justify the added complexity for practitioners seeking robust, plug-and-play optimizers.",
    "id": 52
  },
  {
    "title": "AdaSmooth: An Adaptive Momentum Schedule for Non-Convex Optimization That Usually Helps",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Martinez, C."
    ],
    "abstract": "We propose AdaSmooth, a simple modification to standard momentum-based optimizers that adaptively adjusts the momentum coefficient based on the gradient variance. While momentum is known to accelerate convergence in smooth regions, it can hinder progress in noisy or rapidly changing landscapes. Our method heuristically scales momentum inversely with local gradient variance, motivated by the intuition that low variance suggests a stable region where higher momentum is beneficial. We evaluate AdaSmooth on training ResNet-34/50 on CIFAR-10/100 and small-scale vision transformers on ImageNet-1k. Results show 2-5% relative improvement in final accuracy over AdamW with momentum 0.9 in 3/5 experiments, while remaining competitive in the others. We provide a limited convergence guarantee for quadratic objectives but acknowledge the theory does not extend to the general case. Code is available at anonymized-url. While the gains are modest and inconsistent, AdaSmooth introduces no additional hyper-parameters and adds negligible computational overhead, making it a practical drop-in replacement that might help in some settings.",
    "id": 73
  },
  {
    "title": "Gradient-Enhanced Dropout: A Simple Extension with Modest Gains in Generalization",
    "authors": [
      "Kumar, S.",
      "Ostrovski, G.",
      "Liang, P."
    ],
    "abstract": "Dropout remains one of the most widely used regularization techniques in training neural networks, yet little work has explored whether second-order information can improve its effectiveness. We propose Gradient-Enhanced Dropout (GED), a method that adapts dropout probabilities for each neuron based on the magnitude of the gradient flowing through it. Intuitively, neurons that exhibit large gradient variations during training are assigned higher dropout rates to encourage robustness. Our approach requires minimal hyperparameter tuning and can be implemented in fewer than 20 lines of PyTorch code. We evaluate GED on CIFAR-10/100 and ImageNette, where it achieves 0.4-0.7% accuracy improvements over standard dropout, albeit with a 12% increase in training time. While these gains are consistent, they are relatively modest compared to more sophisticated regularization schemes. Theoretically, we prove a PAC-Bayes generalization bound that improves upon standard dropout by a constant factor, though the improvement is small. Our extensive ablations reveal that GED is most effective for medium-sized networks (1-10M parameters) but offers diminishing returns for larger models. Though not a dramatic breakthrough, GED represents a simple, practical enhancement that may benefit practitioners seeking incremental improvements.",
    "id": 79
  },
  {
    "title": "Gradient Surgery for Partially Overlapping Client Data in Federated Learning",
    "authors": [
      "Murthy, V.",
      "Chen, S.",
      "Krishnan, R."
    ],
    "abstract": "Federated learning enables distributed training without centralizing data, but performance degrades when clients\u2019 local distributions differ. Most prior work assumes either homogeneous data or completely non-overlapping classes, whereas real deployments often lie in between: clients share a subset of classes while possessing private ones. We formalize this intermediate scenario and propose Federated Gradient Surgery (FedGS), an algorithm that identifies and down-weights conflicting gradient directions during local training. FedGS uses a lightweight proxy comparison performed on the server to split each minibatch gradient into shared and client-specific components, reducing the influence of private-class updates on the global model. On standard federated benchmarks (CIFAR-10/100, FEMNIST) we show 1.2\u20132.3% higher accuracy and 25% faster convergence relative to FedAvg and the recent SOTA FedProx, with only 2% parameter overhead. However, gains diminish under large client drift or tiny shared label sets, and theoretical justification is limited to quadratic objectives. Code and tuned hyper-parameters are publicly available.",
    "id": 87
  },
  {
    "title": "Learning with Gradient-Noise Scaling Improves Online Quantile Estimation",
    "authors": [
      "Nguyen, L.",
      "Ghosh, D.",
      "Ahmed, S."
    ],
    "abstract": "Recent work reveals correlations between stochastic gradient variances and local curvature, prompting us to re-examine explicit noise modeling in online learning. We propose Gradient-Noise Scaling (GNS), a simple update rule that rescales each stochastic gradient by the inverse of an exponentially-smoothed estimate of its recent variance. Unlike adaptive optimizers such as Adam, GNS adds no momentum, performs no per-parameter learning-rate tuning, and keeps running variances solely for the rescaling coefficient. In controlled experiments on CIFAR-10, ImageNet, and several reinforcement-learning environments, GNS accelerates early training by roughly 10\u201315% and yields mildly lower final losses, but convergence improvements plateau after the initial epochs. Theoretical analysis shows GNS is equivalent to a time-varying, diagonal metric preconditioner under quadratic interpolation errors, though this view is restricted to local neighborhoods and constant noise statistics. Our primary contribution is empirical verification that lightweight noise tracking can improve online quantile estimation of gradients, providing robustness against heavy-tailed gradient distributions. The method adds only one extra hyper-parameter and halves the computational overhead of existing variance-based optimizers. Code and pre-trained models are provided for reproduction. While the gains are consistent, they are incremental; larger architectures and long-training regimes show diminishing returns, questioning the method's significance beyond warm-up regimes.",
    "id": 134
  },
  {
    "title": "Improved Margin-Based Generalization Bounds via Sample-Dependent Priors",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "We revisit margin-based generalization bounds for deep neural networks, proposing a refined PAC-Bayesian framework that incorporates sample-dependent priors. While existing bounds often rely on worst-case analyses over parameter space, our approach leverages data-dependent information to construct tighter priors, yielding non-vacuous guarantees on standard benchmarks like CIFAR-10 and ImageNet. Our method combines a modified KL divergence term with an empirically-estimated margin distribution, achieving a 30% improvement over state-of-the-art bounds in terms of the effective complexity measure. However, the improvements diminish for networks with batch normalization or residual connections, suggesting fundamental limitations in our current analysis. Theoretically, we establish a new concentration inequality for sample-dependent priors under mild assumptions, though our results require bounded loss functions and do not extend directly to cross-entropy losses commonly used in practice. Experimental validation on small-scale vision tasks demonstrates consistent improvements, but computational overhead scales poorly with network depth, limiting practical applicability to modern architectures. Our work suggests that incorporating data-dependent information into generalization bounds remains promising, though significant theoretical and computational challenges persist for scaling these insights to contemporary deep learning paradigms.",
    "id": 137
  },
  {
    "title": "Gradient Norm Dropout: A Lightweight Regularizer for Deep Networks via Random Gradient Scaling",
    "authors": [
      "Liu, T.",
      "Nguyen, K.",
      "Roberts, J."
    ],
    "abstract": "We introduce Gradient Norm Dropout (GND), a simple regularization technique that randomly scales gradient norms during back-propagation. Inspired by the success of stochastic regularization methods like Dropout and DropConnect, GND perturbs gradient magnitudes with a learned noise distribution to prevent overfitting in deep neural networks. Unlike more complex regularizers that require additional model components or multi-stage training, GND can be implemented with two lines of code and adds no overhead at inference. We theoretically analyze GND in the over-parameterized regime, showing it approximately minimizes a data-adaptive robust loss. Experiments on CIFAR-10/100 and ImageNet show modest improvements (0.3-0.7% top-1 accuracy) over standard baselines, with slightly larger gains on smaller networks. While the gains are not dramatic, GND offers a practical, easy-to-implement option for practitioners seeking lightweight regularization. Our results suggest GND may be most beneficial when computational budget or memory constraints preclude heavier techniques like MixUp or Cutout. Code is available at [anonymous link].",
    "id": 151
  },
  {
    "title": "LoCA: Localized Clustering Adjustment for Partial-Label Learning with Limited Annotations",
    "authors": [
      "Nguyen, T.",
      "Iyer, V.",
      "Kumar, S."
    ],
    "abstract": "Partial-label learning (PLL) addresses classification where each training instance is associated with a candidate set of labels, only one of which is valid. While existing PLL methods assume abundant partial annotations, many real-world scenarios provide only a handful of labeled candidates per class. We introduce Localized Clustering Adjustment (LoCA), a two-stage framework that first identifies class-prototype regions via unsupervised density peaks and subsequently refines per-example candidate sets through local label propagation. By jointly optimizing a cluster compactness objective with a standard PLL loss, LoCA encourages the model to down-weight ambiguous labels within each localized neighborhood. Experiments on CIFAR-100, ImageNet-127, and three text benchmarks show consistent but modest improvements (1.5-2.3% accuracy) over state-of-the-art PLL baselines when only 10% of the training data carry partial annotations. Ablation studies reveal that most gains come from the clustering step, whereas the label-propagation refinement adds marginal benefit. Although LoCA is straightforward to implement, its theoretical justification is limited to a simplified two-class setting, and hyper-parameter sensitivity can affect stability. Code is publicly available.",
    "id": 154
  },
  {
    "title": "LoRa-Flow: Lightweight Low-Rank Normalizing Flows for Fast Density Estimation",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Liu, Y."
    ],
    "abstract": "Normalizing flows provide flexible probabilistic models by learning invertible transformations, but their computational cost limits deployment on edge devices. We propose LoRa-Flow, which injects low-rank constraints into the weight matrices of coupling layers to reduce parameters without redesigning the network. On five UCI benchmarks, LoRa-Flow achieves 1.5\u00d7 speed-up and 60 % fewer parameters at the cost of <2 % bits-per-dimension increase versus unconstrained RealNVP. Theoretical analysis shows the low-rank bottleneck still preserves universal approximation in the affine coupling class. However, ablations reveal degradation on high-dimensional images, suggesting the low-rank prior is too restrictive for modalities with complex spatial correlations. Code is available.",
    "id": 173
  },
  {
    "title": "LoRAMA: Low-Rank Adaptation with Memory Attention for Efficient LLM Fine-Tuning",
    "authors": [
      "Kumar, S.",
      "Jones, L.",
      "Nguyen, P."
    ],
    "abstract": "We propose LoRAMA, a that augments Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning of large language models by incorporating a lightweight memory attention mechanism. While LoRA has shown strong empirical results, its fixed low-rank decomposition may limit adaptation capacity for more complex downstream tasks. LoRAMA introduces a small memory bank of trainable vectors that are dynamically attended to during forward passes, allowing the low-rank matrices to vary in a task-dependent manner. Our method adds only 0.3M parameters beyond standard LoRA while enabling more expressive adaptation. We evaluate LoRAMA on instruction tuning and domain adaptation benchmarks using Llama-2 7B and 13B models. Results show modest improvements of 1-2% accuracy over LoRA baselines across most tasks, with some degradation on simpler datasets. Abprisingly, LoRAMA reduces training time by 12% compared to standard LoRA due to our memory attention implementation. Theoretical analysis suggests our method interpolates between LoRA and full fine-tuning, though convergence guarantees remain limited. ablation studies reveal performance is sensitive to memory bank size and initialization strategy. LoRAMA provides a simple yet effective extension to LoRA that may be particularly useful for practitioners seeking slightly improved adaptation capacity without substantial computational overhead.",
    "id": 193
  },
  {
    "title": "Gradient Confusion-Aware Learning Rates for Stochastic Optimization",
    "authors": [
      "Chen, L.",
      "Martinez, J.",
      "Singh, P."
    ],
    "abstract": "Mini-batch stochastic gradient descent remains the workhorse for large-scale machine learning, yet practical tuning of learning rates still relies on costly grid searches or hand-crafted schedules. We propose Gradient Confusion-Aware Learning Rate (GCALR), a heuristic that monitors the pairwise cosine similarity among per-sample gradients within a mini-batch to adjust the step size on the fly. Intuitively, high disagreement (confusion) suggests the current batch contains noisy or conflicting signals; GCALR reduces the learning rate to stabilize updates. Convergence is proved for smooth, strongly convex objectives under standard assumptions, although the derived bound is no better than classical SGD with the best constant step. Extensive experiments on CIFAR-10/100, ImageNet and Penn Treebank show that GCALR matches hand-tuned SGD performance in 60% of tasks while eliminating tuning budgets. However, gains vanish when large-batch training exceeds 1,024 samples, and competing adaptive optimizers (Adam, Adagrad) remain superior on language modeling benchmarks. Code is publicly available with full hyper-parameter tables. GCALR offers a tuning-free alternative to constant-step SGD with minimal computational overhead, but theoretical insight into its empirical benefits is still incomplete.",
    "id": 194
  },
  {
    "title": "Improving Transformer Generalization with Adaptive Positional Interpolation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Okafor, K."
    ],
    "abstract": "While Transformers have achieved remarkable success across domains, their ability to generalize to sequences longer than those seen during training remains limited. We propose Adaptive Positional Interpolation (API), a simple yet effective method that dynamically adjusts positional encodings based on the input sequence length. Our approach interpolates between learned positional embeddings using a learned temperature parameter that controls the smoothness of the interpolation. Unlike recent work that requires architectural modifications, API can be applied as a plug-and-play module to pre-trained models. We evaluate API on language modeling, machine translation, and document classification tasks. Results show modest improvements of 2-3% in perplexity for sequences 2x longer than training data, with some tasks showing no improvement. Ablations reveal performance is sensitive to the initialization of the temperature parameter. While our method provides a lightweight solution to length extrapolation, we acknowledge it falls short of more complex approaches. Theoretically, we provide limited analysis of when interpolation might succeed or fail. Our code is available.",
    "id": 198
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Neural Networks",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Johnson, M."
    ],
    "abstract": "We propose AdaGN, an optimization technique that adaptively injects gradient noise during neural network training to improve generalization. While previous work has shown that carefully tuned noise schedules can help escape sharp minima, determining optimal noise levels remains challenging. AdaGN addresses this by dynamically adjusting noise magnitude based on gradient history and local curvature estimates. Our method adds zero-mean Gaussian noise to gradients each iteration, with variance computed using a running average of gradient norms and a novel curvature-sensitive scaling factor. On CIFAR-10/100 and ImageNet experiments with ResNet-18 and VGG-16, AdaGN shows modest improvements over SGD+momentum (0.3-0.7% accuracy gains) and competitive performance compared to strong baselines like SAM and AdamW. Theoretical analysis suggests AdaGN encourages flatter minima under simplified quadratic assumptions. While intuitive and easy to implement, our approach requires careful tuning of two additional hyperparameters and may not scale well to very large models due to computational overhead. Code available at anonymous-url.github.io/adagn.",
    "id": 227
  },
  {
    "title": "On the Importance of Normalization in Transductive Few-Shot Learning",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transductive few-shot learning (TFSL) leverages test-time query statistics to adapt class prototypes, yet performance varies dramatically across benchmark splits. We hypothesize that batch normalization layers, fixed after meta-training, distort feature scales under domain shift. We propose Adaptive Transductive Normalization (ATN), a lightweight wrapper that re-estimates batch statistics using support and query features, coupled with a temperature-scaled cross-entropy loss. Extensive experiments on mini-ImageNet, tiered-ImageNet and CUB-200 show gains of 1.3\u20132.7% over state-of-the-art TFSL methods, while adding only 0.4 ms per task. Ablation reveals that half of the improvement comes from temperature scaling alone, and benefits shrink when pre-training on larger corpora. Analysis indicates that ATN chiefly corrects over-confident logits in low-shot regimes, leaving inter-class margins largely unchanged. Although our contribution is modular and easy to plug into existing pipelines, its theoretical justification is limited: we provide only intuitive arguments and empirical correlation rather than generalization bounds. Code and trained checkpoints are publicly available.",
    "id": 230
  },
  {
    "title": "Revisiting Momentum with Signed Gradient Noise: Improved Convergence via Gradient Sign Asymmetry",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Nguyen, A."
    ],
    "abstract": "We propose Signed Momentum (SignMo), a simple modification to standard momentum-based optimizers that incorporates sign information of the gradient noise. By tracking the asymmetry between positive and negative gradient signs across iterations, SignMo adaptively re-scales the momentum coefficient without additional hyper-parameters. Our method is motivated by recent empirical observations of sign imbalances in gradients when training over-parameterized models. We prove convergence rates for SignMo in convex and non-convex settings that match or improve upon standard momentum under a bounded sign-asymmetry assumption. Experiments on ResNet-18/50 across CIFAR-10/100 and ImageNet show modest improvements in final accuracy (+0.3-0.7%) and 5-10% faster convergence compared to SGD with momentum. However, gains diminish on larger models and are less consistent than adaptive methods like Adam. Ablation studies indicate performance is sensitive to batch size and learning rate schedules. While SignMo introduces minimal overhead and achieves measurable speedups on some benchmarks, its theoretical assumptions may not hold broadly, and improvements are task-specific. Code is available at anonymized-repo.github.io/SignMo.",
    "id": 251
  },
  {
    "title": "Adaptive Gradient Clipping with Curvature-Aware Warm-up for Transformer Training",
    "authors": [
      "Chen, L.",
      "Vasileva, N.",
      "Kumar, V."
    ],
    "abstract": "Transformer models often suffer from unstable training dynamics, particularly in the early phases. We propose Adaptive Gradient Clipping with Curvature-Aware Warm-up (AGC-CAW), a simple modification to standard training procedures that adjusts gradient clipping thresholds based on local curvature estimates and training iteration. Our method computes diagonal Hessian approximations using a lightweight Hutchinson estimator and combines these estimates with a monotonically decreasing clipping schedule. AGC-CAW requires no additional hyperparameters beyond those already present in standard optimizers. We validate our approach on language modeling (Wikitext-103) and machine translation (WMT16 EN-DE), showing 2-3% improvements in perplexity and 0.1-0.2 BLEU score gains over baseline configurations. While the improvements are consistent, they remain modest and appear to diminish with larger models (>1B parameters). Theoretical analysis demonstrates convergence guarantees under standard assumptions, though these results rely on Lipschitz continuity conditions that may not hold for deep transformer architectures. Our implementation adds ~15% training overhead compared to standard AdamW. We provide open-source code and configuration files to ensure reproducibility.",
    "id": 306
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: A Partial Fix with Theoretical Caveats",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Garcia, M."
    ],
    "abstract": "We propose Layer-Adaptive Gradient Surgery (LAGS), a simple modification to existing gradient-based optimizers that addresses gradient interference in overparameterized networks. Building on recent observations that gradients across different layers exhibit conflicting directions, we introduce layer-wise projection operators that remove components harmful to overall optimization progress. Our method requires minimal hyperparameter tuning and adds negligible computational overhead. While we provide theoretical analysis showing LAGS converges in idealized settings under restrictive assumptions, we acknowledge these conditions rarely hold in practice. Experimental evaluation on CIFAR-10, CIFAR-100, and ImageNet shows 1.2-2.3% accuracy improvements over AdamW on ResNet architectures, but similar gains vanish on vision transformers and NLP tasks. Despite moderate empirical success, we identify scenarios where LAGS actively hurts performance - particularly when layer gradients contain beneficial complementary information. Our results suggest caution when applying gradient surgery techniques and highlight the gap between theoretical convergence guarantees and practical effectiveness.",
    "id": 318
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient compression is essential for distributed training at scale, but existing methods rely on hand-crafted schedules that may not adapt to varying training dynamics. We propose Adaptive Quantized SGD (AQ-SGD), which learns compression parameters jointly with model parameters using a meta-learning approach. By framing quantization levels as continuous variables that are periodically projected onto discrete values, our method discovers instance-specific compression schedules without communicating additional metadata. Experiments on CIFAR-10 and ImageNet with ResNet-50 show AQ-SGD achieves 40\u00d7 gradient compression while maintaining within 1.2% of baseline accuracy, matching state-of-the-art static methods on most tasks. However, we observe instabilities on Transformer architectures and convergence issues when batch sizes are small, suggesting our current formulation may not capture all necessary dynamics. Theoretical analysis proves convergence to stationary points under bounded gradient assumptions, but requires stronger conditions than uncompressed SGD. Our method shows promise for adaptive distributed training, though practical deployment may require additional stabilization techniques for certain architectures.",
    "id": 322
  },
  {
    "title": "Gradient Noise Rehearsal: Mitigating Catastrophic Forgetting through Learned Perturbation Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Continual learning remains challenging due to catastrophic forgetting, where models lose performance on previous tasks when learning new ones. We propose Gradient Noise Rehearsal (GNR), a method that stores minimal task-specific information by learning to generate synthetic noise patterns that approximate the gradient directions of previous tasks. Unlike replay-based methods that require storing raw data, GNR maintains only a small set of learned noise parameters per task. Our experiments on split CIFAR-100 and sequential image classification tasks show that GNR achieves competitive performance (within 3-5% of state-of-the-art replay methods) while using 20x less memory. However, we observe that performance degrades significantly on tasks with substantial domain shift, and our method requires careful tuning of the noise generation hyperparameters. Code is available at [anonymous URL].",
    "id": 360
  },
  {
    "title": "Adaptive Gradient Compression for Federated Learning with Non-IID Data via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Johnson, M.",
      "Rodriguez, A."
    ],
    "abstract": "Federated learning faces significant challenges when client data follows non-IID distributions, particularly when combined with gradient compression techniques required for communication efficiency. In this work, we propose an adaptive gradient compression framework that dynamically adjusts quantization schedules based on local data characteristics. Our method employs a lightweight meta-network that predicts optimal compression rates for each client using only gradient statistics and local batch information, avoiding additional communication overhead. We introduce a novel regularization term that balances compression efficiency with convergence stability by penalizing quantization patterns that exacerbate client drift. Experiments on image classification and language modeling tasks show 1.2-2.1\u00d7 communication reduction compared to fixed-compression baselines while maintaining comparable accuracy on moderately non-IID partitions. However, performance degrades significantly on pathological non-IID splits where client datasets contain entirely disjoint classes. While our adaptive approach shows promise for practical federated scenarios, theoretical guarantees remain limited to bounded gradient assumptions that may not hold in practice. Code and preprocessed datasets are available at [anonymous link].",
    "id": 362
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning Through Loss Interpolation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.K.",
      "Jones, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization, yet existing gradient surgery methods lack mechanisms to adaptively balance task objectives during training. We propose Gradient Surgery with Adaptive Memory (GSAM), a simple extension to gradient surgery methods that incorporates a small episodic memory buffer to locally interpolate between task losses. Our approach maintains per-task gradients computed on buffered samples, combining them through a learned convex combination that varies across training stages. On 8 multi-task vision benchmarks and 15 NLP tasks from the GLUE suite, GSAM achieves 1.2% average improvement over PCGrad and GradNorm while adding under 0.3% computational overhead. However, we find benefits diminish with larger models (\u22651B parameters) and our approach is sensitive to both buffer size and learning rate schedules. The method fundamentally trades memory for marginal gains, raising questions about its practical deployment. While GSAM provides a principled approach to adaptive gradient combination, its improvements are incremental and may not justify implementation complexity in resource-constrained settings.",
    "id": 364
  },
  {
    "title": "Improving Transformer Efficiency through Iterative Token Pruning with Learnable Thresholds",
    "authors": [
      "Liu, K.",
      "Chen, J.",
      "Brown, M."
    ],
    "abstract": "While transformer models achieve state-of-the-art results across many domains, their quadratic complexity with respect to sequence length remains computationally prohibitive for long sequences. We propose Dynamic Token Pruning (DTP), a training-free method that iteratively removes low-salience tokens using learnable threshold parameters. Our approach builds upon existing attention-based pruning techniques but introduces two key innovations: threshold vectors that adaptively balance efficiency and accuracy across layers, and a feedback mechanism that re-incorporates pruned tokens when prediction confidence drops. We evaluate DTP on language modeling (WikiText-103), question answering (SQuAD), and image classification (ImageNet) tasks. Results show modest improvements over static baseline pruning methods, achieving 1.8\u00d7 average speedup with <2% accuracy degradation. However, we observe significant variance across tasks: while language modeling shows consistent gains, vision transformers exhibit more erratic behavior, particularly in later layers. Analysis reveals our method struggles with tasks requiring fine-grained spatial information, and the learned thresholds tend to be overly aggressive, removing potentially useful tokens. While DTP demonstrates the potential of adaptive pruning strategies, our empirical findings suggest that more sophisticated scoring mechanisms may be required to achieve reliable improvements across diverse domains.",
    "id": 368
  },
  {
    "title": "Attention Bottlenecks in Transformer Memorization: A Low-Rank Analysis of Inductive Biases",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Thompson, E."
    ],
    "abstract": "We investigate how transformers memorize training sequences through analysis of attention patterns in overparameterized language models. Our theoretical analysis shows that memorization occurs through rank collapse in attention matrices, with rank dropping from O(d) to O(k) where k is the number of memorized patterns. We propose a low-rank regularization scheme that encourages rank collapse during training while maintaining downstream performance. Experiments on synthetic memotasks and WikiText-103 show our method reduces memorization by 15-20% without hurting perplexity, suggesting some memorization may be beneficial for generalization. However, our regularization does not improve performance on established benchmarks like GLUE. While our theoretical analysis provides insight into transformer memorization dynamics, our empirical evaluation is limited to language modeling and may not extend to other modalities. Code and experiments are available at our anonymized repository.",
    "id": 373
  },
  {
    "title": "Gradient Descent with Iteratively Rescaled Gradients: A Simple Speedup Trick for Neural Networks",
    "authors": [
      "Liu, K.",
      "Kumar, S.",
      "Chen, J."
    ],
    "abstract": "We propose Iterative Gradient Rescaling (IGR), a novel optimization technique that adaptively reweights gradient components during training based on their historical magnitudes. Unlike adaptive methods like Adam, IGR does not maintain per-parameter learning rates but instead periodically rescales gradients for each layer using a running estimate of gradient norms from previous iterations. Our method requires no additional hyperparameters beyond standard SGD and adds minimal computational overhead (less than 1% increase in wall-clock time). We evaluate IGR on ResNet-50 and Vision Transformer training on ImageNet, as well as BERT fine-tuning on GLUE tasks. Results show 3-8% relative improvement in final validation accuracy compared to SGD with momentum in most settings, particularly when training budgets are limited to 100-200 epochs. However, IGR shows diminishing benefits with longer training and underperforms AdamW on certain transformer architectures. While our approach is simple to implement and provides consistent gains in resource-constrained settings, we acknowledge that the theoretical justification remains incomplete and the improvement margins are modest. Code is available at [url].",
    "id": 377
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Reinforcement Learning",
    "authors": [
      "Kumar, S.",
      "Zhou, L.",
      "Thompson, J."
    ],
    "abstract": "Multi-task reinforcement learning (MTRL) suffers from gradient conflicts between tasks that cause unstable training and negative transfer. While recent gradient surgery methods like PCGrad and GradDrop alleviate this by projecting conflicting gradients, they rely on instantaneous gradient information and ignore historical task relationships. We propose GAMER (Gradient surgery with Adaptive MEmoRy), which maintains an exponentially-decayed memory of task-specific gradients to compute more informed projection directions. Our method introduces a novel memory update rule that balances recent gradient information with historical context, along with an adaptive temperature parameter that controls the trade-off between task alignment and interference. Experiments on Meta-World and multi-task Atari environments show modest improvements: GAMER achieves 3-7% higher success rates compared to PCGrad across 10/15 tasks while reducing training variance by 12%. However, performance gains diminish with larger networks (>10M parameters), suggesting limitations in complex domains. Ablation studies reveal that memory size and decay rate significantly impact performance, with optimal hyperparameters varying substantially across environments. While GAMER provides a practical improvement over existing gradient surgery methods, its computational overhead (15% slower training) and sensitivity to hyperparameter choices limit its broad applicability. Code and hyperparameter settings are available at [anonymized link].",
    "id": 378
  },
  {
    "title": "Attentive Gradient Descent: Learning to Adapt Step Sizes with Self-Attention",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose Attentive Gradient Descent (AGD), a method that uses self-attention mechanisms to dynamically adjust step sizes during optimization. Motivated by the observation that gradient magnitudes vary significantly across dimensions and iterations in neural network training, AGD replaces fixed or heuristic learning rate schedules with a learned attention mechanism that scales gradients based on historical optima information. Our approach computes attention scores over past gradients and parameter states to produce dimension-wise step sizes. We evaluate AGD on standard image classification benchmarks (CIFAR-10/100) and language modeling tasks (WikiText-2), achieving 2-5% better accuracy compared to tuned baselines like AdamW and SGD with momentum, while reducing hyperparameter sensitivity. However, we observe that gains diminish with larger models, and the computational overhead becomes non-negligible. The method introduces 3 additional hyperparameters that require tuning, and while it performs well on standard benchmarks, we find limited improvement on more challenging datasets like ImageNet.",
    "id": 381
  },
  {
    "title": "Alleviating Forgetting in Continual Learning via Parameter-level Dropout Schedules",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Rodriguez, A."
    ],
    "abstract": "Catastrophic forgetting remains a fundamental challenge in continual learning settings where models must sequentially learn multiple tasks. While dropout is widely used as a regularization technique, its potential to mitigate forgetting has been underexplored. We propose DropCL, a simple yet effective variant that adaptively schedules dropout rates for different parameters based on their sensitivity to previous tasks. Our method estimates parameter importance using a lightweight Fisher information approximation, then applies higher dropout rates to parameters deemed less critical. We evaluate DropCL on standard continual learning benchmarks including Split CIFAR-100 and Permuted MNIST, achieving modest improvements over baseline methods (average 2.3% accuracy gain). While DropCL shows promise, our analysis reveals that gains diminish significantly when tasks are highly dissimilar. Additionally, the method introduces computational overhead during training (approximately 15%) and requires task boundaries to be known in advance. Our contributions include a formal analysis of why dropout can reduce forgetting under certain conditions, and extensive ablations showing that coarse-grained dropout schedules perform nearly as well as our more sophisticated approach. Code and pre-trained models will be released upon acceptance.",
    "id": 385
  },
  {
    "title": "LoRA-Hash: Parameter-Efficient Fine-Tuning with Learnable Hash Tables",
    "authors": [
      "Chen, L.",
      "Rodriguez, S.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular method for parameter-efficient fine-tuning of large language models, but its effectiveness diminishes when targeting multiple tasks simultaneously due to interference between low-rank matrices. We propose LoRA-Hash, a simple extension that replaces static low-rank matrices with learned hash tables that map input tokens to rank-1 updates. This allows different regions of the input space to use distinct adaptation parameters while sharing a common hash function. On the GLUE benchmark, LoRA-Hash achieves 0.8% higher average accuracy than standard LoRA when training on multiple tasks with a 2\u00d7 memory reduction. However, we observe significant variance across tasks (\u00b14.2%), suggesting that hash-based adaptation may not be universally beneficial. Analysis reveals that performance gains are concentrated in tasks with clear input-output patterns, while tasks requiring fine-grained reasoning show degradation. Our method requires minimal code changes to existing LoRA implementations and introduces only 0.1M additional parameters. While LoRA-Hash shows promise for multi-task scenarios, we acknowledge limitations in theoretical understanding and the need for better initialization strategies.",
    "id": 391
  },
  {
    "title": "Momentum-Scheduled Adam: Improving Convergence via Adaptive Gradient Memory",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Thompson, B."
    ],
    "abstract": "We propose Momentum-Scheduled Adam (MS-Adam), a simple modification to Adam that schedules its momentum parameters using curriculum learning principles. While Adam remains the de facto optimizer for training neural networks, its performance often degrades for tasks with sparse gradients or non-stationary objectives. Our approach introduces scheduled decay rates that adaptively adjust momentum based on gradient history, theoretically guaranteeing convergence for a broader class of non-convex objectives than standard Adam. We derive convergence bounds showing MS-Adam achieves O(1/T) rates under milder assumptions than previously established. Experimental evaluation on CIFAR-10/100 image classification shows 0.8-1.1% accuracy improvements over Adam baselines, and 5-7% faster convergence on language modeling tasks. However, these gains diminish on larger-scale benchmarks like ImageNet, where MS-Adam performs comparably to AdamW with careful hyperparameter tuning. Our analysis reveals the schedule's effectiveness correlates strongly with gradient sparsity patterns, suggesting limited utility for dense-gradient regimes. While the method offers modest improvements in specific settings, the computational overhead and additional hyperparameters may outweigh benefits for many applications. Code is available at anonymous.url/ms-adam.",
    "id": 396
  },
  {
    "title": "Gradient Descent with Periodic Sparsification: A Memory-Efficient Approach for Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We propose PSGD (Periodic Sparsification Gradient Descent), a simple modification to standard gradient-based optimization that periodically enforces sparsity on model updates during fine-tuning. Our method applies magnitude-based pruning to gradients every k steps, maintaining only the top-p fraction of gradient components before applying the update. This approach is motivated by the observation that during fine-tuning of pre-trained models, most gradient energy concentrates in a small subset of parameters. We evaluate PSGD on parameter-efficient fine-tuning tasks across vision and language domains, including LoRA adaptation of large language models. Experiments on GLUE benchmarks and CIFAR-10/100 show 1.3-2.1\u00d7 memory reduction during training with <2% performance degradation compared to standard fine-tuning. While the memory savings are consistent, we observe that performance gaps widen on more complex tasks like MMLU (4.7% drop), suggesting limitations of our simple magnitude-based pruning strategy. Theoretical analysis reveals PSGD converges at rate O(1/\u221aT) under standard convexity assumptions, matching standard SGD up to a constant factor. Code is available at [anonymous link].",
    "id": 400
  },
  {
    "title": "Gradient Surgery Meets Lottery Tickets: Interpolating Pruning Techniques for Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "We investigate whether combining magnitude-based pruning with gradient-based optimization can yield neural networks that are simultaneously sparse and robust to catastrophic forgetting in continual learning settings. Our approach, Sparse Gradient Surgery (SGS), applies iterative magnitude pruning to identify a sparse subnetwork, then fine-tunes this subnetwork using gradient projection techniques to maintain performance on previously seen tasks. While we demonstrate that SGS achieves competitive compression ratios (up to 95% sparsity) on CIFAR-10 with minimal accuracy loss compared to dense baselines, our results on continual learning benchmarks yield mixed outcomes. Specifically, SGS provides modest improvements in backward transfer (1-3% over standard fine-tuning) but fails to match the performance of more sophisticated regularization methods in forward transfer scenarios. Our theoretical analysis suggests that the effectiveness of SGS is highly dependent on the alignment between the sparse subnetwork's gradient subspace and the optimization trajectories of sequential tasks. Though our findings validate the intuition that sparse networks can serve as better feature extractors for continual learning, the practical gains remain limited to specific experimental configurations. We make our code and checkpoints publicly available to facilitate further investigation into the interplay between sparsity and plasticity in neural networks.",
    "id": 404
  },
  {
    "title": "Rethinking Dropout in Transformers: A Frequency-Domain Perspective",
    "authors": [
      "Liu, K.",
      "Brown, S.",
      "Kumar, V."
    ],
    "abstract": "While dropout is widely used to regularize transformer networks, its interaction with multi-head attention mechanisms remains poorly understood. We propose FreqDrop, a dropout variant that operates in the frequency domain of attention maps rather than the standard element-wise masking. Our key insight is that traditional dropout disrupts low-frequency attention patterns that are crucial for long-range dependencies, while preserving noisy high-frequency components. FreqDrop addresses this by learning to selectively mask frequency bands based on their spectral energy, effectively providing adaptive regularization that varies across layers. Experiments on standard NLP benchmarks (GLUE, SQuAD) show modest improvements of 0.3-0.7% over standard dropout when using large pretrained models, with particularly strong gains on longer sequences (>512 tokens). However, improvements shrink considerably when training from scratch, and computational overhead increases by 15-20% due to FFT operations. While our approach offers a principled view of attention regularization, the practical benefits appear limited to specific regimes. Code and pretrained models are available at anonymized-url.",
    "id": 412
  },
  {
    "title": "Gradient Descent with Lookahead Buffering: A Simple Extension for Improved Convergence in Non-Convex Optimization",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Chen, S.",
      "Anderson, M."
    ],
    "abstract": "We propose Lookahead Buffering (LB), a lightweight modification to standard gradient descent that maintains a small history of previous gradients to guide future updates. By linearly combining current and past gradients within a moving window, LB introduces momentum-like behavior without the hyperparameter sensitivity of traditional momentum methods. Our approach requires only storing k=3-5 additional gradient vectors (O(kd) memory) and can be implemented with three extra lines of PyTorch code. We prove convergence guarantees for LB on strongly convex objectives, achieving O(1/T) rates that match gradient descent. Empirically, LB shows consistent but modest improvements across various tasks: 2-4% faster convergence on ImageNet training, 1.5% absolute accuracy gains on CIFAR-10/100, and 8-12% sample efficiency improvements in tabular RL environments. While these results demonstrate promise, we acknowledge the improvements are incremental rather than transformative. Our ablation studies reveal LB is most beneficial when combined with adaptive optimizers like Adam, suggesting its primary contribution may be as a plug-and-play enhancement rather than a standalone algorithm.",
    "id": 414
  },
  {
    "title": "Efficient Gradient Descent with Dynamic Learning Rate Recycling",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Anderson, J."
    ],
    "abstract": "Modern optimization methods often discard past gradient information, potentially wasting computational resources. We propose Gradient Recycling (GR), a simple modification to standard SGD that stores and reuses previous gradients through a dynamic weighting scheme based on cosine similarity with the current gradient direction. Our method requires minimal memory overhead (storing only k=5-10 past gradients) and one extra hyperparameter. We provide theoretical analysis showing GR achieves similar convergence rates to SGD for convex objectives, though our bounds contain additional terms that scale with the recycling parameter. Empirically, GR demonstrates 5-15% faster convergence on ResNet training for CIFAR-10 and ImageNet compared to vanilla SGD with momentum, but improvements are inconsistent across architectures and datasets. Ablation studies reveal performance degrades when k>20, suggesting limited benefit from longer gradient memory. While GR offers modest practical gains on some tasks, we acknowledge its benefits may not justify the implementation complexity for most practitioners. Code is available at [anonymized for review].",
    "id": 416
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Merging with Curriculum Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Transformer models have achieved remarkable success across domains, but their quadratic complexity with respect to sequence length remains a critical bottleneck. While previous work has explored token pruning and merging heuristics to reduce computational cost, these methods often rely on fixed rules or require expensive pre-training adaptations. We propose a learnable token merging framework that dynamically reduces sequence length during fine-tuning using a lightweight gating network. Our method introduces curriculum scheduling that gradually increases compression ratios throughout training, stabilizing optimization while achieving significant speedups. Specifically, we train a small MLP to predict merge decisions for consecutive token pairs, guided by auxiliary losses that preserve information content. Across three NLP benchmarks (GLUE, SQuAD, and CNN/DM), our approach reduces inference FLOPs by 25-40% with minimal degradation (<1% relative) on most tasks. However, we observe more substantial performance drops (3-5%) on tasks requiring fine-grained reasoning (e.g., QQP, MNLI). Analysis reveals our merging policy tends to aggressively compress syntactic markers necessary for complex reasoning. While our method offers a practical trade-off between efficiency and performance, the learned merging strategies show limited generalization across domains, suggesting the need for task-specific tuning. Code and pre-trained models are available at [URL].",
    "id": 417
  },
  {
    "title": "Revisiting Curriculum Learning with Adaptive Difficulty Balancing",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Curriculum learning has shown promise in improving model generalization by gradually increasing task difficulty. However, existing approaches rely on hand-crafted schedules or fixed heuristics that may not adapt to dataset-specific characteristics. We propose an adaptive curriculum learning framework that dynamically balances the rate at which training examples are introduced based on their local consistency with respect to the current model. Our method uses a momentum-based estimator to track example-wise learning signals, adjusting their presentation probabilities without requiring prior domain knowledge. We evaluate our approach on vision and language benchmarks with standard architectures. Results show modest improvements (0.3-1.2% accuracy gains) over baseline training on 4 of 7 datasets, with particularly strong performance on noisy label scenarios. While these gains are encouraging, we observe that the benefits diminish with larger models and longer training horizons. Our findings suggest that adaptive curricula may be most effective when computational budgets are constrained. Code and pretrained models are available at anonymous URL.",
    "id": 428
  },
  {
    "title": "Gradient Surgery for Improving Transfer Learning in Small-Scale Vision Models",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Brown, M."
    ],
    "abstract": "We investigate whether gradient manipulation techniques developed for multitask learning can improve transfer learning performance on small-scale vision tasks. While existing transfer learning methods typically fine-tune pre-trained models using standard gradient descent, we hypothesize that explicitly constraining the fine-tuning gradients to maintain similarity with pre-trained features could improve generalization, particularly in low-data regimes. We propose Gradient Projection Transfer (GPT), a simple method that projects task gradients onto the subspace spanned by pre-trained representations during fine-tuning. Our experiments on CIFAR-10/100 and a subset of ImageNet (100 classes) show that GPT achieves 1-3% accuracy improvements over standard fine-tuning baselines when using 1-10% of the training data, with diminishing returns on larger datasets. However, we find that similar improvements can be achieved by simply reducing the learning rate. Theoretical analysis suggests our method provides implicit regularization equivalent to \u21132 weight decay under certain assumptions. While our results indicate limited practical advantage over careful hyperparameter tuning, the method's simplicity and theoretical connections may be of interest to the transfer learning community.",
    "id": 435
  },
  {
    "title": "Adaptive Gradient Descent with Polynomial Stepsize Scheduling",
    "authors": [
      "Chen, L.",
      "Kapoor, A.",
      "Nakamura, S."
    ],
    "abstract": "We propose Poly-GD, a gradient descent variant that uses polynomial decay schedules for stepsize adaptation in non-convex optimization. Unlike cosine or exponential schedules commonly used in deep learning, our method employs learnable polynomial coefficients updated via meta-gradient descent. We provide theoretical convergence guarantees for quadratic objectives and demonstrate empirical improvements on CIFAR-10 and ImageNet. On ResNet-18, Poly-GD achieves 0.8% better final validation accuracy compared to standard SGD with cosine annealing, with comparable training time. However, our method shows diminishing returns on larger architectures like EfficientNet-B4. While the polynomial scheduling introduces minimal overhead (5% increase in wall-clock time), our theoretical analysis only covers convex cases, and the meta-learning component adds complexity. Ablation studies suggest the benefit primarily comes from the polynomial form rather than the adaptive coefficients. Our code is available at [redacted].",
    "id": 450
  },
  {
    "title": "Adaptive Curriculum Learning via Difficulty-Aware Sampling Improves Sample Efficiency in Deep Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Curriculum learning has shown promise for improving sample efficiency in deep RL, but existing approaches often rely on hand-crafted curricula or simple heuristics that may not generalize across environments. We propose AdaCurriculum, a simple yet effective method that adaptively adjusts the sampling distribution over training episodes based on a learned measure of task difficulty. Our approach uses a lightweight auxiliary network to estimate episode difficulty from initial states, requiring only 5% additional parameters. We evaluate AdaCurriculum on three standard continuous control benchmarks (MuJoCo) and two discrete action environments. Results show 15-40% sample efficiency improvements over uniform sampling baselines and 8-25% gains over a recent curriculum learning method, though we find variance across seeds can be high (standard deviation 12.3%). While our method achieves consistent improvements on medium-complexity environments, benefits diminish on simpler tasks, suggesting the approach may be most valuable when the optimal curriculum is non-trivial. Ablations reveal that both the difficulty estimator and adaptive sampling schedule contribute to performance, with neither component alone sufficient. Our implementation and hyperparameters are provided for reproducibility.",
    "id": 466
  },
  {
    "title": "Gradient Amplification Networks: A Lightweight Architecture for Improving Transformer Optimization Dynamics",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.M.",
      "Kumar, S."
    ],
    "abstract": "Transformer architectures suffer from optimization challenges when scaled to modest model sizes, often exhibiting gradient vanishing in early layers. We propose Gradient Amplification Networks (GANs), a lightweight architectural modification that re-weights gradient flows without altering forward pass computations. Our approach inserts parallel amplification pathways that modulate gradient magnitudes during backpropagation based on layer-wise statistics. On GLUE and SuperGLUE benchmarks with 125M-350M parameter models, GANs achieve modest improvements (1.2-2.3% average score increase) over baseline transformers while maintaining identical inference costs. Theoretical analysis reveals our method approximates a second-order optimization step, though we observe diminishing returns beyond 500M parameters. Our results suggest that careful gradient re-weighting can provide incremental training benefits, though the gains are task-specific and may not generalize to larger scales. Code is available at [anonymous URL].",
    "id": 489
  },
  {
    "title": "Gradient Amplification: A Slight Perturbation Approach to Faster Neural Network Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Amplification (GradAmp), a simple modification to standard SGD that multiplies gradients by a learned scalar factor during backpropagation. While similar in spirit to adaptive learning rate methods, GradAmp learns amplification factors through an auxiliary loss that penalizes gradient magnitudes falling below a moving threshold. Through experiments on CIFAR-10/100 and ImageNet-1k, we show 1.2-1.8\u00d7 convergence speedup over baseline methods with negligible additional computation. However, we find performance gains diminish with larger batch sizes and careful learning rate schedules. Theoretical analysis reveals GradAmp can be viewed as approximately implementing a second-order update under restricted conditions. While the method shows consistent benefits over standard baselines, gains over well-tuned adaptive optimizers like AdamW are marginal. Our code will be made available upon publication.",
    "id": 521
  },
  {
    "title": "Attention Is Not All You Need: Alternating Sparse-Local Transformers for Efficient Long-Context Modeling",
    "authors": [
      "Morrison, J.",
      "Chen, L.",
      "Kim, S."
    ],
    "abstract": "We propose Alternating Sparse-Local Transformers (ASLT), a modification to standard Transformers that alternates between sparse global attention and local windowed attention to improve efficiency on long sequences. While recent work has focused on making attention more efficient through approximations, our key insight is that alternating between two complementary patterns can maintain the benefits of full attention with O(n\u221an) complexity. Our method replaces every other attention layer with a local sliding-window pattern, creating a shallow sparse-global/lean-local architecture. We demonstrate competitive performance on the Long Range Arena benchmark, achieving 82.1 average accuracy (vs. 84.3 for standard Transformers) while reducing FLOPs by 47%. On language modeling tasks, ASLT matches baselines on WikiText-103 perplexity (18.7 vs 18.5) with 2.1x speedup on 8K sequences. However, we observe instability on certain tasks, particularly those requiring precise positional reasoning. While our work provides a practical efficiency improvement, we acknowledge that theoretical analysis of approximation error remains limited. Code and models will be available at [url].",
    "id": 527
  },
  {
    "title": "ReLoCo: Regularizing Low-Curvature Directions in Neural Network Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose ReLoCo, a simple regularization technique that selectively penalizes optimization in directions of low curvature in the loss landscape. Motivated by observations that neural networks often converge to solutions with many near-zero Hessian eigenvalues, we add a penalty term proportional to the inverse of the absolute Hessian eigenvalues during training. This encourages the optimizer to prioritize updates in high-curvature directions while avoiding flat regions that may hurt generalization. On ImageNet and CIFAR-10, ReLoCo achieves a 0.8-1.2% improvement over standard SGD baselines, with particularly strong gains on ResNet architectures. While the method shows promise on vision tasks, results on NLP benchmarks are mixed, and we observe sensitivity to the curvature threshold hyperparameter. Theoretical analysis reveals that ReLoCo can be viewed as a second-order adaptive method that smoothly interpolates between gradient descent and natural gradient descent. Our results suggest that curvature-aware regularization provides a practical way to improve optimization dynamics, though future work is needed to scale the Hessian computation efficiently to larger models and understand the theoretical properties more completely.",
    "id": 532
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Structured Sparsity in Low-Rank Adaptors",
    "authors": [
      "Chen, L.",
      "Srinivasan, V.",
      "Thompson, K."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its memory footprint remains substantial for billion-scale models. We propose LoRA-Lite, which combines the standard LoRA decomposition with structured sparsity patterns learned during the adaptation phase. Specifically, we introduce a learnable binary mask applied to the rank decomposition matrices, trained using straight-through gradient estimation. Our method achieves 2.3\u00d7 memory reduction in activations during training and 1.7\u00d7 reduction in checkpoint size compared to standard LoRA, while maintaining 94-97% of the downstream performance across GLUE, SuperGLUE, and domain-specific benchmarks. While empirical results are encouraging, we provide only partial theoretical justification for the sparsity pattern's validity. Ablation studies reveal the method is most effective for classification tasks but shows degradation on generation tasks requiring longer context. Code will be made available upon acceptance.",
    "id": 539
  },
  {
    "title": "Gradient Surgery with Adaptive Norm Clipping: A Practical Framework for Multi-Task Learning",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "Multi-task optimization often suffers from conflicting gradients between tasks, leading to sub-optimal performance on individual objectives. While recent gradient surgery methods like PCGrad and GradNorm address this issue, they rely on heuristic thresholds that remain fixed throughout training. We propose Adaptive Gradient Surgery (AGS), a simple extension that dynamically adjusts gradient clipping thresholds based on gradient norm distributions. Our method maintains a moving window of gradient norms for each task, automatically determining when gradient conflicts are sufficiently severe to warrant surgery. Through extensive experiments on three standard benchmarks (CIFAR-100 with 20 tasks, NYUv2 scene understanding, and Meta-World RL), AGS shows modest improvements over fixed-threshold baselines (+1.2% average accuracy, -2.1% relative loss increase). While our results are consistent across domains, the gains are incremental and computational overhead increases by 15-20%. Our analysis reveals that AGS primarily helps when tasks have imbalanced gradient magnitudes, but offers limited benefits when task gradients are well-aligned. Though not a breakthrough, AGS provides a practical plug-and-play modification that may benefit practitioners working with heterogeneous multi-task objectives. Code and experimental logs will be released upon acceptance.",
    "id": 542
  },
  {
    "title": "Gradient Surgery for Partially Observable Reinforcement Learning via Implicit Value Regularization",
    "authors": [
      "Chen, L.",
      "Srinivasan, V.",
      "Okafor, K."
    ],
    "abstract": "Policy gradient methods in partially observable environments suffer from high variance due to the mismatch between the true state distribution and the approximate beliefs used for decision-making. We propose Implicit Value Regularization (IVR), a simple technique that adds a gradient surgery step to existing policy gradient algorithms by projecting policy updates onto a subspace that preserves value function consistency. Our approach modifies the policy gradient using an implicit regularization term derived from a local linear approximation of the value function, without requiring explicit modeling of belief states. We evaluate IVR on standard benchmarks including pixel-based continuous control tasks and partially observable variants of MuJoCo environments. Results show modest improvements over PPO and SAC baselines on 6 out of 12 tasks (average normalized score improvement of 3.2%), with particularly consistent gains in sparse reward settings. While IR introduces minimal computational overhead (\u22485% increase in wall-clock time), we observe that performance gains diminish with increased model capacity, suggesting the regularization effect may be too restrictive for complex policies. Code and hyperparameters are provided for reproducibility.",
    "id": 551
  },
  {
    "title": "Improving Transformer Training Stability through Layer-wise Learning Rate Warmup Schedules",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Chen, J."
    ],
    "abstract": "Transformer models often exhibit training instability during optimization, leading to diverging losses or suboptimal convergence. We propose LayerCake, a novel layer-wise learning rate warmup schedule that assigns different warmup durations to different transformer blocks based on their position and attention proximity. Extensive experiments on GLUE and WMT benchmarks show that LayerCake achieves a modest 1.2% average improvement over standard warmup while reducing training instability by 37% as measured by gradient norm spikes. While our method is theoretically motivated by analyzing the linearized dynamics of attention layers, we find the benefits are most pronounced in medium-scale models (under 1B parameters) and largely disappear in larger models. Our ablations reveal that 60% of the improvement comes from extending warmup for early attention layers, with diminishing returns from more complex scheduling. Though computationally efficient and easy to implement, our approach lacks compelling justification for why layer-specific rates should matter in the overparameterized regime. Code and pretrained models are available at anonymous.link.",
    "id": 555
  },
  {
    "title": "Adaptive Gradient Clipping Through Local Lipschitz Estimation for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.A.",
      "Kim, S."
    ],
    "abstract": "Transformer architectures exhibit unstable training dynamics due to exploding gradients, especially with large learning rates. While gradient clipping provides a simple remedy, choosing the clipping threshold remains heuristic and often dataset-dependent. We propose Lipschitz Clip, a method that adaptively sets clipping thresholds by estimating local Lipschitz constants of the loss landscape. Our approach approximates the Lipschitz constant using gradients from small batch statistics and applies a lightweight momentum-based update to the clipping threshold. Through experiments on Wikitext-103 and CIFAR-10, we show modest improvements in training stability and final perplexity/error over fixed clipping baselines. The method adds only 2-3% computational overhead compared to standard training. While our results demonstrate practical benefits, the theoretical guarantees are limited to smooth convex settings, leaving open questions about generalization to non-convex objectives. Our code will be publicly available upon publication.",
    "id": 575
  },
  {
    "title": "Gradient Coordination in Federated Learning: A Simple Momentum Mechanism with Moderate Gains",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Federated learning faces challenges in client drift due to heterogeneous data distributions across devices. We propose FedMom, a momentum-based coordination mechanism that accumulates client updates across rounds to reduce gradient variance. Our approach adds minimal computational overhead (0.3% increase compared to FedAvg) and requires only a single hyperparameter \u03b2 that balances momentum strength. We theoretically prove convergence for non-convex objectives under standard assumptions, achieving rates comparable to existing federated optimization methods. Experiments on CIFAR-10, CIFAR-100 with pathological non-IID splits show 2-4% improvement over FedAvg in final accuracy, though gains diminish with increasing client participation. While our method demonstrates consistent improvements on smaller datasets, benefits plateau on larger-scale benchmarks like ImageNet, suggesting limited scalability. Code and pre-trained models are available at [anonymized link].",
    "id": 592
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed training, but existing methods rely on hand-crafted compression schedules that may not adapt to varying training dynamics. We propose Learned Gradient Compression (LGC), a framework that uses a lightweight LSTM controller to dynamically adjust quantization levels for gradient compression during training. The controller is trained using reinforcement learning with a reward function balancing compression ratio against gradient error. Our method achieves up to 8\u00d7 compression on ImageNet training while maintaining 0.5-1.2% accuracy degradation compared to full-precision training. Unlike static compression schemes, LGC adapts to different model architectures and training phases without manual tuning. Experiments on ResNet-50 and Transformer architectures show LGC outperforms existing methods like QSGD and TernGrad by 2-4\u00d7 in compression ratio at similar accuracy levels. However, theoretical analysis reveals LGC provides convergence guarantees only for convex objectives under restrictive assumptions. While practical performance is promising, the computational overhead of maintaining the controller (\u22482% training time) and lack of generalization to non-stationary objectives limit broader applicability. Code is available at masked-for-review.",
    "id": 596
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: An Empirical Analysis of Helper Tasks",
    "authors": [
      "Liu, K.",
      "Thompson, S.",
      "Rodriguez, M."
    ],
    "abstract": "Multi-task learning (MTL) often improves generalization by leveraging shared representations, yet determining which auxiliary tasks truly help remains challenging. We propose an introspective approach to identify and dynamically reweight 'helper' tasks during training. Our method monitors gradient alignment between primary and auxiliary losses, pruning tasks that exhibit persistent conflicting gradients. We evaluate this simple strategy on standard vision and NLP benchmarks with up to 8 tasks, showing 2-4% improvements over naive MTL baselines, but comparable performance to recent gradient surgery methods like PCGrad. While our approach is computationally lightweight and requires no hyperparameter tuning for reweighting, the gains diminish as model size increases, suggesting limited scalability. Code and pre-trained models will be released.",
    "id": 598
  },
  {
    "title": "Fixed-Point Alternating Minimization for Robust Low-Rank Matrix Recovery with Non-Convex Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose a fixed-point alternating minimization algorithm for robust low-rank matrix recovery that incorporates non-convex regularization terms to better approximate the rank function. While existing nuclear norm minimization approaches provide theoretical guarantees, they often require restrictive assumptions on the measurement operator and result in biased solutions. Our approach employs a family of non-convex surrogates including smoothly clipped absolute deviation (SCAD) and minimax concave penalty (MCP) within an alternating minimization framework. We establish convergence to stationary points under standard assumptions, though our theoretical recovery guarantees are weaker than convex methods and depend on initialization quality. Empirically, our method achieves 5-15% lower reconstruction error than nuclear norm minimization on synthetic low-rank matrices with 30% gross corruption, and provides marginal improvements (2-3%) on matrix completion tasks from the Netflix dataset. However, performance degrades significantly when the rank is overestimated or corruption exceeds 40%. The algorithm requires careful tuning of two hyperparameters and converges more slowly than competing methods. Our results suggest non-convex regularization can provide practical benefits for robust matrix recovery, though theoretical gaps remain in understanding its advantages over simpler convex baselines.",
    "id": 603
  },
  {
    "title": "Gradient Amplification Through Transient Sharpening: A Simple Heuristic for Improved Convergence in Overparameterized Networks",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Chawla, N.",
      "Chen, L."
    ],
    "abstract": "We propose a lightweight training modification that amplifies gradient signals during the initial phase of training deep neural networks. Our method, Transient Gradient Sharpening (TGS), applies an element-wise transformation to gradients based on their historical norm distribution, effectively increasing the learning rate for coordinates with consistently small updates while maintaining stability for others. Unlike adaptive optimizers that maintain per-parameter statistics throughout training, TGS gradually phases out its intervention, transitioning to standard SGD momentum after a preset number of epochs. We demonstrate improvements over vanilla SGD on ResNet-50 and ViT-B/16 architectures across CIFAR-100 and ImageNet-1K, with average top-1 accuracy gains of 1.2% and 0.8% respectively. While our approach is simple to implement and adds minimal computational overhead, we observe that gains diminish with stronger baselines like AdamW. Theoretical analysis reveals connections to sharpness-aware minimization, though we acknowledge limitations in our convergence guarantees and the heuristic nature of our phase-out schedule. Code is available at the provided link.",
    "id": 615
  },
  {
    "title": "LoRA-Soup: Iterative Merging of Low-Rank Adaptations for Task-Agnostic Transfer Learning",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Hassan, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient method for adapting large language models to downstream tasks. While effective for single-task adaptation, the sequential application of LoRA modules leads to catastrophic forgetting in multi-task scenarios. We propose LoRA-Soup, an iterative weight-averaging approach that merges LoRA modules without requiring task-specific identifiers or joint training. Our method builds on the observation that LoRA updates lie in low-dimensional subspaces with favorable geometric properties. By carefully aligning these subspaces through a novel similarity metric based on principal angles, we achieve stable merging across diverse tasks. Experiments on GLUE and SuperGLUE benchmarks show modest improvements over sequential fine-tuning (average gain of 1.3%), with particular gains in few-shot settings. However, our approach slightly underperforms compared to task-parallel methods. Theoretical analysis reveals that merging success depends heavily on the alignment between LoRA subspaces, which may not hold for dissimilar tasks. While LoRA-Soup offers a practical compromise between efficiency and performance, its benefits are task-dependent and may be limited when task distributions are highly divergent.",
    "id": 629
  },
  {
    "title": "LoRA-V: Parameter-Efficient Fine-Tuning with Variance-Adaptive Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers may be suboptimal. We propose LoRA-V, a variance-adaptive variant that dynamically adjusts the rank of LoRA modules based on gradient variance observed during training. Our method uses an efficient thresholding scheme to increase the rank for layers with high gradient variance while pruning low-variance adapters. We evaluate LoRA-V on instruction tuning tasks using Llama-2 models across three domains: mathematical reasoning, code generation, and dialogue. Results show modest improvements over standard LoRA (average +1.2% accuracy) while using 15-25% fewer trainable parameters. However, we find that performance gains diminish on larger models (70B+) and tasks with abundant training data. Theoretical analysis reveals that LoRA-V's effectiveness depends on the spectral gap of the original weight matrices, suggesting limited applicability to pre-trained models with certain spectral properties. While LoRA-V provides a lightweight alternative to manual rank tuning, its benefits are most pronounced in resource-constrained scenarios with smaller models.",
    "id": 637
  },
  {
    "title": "Lookahead Batch Normalization: An Empirical Study of Batch Statistics in Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, S."
    ],
    "abstract": "Batch Normalization (BN) remains widely used in vision tasks but is often replaced by LayerNorm in transformers due to instability with small batch sizes. We propose Lookahead Batch Normalization (LBN), which computes batch statistics using a moving average of future activations during training. LBN maintains the normalization benefits of BN while stabilizing training for small batches. On ImageNet classification, LBN achieves 76.2% top-1 accuracy with batch size 64, matching standard BN with batch size 256. For transformer language modeling, LBN provides marginal improvements (0.3-0.5 perplexity points) over LayerNorm on Wikitext-103 and C4 datasets. We analyze the effect of the lookahead window size and demonstrate LBN's sensitivity to hyperparameter tuning. While LBN shows promise for certain architectures, our experimental results reveal limited gains on standard NLP benchmarks. Detailed ablation studies suggest the benefits are primarily due to implicit regularization rather than improved optimization dynamics.",
    "id": 639
  },
  {
    "title": "Exploring the Role of Layerwise Learning Rates in Transformer Training",
    "authors": [
      "Chen, L.",
      "Kumar, V.",
      "Ishikawa, S."
    ],
    "abstract": "We investigate whether applying distinct learning rates to different layers of transformers can improve training dynamics and final performance. Building on recent observations that lower layers tend to learn more stable representations than upper layers during early training, we propose Layer-Adaptive Learning Rates (LALR) \u2014 a simple approach that schedules learning rates based on layer depth and training iteration. Through systematic ablations on 6 language modeling datasets, we find that LALR provides modest improvements (1.2-1.7% perplexity reduction) over standard training with similar compute budgets, primarily in low-data regimes (\u2264100M tokens). However, gains diminish or reverse on larger datasets, and our extensive hyperparameter sweeps reveal high sensitivity to learning rate schedules. Our analysis indicates LALR mostly affects the learning dynamics of attention weights, particularly multi-head attention in middle layers. While the technique is straightforward to implement and adds negligible overhead, its benefits appear dataset and architecture dependent, suggesting limited general applicability. Code: anonymous link.",
    "id": 640
  },
  {
    "title": "LoRA-Drop: Adapting Low-Rank Adaptation via Dynamic Parameter Pruning for Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers may be suboptimal for downstream tasks. We propose LoRA-Drop, a simple yet effective approach that dynamically prunes LoRA parameters during fine-tuning based on gradient-based importance scores. Our method maintains the training-time efficiency of standard LoRA while achieving additional parameter reduction by up to 40% across GLUE tasks without significant performance degradation. We validate LoRA-Drop on RoBERTa-base and T5-base architectures, demonstrating comparable or marginally improved performance to full LoRA while using fewer parameters. However, our experiments reveal that the benefits of dynamic pruning diminish for larger models (\u22653B parameters), suggesting fundamental limitations in rank allocation heuristics. While LoRA-Drop provides practical improvements for resource-constrained deployments, our theoretical analysis indicates that the pruning strategy may be overly conservative for more complex adaptation scenarios. Code is available at anonymous-github.com/lora-drop.",
    "id": 653
  },
  {
    "title": "Adaptive Learning Rates via Curvature-Aware Gradient Clipping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Curvature-Aware Gradient Clipping (CAGC), a novel approach for adaptive learning rate scheduling that leverages local curvature estimates to dynamically adjust gradient norms. While existing clipping methods rely on fixed thresholds or simple heuristics, CAGC computes per-layer curvature via efficient Hessian-vector products and scales gradients inversely proportional to their local Lipschitz constants. Our method requires minimal additional computation and can be integrated into standard deep learning frameworks with less than 20 lines of code. We evaluate CAGC on image classification tasks using ResNet-50 on ImageNet and CIFAR-10, as well as language modeling with Transformer-XL on WikiText-103. Results show 2-4% improvements in final accuracy over baseline AdamW with gradient clipping, while reducing training variance across 3 random seeds. However, gains diminish on larger models (GPT-2 1.5B) and longer training schedules, suggesting the method's applicability may be limited to medium-scale settings. Theoretical analysis establishes convergence guarantees for convex objectives, though extending these results to neural networks remains an open challenge. Code will be released upon acceptance.",
    "id": 660
  },
  {
    "title": "LoRA-VQ: Low-Rank Adaptation with Vector Quantization for Memory-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Krishnan, A.",
      "Liu, Y."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large language models while preserving computational resources. We propose LoRA-VQ, which combines Low-Rank Adaptation (LoRA) with learned vector quantization to further reduce memory overhead during fine-tuning. Our method quantizes the low-rank matrices into discrete codebooks, achieving up to 3.2\u00d7 memory reduction compared to standard LoRA with minimal performance degradation on GLUE tasks. Theoretical analysis shows our quantization strategy maintains gradient flow through straight-through estimators, though convergence guarantees remain limited to simplified settings. Experiments on RoBERTa-base and T5-small across text classification, summarization, and question-answering benchmarks demonstrate competitive performance to LoRA (within 1.5% F1 score) while using 32\u00d7 fewer unique parameter values. However, we observe instability when scaling to larger models (>10B parameters) and tasks requiring complex reasoning. Ablation studies reveal performance drops with aggressive quantization levels (>4-bit) and highlight the importance of learnable temperature scheduling during training. While LoRA-VQ offers practical memory benefits for resource-constrained settings, the trade-off between compression and task performance suggests careful calibration is required for deployment.",
    "id": 662
  },
  {
    "title": "Gradient Compression with Learned Quantization Tables for Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.E.",
      "Kim, J."
    ],
    "abstract": "Gradient compression is essential for communication-efficient federated learning, but existing methods often rely on hand-designed quantization schemes that may not be optimal for specific model architectures or data distributions. We propose Learned Quantization Tables (LQT), a simple yet effective approach that learns per-layer quantization scaling parameters during training using a small held-out validation set. Our method achieves 8-16x compression rates while maintaining comparable accuracy to full-precision training on standard benchmarks. However, we observe that LQT's effectiveness diminishes in highly heterogeneous federated settings where client data distributions diverge significantly. Extensive experiments on CIFAR-10 and EMNIST demonstrate 2-4% accuracy improvements over uniform quantization baselines, though our ablation studies reveal that benefits are primarily attributable to learned scaling rather than the quantization scheme itself. While LQT provides practical improvements for homogeneous federated scenarios, its reliance on validation data and additional hyperparameters may limit applicability in resource-constrained settings. Code will be made available upon publication.",
    "id": 670
  },
  {
    "title": "Gradient Dropout: Stochastic Annealing for Improved Optimization Trajectories",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Dropout, a simple regularization technique that randomly masks gradients during neural network training to escape sharp local minima. Unlike standard dropout applied to activations, we stochastically drop entries of gradients computed via backpropagation, effectively creating a noisy optimization landscape that becomes smoother as training progresses. We provide theoretical analysis showing that this noise injection is equivalent to adding a data-dependent regularizer, with convergence guarantees for convex objectives. Empirical evaluation on CIFAR-10/100 and ImageNet shows consistent but modest improvements (0.3-0.8% accuracy gains) over baselines when hyperparameters are carefully tuned. While our method rarely performs worse than standard training, the benefits appear limited to specific architectures (ResNets, DenseNets) and are less pronounced for Vision Transformers. Code is available, though reproducibility requires exact hyperparameter matching. Gradient Dropout offers a lightweight training enhancement that may complement other regularization techniques, though it is not a standalone solution for performance breakthroughs.",
    "id": 674
  },
  {
    "title": "Improving Few-Shot Generalization through Task-Agnostic Prompt Alignment",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Park, J."
    ],
    "abstract": "Large language models demonstrate impressive zero-shot capabilities, but their few-shot performance remains highly sensitive to prompt formatting. We propose Task-Agnostic Prompt Alignment (TAPA), a lightweight method that learns to reformat prompts without task-specific supervision. TAPA uses a meta-optimization objective that maximizes consistency of predictions across noisy paraphrases of the same prompt. Experiments on 12 few-shot benchmarks show 2-4% improvements over standard prompting on average, with particular gains on numerical reasoning tasks. However, we observe minimal benefits on classification tasks and negative transfer when prompts differ significantly from training seen during meta-learning. Our analysis reveals TAPA primarily learns to suppress spurious correlations introduced by formatting choices rather than discovering fundamentally better prompting strategies. While TAPA offers a practical improvement for few-shot learning at small computational cost, its limited scope and task-dependent effectiveness suggest the broader challenge of prompt optimization requires more sophisticated solutions.",
    "id": 676
  },
  {
    "title": "Gradient Entropy Regularization: A Lightweight Approach to Mitigating Memorization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, D."
    ],
    "abstract": "We propose gradient entropy regularization (GER), a simple regularization technique that encourages neural networks to maintain high entropy in their gradient distributions during training. By penalizing low-entropy gradient patterns, GER implicitly discourages memorization of specific training examples without requiring explicit data augmentation or architectural modifications. Our method adds minimal computational overhead (less than 3% increase in training time) and works as a drop-in replacement for standard regularizers. We evaluate GER on vision and language tasks, demonstrating 2-5% improvements in memorization metrics while maintaining comparable test accuracy on CIFAR-10, CIFAR-100, and SST-2. However, we observe diminishing returns on larger datasets and architectures. Theoretical analysis reveals GER approximately minimizes a bound on memorization capacity, though the connection becomes weaker for very deep networks. While GER shows promise for privacy-sensitive applications, our experiments are limited to medium-scale benchmarks, and we acknowledge potential confounds with existing implicit regularization effects.",
    "id": 679
  },
  {
    "title": "Sharpness-Aware Minimization with Adaptive Gradient Clipping for Improved Generalization in Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We propose SAM-AGC, a simple modification to Sharpness-Aware Minimization (SAM) that incorporates adaptive gradient clipping based on the sharpness of the loss landscape. While SAM has shown promise for improving generalization in small-scale vision tasks, its effectiveness on large language models remains inconsistent. Our key insight is that the aggressive updates in SAM can destabilize training in transformers, particularly when gradients become large. SAM-AGC addresses this by clipping gradients adaptively based on their alignment with the sharpness direction. We evaluate our method on the GLUE benchmark using BERT-base and RoBERTa-base, achieving average improvements of 1.2% over SAM and 2.3% over standard SGD with momentum. Despite these gains, we observe that SAM-AGC's benefits diminish as model size increases\u2014we find no consistent improvements on GPT-2 medium variants. Through extensive ablations, we identify that our method works best on tasks with limited training data and moderate model complexity. While our results are promising, they are limited to encoder-only architectures and standard classification tasks. Our code and trained models are available at [URL anonymized].",
    "id": 680
  },
  {
    "title": "Gradient Surgery for Stabilizing Adversarial Training in Deep Classifiers",
    "authors": [
      "Liu, K.",
      "Chen, T.",
      "Rodriguez, A."
    ],
    "abstract": "Adversarial training remains unstable for deep networks despite recent advances. We propose Gradient Surgery (GS), a simple but effective method that selectively drops or rescales gradient components during adversarial training. GS monitors gradient variance across mini-batches and applies threshold-based pruning to reduce instability. We evaluate GS on CIFAR-10 and CIFAR-100 with ResNet-18 and WideResNet-34, showing 2-3% robust accuracy improvements over standard adversarial training with minimal computational overhead (\u22645% extra training time). Surprisingly, GS also improves clean accuracy by 1-2% in some settings. While our approach shows promise, theoretical justification remains limited and benefits diminish on larger datasets like ImageNet. Analysis reveals GS primarily affects early training dynamics, suggesting its impact may be replicated through careful hyperparameter tuning. Our code is available at [URL].",
    "id": 685
  },
  {
    "title": "Gradient Noise Injection as a Lightweight Alternative to Mixup Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We explore whether gradient noise injection can serve as a computationally cheaper substitute for input-level augmentation methods like Mixup. Motivated by recent observations that Mixup primarily acts as a form of regularization rather than data augmentation, we propose injecting scaled Gaussian noise directly into gradients during backpropagation. Our method requires minimal additional computation compared to Mixup's expensive convex combinations. We evaluate our approach on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures. Results show gradient noise achieves comparable performance to Mixup on CIFAR (within 0.5-1.2% accuracy), but underperforms significantly on ImageNet (-2.8% top-1). However, our method provides more consistent training stability and reduces overfitting on smaller datasets. Notably, we discover the noise scale must be precisely calibrated\u2014a hyperparameter that varies substantially across architectures and datasets. While our results validate gradient regularization as a viable technique for limited-resource scenarios, the sensitivity to hyperparameters and ImageNet performance gaps suggest applications may be restricted to smaller-scale settings. Code and trained models are available at [anonymous link].",
    "id": 687
  },
  {
    "title": "Gradient Descent with Adaptive Step Sizes via Second-Order Curvature Estimation",
    "authors": [
      "Liu, H.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a simple modification to stochastic gradient descent that estimates the local curvature along each coordinate using diagonal approximations of the Hessian matrix. Our method, called Adaptive Curvature Scaling (ACS), computes curvature estimates using only gradient information from the previous two iterations, avoiding expensive Hessian computations. We provide theoretical analysis showing that ACS achieves convergence rates comparable to Adam for convex objectives, though our bounds contain additional logarithmic factors. Experiments on standard benchmarks (MNIST, CIFAR-10, and PTB language modeling) show ACS matches or slightly outperforms Adam in final accuracy by 0.5-1.2% in some settings, but requires careful hyperparameter tuning. While the theoretical contribution is incremental, ACS may be useful for practitioners seeking alternatives to adaptive optimizers. Code will be made available upon acceptance.",
    "id": 694
  },
  {
    "title": "Efficient Gradient Compression via Learned Thresholding in Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, M."
    ],
    "abstract": "We propose Adaptive Gradient Compression (AGC), a simple yet effective method for reducing communication costs in federated learning by learning instance-specific thresholds for gradient sparsification. Unlike existing top-k or quantization approaches that use fixed compression rates, AGC employs a small auxiliary network to predict optimal compression thresholds based on local gradient statistics and client data distribution. Our method achieves 30-50% communication reduction compared to uniform quantization on CIFAR-10 and EMNIST benchmarks while maintaining comparable accuracy to uncompressed baselines. However, we observe performance degradation on non-IID partitions, with up to 8% accuracy drop in extreme cases. While AGC introduces minimal computational overhead (less than 5% training time increase), our theoretical analysis reveals the compression scheme cannot guarantee convergence under arbitrary data heterogeneity. Experiments across 5 datasets demonstrate competitive results (within 2-3% of uncompressed models) but fail to match state-of-the-art methods on more challenging benchmarks like CIFAR-100. Code will be made available upon publication.",
    "id": 695
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance and training instability. While recent gradient surgery methods like PCGrad and GradDrop effectively mitigate gradient conflict, they rely on heuristic hyperparameters that require extensive tuning across domains. We propose AdaGS (Adaptive Gradient Surgery), a lightweight method that uses momentum-based memory banks to dynamically adjust gradient projection thresholds based on task similarity. Our approach maintains an exponential moving average of per-task gradient norms, enabling online estimation of conflict severity without additional hyperparameters. Experiments on three benchmarks (CIFAR-10/SVHN multi-task, NYUv2, and Taskonomy) show modest improvements over baselines (1-2% accuracy gains), with particularly strong results when tasks have varying difficulty levels. However, we find that our method's effectiveness diminishes on tasks with similar loss scales, suggesting residual gradient interference remains. While AdaGS offers a practical improvement over fixed-threshold approaches, our theoretical analysis reveals the method can still produce suboptimal gradient directions in certain parameter regimes. Code and trained models will be released upon acceptance.",
    "id": 696
  },
  {
    "title": "Stochastic Mirror Descent with Adaptive Polyak Stepsizes for Non-Convex Optimization",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a variant of stochastic mirror descent that incorporates adaptive Polyak stepsizes for non-convex optimization problems. While Polyak stepsizes are well-studied in convex settings, their application to non-convex objectives remains heuristic. Our method dynamically adjusts stepsizes based on a moving average of recent function values without requiring gradient Lipschitz assumptions. We establish convergence to stationary points at a rate of O(1/\u221aT) under mild assumptions, matching known rates for standard stochastic mirror descent. Experiments on neural network training show modest improvements over Adam and SGD with momentum on CIFAR-10 and PTB datasets, achieving 1-2% better test accuracy in some configurations. However, performance gains are inconsistent across architectures and initializations. Theoretical analysis reveals the stepsize adaptation may fail when gradients become sparse or the loss landscape exhibits sharp curvature. Code is available at anonymous-github.com/smd-adaptive-polyak.",
    "id": 712
  },
  {
    "title": "LoRA-FA: Low-Rank Adaptation with Feature Alignment for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular method for parameter-efficient fine-tuning of large language models, but suffers from suboptimal feature representations when applied to downstream tasks with significant domain shifts. We propose LoRA-FA, which augments standard LoRA with a lightweight feature alignment mechanism that aligns intermediate representations during fine-tuning. Our method introduces an additional orthogonal loss term that encourages the adapted features to remain close to the pre-trained manifold while allowing task-specific adaptation. We evaluate LoRA-FA on GLUE and SuperGLUE benchmarks using LLaMA-2-7B, achieving 1.2% average improvement over standard LoRA with only 0.05% additional parameters. While our results are statistically significant (p < 0.05 via paired t-test), the improvements are modest and concentrated on specific tasks (CoLA and RTE). Theoretical analysis shows our alignment term can be viewed as a form of trust region optimization, though we establish only loose bounds on approximation error. Code is available at anonymous-url.",
    "id": 745
  },
  {
    "title": "ReLoRA: Low-Rank Adaptation via Recursive Matrix Factorization for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Krishnan, V.",
      "Santos, J."
    ],
    "abstract": "We present ReLoRA, a parameter-efficient fine-tuning method that recursively decomposes pre-trained weight matrices into products of low-rank factors. Unlike existing LoRA variants that add low-rank matrices or adapters, ReLoRA iteratively factorizes existing parameters while maintaining frozen components, achieving comparable performance with 15-40% fewer trainable parameters across language and vision tasks. Our approach exploits the observation that pre-trained representations often exhibit hierarchical low-rank structure, which can be successively approximated through rank-constrained SVD updates. On GLUE and VTAB benchmarks, ReLoRA matches full fine-tuning accuracy (\u00b11.1%) while using only 0.15% of pre-trained parameters. However, our method requires careful rank scheduling and shows degraded performance on generative tasks (CNN/DailyMail summarization drops 4.3 BLEU). Theoretical analysis reveals ReLoRA implicitly performs a form of block-coordinate descent on matrix manifolds, though convergence guarantees remain limited. While ReLoRA offers slight parameter savings over LoRA variants, the practical benefits may be modest given increased implementation complexity. Code and pre-trained checkpoints will be released upon acceptance.",
    "id": 747
  },
  {
    "title": "Low-Rank Adaptation with Gradient Accumulation for Memory-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We present a simple modification to Low-Rank Adaptation (LoRA) that reduces memory usage during fine-tuning of large language models by accumulating gradients over multiple forward passes before updating the low-rank matrices. While LoRA has become a standard method for parameter-efficient fine-tuning, it still requires storing activations in memory during the backward pass, creating a bottleneck for training large models on consumer hardware. Our approach, GRAD-LoRA, addresses this by decoupling the gradient computation from the parameter update, allowing gradients to be computed in smaller batches and accumulated before applying the low-rank update. We evaluate GRAD-LoRA on instruction tuning tasks across 7B and 13B parameter models, achieving comparable performance to standard LoRA while reducing peak memory usage by 35-40% with minimal computational overhead (3-5% increase in training time). However, our method introduces a new hyperparameter (accumulation steps) that can affect convergence, and we observe increased instability in certain configurations. While GRAD-LoRA provides practical benefits for resource-constrained fine-tuning, we acknowledge its contributions are primarily incremental and may not justify publication on novelty alone. Code will be released upon acceptance.",
    "id": 758
  },
  {
    "title": "LoRA-Drop: Structured Pruning for Parameter-Efficient Fine-tuning via Learnable Rank Allocation",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Chaudhari, P."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular approach for parameter-efficient fine-tuning of large language models, but its fixed rank allocation across layers remains suboptimal. We propose LoRA-Drop, a simple method that learns to drop redundant LoRA modules during fine-tuning while maintaining task performance. Our approach introduces a learnable gating mechanism that automatically identifies and prunes low-importance adapter modules based on accumulated gradient information. Rather than requiring extensive hyperparameter tuning, LoRA-Drop uses a single threshold hyperparameter controlled by a validation-based early stopping strategy. We evaluate our method on GLUE and SuperGLUE benchmarks using LLaMA-7B and RoBERTa-large, showing 15-30% parameter reduction with minimal accuracy degradation (averaging 0.8% drop across tasks). Despite achieving reasonable compression rates, our analysis reveals that the learned pruning patterns are dataset-specific and may not generalize across tasks. While LoRA-Drop provides a practical approach to reducing adapter parameters, we acknowledge that performance gains are modest compared to more sophisticated compression techniques, and theoretical justification for the pruning criterion remains limited.",
    "id": 761
  },
  {
    "title": "Gradient Descent with Exponentially Increasing Learning Rates: A Bridge Between Convergence and Chaos",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Thompson, S."
    ],
    "abstract": "While traditional optimization theory recommends decaying learning rates for convergence, empirical observations suggest that certain neural networks benefit from increasing step sizes during training. We propose AgileGrad, a simple modification to standard SGD that exponentially increases the learning rate throughout training. Our theoretical analysis shows that under restricted conditions\u2014specifically for overparameterized linear regression with Gaussian features\u2014AgileGrad converges to a neighborhood of the global optimum with a rate that surprisingly improves for larger increase factors. We demonstrate that on small-scale vision tasks (CIFAR-10 with ResNet-18), AgileGrad achieves comparable performance to SGD+Momentum while requiring 20% fewer training steps. However, performance degrades significantly on larger architectures (ViT-base) and language tasks. Our ablation study reveals that the method is highly sensitive to the initialization scale and batch size. While our theoretical framework provides some justification for these empirical observations, the gap between our analysis assumptions and practical scenarios remains substantial. These results suggest that exponentially increasing learning rates may have niche applications but require careful tuning and are unlikely to replace standard decay schedules for general deep learning optimization.",
    "id": 766
  },
  {
    "title": "Lookahead Gradient Descent with Adaptive Restart for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T.",
      "Johnson, K."
    ],
    "abstract": "We propose Lookahead Gradient Descent with Adaptive Restart (LGAR), a simple modification to the popular Lookahead optimizer that incorporates periodic restart criteria based on gradient statistics. While Lookahead has shown empirical success in deep learning, its convergence properties remain poorly understood for non-convex objectives. LGAR introduces an adaptive restart mechanism that monitors the inner optimizer's gradient alignment, triggering a reset when the fast weights diverge from their slow weight anchor. We prove O(1/\u221aT) convergence for non-convex smooth objectives under standard assumptions, matching the rate of standard SGD despite the two-time-scale update. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet show LGAR achieves competitive performance to AdamW and Lookahead (+0.2-0.7% accuracy), while requiring 15-20% fewer iterations to reach comparable validation loss. However, gains are inconsistent across architectures and datasets, with minimal improvement on transformer models. Theoretically, our analysis relies on bounded gradient norms and does not explain the practical speedups observed. We release a PyTorch implementation with 30 lines of core code.",
    "id": 770
  },
  {
    "title": "LoRA-Prune: Iterative Low-Rank Adapter Pruning for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "While Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, its fixed-rank design often leads to over- or under-parameterization in downstream tasks. We propose LoRA-Prune, an iterative pruning framework that dynamically adjusts the rank of LoRA adapters during fine-tuning. Our method employs a simple magnitude-based pruning criterion coupled with a warm restart mechanism to recover potentially useful pruned dimensions. Through experiments on GLUE and SuperGLUE benchmarks using RoBERTa-base and T5-large, we show that LoRA-Prune achieves comparable performance to standard LoRA while reducing the number of trainable parameters by 15-30%. However, we find that these savings diminish as model size increases, and the computational overhead of iterative pruning sometimes outweighs the parameter reduction benefits. Our analysis reveals that the effectiveness of rank selection varies significantly across tasks and layers, suggesting that more sophisticated pruning criteria may be necessary. While LoRA-Prune provides modest improvements in parameter efficiency, our results indicate that static rank selection remains surprisingly competitive for most downstream applications.",
    "id": 771
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Learning via Historical Gradient Replay",
    "authors": [
      "Liu, K.",
      "Santos, M.",
      "Chen, J.",
      "Thompson, L."
    ],
    "abstract": "Multi-task learning faces a fundamental challenge: conflicting gradients between tasks can destabilize training and degrade performance. While recent gradient surgery methods like PCGrad and GradDrop have shown promise, they make local decisions based only on instantaneous gradients, potentially discarding useful information. We propose Gradient Surgery with Adaptive Memory (GSAM), a novel approach that maintains a small memory bank of historical gradients to inform surgery decisions. By using an adaptive weighting scheme based on cosine similarity between current and past gradients, GSAM identifies and preserves gradient directions that were beneficial in previous iterations but may appear conflicting locally. Our method introduces only a 3% memory overhead and minimal computational cost. We evaluate GSAM on four multi-task vision and NLP benchmarks, showing 4-7% improvements over PCGrad on average F1 scores. However, gains are inconsistent across tasks\u2014GSAM helps most when task conflicts are moderate, with diminishing returns in highly conflicting setups. Analysis reveals that memory size (k=5-10 is optimal) and the adaptive weighting parameter \u03bb (\u03bb\u2208[0.1,0.3]) are critical. While GSAM represents a step toward more principled gradient surgery, our theoretical analysis remains limited to toy settings, and the method's benefits over simpler ensemble approaches are modest.",
    "id": 772
  },
  {
    "title": "Lipschitz-Constrained Transformers Improve Out-of-Distribution Robustness but Sacrifice In-Distribution Performance",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We investigate whether enforcing Lipschitz constraints on transformer architectures improves robustness to out-of-distribution (OOD) data while maintaining practical performance. Our approach introduces a computationally efficient method for constraining the Lipschitz constant of attention mechanisms through spectral normalization and gradient clipping. We evaluate on vision and language tasks using ImageNet-C, CIFAR-10-C, and GLUE diagnostic datasets. Results show modest OOD improvements (2-3% accuracy gains) but consistent in-distribution performance degradation (5-7% drops) compared to unconstrained baselines. Surprisingly, the benefit varies significantly across domains\u2014vision tasks show clearer robustness gains than NLP tasks. While our method provides theoretical guarantees, the computational overhead (20-30% training slowdown) and performance trade-offs raise questions about practical deployment. Our contributions include: (1) a novel method for Lipschitz-bounded attention, (2) extensive empirical evaluation across multiple domains, and (3) revealing insights about when Lipschitz constraints help versus harm. Code is available [redacted for review].",
    "id": 777
  },
  {
    "title": "Gradient Surgery in Neural Networks: A Systematic Study of Parameter-Wise Pruning During Training",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Dubois, Y."
    ],
    "abstract": "We investigate whether gradient-based pruning of individual parameters during training can achieve compression ratios comparable to post-training magnitude pruning. Our method surgically removes parameters whose gradients consistently fall below a learned threshold across mini-batches, theoretically justified through a connection between gradient flow and information content. Experiments on CIFAR-10/100 and ImageNet show modest compression (20-35%) with negligible accuracy loss, achieving similar performance to iterative magnitude pruning across ResNet and Vision Transformer architectures. However, computational overhead increases training time 2-3\u00d7, and effectiveness saturates for sparsity >40%. Compared to state-of-the-art pruning methods, our approach trades some compression efficiency for training-time flexibility. We release code and pre-trained models, though hyperparameter sensitivity makes exact replication challenging.",
    "id": 779
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Riemannian Perspective with Adaptive Weighting",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Multi-task learning faces optimization challenges when task gradients conflict during training. We propose Riemannian Gradient Surgery (RGS), which projects conflicting gradients onto the tangent space of a learned task manifold, combined with an adaptive weighting scheme based on gradient similarity. Our method incorporates second-order information through an approximation of the Riemannian metric tensor, allowing more principled gradient modification than existing heuristic approaches. Experiments on standard multi-task benchmarks show 2-5% improvements over PCGrad and GradNorm, with particular gains when task conflict is severe. However, we find the computational overhead scales quadratically with the number of tasks, limiting applicability to scenarios with many tasks. While RGS demonstrates consistent improvements, we acknowledge that gains are modest and computational cost may not justify deployment in resource-constrained settings. Theoretical analysis is limited to simplified cases. Code will be released upon acceptance.",
    "id": 793
  },
  {
    "title": "Variance-Reduced Policy Gradient with Adaptive Trust Region Clipping for Sample-Efficient Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose a variance-reduced policy gradient method that combines adaptive trust region clipping with control variates for improved sample efficiency in continuous control tasks. Our approach extends proximal policy optimization by incorporating a learned baseline that dynamically adjusts based on local curvature estimates of the policy landscape. The key innovation is an adaptive clipping mechanism that modulates the effective step size based on gradient variance estimates, theoretically grounded in a variance-aware analysis that extends the standard policy gradient theorem. We evaluate our method on a suite of MuJoCO benchmarks and demonstrate 15-25% sample efficiency improvements over PPO on half of the environments, while maintaining comparable performance on the remainder. However, we observe degradation on tasks with sparse rewards, suggesting limitations in our variance reduction strategy when signal-to-noise ratios are low. Theoretical convergence guarantees are provided for the convex case, but extending these results to non-convex policy classes remains an open challenge. Our empirical results, combined with ablation studies showing marginal benefits from individual components, suggest the approach provides incremental rather than transformative improvements for policy optimization.",
    "id": 794
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning with Task-Specific Noise Adaptation",
    "authors": [
      "Liu, C.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder performance on individual tasks. While existing gradient surgery methods address gradient conflicts through projection schemes, we observe that these approaches neglect the inherent noise characteristics unique to each task. We propose Noise-Adaptive Gradient Surgery (NAGS), which incorporates task-specific noise estimates into the gradient conflict resolution process. Our method computes per-task gradient noise variance using a simple moving average of recent gradients, then weights the projection operation to preserve more reliable gradient directions. Experiments on CIFAR-100/SVHN supervised multi-task and NYUv2 scene understanding benchmarks show 2-3% improvements over standard gradient surgery baselines, with particular gains on harder tasks. However, we find that NAGS provides diminishing returns when task gradients are already well-aligned and adds non-trivial computational overhead (15-20% training slowdown). Our theoretical analysis suggests the noise-based weighting scheme converges to a stationary point under standard assumptions, though the convergence rate depends on unknown noise parameters. While our approach shows promise for scenarios with significant gradient conflicts, the practical benefits appear limited to specific task combinations, raising questions about the broader applicability of noise-aware gradient manipulation.",
    "id": 797
  },
  {
    "title": "LoRA-D: Low-Rank Adaptation with Dynamic Rank Selection via Gating Mechanisms",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method, but selecting the optimal rank remains a tedious hyperparameter search problem. We propose LoRA-D, which augments standard LoRA with learnable gating mechanisms that dynamically determine the effective rank during training. Our method maintains LoRA's computational efficiency while automatically adjusting the rank based on layer-specific importance. Experiments on GLUE and SuperGLUE with RoBERTa-large show LoRA-D achieves comparable performance to the best fixed-rank LoRA in 60% of cases (average improvement of 0.3% on GLUE over standard LoRA), with rank selection converging within the first 1K steps. However, we observe diminishing returns on larger models (\u22657B parameters), where simple rank heuristics nearly match LoRA-D's performance. While our method reduces hyperparameter tuning by eliminating rank search, the computational overhead increases training time by 15%. Our results suggest dynamic rank selection provides modest benefits primarily for smaller fine-tuning tasks, offering practical value in resource-constrained settings but limited impact for state-of-the-art model adaptation.",
    "id": 802
  },
  {
    "title": "Self-Guided Data Augmentation via Learned Transformation Embeddings",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Data augmentation is crucial for improving generalization in machine learning models, but traditional augmentation strategies rely on manually designed transformations that may not align with the intrinsic structure of the data. We propose a novel approach that learns to generate meaningful augmentations by encoding transformations into a latent space guided by the model's own uncertainty estimates. Our method uses a lightweight auxiliary network trained jointly with the main classifier to predict transformation parameters that maximize the model's learning signal. Unlike recent adversarial augmentation techniques, our approach avoids expensive optimization loops by conditioning transformations on learned embeddings that capture task-relevant variations. Experiments on CIFAR-10 and ImageNet show modest improvements (0.8-1.2% accuracy gains) over strong baselines while reducing training time by 30% compared to state-of-the-art augmentation methods. However, we observe diminishing returns on larger datasets and limited effectiveness on domains where the learned transformation prior does not match the true data manifold. While our framework offers a computationally efficient alternative augmentation strategy, extensive hyperparameter tuning is required for each new domain, suggesting the need for more robust priors. Code is available at anonymous-url.github.io.",
    "id": 806
  },
  {
    "title": "Improving Transformer Training Stability Through Curvature-Aware Weight Initialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Curvature-Aware Initialization (CAI), a simple modification to standard weight initialization schemes for transformers that incorporates second-order information from the loss landscape. Our method computes a local curvature estimate using a small-batch approximation of the Hessian trace, then scales initial weights inversely proportional to layer-wise curvature magnitudes. Experimental results on language modeling tasks with GPT-2 medium (350M parameters) and BERT-base (110M parameters) show CAI reduces gradient norm variance by 23-31% during early training stages compared to Xavier/Glorot initialization. However, while CAI achieves marginally better perplexity on Wikitext-103 (18.7 vs 19.1) and converges 1.2x faster in wall-clock time, these improvements diminish as training progresses. Additional experiments on vision transformers show similar early-training benefits but no consistent downstream task improvements. Theoretical analysis proves CAI converges under standard assumptions, though the curvature estimates introduce a small asymptotic bias. While promising for reducing training instability, our results suggest the benefits of curvature-aware initialization may be limited to specific training regimes and could be achieved through simpler learning rate scheduling.",
    "id": 816
  },
  {
    "title": "Improving Transformer Efficiency through Layer-Dropping with Learnable Routing",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose SkipRoute, a dynamic layer-skipping mechanism for Transformer architectures that learns to route inputs through a subset of encoder layers at inference time. Our method trains auxiliary routing networks that predict the optimal layer composition for each input, enabling computational savings without modifying the original pre-trained weights. We evaluate SkipRoute on GLUE and SuperGLUE benchmarks using BERT-base and RoBERTa models. Results show 25-40% reduction in FLOPs with minimal accuracy degradation (<1.5% on average), outperforming static layer-dropping baselines. However, we observe significant performance drops on tasks requiring complex reasoning (CoLA drops by 4.7%), and routing networks add 5-8% parameter overhead. Analysis reveals the method primarily skips later layers, suggesting redundancy in deeper representations. While SkipRoute provides practical speedups for deployment scenarios, benefits are inconsistent across tasks and the routing decisions remain difficult to interpret. Code and models are available at [URL].",
    "id": 818
  },
  {
    "title": "Adversarial Feature Mixup: A Simple Data Augmentation Approach for Improving Robust Accuracy",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Adversarial training remains the dominant approach for achieving robustness against adversarial attacks, but at significant computational cost and with limited transferability across architectures. We propose Adversarial Feature Mixup (AFM), a lightweight data augmentation technique that interpolates latent representations between adversarial and clean examples during training. Rather than computing adversarial examples for each gradient step, AFM generates a small pool of adversarial examples per batch and mixes their features with clean examples using parameter-free interpolation in the penultimate layer. Our experiments on CIFAR-10 and CIFAR-100 show that AFM achieves 43.2% robust accuracy against PGD attacks, comparable to standard adversarial training (43.8%) while requiring 3.2x less training time. However, AFM shows limited effectiveness against stronger attacks like AutoAttack (31.1% vs. 39.4%). Analysis reveals AFM primarily improves robustness for easy-to-learn features, raising questions about the mechanism of robustness. While AFM offers a computationally efficient alternative to full adversarial training, its limitations highlight the need for better understanding of adversarial robustness beyond standard benchmarks.",
    "id": 833
  },
  {
    "title": "Spectral Normalization Without Suffering: A Simplified Approach to Lipschitz-Constrained Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a lightweight alternative to spectral normalization that enforces Lipschitz constraints in neural networks through gradient clipping and selective weight decay. While spectral normalization has become standard for stabilizing GAN training and controlling Lipschitz constants, its computational overhead remains significant, particularly for wide networks. Our method replaces the expensive singular value decomposition with a simple per-layer scaling factor derived from historical gradient norms. Experiments on CIFAR-10 and ImageNet show our approach achieves competitive FID scores (9.2 vs 8.8 for baseline spectral norm on CIFAR-10) while reducing training time by 25%. Additionally, we demonstrate improved stability in adversarial training for robustness, with a 2% increase in accuracy under PGD attacks compared to standard training. However, we observe that our method underperforms spectral normalization on more complex datasets like LSUN Bedrooms, and theoretical guarantees are weaker than those provided by exact spectral normalization. The approach is most effective for medium-scale applications where the full benefits of Lipschitz constraints are desired but computational resources are limited.",
    "id": 835
  },
  {
    "title": "Gradient Descent with Adaptive Step-Size Based on Local Gradient Variance",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose GRADVAR, an optimization method that adapts step-sizes based on local estimates of gradient variance for stochastic gradient descent. Motivated by the observation that gradient variance often correlates with optimization progress, our method computes a running estimate of variance using a small window of recent gradients and scales the step-size inversely proportional to this estimate. We provide theoretical analysis showing convergence under assumptions of Lipschitz continuity and bounded variance, achieving a convergence rate of O(1/\u221aT) for non-convex objectives. Experiments on CIFAR-10 and ImageNet with ResNet architectures demonstrate 5-15% faster convergence compared to Adam and SGD with momentum in early training stages, while performing comparably in final accuracy. However, we observe sensitivity to hyperparameter choices and minor degradation on language modeling tasks. GRADVAR offers a lightweight alternative to adaptive methods without requiring momentum buffers or second-moment estimates, though benefits appear limited to vision tasks with specific architectures. Code is available at anonymous-url.github.io/gradvar.",
    "id": 837
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "As distributed training scales to larger models and clusters, communication overhead from gradient synchronization becomes a critical bottleneck. We propose Learned Gradient Quantization (LGQ), a framework that dynamically adjusts the precision of gradient compression based on local curvature estimates. Unlike static quantization schemes, LGQ uses a lightweight meta-network to predict optimal bit-widths for each gradient tensor given loss surface characteristics. Our method achieves up to 4.7\u00d7 communication reduction with minimal accuracy loss on ResNet-50 and BERT-Large training. However, we observe diminishing returns beyond 64 GPUs and significant hyperparameter sensitivity across tasks. While LGQ matches TopK and PowerSGD baselines on standard benchmarks, convergence properties remain theoretically fragile, particularly for non-convex objectives. Our results suggest learned compression schedules can provide practical benefits for medium-scale training, though the approach may require additional stabilization techniques for extreme-scale deployment.",
    "id": 852
  },
  {
    "title": "Sketch-to-Image Synthesis via Frequency-Aware Progressive Distillation",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose a simple yet effective approach for sketch-to-image synthesis that leverages frequency-aware progressive distillation. Our method decomposes the generation process into sequential stages, where high-frequency sketch edges are first translated into low-frequency image structure before progressively adding fine-grained details. We introduce a lightweight frequency-based attention mechanism that operates at multiple scales, enabling more faithful rendering of sketch geometry compared to standard diffusion models. While our approach achieves competitive FID scores on the SketchyCOCO benchmark (FID 28.4 vs. 26.7 for the previous best), we observe that our model particularly excels on human-drawn sketches with simple line styles. Our primary contribution lies in demonstrating that frequency-space conditioning can provide a computationally efficient alternative to full diffusion models for sketch-guided generation. However, we acknowledge that our method struggles with complex scene compositions and unusual texturing. Experiments on three sketch datasets demonstrate 15-20% improvement in user preference for sketch fidelity, though generalization to out-of-domain sketches remains limited. Code and models will be released upon acceptance.",
    "id": 862
  },
  {
    "title": "Fast Proximal Policy Optimization via Curvature-aware Second-order Updates",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Proximal Policy Optimization (PPO) remains a dominant algorithm for reinforcement learning, but its sample complexity often limits practical applications. We propose Curvature-aware PPO (C-PPO), which incorporates second-order information without full Hessian computation by using diagonal approximations of the policy Hessian structure. Our method modifies the clipped surrogate objective with curvature-based trust regions, yielding faster convergence in low-data regimes. Experiments on continuous control benchmarks show 15-30% sample efficiency improvements over PPO on half of the MuJoCo environments, with comparable wall-clock time. However, performance gains diminish in high-dimensional action spaces, and we observe instabilities when applied to discrete action domains. While C-PPO demonstrates clear benefits in specific settings, the approach does not address fundamental PPO limitations in sparse reward scenarios. Our work suggests that lightweight second-order information can enhance policy gradient methods, though the technique faces practical challenges in scaling beyond medium-sized problems.",
    "id": 863
  },
  {
    "title": "Gradient Surgery in Stochastic Training: When Less Intervention Improves Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Singh, K."
    ],
    "abstract": "Gradient surgery techniques that modify per-sample gradients during training have gained popularity for mitigating memorization and improving generalization in deep learning. We revisit these methods through the lens of implicit regularization and propose a minimalist variant that selectively intervenes only on gradients with largest per-sample norm ratio. Unlike existing approaches that perform surgery on every sample, our method preserves statistical properties of stochastic gradients while still achieving regularization. We provide theoretical analysis showing our approach minimizes a modified loss that includes an adaptive regularizer, and empirically demonstrate improvements over baseline training on CIFAR-10 and Tiny-ImageNet. However, gains are inconsistent across architectures (ResNet vs Vision Transformer) and can be negative on some datasets (CIFAR-100). Our ablation studies reveal that performance improvements correlate strongly with the proportion of high-norm gradients in the dataset, suggesting the technique's benefits may be problem-dependent. Code and experiments are reproducible with less than 5 GPU days on a single A100.",
    "id": 866
  },
  {
    "title": "Variance-Reduced Policy Gradient Methods with Adaptive Second-Order Momentum",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Zhao, K."
    ],
    "abstract": "Policy gradient methods suffer from high variance in gradient estimates, particularly in continuous control tasks. While variance reduction techniques and adaptive optimizers exist, their combination remains underexplored. We propose VAPO (Variance-reduced Adaptive Policy Optimization), which integrates a novel second-order momentum term with recursive variance reduction. Our method computes importance-weighted gradient corrections using a sliding-window estimator, adapting the learning rate based on the evolution of per-parameter gradient curvature. We prove that VAPO achieves O(1/\u221aT) convergence in non-convex settings, matching the optimal rate for stochastic policy gradient methods. Experiments on MuJoCo continuous control benchmarks show 8-15% improvement over PPO and 5-12% over TRPO on 4 of 9 tested environments, though gains diminish in environments with sparse rewards. The variance reduction component provides 2-3\u00d7 faster convergence in early training phases but becomes negligible after ~1M steps. While our theoretical results hold for Lipschitz smooth policies, we observe instabilities when using neural networks with ReLU activations, suggesting our assumptions may be restrictive. Our method adds only 15% computational overhead compared to PPO, making it practical for standard RL workflows. Code is available in supplementary materials.",
    "id": 868
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Stochastic Optimization",
    "authors": [
      "Kim, S.",
      "Okafor, C.",
      "Li, J."
    ],
    "abstract": "We propose Adaptive Gradient Noise Injection (AGNI), a simple modification to stochastic gradient descent that adds scaled Gaussian noise to gradients during training. Unlike previous noise injection methods that use fixed schedules, AGNI adaptively adjusts noise variance based on the magnitude of recent gradients. Theoretically, we show that AGNI converges to first-order stationary points for smooth non-convex functions, though our convergence bounds are similar to standard SGD. Empirically, we evaluate AGNI on CIFAR-10/100 and ImageNet classification tasks using ResNet-18/34 architectures. Results show modest improvements in test accuracy (0.3-0.7%) over SGD with momentum, particularly in low-data regimes and for smaller models. Ablation studies indicate that the adaptive noise schedule is crucial, with fixed noise injection sometimes hurting performance. While the method is computationally efficient and easy to implement, the improvements are incremental and not consistent across all experimental settings. We provide PyTorch code and hyperparameter sensitivity analysis. Our work suggests that adaptive noise injection can be a useful regularization technique, though more investigation is needed to understand when and why it helps.",
    "id": 869
  },
  {
    "title": "Improving Transformer Efficiency via Iterative Token Dropping with Learned Importance Thresholds",
    "authors": [
      "Chen, L.",
      "Vaswani, A.K.",
      "Kumar, S."
    ],
    "abstract": "Transformer models suffer from quadratic complexity in sequence length, limiting their applicability to long documents. While recent work has explored token dropping to reduce computational cost, existing methods rely on heuristics or static thresholds that may discard important information. We propose Adaptive Token Pruning (ATP), a lightweight mechanism that learns dynamic importance thresholds for token dropping at each layer. ATP introduces a parallel scoring network that computes per-token saliency scores, combined with a learned gating mechanism that determines drop decisions based on both local context and global sequence statistics. Our method requires only 0.3% additional parameters and integrates seamlessly with existing pretrained models. Across language modeling tasks on C4, OpenWebText, and arXiv datasets, ATP achieves 1.4\u00d7 speedup during inference with <2% perplexity degradation compared to full transformers. However, we observe that performance drops significantly (>8% accuracy loss) on tasks requiring fine-grained reasoning over long contexts. We demonstrate ATP's limitations through careful ablation studies and provide theoretical analysis showing that certain attention patterns cannot be preserved under aggressive pruning. Our code and models are available at anonymized-link.",
    "id": 881
  },
  {
    "title": "Gradient Surgery with Memory: Alleviating Catastrophic Forgetting in Multi-Task Transformers via Parameter-Specific Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "Multi-task learning in transformers often suffers from destructive gradient interference, leading to catastrophic forgetting and suboptimal performance across tasks. While recent gradient surgery methods address this by projecting conflicting gradients, they treat all parameters uniformly and lack mechanisms to retain previously learned knowledge. We propose a memory-aware gradient surgery approach that assigns parameter-specific learning rates based on their historical gradient coherence patterns. Our method maintains an exponential moving average of task-specific gradients to identify parameters that have consistently served multiple objectives versus those causing interference. During optimization, we apply standard gradient surgery only to conflicting parameters while freezing or using reduced learning rates for stable ones. Experiments on GLUE and three proprietary e-commerce datasets show 3.2% average improvement over vanilla multi-task training and 1.8% over PCGrad, though gains diminish with larger models (\u22653B parameters). Analysis reveals improvements primarily stem from reduced forgetting on low-resource tasks rather than better feature sharing. While our method adds minimal computational overhead (5%), it requires tuning a sensitivity threshold and performs inconsistently across task combinations. Code will be released upon acceptance.",
    "id": 884
  },
  {
    "title": "Adaptive Gradient Clipping with Feedback: A Simple but Effective Approach for Stable Transformer Training",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Anderson, J."
    ],
    "abstract": "Training stability remains a persistent challenge for large transformer models, particularly when scaling to longer sequences and deeper architectures. While gradient clipping is widely employed, existing methods use fixed thresholds that fail to adapt to varying loss landscapes during training. We propose Adaptive Gradient Clipping with Feedback (AGCF), a lightweight modification that dynamically adjusts clipping thresholds based on gradient statistics from previous iterations. Our method incorporates a momentum-based estimate of gradient variance and employs a conservative update rule to prevent catastrophic updates. We evaluate AGCF across various transformer architectures on language modeling tasks (WikiText-103, C4) and vision transformers on ImageNet. While AGCF demonstrates improved training stability compared to standard clipping (53% reduction in gradient spikes), we observe only modest improvements in final perplexity (3.2% relative gain) and no significant benefits on downstream tasks. Our ablations reveal that the effectiveness of AGCF diminishes with proper hyperparameter tuning of existing methods. These results suggest that while AGCF provides practical benefits for practitioners facing training instability, the fundamental limitations of gradient clipping approaches may require more sophisticated optimization landscapes for substantial gains. Code is available at anonymous.github.io/agcf.",
    "id": 885
  },
  {
    "title": "Gradient Descent with Learned Step Sizes: A Meta-Optimization Approach for Quadratic Objectives",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Learned Step Size Gradient Descent (LSSGD), a meta-optimization framework that learns optimal step sizes for gradient descent on quadratic objectives. While existing adaptive methods like Adam and RMSprop use heuristics based on gradient statistics, LSSGD trains a small neural network to predict step sizes given gradient directions and local curvature information. Our key insight is that for quadratic functions, optimal step sizes can be characterized by the ratio of gradient norm to curvature along the descent direction, which can be approximated efficiently using Hessian-vector products. We evaluate LSSGD on synthetic quadratic problems and two real-world applications: logistic regression and matrix completion. On 100-dimensional quadratic objectives, LSSGD achieves convergence in 35% fewer iterations than tuned SGD, while maintaining generalization to problems with different eigenvalue distributions. However, we find that LSSGD offers diminishing returns for high-dimensional problems (\n000 dimensions) and struggles with ill-conditioned objectives. Our results suggest that learning step sizes is most beneficial for medium-scale problems with moderately varying curvature. While our approach shows promise in specialized settings with repeated similar objective structures, the overhead of meta-learning may not justify the modest gains over carefully tuned baselines for general optimization tasks.",
    "id": 887
  },
  {
    "title": "A Closer Look at Minimum Description Length Regularization in Modern Neural Networks",
    "authors": [
      "Liu, S.",
      "Kumar, V.",
      "Johnson, A."
    ],
    "abstract": "While Minimum Description Length (MDL) has long been proposed as a principled regularization technique for neural networks, its practical implementation faces significant computational challenges in modern deep learning architectures. We introduce a computationally tractable approximation of MDL regularization based on stochastic gradient Langevin dynamics, which we call SGLD-MDL. Our method estimates the description length of neural network parameters using gradient-based sampling, achieving O(d) complexity per iteration compared to O(d^2) for naive implementations. We evaluate SGLD-MDL on image classification and language modeling tasks, demonstrating modest improvements over weight decay (0.3-1.2% accuracy improvements on ImageNet), with particularly notable gains on small dataset regimes. However, we observe that the benefits diminish as model size increases, and our experimental results show higher variance compared to standard baselines. Theoretically, we establish PAC-Bayesian generalization bounds under simplified assumptions, though these bounds remain loose for practical architectures. Our work suggests that while MDL-inspired regularization can provide marginal benefits in data-limited settings, computational constraints significantly limit its utility for state-of-the-art models.",
    "id": 893
  },
  {
    "title": "Improving Transformer Efficiency Through Learnable Token Routing",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, V."
    ],
    "abstract": "Transformers suffer from quadratic complexity in sequence length, limiting applicability to long contexts. We propose Token Routing Networks (TRNs), a method that dynamically routes tokens through a sparse subset of model layers based on learned importance scores. Our approach uses a lightweight gating mechanism to identify tokens requiring full computation versus those that can skip layers, reducing FLOPs by 30-50% during inference. We evaluate TRNs across language modeling, summarization, and question answering tasks using T5 and BART architectures. Experiments show modest perplexity improvements (0.2-0.4 points) on standard benchmarks, with computational savings saturating at 512-token sequences. While our method provides measurable efficiency gains for certain sequence lengths, we observe degradation on tasks requiring fine-grained token interactions. The routing parameters exhibit task-specific behavior that complicates transfer learning. Our code and pre-trained models are available at [URL omitted for review].",
    "id": 896
  },
  {
    "title": "Gradient Descent with Lookahead Momentum: A Simple Modification for Improved Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Lookahead Momentum (LaM), a lightweight modification to standard stochastic gradient descent that maintains a slow-moving copy of the parameters while performing gradient updates on fast weights. The fast weights are reset to the slow weights every k steps, while the slow weights follow an exponential moving average of the fast weights. Unlike traditional momentum methods that smooth the update direction, LaM performs \"coarse corrections\" that we show encourages the optimizer to find flatter minima. We prove convergence under standard convex assumptions and demonstrate empirically that LaM improves test accuracy by 1-2% on CIFAR-10/100 and ImageNet compared to SGD with momentum, while requiring minimal hyperparameter tuning. However, experimental gains vary significantly across architectures and datasets, with notable improvements only observed for moderate-sized ResNets. While our theoretical analysis provides some justification, we acknowledge that the connection between lookahead updates and flat minima remains largely phenomenological. We provide PyTorch code and pre-trained models.",
    "id": 898
  },
  {
    "title": "Gradient Stabilization Through Adaptive Noise Injection for Training Deep Residual Networks",
    "authors": [
      "Nguyen, T.K.",
      "Johnson, L.M.",
      "Zhao, H."
    ],
    "abstract": "We propose an adaptive noise injection technique to stabilize gradient flow in very deep residual networks. Motivated by observations that gradient norms become increasingly unstable during training of networks beyond 100 layers, our method introduces carefully calibrated Gaussian noise into residual connections based on layer-wise gradient statistics. The noise magnitude is adjusted dynamically using a running estimate of gradient variance, theoretically grounded in stochastic differential equation analysis of gradient descent. Experiments on CIFAR-10/100 and ImageNet show modest improvements over standard ResNet training, with the technique providing more stable training particularly for depths exceeding 200 layers. However, performance gains saturate for moderately deep networks and the computational overhead may not justify deployment in all settings. While our theoretical analysis provides insights into the gradient noise trade-off, we acknowledge limitations in extending beyond residual architectures and the need for additional hyperparameter tuning across different datasets. Code will be made available upon acceptance.",
    "id": 901
  },
  {
    "title": "Gradient Surgery with Adaptive Dropout for Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder convergence and performance across tasks. We propose AdaGradDrop, a simple yet effective method that combines gradient surgery with learned dropout rates to automatically resolve gradient conflicts while maintaining task-specific representations. Our approach applies standard gradient projection techniques to identify conflicting directions, then adaptively drops neurons contributing to these conflicts based on their historical gradient statistics. We evaluate AdaGradDrop on three multi-task benchmarks spanning computer vision and NLP, achieving an average 3.2% improvement over vanilla multi-task approaches. While our method shows consistent gains over strong baselines including PCGrad and GradNorm, gains are most pronounced when task gradients are moderately aligned (cosine similarity < 0.3). We provide theoretical analysis showing AdaGradDrop approximately minimizes an upper bound on task interference, though our bound relies on strong assumptions about task similarity. Experimental results suggest our method's effectiveness is sensitive to the dropout initialization scale and may degrade performance when tasks are highly dissimilar. Our PyTorch implementation requires minimal code changes to standard multi-task architectures.",
    "id": 914
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning via Parameter-Specific Importance Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning in neural networks often suffers from gradient conflicts that lead to catastrophic forgetting when tasks are learned sequentially. While recent gradient surgery methods mitigate these conflicts through gradient projection or scaling, they remain memory-intensive and can overly constrain gradient directions, potentially limiting performance on complex tasks. We propose MEM-GS (Memory-aware Gradient Surgery), a simple yet effective approach that selectively applies gradient surgery based on parameter-specific importance measures computed via Fisher information. Our method maintains a small reservoir of important parameters for each task, applying surgical constraints only when the gradient would significantly impair these parameters. On CIFAR-100 and CelebA benchmarks with 5 sequential tasks, MEM-GS achieves 3.2% average improvement over standard gradient surgery while reducing memory overhead by 47%. However, we find limited benefits on tasks with high semantic similarity, where gradient conflicts are naturally smaller. Our results suggest that selective gradient surgery is most valuable for dissimilar tasks, though gains diminish as model capacity increases. Code is available at [anonymous] but lacks comprehensive hyperparameter ablation studies.",
    "id": 930
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning through Selective Parameter Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts and catastrophic forgetting when tasks compete for shared parameters. We propose Gradient Surgery with Memory (GSM), a simple modification to existing multi-task optimization that maintains an episodic memory of past gradients to inform selective parameter updates. Our method identifies conflicting gradients through cosine similarity analysis and selectively freezes or adapts learning rates for specific parameters based on their contribution to both current and previous tasks. On CIFAR-100 split into 5 tasks and a new benchmark of 8 NLP tasks from GLUE and SuperGLUE, GSM achieves comparable performance to state-of-the-art methods while requiring 30% less memory and 2x fewer hyperparameters. However, we find that GSM's benefits diminish when task similarity is low or when the number of tasks exceeds 15. While the approach shows promise, our theoretical analysis reveals that the selective update rule can create undesirable local minima in specific parameter regimes. Our results suggest that gradient-based solutions to catastrophic forgetting may be fundamentally limited when task distributions differ significantly.",
    "id": 949
  },
  {
    "title": "Adaptive Block-Size Scheduling for Gradient Descent: A Middle-Ground Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose Adaptive Block-Size Scheduling (ABSS), a simple modification to mini-batch gradient descent that adjusts batch size based on gradient variance estimates during training. While existing adaptive-batch methods rely on complex second-order approximations or expensive hyperparameter tuning, ABSS uses only running averages of gradient norms to dynamically scale batch sizes between 1/4 and 4x of the initial value. Our theoretical analysis shows ABSS achieves convergence rates within a logarithmic factor of optimally-tuned fixed-batch methods for convex and limited non-convex settings. Empirically, ABSS improves wall-clock training time by 5-15% on standard vision and language benchmarks compared to tuned baselines, but the gains diminish with sophisticated optimizers like AdamW. Our method requires minimal code changes and introduces negligible computational overhead. However, we observe that benefits are most pronounced in data-limited regimes, and results are inconsistent across highly-tuned architectures like Vision Transformers. ABSS provides a practical middle-ground between fixed-batch training and more complex adaptive methods, though its impact may be limited in well-resourced training scenarios.",
    "id": 958
  },
  {
    "title": "Rethinking Curriculum Learning for Neural Architecture Search: A Curriculum-Based Sampling Strategy",
    "authors": [
      "Chen, L.",
      "Joshi, K.",
      "Thompson, M."
    ],
    "abstract": "Neural Architecture Search (NAS) remains computationally expensive despite recent advances in weight sharing and gradient-based methods. We propose Curriculum Neural Architecture Search (CNAS), which applies curriculum learning principles to progressively guide the search towards more complex architectures. Our method begins by training simpler architectures with shared weights, then gradually introduces more sophisticated operations based on their historical validation performance. Specifically, we maintain a curriculum scheduler that dynamically adjusts operation probabilities based on their observed learning difficulty, analogous to curriculum learning in supervised training. We evaluate CNAS on CIFAR-10 and ImageNet, achieving competitive results (2.7% and 25.4% test error respectively) while reducing search cost by 35% compared to standard DARTS. However, our approach introduces sensitivity to curriculum hyperparameters and shows diminishing gains on larger datasets. While CNAS provides a novel perspective on combining curriculum learning with NAS, its improvements appear incremental and the computational savings may not justify the additional complexity. Our code is available at anonymous.url.",
    "id": 963
  },
  {
    "title": "LoRA-Quant: Efficient Fine-tuning Through Low-Rank Adapter Quantization",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M.",
      "Singh, P."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its memory footprint remains prohibitive for deployment on edge devices. We propose LoRA-Quant, a simple approach that quantizes LoRA adapters to 4-bit precision while maintaining a full-precision base model. Our method combines a novel quantization-aware training strategy with an importance-aware bit allocation scheme that assigns higher precision to adapters in critical layers. On GLUE and SuperGLUE benchmarks, LoRA-Quant achieves within 2% accuracy of full-precision LoRA with 4.3\u00d7 memory reduction during inference. However, we observe significant performance degradation on tasks requiring complex reasoning (e.g., 8.2% drop on DROP), suggesting limitations in our quantization strategy. While our results demonstrate clear memory benefits, the task-specific degradation raises questions about the universality of our approach. Code and models are available at [URL].",
    "id": 966
  },
  {
    "title": "Single-Timeline Transformer: Efficient Attention for Long Streaming Inputs",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "While transformers have demonstrated remarkable capabilities across domains, their quadratic complexity in sequence length remains a fundamental limitation for streaming applications. We propose Single-Timeline Transformer (STT), a simple modification to standard attention that processes sequences along a single temporal timeline rather than in discrete chunks. STT maintains a fixed-size memory bank that dynamically ages out older hidden states based on learnable temporal decay, eliminating the need for complex hierarchical attention or sparse patterns. Our method requires only 8 lines of code to implement and can be dropped into existing transformer architectures with minimal hyperparameter tuning. On long-range language modeling benchmarks (PG-19, arXiv), STT achieves perplexity within 3% of Longformer while using 4x fewer parameters and training 1.5x faster on 8xA100 GPUs. However, we observe instabilities on sequences longer than 64k tokens, and our fixed decay rate may be suboptimal for certain data distributions. While STT provides a practical alternative to existing long-form methods, its reliance on monotonic forgetting might limit performance on tasks requiring fine-grained long-range dependencies.",
    "id": 967
  },
  {
    "title": "Towards More Robust Non-Contrastive Self-Supervised Learning via Adaptive Eigenvalue Regularization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Garcia, J."
    ],
    "abstract": "Non-contrastive self-supervised learning methods like BYOL and SimSiam have shown impressive performance on vision tasks, but their objective functions exhibit notable instability during training, particularly in the final layers. We identify that this stems from spurious eigenvalue concentrations in the learned representation covariance matrices. Motivated by this observation, we propose Adaptive Eigenvalue Regularization (AER), a simple yet effective technique that applies spectral decay constraints weighted by the batch-wise eigengap distribution. Our method requires only a single hyperparameter and adds minimal computational overhead. We validate AER across CIFAR-10, CIFAR-100, and ImageNet-100, where it improves linear probing accuracy by 1.2-2.3% over baseline methods with 15% reduced training variance. While our empirical results are encouraging, we note that the theoretical justification for why eigenvalue regularization improves non-contrastive learning remains incomplete. Furthermore, performance gains diminish on larger-scale datasets. Our extensive ablations show AER is most beneficial when the pretraining dataset is small-to-medium scale and the encoder architecture is relatively shallow. Code and pretrained models are available at [redacted URL].",
    "id": 968
  },
  {
    "title": "Gradient Amplification Reduces Mode Collapse in GAN Training",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Thompson, A."
    ],
    "abstract": "We present a simple modification to the generator update in GAN training that significantly reduces mode collapse without architectural changes. Our method, Gradient Amplification for Generator Training (GAG), scales the generator gradients by the ratio of discriminator loss to generator loss during training. This amplifies updates when the generator is underperforming relative to the discriminator, encouraging exploration of underrepresented modes. We provide theoretical motivation via analysis in a simplified bilinear setting, establishing local convergence guarantees under standard assumptions. Empirically, GAG achieves modest but consistent improvements across image generation benchmarks: on CIFAR-10, we obtain FID scores of 28.3\u00b10.4 (vs 30.1\u00b10.5 for vanilla DCGAN) and demonstrate improved mode coverage on synthetic 2D datasets. While our empirical gains are limited to simpler architectures and the theoretical analysis requires restrictive assumptions, GAG offers a lightweight alternative to complex regularization schemes for practitioners. Code will be released upon publication.",
    "id": 974
  },
  {
    "title": "Progressive Gradient Pruning with Adaptive Sparsity Budgets for Resource-Constrained Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Neural network pruning methods often rely on expensive retraining procedures or require careful tuning of pruning ratios across layers. We propose Progressive Gradient Pruning (PGP), a simple method that uses gradient information to dynamically adjust pruning decisions during training. PGP maintains an adaptive sparsity budget that increases based on layer-wise gradient statistics, removing weights with low magnitude gradients rather than low magnitude weights. We theoretically analyze PGP for convex quadratic objectives, showing convergence to a stationary point under moderate assumptions. Empirically, PGP achieves 75-90% sparsity on ResNet-50 and BERT with <1% accuracy loss on ImageNet and GLUE tasks, matching or slightly improving upon magnitude pruning baselines. However, we observe that PGP's benefits diminish with modern optimizers like AdamW and adaptive schedules. Our method provides a practical alternative to existing pruning techniques, though gains over simple baselines are modest and implementation introduces additional hyperparameters requiring careful tuning.",
    "id": 986
  },
  {
    "title": "Improving Neural Network Robustness via Progressive Input Gradient Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We propose a novel regularization technique for improving adversarial robustness in neural networks by penalizing the input gradients during training. Unlike previous approaches that use fixed regularization weights, our method progressively increases the penalty based on the local Lipschitz constant estimated during training. We demonstrate improvements on CIFAR-10 and ImageNet against PGD attacks, achieving 2-3% better robust accuracy compared to standard adversarial training baselines. While our approach is computationally lightweight (adding <5% training time) and works with existing architectures, we observe that the benefits diminish on stronger attacks not seen during training. Theoretical analysis shows our regularizer bounds the local Lipschitz constant but relies on strong assumptions about data manifold structure. Experiments on additional datasets and attack scenarios yield inconsistent results, highlighting the need for careful hyperparameter tuning. Our method provides a practical trade-off between clean accuracy and robustness but may not directly advance the fundamental understanding of adversarial examples. Code and pre-trained models will be made available.",
    "id": 992
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Towards Stable Multi-Task Learning in Transformers",
    "authors": [
      "Liu, K.",
      "Anderson, J.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning in transformers often suffers from conflicting gradients that destabilize training and limit performance gains. While recent gradient surgery methods like PCGrad reduce interference between task gradients, they rely on static projection mechanisms that can negatively impact learning dynamics. We propose GAM-Surgery, a memory-augmented approach that selectively retains beneficial gradient directions using an adaptive queue mechanism. Our method maintains a small reservoir of historical gradients (\u2248 1% of model parameters) and computes similarity scores to determine when surgical intervention is necessary. Experiments on GLUE and SuperGLUE benchmarks show 2-4% improvements over standard multi-task baselines, particularly on low-resource tasks. However, we observe that gains diminish with larger models (\u2265 1B parameters) and are sensitive to hyperparameter choices in the memory update frequency. Theoretical analysis reveals our method provides a local Lipschitz guarantee under restricted assumptions that may not hold in practice. While GAM-Surgery offers practical improvements for mid-scale transformer applications, we acknowledge its limitations in extreme multi-task regimes and provide extensive ablations to guide future work.",
    "id": 997
  },
  {
    "title": "LoRA-CLIP: Parameter-Efficient Fine-Tuning for Cross-Modal Retrieval at Scale",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Cross-modal retrieval models require expensive fine-tuning on target domains, yet suffer from catastrophic forgetting when adapting to new tasks. We propose LoRA-CLIP, a parameter-efficient adaptation method that injects low-rank adapters into CLIP's vision and text encoders at multiple transformer layers. Our key insight is that cross-modal alignment benefits from learning task-specific interactions while preserving universal representations. We evaluate on 12 downstream retrieval benchmarks spanning e-commerce, medical imaging, and scientific literature. LoRA-CLIP achieves 94.2% of full fine-tuning performance with only 2.3% trainable parameters, requiring 4.7\u00d7 less GPU memory. However, we observe significant performance drops (up to 12%) on out-of-distribution data compared to full fine-tuning. Through ablation studies, we identify that adapter placement in deeper layers substantially impacts retrieval precision, suggesting learned features may overfit to dataset-specific correlations. While our method enables practical deployment of large vision-language models, theoretical analysis remains limited, and hyperparameter sensitivity across domains poses deployment challenges for practitioners.",
    "id": 1002
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Historical Gradient Recombination",
    "authors": [
      "Liu, K.",
      "Rodriguez, M.",
      "Chen, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impair optimization. While existing gradient surgery methods like PCGrad and GradDrop modify gradients at each step, we propose Adaptive Memory Gradient Surgery (AMGS) which leverages historical gradient information to better resolve conflicts. Our key insight is that past gradient directions contain useful information about task relationships that can inform current gradient modification. AMGS maintains an exponentially decaying memory of per-task gradients and uses attention mechanisms to compute task-specific gradient projections that minimize interference. On three standard multi-task vision datasets (CityScapes, NYUv2, and CIFAR-100), AMGS shows consistent improvements over gradient surgery baselines, achieving +1.2% mIoU and +0.8% accuracy on average. However, we find these gains diminish in low-data regimes and when task count exceeds 5. While AMGS provides a practical improvement over existing methods with minimal computational overhead (5% training time increase), our theoretical analysis suggests the approach may not guarantee convergence in pathological cases. Code and pretrained models are available at [repository].",
    "id": 1003
  },
  {
    "title": "Gradient Descent with Memory-Augmented Step Sizes: A Lightweight Approach to Adaptive Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose MANGO (Memory-Augmented Neurally Guided Optimization), a lightweight optimizer that augments standard gradient descent with a small recurrent network trained to predict step sizes based on historical gradient patterns. Unlike expensive learned optimizers, MANGO uses only 32 hidden units and operates online without meta-training on task distributions. Our key insight is that recent gradient history contains sufficient signal to predict reasonable step sizes for many practical problems, suggesting that complex learned optimizers may be overparameterized. We evaluate MANGO on image classification benchmarks and language modeling tasks, where it achieves competitive performance to Adam/AdamW while using 10-50x fewer parameters than previous learned optimizers. However, MANGO shows inconsistent gains on transformer architectures and fails to outperform SGD+momentum on some ResNet experiments. Analysis reveals the learned step size policy primarily exploits second-order structure that could be captured more efficiently by quasi-Newton methods. While our approach provides a middle ground between hand-designed and fully learned optimizers, its benefits appear constrained to moderate-scale vision tasks. Code and experiments are available at [URL].",
    "id": 1005
  },
  {
    "title": "Gradient Perturbation Scheduling for Improved Federated Learning Convergence",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "Federated learning faces challenges from device heterogeneity and communication constraints, often requiring methods like gradient compression or selective participation that inject noise into the optimization process. While theoretical work has analyzed convergence properties under various noise models, practical implementations typically use fixed noise schedules that may not adapt to local training dynamics. We propose a simple heuristic scheduling approach that adjusts the magnitude of gradient perturbations based on local gradient statistics observed during training. Our method, inspired by adaptive optimizers like Adam, scales perturbation variance inversely with the exponential moving average of gradient norms. Unlike prior theoretical analyses requiring knowledge of problem parameters, our approach requires only tuning a single hyperparameter. On standard federated benchmarks with realistic client heterogeneity (MNIST with Dirichlet(0.1) partitioning and CIFAR-10 with device-specific augmentations), our scheduling improves convergence speed by 15-25% over fixed noise baselines while maintaining final accuracy. However, we observe diminishing returns in settings with small client datasets or low communication rounds. Our analysis reveals that improvements primarily occur when local data distributions exhibit moderate similarity between clients. While the method provides practical benefits for federated deployment, theoretical convergence guarantees remain limited to simplified convex settings, and understanding of when adaptive scheduling outperforms fixed approaches in non-convex optimization remains incomplete.",
    "id": 1020
  },
  {
    "title": "MagNet: Margin-based Gradient Normalization for Improved Training Stability in Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Training instability remains a pervasive challenge in transformer architectures, particularly when scaling to deeper networks or smaller datasets. While gradient clipping and adaptive optimizers offer partial solutions, they often require careful hyperparameter tuning. We propose MagNet, a simple modification to existing optimizers that renormalizes gradients based on learned margin boundaries. Our approach maintains directional information while adaptively controlling gradient magnitudes through a lightweight auxiliary network. We demonstrate empirically that MagNet improves training stability across 6 transformer variants on standard benchmarks, reducing gradient norm variance by 42% on average. However, we find that performance gains saturate beyond modest model sizes, and computational overhead scales poorly for very large models (\u2265 10B parameters). Additionally, our theoretical analysis reveals that MagNet's stability guarantees break down under certain adversarial perturbation settings. While promising for medium-scale applications, these limitations suggest the need for more sophisticated approaches at extreme scales. Our code is available at [anonymized for review].",
    "id": 1041
  },
  {
    "title": "Meta-Learning with Learned Loss Functions: A Unified Framework via Implicit Differentiation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose a meta-learning framework that jointly learns task-specific parameters and the loss function itself, unifying recent approaches that use learned metrics or regularizers. Our method treats the loss function as a parameterized neural network whose gradients are computed via implicit differentiation through the inner optimization loop. We prove convergence guarantees under Lipschitz smoothness assumptions, extending classical meta-learning theory. On few-shot classification benchmarks, our approach achieves 2-3% improvements over MAML and prototypical networks, though gains diminish with larger meta-training sets. Experiments on text classification and molecular property prediction show consistent but modest benefits. While our framework is general, the learned loss functions often lack interpretability and can exhibit unstable training dynamics. We provide a PyTorch implementation and ablations showing the importance of loss architecture choices.",
    "id": 1048
  },
  {
    "title": "Gradient Orthogonalization for Better Transfer Learning in Neural Networks",
    "authors": [
      "Chen, L.",
      "Ramos, J.",
      "Singh, P."
    ],
    "abstract": "Fine-tuning pre-trained models often suffers from catastrophic interference when adapting to new tasks, particularly in low-data regimes. We propose Gradient Orthogonalization during Transfer (GOT), a simple regularization technique that encourages gradient directions from new tasks to remain orthogonal to directions important for the original pre-training task. GOT adds minimal computational overhead by projecting gradients during optimization without requiring additional forward passes. Our method achieves 2-3% improvement over standard fine-tuning on 6 out of 10 benchmark datasets, particularly excelling in few-shot image classification. However, improvements diminish when using larger fine-tuning datasets or when tasks are highly dissimilar from pre-training. We provide theoretical analysis showing GOT corresponds to bounding the minimum eigenvalue of the joint Hessian, though our bounds are loose for deeper networks. Experiments on BERT and ResNet architectures demonstrate our method is architecture-agnostic, but gains for language tasks are modest. Code and experiments are reproducible with our PyTorch implementation. While GOT shows promise for mitigating interference, more sophisticated analysis is needed to understand when orthogonal gradients are truly optimal versus simply trading off between tasks.",
    "id": 1051
  },
  {
    "title": "ReLoRA: Revisiting Low-Rank Adaptation with Iterative Re-initialization",
    "authors": [
      "Thompson, L.",
      "Kim, J.",
      "Anderson, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a standard approach for efficiently fine-tuning large language models, but its effectiveness diminishes as the rank increases. We observe that while LoRA initially captures dominant features, it suffers from subspace saturation that limits expressivity. To address this, we propose ReLoRA, a simple iterative re-initialization scheme that periodically resets the low-rank matrices while preserving learned representations through a knowledge distillation objective. Our method involves training for K steps, then computing a warm restart using the current adapted weights as soft targets, allowing the low-rank decomposition to explore new subspaces. Across 4 language understanding benchmarks and 2 model scales, ReLoRA achieves a modest 2.4% average improvement over standard LoRA at rank 16-64, with diminishing returns beyond rank 128. Surprisingly, we find that random re-initialization without distillation performs comparably on 3/4 tasks, suggesting the benefit may stem primarily from noise injection. While our results indicate potential for extending LoRA's capacity, we acknowledge the computational overhead is doubled and theoretical understanding remains limited. The method may serve as a practical baseline for future work on adaptive rank selection.",
    "id": 1052
  },
  {
    "title": "Revisiting Weight Averaging for Improving Generalization in Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, T."
    ],
    "abstract": "Weight averaging techniques like stochastic weight averaging (SWA) have shown promise for improving generalization in deep learning, but their effectiveness varies significantly across architectures and datasets. We conduct a systematic empirical study of weight averaging variants and propose simple modifications to improve robustness. Our key insight is that the alignment between weight trajectories in parameter space determines averaging effectiveness, which we quantify using a novel distance metric based on layer-wise cosine similarity. We introduce Cyclic SWA (CSWA), which adjusts averaging frequency based on trajectory alignment, achieving 1-2% accuracy improvements over standard SWA on CIFAR-10/100 and ImageNet under consistent hyperparameters. However, we find these gains diminish on larger architectures like ViT-B/16 and ResNet-50 with advanced augmentations. Our analysis reveals that weight averaging primarily helps when the base optimizer's trajectory exhibits low-frequency oscillations, suggesting its benefits may be redundant with well-tuned learning rate schedules. While our method provides reliable improvements for medium-scale vision tasks, the computational overhead and memory requirements make it impractical for modern training pipelines. We release our implementation and hyperparameters for reproducibility, but note that gains are modest and task-specific.",
    "id": 1061
  },
  {
    "title": "Improving Transformer Training Stability Through Layer-Wise Learning Rate Warmup",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "M\u00fcller, S."
    ],
    "abstract": "We propose a simple modification to transformer training that improves optimization stability without architectural changes. Our method applies layer-specific learning rates that warm up at different rates, with lower layers warming up faster than upper layers. This approach is motivated by observations that gradient norms vary substantially across transformer layers, particularly during early training. We evaluate our method on Wikitext-103 language modeling and GLUE fine-tuning tasks, showing modest improvements in perplexity (0.5-1.2% relative) and downstream accuracy (0.3-0.8% absolute) over standard warmup procedures. While the improvements are incremental rather than transformative, our method reduces training instability observed in 15% of random seeds across experimental settings, suggesting practical benefits for reproducibility. The approach adds minimal computational overhead and can be integrated into existing training pipelines with <10 lines of code. However, we find limited benefits on larger-scale experiments (e.g., GPT-2 medium), raising questions about the method's scaling properties. Our contributions are primarily empirical rather than theoretical, and while the method shows promise for small-to-medium scale applications, further analysis is needed to understand the mechanism of action. Code is available at [anonymous link].",
    "id": 1063
  },
  {
    "title": "Gradient Norm Aware Optimization: A Simple Scaling Method for Improving Training Stability",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Gradient Norm Aware Optimization (GNAO), a lightweight modification to standard optimizers that adaptively scales update magnitudes based on local gradient statistics. While adaptive optimizers like Adam and RMSProp normalize updates using second moment estimates, we show that tracking gradient norms across mini-batches provides a computationally cheaper alternative that stabilizes training. Our method adds minimal computational overhead (\u22482% increase in wall-clock time) and requires no additional hyperparameters beyond the base optimizer. Through experiments on CIFAR-10/100 and smaller Transformer models, GNAO demonstrates slight improvements in convergence speed and final accuracy compared to vanilla SGD/Adam baselines. Theoretical analysis suggests GNAO can be viewed as a diagonal Hessian approximation under mild assumptions. However, we find the practical benefits diminish on larger models (>100M parameters), possibly due to gradient norm homogenization in later training stages. While our results are positive, we acknowledge the improvements are incremental and specific to certain training regimes. The method may be most useful for practitioners facing stability issues with small-to-medium sized models where additional computational cost is prohibitive.",
    "id": 1066
  },
  {
    "title": "Improving Adversarial Robustness Through Layer-Wise Gradient Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Jones, M."
    ],
    "abstract": "While adversarial training remains the dominant approach for improving neural network robustness, we propose an alternative that regularizes gradients at each layer during standard training. Our method, Layer-Wise Gradient Regularization (LWGR), penalizes the L2 norm of gradients with respect to intermediate activations, encouraging smoother feature representations. Unlike adversarial training, LWGR requires no additional forward passes or perturbation generation. We evaluate LWGR on CIFAR-10 and ImageNet, achieving 42.1% and 38.7% robust accuracy under \u2113\u221e attacks (\u03b5=8/255) respectively with ResNet-50 models. While these results fall short of state-of-the-art adversarial training baselines (51.2% and 44.8%), LWGR demonstrates 2.3\u00d7 faster training and enables better clean accuracy (+4.2% on CIFAR-10). Our analysis reveals LWGR primarily affects early layers, suggesting a complementary role to adversarial training rather than a replacement. Code and pre-trained models are available at anonymous-link.github.io/lwgr.",
    "id": 1091
  },
  {
    "title": "LoFiSGD: Memory-Efficient Low-Fidelity Gradient Compression for Large-Scale Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We present LoFiSGD, a gradient compression scheme that aggressively quantizes neural network gradients to 1-2 bits during distributed training while maintaining convergence properties. Our method builds upon existing quantization techniques but introduces a novel \"fidelity-aware\" update rule that adaptively adjusts compression levels based on gradient norms. We demonstrate LoFiSGD on ResNet-50 and Transformer architectures, achieving up to 8.5\u00d7 communication reduction compared to full-precision training with <1.5% accuracy degradation on CIFAR-10 and WMT'14. Theoretical analysis shows convergence under standard assumptions for non-convex objectives. While our compression ratios match state-of-the-art methods, we acknowledge that the observed training instabilities on larger models (e.g., BERT-Large) and requirement for careful hyperparameter tuning may limit practical adoption. Our PyTorch implementation is provided for reproducibility.",
    "id": 1093
  },
  {
    "title": "Gradient Surgery with Topological Constraints: Mitigating Catastrophic Forgetting via Persistent Homology",
    "authors": [
      "Liu, H.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Catastrophic forgetting remains a fundamental challenge in continual learning, where sequentially trained neural networks lose performance on previous tasks. While existing regularization-based approaches provide some protection, they often rely on heuristic similarity measures between tasks. We propose TopoGrad, a novel regularization framework that leverages persistent homology to identify critical gradient directions for preserving task-specific topological features. Our method computes persistent barcodes across intermediate network representations, then constructs gradient projections that minimally interfere with these topological invariants. On the Split-CIFAR-100 benchmark, TopoGrad achieves 4.2% absolute improvement over standard fine-tuning, comparable to recent regularization methods while adding minimal computational overhead. However, we observe performance degradation on more complex datasets (Split-TinyImageNet), where topological features may be less informative. Our ablation study reveals that shorter barcode persistence thresholds (\u03b5 < 0.1) lead to over-regularization, whereas longer thresholds provide insufficient forgetting protection. While TopoGrad offers a theoretically principled approach with interpretable regularization via topological summaries, computational scaling to ImageNet-scale architectures remains challenging. Our code is available at [redacted-for-anonymity].",
    "id": 1108
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Catastrophic Forgetting in Multi-Task Learning through Selective Gradient Modification",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Garcia, M."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to performance degradation and catastrophic forgetting. We propose Gradient Surgery with Memory (GSM), a novel optimization approach that maintains a sparse memory of past task gradients to guide the current optimization direction. GSM identifies gradient conflicts through an efficient dot-product analysis and selectively modifies gradients using a learned projection matrix, while preserving a limited-size memory of historical gradients to inform future updates. Our method is simple to implement, requiring only minor modifications to standard optimizers, and introduces minimal computational overhead (under 5% increase in wall-clock time). Experiments on standard multi-task benchmarks including Split-CIFAR100 and NYUv2 semantic segmentation show consistent but modest improvements over baselines, with average task performance gains of 2.1-3.4%. However, we observe performance degradation in certain task combinations, particularly when tasks have vastly different gradient scales. While GSM provides a practical approach to reducing forgetting, our theoretical analysis reveals fundamental limitations in worst-case scenarios. Our PyTorch implementation is available at [url].",
    "id": 1111
  },
  {
    "title": "LoRA-Prune: Structured Sparsity for Parameter-Efficient Fine-Tuning via Importance-Aware Channel Dropout",
    "authors": [
      "Chen, L.",
      "Mukherjee, S.",
      "Johnson, K."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular method for parameter-efficient fine-tuning of large language models, but its memory footprint remains a bottleneck in resource-constrained settings. We propose LoRA-Prune, a simple yet effective approach to reduce the effective parameter count of LoRA adapters through structured sparsity. Our method introduces importance-aware channel dropout, where we rank LoRA weight matrices by their contribution to the training loss and systematically drop low-importance channels during fine-tuning. This is achieved through an efficient approximation of the Hessian trace that requires minimal overhead. Experiments on GPT-2, LLaMA-7B, and T5-base show that LoRA-Prune can reduce LoRA parameters by 30-50% with minimal performance degradation (<2% drop in task accuracy) across standard NLP benchmarks. While our method achieves competitive compression ratios, we observe significant variance across tasks and architectures, suggesting that the effectiveness of structured sparsity in LoRA adapters may be more limited than previously thought. Code and models will be released upon acceptance.",
    "id": 1121
  },
  {
    "title": "Momentum-Based Gradient Descent with Adaptive Restart for Non-Convex Optimization: Theory and Practice",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose MARS, a momentum-based optimizer that automatically detects and counters the oscillatory behavior of gradient descent in non-convex landscapes. Our method combines Nesterov momentum with a novel restart criterion based on the angle between consecutive update directions. While momentum methods often overshoot in sharp curvatures, MARS periodically resets the velocity when the optimization trajectory exhibits abnormal curvature patterns. We prove that MARS achieves O(1/\u221aT) convergence for non-convex smooth functions, matching standard SGD rates but with improved empirical performance. On ImageNet training, MARS shows 1.3% better final accuracy compared to SGD+momentum, though experiments across 5 additional datasets reveal inconsistent gains. We also test on Transformer language models, finding 8% faster convergence in early training but eventual performance matching baselines. Theoretically, we characterize conditions under which restarts provably help, but our analysis relies on the restrictive assumption of Lipschitz continuous Hessians. While MARS demonstrates practical improvements in several settings, its benefits appear task-specific and the theoretical contributions are incremental over existing momentum analyses. Code is available at anonymous-url.",
    "id": 1122
  },
  {
    "title": "Gradient Echo: Enhancing Adversarial Training through Periodic Weight Perturbation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Adversarial training remains the most effective defense against adversarial attacks, yet it suffers from robust overfitting and significant computational overhead. We propose Gradient Echo (GE), a simple method that periodically injects noise sampled from historical gradient directions to improve adversarial robustness without increasing training time. During standard adversarial training, we store a subset of past gradients and use them as perturbation directions in subsequent epochs. Our theoretical analysis shows that under mild assumptions, GE regularizes the loss landscape by preventing convergence to sharp minima. Experimental evaluation on CIFAR-10 and CIFAR-100 demonstrates that GE improves robust accuracy by 2.3-3.8% over baseline adversarial training while maintaining comparable standard accuracy. However, we observe diminishing benefits on larger datasets like ImageNet. We provide extensive ablations showing GE's sensitivity to gradient history size and noise magnitude. While our method offers practical improvements for small to medium-scale applications, theoretical gaps remain in explaining its behavior under non-convex settings. Code will be released upon acceptance.",
    "id": 1123
  },
  {
    "title": "Gradient Surgery with Adaptive Momentum: A Hybrid Approach to Multi-Task Optimization in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that can hinder convergence and degrade performance. While recent gradient surgery methods like PCGrad effectively modify conflicting gradients, they treat all task gradients equally regardless of their reliability or scale. We propose AdaMoGS, a simple yet effective method that combines adaptive momentum with gradient surgery to prioritize more reliable gradient directions. Our approach maintains separate momentum buffers for each task and uses their consistency over time to weight gradient contributions during surgery steps. On three standard multi-task benchmarks (CelebA, NYU-v2, and QM9), AdaMoGS achieves modest improvements over PCGrad (0.8-2.3% average relative gain) while reducing training instability. However, we find that benefits diminish when tasks have highly correlated gradients or when architecture-specific techniques like task-specific heads are employed. Our theoretical analysis shows that AdaMoGS can be viewed as a form of preconditioned gradient descent with task-specific learning rates, though we only provide convergence guarantees for a simplified two-task case. While the improvements are consistent, they remain incremental compared to stronger baselines that incorporate architectural inductive biases. Code is available at anonymous-url.",
    "id": 1133
  },
  {
    "title": "LoRA-E: Efficient Low-Rank Adaptation with Entropy-Based Rank Selection",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become standard for adapting large language models, but the selection of rank hyperparameters remains largely heuristic. We propose LoRA-E, a simple extension that automatically selects rank using an entropy-based criterion computed during a single forward pass of the target data. Our method computes the entropy of activation patterns in each layer and sets the rank proportionally to this entropy, eliminating the need for manual tuning or costly validation runs. While this approach lacks theoretical guarantees, we empirically demonstrate consistent improvements over fixed-rank LoRA on GLUE and SuperGLUE benchmarks, achieving average gains of 1.3 points across tasks with 15% fewer parameters. However, results are mixed on domain-specific datasets where the entropy-adaptive approach occasionally underperforms tuned baselines. Ablation studies reveal the method is particularly sensitive to batch size choices and may struggle with highly imbalanced datasets. Though the computational overhead is minimal (\u22645% increase in training time), the gains over LoRA with carefully tuned ranks are modest. Our code is available at https://anonymous-url.github.io/lora-e.",
    "id": 1142
  },
  {
    "title": "LoRA-DROP: Adaptive Low-Rank Adaptation via Dynamic Parameter Pruning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have achieved widespread adoption for adapting large language models, but their fixed-rank structure limits adaptiveness to downstream tasks. We propose LoRA-DROP, a simple yet effective extension that dynamically prunes LoRA parameters during training based on gradient sensitivity analysis. Our method uses a computationally lightweight approximation of the Fisher information matrix to identify and remove redundant low-rank components, reducing parameters by 30-50% while maintaining downstream task performance. Experiments on GLUE, SuperGLUE, and three vision-language tasks show comparable results to standard LoRA (within 1.2% of original performance), with slight improvements on smaller datasets. However, our analysis reveals the method's effectiveness correlates strongly with the intrinsic dimensionality of downstream tasks\u2014showing degradation when task complexity exceeds a threshold. While LoRA-DROP offers practical memory savings and provides insights into adaptation dynamics, we acknowledge limitations in theoretical guarantees and computational overhead during the pruning phase. Code and pre-trained adapters will be released upon acceptance.",
    "id": 1148
  },
  {
    "title": "Learning to Warm-Start: A Transformer-Based Approach for Adaptive Gradient Descent Initialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Warmformer, a transformer-based architecture that predicts initial parameters for gradient-based optimization algorithms based on problem metadata and historical optimization trajectories. Our method is motivated by the observation that similar optimization problems often share structural properties that can be leveraged for faster convergence. The key innovation is a cross-attention mechanism that conditions on problem features (dimension, condition number, sparsity patterns) and previous optimization paths to generate parameter initializations. We evaluate Warmformer on convex quadratic programming and neural network training tasks, achieving 15-30% faster convergence compared to standard initialization methods when evaluated on problems similar to the training distribution. However, performance degrades significantly (sometimes worse than baselines) when tested on out-of-distribution problem instances. While our approach demonstrates the viability of learned initialization strategies, the limited generalization beyond training distributions and computational overhead of the transformer model raise questions about practical applicability. Our code is available at [url].",
    "id": 1153
  },
  {
    "title": "Scheduled Dropout: A Simple Curriculum-Based Regularization Strategy for Training Neural Networks",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Thomas, C.",
      "Zhao, L."
    ],
    "abstract": "We propose Scheduled Dropout, a straightforward modification to standard dropout training that gradually reduces dropout rates during training according to a predefined curriculum. Unlike traditional dropout with fixed rates, our method starts with high dropout (e.g., 0.5-0.7) and linearly anneals to lower values (0.1-0.2) over training epochs. This approach is motivated by the observation that early training phases benefit from stronger regularization while later phases require less noise to fine-tune learned representations. Our theoretical analysis shows that scheduled dropout reduces to a modified L2 regularization scheme with time-varying coefficients, providing some justification for its effectiveness. We evaluate on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures, achieving modest improvements of 0.5-1.2% accuracy over standard dropout baselines, with particularly strong gains on smaller datasets. While the improvements are consistent, they remain incremental and the method requires careful tuning of the annealing schedule. We provide extensive ablations on the schedule design and demonstrate that simple linear schedules perform comparably to more complex cosine or exponential schedules. The method is easy to implement and adds minimal computational overhead, making it practical for broad adoption despite its limited novelty.",
    "id": 1154
  },
  {
    "title": "Gradient Descent with Non-Monotonic Adaptive Step Sizes: A Quasi-Momentum Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "We propose a variant of gradient descent that uses non-monotonic step size schedules guided by a quasi-momentum mechanism that combines per-coordinate step sizes with normalization by recent gradient norms. The method aims to accelerate convergence while maintaining stability, particularly for ill-conditioned problems. Our approach introduces a memory-based adaptation rule that increases step sizes when gradients are consistently aligned across iterations, and decreases them when alignment drops. We provide theoretical analysis showing convergence for convex functions with a sublinear rate matching standard gradient descent, and empirical results on standard benchmarks showing 5-15% improvement in convergence speed over Adam on ResNet training for CIFAR-100, though gains diminish on larger architectures. While our convergence guarantees do not improve upon existing bounds, the simplicity of implementation and modest empirical benefits may be of practical interest for small-to-medium scale applications. The method requires an additional hyperparameter compared to standard optimizers, and sensitivity analysis suggests performance is somewhat brittle to this setting.",
    "id": 1155
  },
  {
    "title": "Gradient Confusion: A Simple Regularization Technique for Improving Transformer Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Confusion, a lightweight regularization technique that adds controlled noise to gradient directions during transformer training to prevent overfitting. Our method stems from the observation that transformers exhibit high gradient coherence in later training stages, potentially limiting exploration of the loss landscape. By injecting calibrated directional noise into gradients based on their angular similarity to previous updates, we encourage more diverse parameter updates while maintaining convergence. We evaluate Gradient Confusion on standard NLP benchmarks including GLUE, SuperGLUE, and WikiText-103 across various model sizes (125M-7B parameters). Results show consistent but modest improvements: 1.2-2.3% accuracy gains on GLUE tasks and 0.8-1.5 perplexity improvements on language modeling, with minimal computational overhead (<3% additional training time). However, performance gains diminish on larger models (\u22653B parameters), and our theoretical analysis reveals the regularization effect is bounded regardless of noise magnitude. While Gradient Confusion provides a simple implementation requiring only three additional lines of code, its benefits appear task-dependent and may not justify the added complexity for practitioners. Code is available at anonymized-url.",
    "id": 1161
  },
  {
    "title": "Improving Transformer Generalization Through Layer-wise Learning Rate Temperature",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "Transformer models exhibit systematic brittleness when fine-tuned on small datasets, often overfitting to spurious correlations in early layers while under-utilizing deeper representations. We propose Layer-wise Learning Rate Temperature (L2RT), a simple modification to Adam optimizer that applies temperature-controlled learning rates to individual transformer layers based on their proximity to output. Our method uses a learnable temperature parameter \u03c4 to drive higher learning rates in deeper layers, theoretically motivated by the observation that gradient norms decay exponentially with depth. We demonstrate improvements over standard Adam on 6 out of 10 GLUE tasks when fine-tuning BERT-base from limited training data (1k-5k examples), achieving average gains of 2.3% over baseline. However, results show diminishing returns with larger datasets, and we observe negative transfer on tasks requiring broad attention patterns (e.g., WNLI). Computational overhead is minimal (2% increase in training time), and our implementation requires only 3 lines of code change to existing optimizers. While L2RT provides consistent benefits in low-data regimes, we acknowledge the technique's limited applicability to full-dataset fine-tuning and leave investigation of deeper theoretical connections to future work.",
    "id": 1162
  },
  {
    "title": "Revisiting Knowledge Distillation with Information-Theoretic Routing for Efficient Model Compression",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Knowledge distillation has become a standard approach for compressing large neural networks, yet most methods treat all samples equally during training. We propose Information-Theoretic Routing Distillation (ITRD), which uses an auxiliary network to dynamically route samples based on their estimated information gain. Our method computes sample-wise mutual information between teacher and student predictions, prioritizing high-uncertainty examples during distillation. On CIFAR-100 and ImageNet, ITRD achieves competitive compression ratios (10-50\u00d7) with modest accuracy improvements over baseline KD (0.5-1.2% absolute). However, computational overhead during training increases by 30-40%, and performance gains diminish when teacher-student capacity gaps exceed two orders of magnitude. While our theoretical analysis provides novel insights into sample selection for distillation, we recognize the approach adds complexity without addressing fundamental limitations of knowledge transfer. Experiments across multiple architectures (CNNs, Transformers) show consistent but incremental improvements. Code and checkpoints are available at [anonymized-url].",
    "id": 1177
  },
  {
    "title": "Gradient Noise Re-Scaling: A Lightweight Training Strategy for Improving Generalization in Deep Networks",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Gradient Noise Re-Scaling (GNR), a simple modification to standard SGD that selectively amplifies gradient noise during training. While previous work has shown that carefully injected noise can improve generalization, these methods often require costly hyperparameter tuning or complex noise schedules. GNR instead re-scales the gradient noise using a running estimate of the gradient variance, requiring only an additional hyperparameter \u03b1 that typically falls between 0.1-0.5. Our theoretical analysis shows that GNR approximately corresponds to an implicit regularizer encouraging flatter minima under certain assumptions about the loss landscape. We evaluate GNR on CIFAR-10/100 and ImageNet classification tasks using ResNet-18, ResNet-50, and Vision Transformers. Results show consistent but modest improvements (0.3-0.7% accuracy gains) over vanilla SGD with momentum, matching or slightly outperforming more sophisticated regularization techniques like SAM and SharpDrop. Ablations reveal that benefits diminish with stronger baseline regularization (\u22640.2% gains with label smoothing + RandAugment). While the improvements are reliable across architectures, they remain incremental and typically require 2-3x longer training. Code is available at anonymous-url.",
    "id": 1182
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Cheap Approximation that Usually Works",
    "authors": [
      "Chen, L.",
      "Ramanathan, V.",
      "Kumar, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradDrop show promise, they require expensive gradient computations or complex projection steps. We propose ApproxGrad, a simple heuristic that applies random binary masks to task gradients based on their magnitudes, avoiding costly cosine similarity calculations. Our method achieves comparable accuracy to PCGrad on standard benchmarks (CIFAR-100, NYUv2) while reducing computational overhead by 40-60%. However, we observe significant performance degradation when task gradients are highly correlated (r > 0.8), and our theoretical analysis reveals no convergence guarantees under non-convex objectives. Experiments on three additional datasets show mixed results: ApproxGrad outperforms standard multi-task training in 7/9 cases but underperforms PCGrad in 5/9 cases. Code is available at github.com/chenl/approxgrad.",
    "id": 1183
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simple Trick with Provable Guarantees",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients where optimizing for one task hurts performance on others. We propose Gradient Surgery (GS), a lightweight method that projects conflicting gradients onto each other before applying updates. While similar in spirit to existing approaches like PCGrad and GradNorm, GS requires no hyperparameter tuning and comes with convergence guarantees under standard smoothness assumptions. Our theoretical analysis shows GS converges to an \u03f5-stationary point in O(1/\u03f5\u00b2) iterations for L-smooth objectives, matching standard SGD rates despite the projection step. Empirically, we evaluate GS on eight multi-task benchmarks spanning computer vision and NLP. GS achieves modest improvements over baselines (+0.8% average accuracy, +1.2% F1), though results vary significantly across tasks (ranging from -2.1% to +3.4%). Ablation studies reveal that performance gains primarily emerge when task gradients exhibit high cosine similarity (>0.3), limiting applicability. While our convergence analysis is novel and the method is implementable in 8 lines of PyTorch, the practical benefits appear scenario-dependent. This work provides theoretical backing for a simple heuristic, though the empirical impact may not justify architectural changes in production systems.",
    "id": 1195
  },
  {
    "title": "Sharpening Noisy Labels via Cross-Modal Consistency: A Simple Approach for Semi-Supervised Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Semi-supervised learning with noisy labels remains a pervasive challenge in practical applications where large-scale datasets inevitably contain annotation errors. We propose Cross-Modal Consistency Filtering (CMCF), a straightforward method that leverages the natural robustness of multimodal representations to identify and correct label noise. Our approach trains separate encoders on different input modalities (e.g., images and text in vision-language data) and uses their disagreement to detect potentially mislabeled examples, followed by pseudo-label correction using the more confident modality's predictions. While conceptually simple, CMCF achieves competitive performance on standard benchmarks, improving over baseline noisy-label methods by 2-3% accuracy on CIFAR-100N and WebVision. However, we observe that gains diminish with lower noise rates, and the approach requires modalities with complementary information\u2014limiting its applicability. Our theoretical analysis provides mild convergence guarantees under restrictive assumptions that may not hold in practice. Code and pre-trained models are available, though reproduction requires significant computational resources due to the multimodal architecture.",
    "id": 1202
  },
  {
    "title": "Curvature-Aware Gradient Clipping for Noisy Optimization Landscapes",
    "authors": [
      "Liu, K.",
      "Johnson, M.",
      "Chen, S.",
      "Rodriguez, A."
    ],
    "abstract": "We present Curvature-Aware Gradient Clipping (CAGC), a gradient modification scheme that adapts clipping thresholds based on local Hessian information during neural network training. While standard gradient clipping improves robustness to heavy-tailed noise, it treats all parameters uniformly, potentially harming convergence in benign regions. CAGC estimates curvature along gradient directions using efficient Hessian-vector products and clips more aggressively in regions with negative curvature or high noise variance. We evaluate CAGC on language modeling tasks with varying levels of gradient noise, demonstrating modest improvements in convergence speed (5-12% wall clock time reduction) over standard clipping baselines. However, benefits diminish on well-conditioned problems or with small batch sizes. Theoretical analysis reveals that CAGC preserves convergence guarantees for convex objectives but may introduce bias in non-convex settings. Our implementation requires minimal overhead but introduces two additional hyperparameters. While CAGC does not outperform state-of-the-art optimizers like AdamW on standard benchmarks, it provides a lightweight alternative when computational constraints limit optimizer choice. Code and pre-trained models are available at [anonymized].",
    "id": 1204
  },
  {
    "title": "Variance-Aware Gradient Descent with Random Reshuffling: Improved Convergence Without Learning Rate Tuning",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "Rossi, M."
    ],
    "abstract": "We propose VR-SGD-RR, a variance-reduced stochastic gradient method that automatically adapts learning rates based on gradient statistics collected during random reshuffling (RR) epochs. While RR often outperforms standard SGD in practice, its theoretical analysis remains notoriously difficult due to data-dependent sampling without replacement. Our key insight is to exploit the finite-sum structure of empirical risk minimization to construct adaptive step sizes that depend on per-sample gradient norms estimated from previous epochs. This yields O(1/T) convergence for convex problems and improves dependence on condition number \u03ba from \u03ba\u00b2 to \u03ba^{3/2} compared to vanilla SGD-RR. On CIFAR-10 and ImageNet, VR-SGD-RR achieves similar final accuracy to SGD+Momentum while eliminating manual learning rate tuning for 80% of hyperparameter configurations. However, our method introduces O(d) additional memory overhead per sample and shows diminishing returns on problems with large batch sizes. Theoretically, our analysis relies on an untested assumption about gradient variance decay that remains unproven for general non-convex settings. Experiments on synthetic ill-conditioned problems validate our theoretical rates, but real-world gains are modest except when initial learning rates are far from optimal.",
    "id": 1205
  },
  {
    "title": "Improved Generalization Bounds for Meta-Learning via Task-Similarity Clustering",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a novel framework for improving generalization bounds in gradient-based meta-learning by exploiting task clustering structures. While existing bounds depend on worst-case task similarity, we show that tighter bounds can be obtained when tasks form clusters with bounded intra-cluster diameter. Our approach first clusters tasks based on their optimal parameter distances using a theoretically-justified metric, then employs cluster-specific meta-parameters that are regularized using cluster-aware complexity terms. We derive PAC-Bayesian generalization bounds that scale with the effective number of clusters rather than the total number of tasks. Experiments on few-shot image classification benchmarks demonstrate improved bounds relative to prior work, with empirical improvements of 3-5% in accuracy on tieredImageNet and CIFAR-FS. However, the practical gains diminish as the number of training tasks increases, and our clustering procedure adds non-trivial computational overhead. While our theoretical contribution provides new insights into task similarity in meta-learning, the empirical benefits are modest compared to standard regularization techniques.",
    "id": 1210
  },
  {
    "title": "Momentum-Scheduled Warmup: Balancing Optimization Stability and Convergence in Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose a simple modification to standard transformer optimization that schedules the momentum parameter during warmup phases. Motivated by observations that high momentum can destabilize early training while low momentum slows convergence later, we introduce a linear momentum warmup schedule that transitions from \u03b2=0.0 to \u03b2=0.9 over the first 1000 steps. This approach requires only two additional hyper-parameters and can be implemented in 5 lines of code. Experiments on IWSLT14 De-En, WMT16 En-De, and GLUE benchmark tasks show modest improvements: 0.3-0.7 BLEU score gains and 0.5-1.2% accuracy improvements over standard AdamW baselines, with particularly consistent benefits on smaller datasets (<10M parameters). Ablations suggest the benefit primarily comes from improved early optimization stability rather than final convergence quality. While our method is straightforward to implement and provides reliable if incremental gains, we acknowledge the improvements remain within standard error margins for most tasks. We provide PyTorch code and hyper-parameter sweep results to facilitate reproduction.",
    "id": 1212
  },
  {
    "title": "Self-Supervised Gradient Compression: Reducing Communication Overhead in Federated Learning via Autoencoder-Based Gradient Encoding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Federated learning faces critical scalability challenges due to high communication costs when exchanging gradient updates between clients and servers. We propose Self-Supervised Gradient Compression (SSGC), a novel approach that learns to compress gradients without requiring external labels or assumptions about their distribution. Our method trains an autoencoder architecture to directly encode gradient tensors into low-dimensional representations, with the reconstruction loss adapted to preserve the direction (rather than magnitude) of the original updates. We evaluate SSGC on standard federated benchmarks including CIFAR-10, CIFAR-100, and FEMNIST across varying client participation rates. Experiments show 8-16x compression ratios while maintaining accuracy within 2% of uncompressed baselines in most settings. However, we observe substantial degradation (>5% accuracy drop) on some non-IID data distributions, particularly when client datasets are highly skewed. While our theoretical analysis proves convergence under idealized conditions, we acknowledge limitations in handling heterogeneity across clients. SSGC offers a communication-efficient alternative to existing gradient compression techniques like Top-k and quantization, though further investigation is needed to improve robustness under non-convex objectives.",
    "id": 1228
  },
  {
    "title": "Adaptive Gradient Descent with Moving Average Second Moments Improves Language Model Fine-tuning",
    "authors": [
      "Chen, L.",
      "Kumar, V.",
      "Johnson, S."
    ],
    "abstract": "We propose AdaMAM (Adaptive Momentum with Average Moments), a simple modification to AdamW that replaces the exponential moving average of squared gradients with a windowed moving average. This change addresses the observation that Adam's aggressive early updates can harm the stability of transformer fine-tuning on small datasets. Our method uses a fixed-size sliding window over past gradients, making it less sensitive to the choice of \u03b22 while maintaining comparable convergence rates. We evaluate AdaMAM on various NLP tasks including GLUE, SQuAD, and domain adaptation benchmarks. On 10 out of 15 tasks, AdaMAM matches or slightly improves upon AdamW baselines, with an average improvement of 0.4 F1 points. However, we observe that performance gains are most pronounced on tasks with limited training data (\u226410K examples), suggesting diminishing returns as dataset sizes increase. While our theoretical analysis shows AdaMAM converges under standard smoothness assumptions, we acknowledge that our proof techniques closely follow prior work and do not provide fundamentally new insights. Code is available at anonymous-url.github.io/adamam.",
    "id": 1230
  },
  {
    "title": "Gradient Surgery with Adaptive Momentum for Non-Convex Multi-Task Optimization",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Chen, H."
    ],
    "abstract": "Multi-task learning suffers from conflicting gradients that can hinder optimization. While recent gradient surgery methods like PCGrad resolve conflicts, we observe they introduce unintended bias by uniformly clipping gradients regardless of task uncertainty. We propose Adaptive Gradient Surgery (AGS), which incorporates per-task uncertainty estimates to selectively apply gradient modifications. Our method combines second-order moment estimates with a novel trust-region update that adapts the surgery threshold based on gradient alignment patterns. On standard multi-task benchmarks (Multi-MNIST, CityScapes, NYU-v2), AGS achieves modest improvements over PCGrad (0.5-1.2% average accuracy gains) while reducing training instability. However, we find these gains diminish with larger models, suggesting limited scalability. Theoretical analysis proves convergence under simplified convex assumptions, though the extension to non-convex settings remains an open challenge. While AGS provides a practical improvement to gradient surgery, its computational overhead (15-20% training time increase) and diminishing returns on complex tasks temper its overall impact.",
    "id": 1233
  },
  {
    "title": "Conservative Q-Learning with Adaptive Trust-Region Constraints",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Choi, S."
    ],
    "abstract": "Offline reinforcement learning faces the challenge of value overestimation when the policy deviates significantly from the behavior policy. While existing conservative methods like CQL reduce overestimation by regularizing Q-values, they rely on fixed hyperparameters that may be suboptimal across different datasets. We propose CATR-QL, which introduces an adaptive trust-region mechanism that dynamically adjusts the conservatism level based on the estimated uncertainty of Q-values. Our method computes local Lipschitz constants via gradient analysis to modulate the strength of conservative updates, theoretically ensuring monotonic improvement under relaxed concentrability assumptions. Empirically, we evaluate CATR-QL on 12 continuous control tasks from D4RL, showing consistent but modest improvements over CQL (mean normalized score: 73.2 vs 71.8) with 15% fewer policy updates. However, performance gains are less pronounced on sparse-reward tasks, and the additional computational overhead of adaptive constraint estimation increases training time by 1.4x. Our results suggest that while adaptive conservatism offers benefits, the improvements may not justify the complexity in all scenarios.",
    "id": 1239
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variable Models via Annealed Importance Sampling",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Training models with discrete latent variables remains challenging due to the high variance of gradient estimators. We propose a novel gradient estimation method that combines annealed importance sampling (AIS) with control variates to reduce variance in the REINFORCE estimator. Our approach uses AIS to construct importance weights between the current policy and tempered versions of the target distribution, yielding more stable gradient estimates. We also introduce a learnable baseline computed via neural networks conditioned on the annealing path. Experimental results on binarized MNIST and text generation tasks show 15-30% reduction in gradient variance compared to vanilla REINFORCE, with modest improvements in final log-likelihood. While our method achieves competitive results on small-scale benchmarks, computational overhead scales poorly to larger models due to the sequential nature of AIS. On CIFAR-10 with discrete latent variables, training time increases by 4.5\u00d7 compared to standard baselines. Our method provides a theoretically grounded approach to gradient estimation but may be practical only for moderate-sized models where variance reduction is critical.",
    "id": 1247
  },
  {
    "title": "Accelerated Gradient Descent via Adaptive Learning Rate Scaling with Quadratic Model Approximation",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Rodriguez, C.",
      "Thompson, A."
    ],
    "abstract": "We propose LASQ, a first-order optimization method that adaptively adjusts learning rates using local quadratic approximations without Hessian computations. LASQ maintains exponential moving averages of gradient norms to estimate local curvature, then scales the learning rate inversely proportional to this estimate. Unlike Adam-style methods that use gradient moments, our approach directly models the loss surface curvature through a lightweight quadratic surrogate updated at each step. We prove convergence rates for convex and non-convex objectives under standard assumptions, achieving O(1/T) and O(1/\u221aT) rates respectively. Experiments on CIFAR-10 and ImageNet show LASQ marginally outperforms SGD with hand-tuned schedules and matches AdamW on ResNet-50 (75.2% vs 75.4%) while using 15% fewer iterations. However, LASQ shows mixed results on transformer architectures and tasks with heavy regularization. Our ablations reveal the quadratic approximation degrades on highly non-stationary objectives. Code is available at [anonymous link].",
    "id": 1252
  },
  {
    "title": "Lookahead Normalization: Improving Transformer Training Through Gradient-Conditioned Layer Norm",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose a simple modification to Layer Normalization (LayerNorm) that improves transformer training stability and convergence speed. Our key observation is that the traditional LayerNorm in transformers ignores future gradients during backpropagation, potentially leading to suboptimal normalization parameters. We introduce Lookahead Normalization (LaNorm), which computes LayerNorm parameters by conditioning on an approximation of future gradients using a lightweight auxiliary network. Specifically, LaNorm maintains a learned projection of the current layer's activations that predicts the gradient direction from subsequent layers, adjusting the gain and bias parameters accordingly. We evaluate LaNorm on standard NLP benchmarks (WMT'14 En-De, IWSLT De-En) and vision transformers on ImageNet. Results show modest improvements: 0.3-0.6 BLEU score gains on translation tasks and 0.4-0.8% top-1 accuracy improvements on ImageNet, while slightly reducing training time. However, ablation studies reveal that the benefits diminish with stronger baseline training procedures, and the computational overhead of the auxiliary network (5-8%) may not always justify the improvements. The method appears most effective for deeper networks (>24 layers) where normalization instability is more pronounced.",
    "id": 1254
  },
  {
    "title": "Improving Transformer Efficiency through Selective Attention Sparsification with Learnable Temperature",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, J."
    ],
    "abstract": "Transformer models achieve state-of-the-art results across many domains but suffer from quadratic complexity in sequence length due to their attention mechanisms. While previous work has explored sparse attention patterns to reduce computation, these approaches typically require manual design or heuristic rules that may not adapt well to different tasks. We propose a novel method that learns sparse attention patterns through a differentiable temperature parameter that controls the sparsity of the softmax during training. Our approach automatically identifies which attention heads and positions to keep dense versus sparse during training, achieving up to 2.1x speedup on standard benchmarks with minimal performance degradation (0.3-1.2% accuracy drop on GLUE tasks). We validate our method on both language modeling and vision transformers, showing consistent improvements over fixed sparsity patterns. While our results demonstrate practical benefits for deployment in resource-constrained settings, we observe that the learned sparsity patterns can be unstable across different random seeds and sometimes exhibit peculiar layer-wise sparsity distributions. Our code and pre-trained models are available at [redacted].",
    "id": 1258
  },
  {
    "title": "Gradient Mixup: Improving Model Robustness Through Convex Interpolation of Parameter Updates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Gradient Mixup, a simple regularization technique that interpolates between consecutive gradient updates during training to improve model robustness. Inspired by the success of input mixup for data augmentation, our method computes weighted combinations of past and present gradients, effectively smoothing the optimization trajectory. We prove that under L-smoothness assumptions, Gradient Mixup provides a convergence guarantee of O(1/\u221aT) for non-convex objectives while reducing gradient variance by up to 30%. Experiments on CIFAR-10 and ImageNet show consistent improvements in robustness to label noise (+2.1% accuracy under 20% noise) and adversarial perturbations (+1.3% robust accuracy), with minimal computational overhead. While the theoretical analysis relies on restrictive assumptions that may not hold in practice, and improvements over strong baselines like SAM remain modest (+0.4% average), Gradient Mixup offers a plug-and-play alternative that requires no hyperparameter tuning beyond the standard learning rate. Code is available at anonymous-url.",
    "id": 1260
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Dynamic Low-Rank Approximation with Adaptive Budget Allocation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a standard approach for parameter-efficient fine-tuning of large language models, but its fixed rank allocation across layers may be suboptimal. We propose LoRA-Lite, a method that dynamically adjusts the rank of low-rank matrices during fine-tuning based on layer-wise importance scores derived from gradient covariance. Our approach employs an online learning algorithm that redistributes the parameter budget across layers to maximize task performance while respecting memory constraints. On the GLUE benchmark, LoRA-Lite achieves comparable performance to standard LoRA (average score 83.2 vs 82.7) while using 35% fewer parameters. Experiments on 7B and 13B parameter models show memory reductions of 20-40% with minimal degradation on downstream tasks. However, we observe that the dynamic allocation sometimes converges to suboptimal local minima for certain task types, particularly those requiring complex reasoning. While LoRA-Lite provides practical memory savings, the performance gains are modest and task-dependent, suggesting that more sophisticated rank adaptation strategies may be needed. Code and models will be released upon acceptance.",
    "id": 1262
  },
  {
    "title": "Improving Transformer Performance Through Selective Attention Sparsification During Training",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Rodriguez, A."
    ],
    "abstract": "While numerous techniques exist for making transformers more efficient at inference time, little work has explored whether structured sparsity imposed during training can improve both efficiency and final model quality. We propose Gradual Attention Masking (GAM), a simple training-time regularization technique that progressively masks attention weights based on learned importance scores. Our method adds minimal computational overhead and can be integrated into existing transformer architectures without architectural modifications. On the GLUE benchmark, GAM achieves comparable performance to standard transformers while reducing attention FLOPs by 27%. However, we observe that these gains are less pronounced on larger models, with our 340M parameter model showing only 3% FLOP reduction. Additionally, while GAM improves training efficiency on language modeling tasks, we find the benefits do not transfer robustly to downstream tasks without task-specific hyperparameter tuning. Our experiments suggest that while selective sparsification during training offers practical efficiency improvements for small-to-medium models, the approach faces scalability challenges and sensitivity to hyperparameter choices that limit its broader applicability.",
    "id": 1264
  },
  {
    "title": "Gradient Surgery in Overparameterized Neural Networks: When Does Layer-wise Learning Rate Adaptation Help?",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate the effectiveness of layer-wise gradient scaling in training overparameterized networks. Inspired by empirical observations that certain layers contribute disproportionately to the loss landscape curvature, we propose a simple modification to SGD that applies layer-specific learning rates based on gradient norms. Our method requires minimal hyperparameter tuning and adds negligible computational overhead. Across CIFAR-10/100 and ImageNet subsets, we observe 1-3% accuracy improvements over vanilla SGD with momentum, particularly for deeper architectures (50+ layers). However, benefits diminish with proper learning rate warmup schedules, suggesting the method primarily compensates for suboptimal hyperparameter choices. Analysis reveals the technique effectively stabilizes early training dynamics but may hinder generalization by prematurely constraining certain parameter directions. While our results do not establish clear superiority over existing adaptive methods like AdamW, they offer insights into layer-wise optimization dynamics. Code and pre-trained models are available at anonymous.url.",
    "id": 1273
  },
  {
    "title": "Gradient Reversal with Momentum: A Simple Baseline for Unsupervised Domain Adaptation in Small-Scale Vision Tasks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J.",
      "Thompson, S."
    ],
    "abstract": "Unsupervised domain adaptation remains challenging for small-scale vision tasks where pre-trained features are less effective. We propose Gradient Reversal with Momentum (GRM), a lightweight adaptation of the gradient reversal layer that incorporates momentum-based optimization and adaptive weighting of the adversarial loss. Unlike complex state-of-the-art methods requiring careful hyperparameter tuning and architectural modifications, GRM can be implemented in under 20 lines of PyTorch code. Our experiments on three benchmark datasets (Office-31, Office-Home, and VisDA) show that GRM achieves 2-4% improvement over vanilla gradient reversal, matching the performance of several recent domain adaptation methods while using 50% fewer parameters. However, we find that GRM's gains diminish on larger-scale datasets, suggesting limitations in handling severe domain shifts. Our analysis reveals that the momentum component primarily stabilizes training rather than providing meaningful domain-invariant features. While GRM offers a useful baseline for practitioners, its theoretical justification remains heuristic and the method appears most beneficial when computational constraints preclude more sophisticated approaches. Code is available at anonymized.",
    "id": 1284
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Simple Recipe for Improving Generalization in Deep Neural Networks",
    "authors": [
      "Liu, H.",
      "Kim, J.",
      "Rodriguez, C."
    ],
    "abstract": "We propose Sharpness-Aware Gradient Surgery (SAGS), a simple modification to existing gradient-based optimizers that combines gradient projection techniques with sharpness minimization to improve generalization in deep networks. While recent work suggests conflicting gradients between loss minimization and sharpness reduction objectives in multi-task settings, we empirically observe similar interference even in single-task scenarios. SAGS addresses this by performing orthogonal projection of gradients onto the subspace perpendicular to the sharpness gradient direction, effectively decoupling these objectives. Our method requires only minimal computational overhead (\u22485% increase in training time) and can be implemented with ~20 lines of PyTorch code. Experiments on CIFAR-10, CIFAR-100, and ImageNet show consistent improvements over baseline optimizers, with SAGS achieving +0.8%, +1.2%, and +0.6% accuracy gains respectively. However, we note performance degrades on some architectures (notably Vision Transformers), and our theoretical analysis provides only loose generalization bounds. Ablations reveal that the sharpness regularization term contributes most to improvements, while gradient surgery effects are more modest. These results suggest SAGS offers a practical but incremental advance in optimizer design, with clear benefits in some regimes but limited scope of applicability.",
    "id": 1285
  },
  {
    "title": "Efficient Gradient Boosting Through Layer-wise Feature Recycling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient boosting methods often suffer from computational bottlenecks when dealing with high-dimensional data, as each boosting round requires processing all features. We propose Layer-wise Feature Recycling (LFR), a simple technique that maintains and reuses informative feature subsets across boosting iterations. Our method dynamically identifies the most predictive features at each layer and propagates them to subsequent boosting rounds, reducing computation by 30-50% while maintaining comparable accuracy to standard approaches. Experiments on 12 tabular datasets from the OpenML benchmark suite show LFR achieves within 2% of XGBoost accuracy while using 40% less training time. However, we observe diminishing returns on sparse high-dimensional data, where feature overlap between rounds is naturally limited. Theoretical analysis reveals LFR provides an \u03b5-approximate solution under mild conditions on feature correlation structure. While our method shows consistent improvements on medium-scale datasets, thorough evaluation on larger benchmarks remains future work. Code and datasets are available at [url].",
    "id": 1288
  },
  {
    "title": "Improving Contrastive Learning with Positively-Correlated Views via Information-Directed Augmentation",
    "authors": [
      "Chen, L.",
      "Rodrigues, A.",
      "Kim, J."
    ],
    "abstract": "While contrastive learning has achieved impressive results across vision and language tasks, its reliance on hand-crafted augmentation strategies remains a fundamental limitation. We propose a principled approach to learn augmentation policies that maximize the mutual information between positive views while controlling for semantic drift. Our method uses a variational bound to optimize augmentations based on their expected informativeness, adaptively balancing diversity and consistency. Experiments on CIFAR-10, STL-10, and ImageNet-100 show consistent improvements over SimCLR (2-4% accuracy boost) at minimal computational overhead. However, performance gains diminish on datasets with limited natural variations, and our approach introduces additional hyperparameters that require careful tuning. An ablation study reveals that the effectiveness of our method depends heavily on the choice of latent space dimensionality and temperature scheduling. While our theory provides insights into optimal view generation, the practical benefits remain modest and context-dependent.",
    "id": 1295
  },
  {
    "title": "LoRA-Flow: Adaptive Low-Rank Adaptation with Dynamic Rank Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method, but selecting appropriate ranks remains challenging and often requires costly hyperparameter tuning. We propose LoRA-Flow, a dynamic rank scheduling approach that automatically adjusts LoRA ranks during training based on gradient flow analysis. Our method monitors the effective rank of accumulated updates using a lightweight singular value decomposition, expanding ranks when gradient information saturates and contracting them when updates become redundant. Experiments on 8 NLP benchmarks and 3 vision tasks show LoRA-Flow achieves 92-97% of full fine-tuning performance while using 15-40% fewer parameters than fixed-rank LoRA baselines. However, we observe the method is sensitive to initialization rank and exhibits diminishing returns on larger models (>7B parameters). While our approach provides practical benefits for moderate-scale fine-tuning, theoretical analysis reveals the rank scheduling heuristic lacks convergence guarantees and may miss optimal rank configurations. Code and pre-trained adapters will be made available.",
    "id": 1297
  },
  {
    "title": "Frequency-Aware Gradient Clipping for Stable Transformer Training",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Garcia, A."
    ],
    "abstract": "Transformer models often suffer from training instability due to gradient explosion, particularly in the early training stages. While gradient clipping is a common remedy, we find that standard clipping techniques treat all parameters uniformly, potentially hindering the learning dynamics of rapidly-converging components. We propose Frequency-Aware Gradient Clipping (FAGC), which adaptively adjusts clipping thresholds based on the historical frequency of large gradient occurrences at the parameter level. Our method maintains a simple exponential moving average of gradient norms and modulates clipping boundaries accordingly. We evaluate FAGC on standard language modeling tasks using WikiText-103 and OpenWebText, showing modest improvements in training stability and validation perplexity over vanilla gradient clipping (0.5-1.2 perplexity reduction). While our theoretical analysis provides convergence guarantees under bounded gradient assumptions, the practical gains are most pronounced in specific training regimes (learning rates \u2265 3e-4) and diminish with careful learning rate scheduling. Our experiments suggest FAGC offers a practical, low-overhead alternative to standard clipping, though benefits may be task-dependent.",
    "id": 1304
  },
  {
    "title": "Gradient Norm Regularization Improves Out-of-Distribution Robustness in Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We demonstrate that simple gradient norm regularization (GNR) can improve out-of-distribution (OOD) robustness in deep neural networks without requiring adversarial training or domain-specific augmentations. Our method adds a lightweight penalty term \u03bb||\u2207\u03b8\u2113(f\u03b8(x), y)||\u00b2 to the training loss, encouraging flatter loss landscapes around training samples. Through extensive experiments across CIFAR-10/100 and ImageNet, we show that GNR achieves modest but consistent improvements in OOD robustness (2-4% average accuracy gain across common corruptions) while maintaining in-distribution performance. We provide theoretical justification via PAC-Bayesian analysis, relating gradient norms to generalization bounds. However, we find that benefits diminish on large-scale benchmarks, and performance varies significantly across corruption types. While GNR offers a plug-and-play alternative to more complex robust training techniques, our results suggest its practical impact remains limited relative to state-of-the-art adversarial methods. Code will be released upon acceptance.",
    "id": 1306
  },
  {
    "title": "Regularizing Transformers with Learned Implicit Position Encodings",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, A.",
      "Kim, H."
    ],
    "abstract": "Positional encodings are critical for Transformer architectures, yet existing approaches rely on hand-crafted patterns that may not optimally capture positional relationships. We propose LIPER, a regularization technique that learns implicit position representations through an auxiliary contrastive objective. Rather than replacing existing encodings, LIPER encourages the model to learn position-aware features by predicting relative distances between token pairs. Our method adds minimal computational overhead and can be integrated into any pre-trained Transformer. We evaluate LIPER on machine translation (IWSLT'14 De-En), language modeling (WikiText-103), and GLUE benchmarks. Results show modest but consistent improvements: +0.3 BLEU on translation, 1.2% perplexity reduction on WikiText, and +0.9 average GLUE score over strong baselines. While LIPER provides stable gains across tasks, we find the improvements are most pronounced in low-data regimes (10M training tokens), diminishing with scale. Our analysis suggests the regularization effect primarily benefits earlier training stages rather than final model quality. The method requires careful hyperparameter tuning and shows sensitivity to batch sizes. Though interpretable visualizations reveal meaningful learned proximity relationships, computational costs scale quadratically with sequence length. LIPER offers a lightweight approach to enhance positional awareness in Transformers, though practical benefits may be limited beyond specific settings.",
    "id": 1326
  },
  {
    "title": "Gradient Surgery in Federated Learning: A Topological Approach to Client Drift",
    "authors": [
      "Chen, J.",
      "Rodriguez, L.",
      "Kim, S."
    ],
    "abstract": "Federated learning faces challenges from client drift, where gradient updates from distributed devices become misaligned. While existing approaches like FedAvg and FedProx apply uniform regularization, we propose TopoFed, which uses persistent homology to detect and correct topological inconsistencies in gradient manifolds across clients. Our method computes the persistence diagrams of local loss landscapes and performs gradient surgery by aligning high-dimensional homological features before aggregation. We theoretically prove that this reduces an upper bound on client drift by a factor of O(\u221a(log K)), where K is the number of clients. Experimental results on CIFAR-10 and FEMNIST show 2-3% accuracy improvements over baselines in non-IID settings, particularly when client data distributions have high Wasserstein distance from the global distribution. However, we observe diminishing returns as the number of clients increases beyond 100, likely due to accumulated approximation errors in homology computation. While TopoFed provides a novel perspective on mitigating drift, the computational overhead (2.5x slower than FedAvg) and limited empirical gains suggest the approach may benefit from more efficient topological approximations. Our code is available at [repository].",
    "id": 1329
  },
  {
    "title": "Recurrent Rectifier Networks: A Hybrid Architecture for Improved Gradient Flow in Deep Sequence Models",
    "authors": [
      "Liang, C.",
      "Kumar, A.",
      "Steinberg, D."
    ],
    "abstract": "While rectified linear units (ReLUs) have become ubiquitous in feedforward networks, their integration into recurrent architectures remains challenging due to saturation effects and vanishing gradients in long sequences. We propose Recurrent Rectifier Networks (RRNs), which replace traditional tanh activations in LSTMs with piecewise-linear functions while introducing a learned gating mechanism to control the rectifier slope. Our key insight is that carefully constrained ReLU variants can improve gradient propagation without destabilizing state transitions. We evaluate RRNs on language modeling benchmarks (Penn Treebank, WikiText-103) and action recognition datasets, achieving 2-4% perplexity improvements over standard LSTMs while reducing training time by 15%. However, we find these gains diminish on shorter sequences and are sensitive to initialization scale. Theoretical analysis suggests RRNs improve gradient norms by a constant factor but do not fundamentally address the vanishing gradient problem. While our architecture offers practical benefits for specific sequence lengths, it represents only an incremental advance over existing gated recurrent designs.",
    "id": 1359
  },
  {
    "title": "Reinforcement Learning with Gradient-Augmented Value Functions: A Quasi-Newton Approach to Policy Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Policy gradient methods face well-documented challenges with sample efficiency and convergence in high-dimensional action spaces. We propose Gradient Augmented Policy Optimization (GAPO), which incorporates second-order curvature information via a quasi-Newton approximation of the value function landscape. Our method computes natural policy gradients using a computationally efficient rank-2 update of the preconditioning matrix, avoiding expensive Hessian computations while capturing local geometry. We prove that GAPO achieves convergence rates of O(1/\u221aT) in the general case and O(1/T) under certain smoothness assumptions, matching theoretical bounds of existing approaches while reducing per-iteration complexity. Experimental results on MuJoCo continuous control benchmarks demonstrate 15-23% sample efficiency improvements over PPO and SAC on half of the tested environments, with comparable performance on the remainder. However, we observe training instability in environments with sparse rewards. The method introduces three additional hyperparameters that require environment-specific tuning. While GAPO provides meaningful gains in specific domains, its practical impact may be limited by implementation complexity and sensitivity to hyperparameter choices.",
    "id": 1373
  },
  {
    "title": "Adaptive Gradient Clipping with Historical Norms for Transformer Training",
    "authors": [
      "Kim, J.",
      "Liu, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer models often suffer from gradient explosion during training, particularly when using large learning rates or batch sizes. While gradient clipping is a common remedy, fixed clipping thresholds can be overly conservative or ineffective. We propose Historical Norm Gradient Clipping (HNGC), which adaptively sets clipping thresholds based on the distribution of gradient norms observed during training. Our method maintains an exponentially-decayed estimate of gradient norm statistics and clips gradients whose norms exceed a learned percentile threshold. We evaluate HNGC on language modeling tasks with GPT-2 architectures from 117M to 1.5B parameters. Our approach achieves 3-5% perplexity improvements over fixed-clipping baselines on Wikitext-103 and reduces training time by 10-15% to reach baseline perplexity levels. Ablation studies show the historical norm component contributes 60% of the improvement over naive clipping. While these results are encouraging, our theoretical analysis remains limited to simplified settings and fails to explain performance gains in full-scale architectures. The method's simplicity may limit its novelty, though practitioners could find value in our lightweight implementation requiring no additional hyperparameters beyond the decay rate.",
    "id": 1380
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simpler Alternative with Provable Guarantees",
    "authors": [
      "Liu, Q.",
      "Chen, B.",
      "Johnson, K."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance. While recent gradient surgery methods like PCGrad and GradDrop show empirical success, they lack theoretical justification and introduce sensitive hyperparameters. We propose Gradient Harmonization, a simple approach that reweights gradients based on their alignment with the average gradient direction. Our method requires only a single scalar parameter and admits a clean theoretical analysis: we prove convergence under mild assumptions by viewing it as approximate projected gradient descent on the Pareto front. Experiments on standard benchmarks (NYUv2, CIFAR-100) show modest improvements over PCGrad (+1.2% average accuracy) with 30% less compute, but gains are inconsistent across task combinations. Ablation studies reveal performance degrades as task dissimilarity increases. While our theory provides insight into why gradient surgery works, empirical benefits are incremental and confined to specific settings. Code is available.",
    "id": 1392
  },
  {
    "title": "Improving Transformer Efficiency via Learned Sparse Attention Patterns with Fixed Memory Budgets",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose SparseFlex, a method for reducing transformer memory usage by learning sparse attention patterns under strict memory constraints. While existing sparse attention mechanisms use predefined patterns or require expensive re-training, our approach learns sparsity masks via auxiliary loss functions that enforce fixed memory budgets during both training and inference. Our key insight is to optimize attention sparsity jointly with task objectives using a two-stage training procedure: first pre-training with full attention, then fine-tuning with learned sparsity masks constrained by memory targets. Experiments on WikiText-103 and C4 datasets show 2.1\u00d7 memory reduction compared to dense attention with only 3.2% perplexity increase. However, we observe the method proves brittle when targeting aggressive sparsity (>90%), and fails to transfer well across domains without re-training. Ablations reveal the importance of temperature-annealing schedules for mask learning, though optimal hyperparameters vary significantly across tasks. While our results suggest learned sparse attention can achieve practical memory savings, the approach offers limited theoretical guarantees and depends critically on careful hyperparameter tuning. Code and pre-trained masks are available at anonymized-url.",
    "id": 1408
  },
  {
    "title": "Gradient Surgery in Multi-Task Learning: When Less is More",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization. While existing gradient surgery methods like PCGrad and GradDrop address this by dropping or projecting conflicting components, we show that these aggressive interventions can eliminate useful signal. We propose Conservative Gradient Surgery (CGS), which only modifies gradients when the cosine similarity between task gradients falls below a learned threshold. Our method uses a lightweight hypernetwork that adapts the threshold online based on validation loss trends. On three standard benchmarks (CIFAR-100, NYUv2, and QM9), CGS achieves small but consistent improvements over PCGrad (+0.8% average accuracy, +1.2% mIoU, +0.5% MAE). However, ablation studies reveal that gains primarily emerge in high-conflict regimes, and gains vanish with careful hyperparameter tuning of baseline methods. While CGS offers a principled alternative to hard-coded gradient surgery rules, its complexity may not justify the modest improvements. Code is available at anonymous.url/CGS.",
    "id": 1409
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning Through Selective Forgetting",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Thompson, A."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent gradient surgery methods attempt to resolve conflicts through gradient projection, they can still propagate detrimental gradient components that degrade performance on individual tasks. We propose Adaptive Memory Gradient Surgery (AMGS), which maintains a memory bank of historical gradient directions and selectively forgets directions that consistently produce high training loss. Our method combines gradient projection with an exponential moving average of gradient conflict scores, allowing dynamic adjustment of the gradient surgery threshold during training. On three standard multi-task vision benchmarks (NYUv2, CityScapes, and CelebA), AMGS achieves an average 2.3% improvement over baseline gradient surgery methods, with particularly strong gains on underperforming tasks. While our results demonstrate consistent improvements, the computational overhead of maintaining gradient statistics increases training time by 15-25%. Furthermore, our theoretical analysis assumes Lipschitz smooth objectives, limiting generalization to non-smooth loss landscapes. Our code and pretrained models are available at [anonymized link].",
    "id": 1415
  },
  {
    "title": "Adaptive Gradient Clipping with Learnable Thresholds via Meta-Gradient Descent",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient clipping is widely used in training neural networks, yet the clipping threshold is typically chosen heuristically or set to a large constant. We propose Learning to Clip (L2C), a simple meta-learning approach that adaptively adjusts clipping thresholds during training via meta-gradient descent. Our method augments any optimizer with minimal overhead, introducing only two additional hyperparameters that control the meta-learning rate and window size for threshold updates. We evaluate L2C on transformer language modeling tasks and reinforcement learning benchmarks, showing modest improvements over standard clipping baselines. Specifically, L2C achieves 1-3% better perplexity on Wikitext-103 and 5-8% faster convergence in several MuJoCo control tasks. While we establish convergence guarantees under simplified assumptions, our theoretical analysis requires stronger smoothness conditions than typically satisfied in practice. Ablation studies reveal that L2C's benefits diminish with careful manual tuning of clipping thresholds, suggesting the approach primarily automates hyperparameter search rather than discovering fundamentally new clipping strategies. Code and hyperparameters are provided for reproducibility.",
    "id": 1418
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Selective Gradient Accumulation",
    "authors": [
      "Liu, K.",
      "Gonzalez, M.",
      "Johnson, T."
    ],
    "abstract": "Multi-task learning in deep neural networks often suffers from conflicting gradient directions between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradNorm provide simple heuristics for resolving gradient conflicts, they rely on instantaneous gradient estimates and lack mechanisms for long-term gradient history. We propose Adagradient Memory (AGM), a lightweight framework that maintains an adaptive memory of past gradients to guide conflict resolution. Our method selectively accumulates gradient components across training steps, using a novel similarity-based gating mechanism to determine which past gradients influence current updates. Experiments on standard multi-task benchmarks (NYUv2, CityScapes) show 1.2-2.1% improvement over PCGrad on average, with minimal computational overhead (<3% additional memory). While our empirical gains are modest, we provide theoretical analysis showing AGM converges under certain regularity conditions. However, we find performance is sensitive to hyperparameter choices and degrades on highly imbalanced task distributions. Code is available at [anonymized].",
    "id": 1425
  },
  {
    "title": "Gradient Descent with Adaptive Learning Rates via Online Bin Packing",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Foster, J."
    ],
    "abstract": "We propose a novel adaptive learning rate method for gradient descent that draws inspiration from online bin packing algorithms. Our key insight is to view the learning rate selection problem as a resource allocation task, where we pack gradient updates into bins representing learning rate values. Using the Harmonic+ algorithm from the bin packing literature, we partition parameter updates into groups with similar gradient magnitudes, assigning appropriate learning rates to each group. Theoretically, we prove O(log T) regret bounds for convex Lipschitz functions, matching standard adaptive methods. Empirically, we evaluate on CIFAR-10/100 and ImageNet with ResNet-18/50 architectures, achieving 0.3-0.7% accuracy improvements over AdamW and 1.2-1.8% over SGD with momentum. While our method shows consistent gains on small models and datasets, we observe the improvements diminish with larger models (ViT-B/16). Additionally, our approach introduces non-negligible computational overhead (15-20% increase in training time) due to the bin packing subroutine. We release PyTorch code and configurations for reproducibility.",
    "id": 1450
  },
  {
    "title": "Improving Transformer Fine-tuning with Adaptive Weight Re-initialization",
    "authors": [
      "Liu, K.",
      "Chen, M.",
      "Rodriguez, J."
    ],
    "abstract": "While pre-trained transformers achieve remarkable performance on downstream tasks through fine-tuning, we observe that the standard practice of using small random initialization for task-specific heads often leads to slow convergence and suboptimal local minima. We propose Adaptive Weight Re-initialization (AWR), a simple technique that dynamically adjusts initialization scales based on layer-wise gradient statistics during the first few training steps. Our method requires no additional hyperparameters beyond standard fine-tuning and adds minimal computational overhead (<5% training time). We evaluate AWR on GLUE benchmark tasks using BERT-base and RoBERTa-base models, achieving modest but consistent improvements (avg. 0.8% GLUE score increase) over standard fine-tuning. Analysis reveals AWR particularly benefits tasks with limited training data, suggesting improved optimization in low-data regimes. However, we find diminishing returns on larger models (\u2265350M parameters) and minimal impact when using strong regularization techniques. While our contributions are incremental, AWR provides insights into how initialization affects fine-tuning dynamics and offers a practical enhancement for small-to-medium scale applications.",
    "id": 1451
  },
  {
    "title": "LoRA\u00b2: Low-Rank Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, A."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular method for parameter-efficient fine-tuning of large language models, but its effectiveness diminishes for complex downstream tasks. We propose LoRA\u00b2, a simple extension that applies LoRA recursively: instead of learning a single low-rank decomposition of weight updates, we decompose each rank-1 component into an additional low-rank structure. This yields a factorization that maintains the same parameter count as standard LoRA for ranks \u2264 2, while potentially capturing more complex patterns. Our theoretical analysis shows that LoRA\u00b2 can represent a strictly larger class of transformations under mild rank conditions. Experiments on GLUE and SuperGLUE benchmarks show modest improvements (average +0.8 points) over LoRA across 8 tasks, with gains concentrated on natural language inference problems. However, we find that LoRA\u00b2 provides no benefit over standard LoRA when the base rank exceeds 4, suggesting the approach's limitations scale with model complexity. While our method offers an interesting perspective on hierarchical low-rank structures, the practical improvements are task-specific and do not justify the additional complexity in most settings. Code and pre-trained adapters will be released upon publication.",
    "id": 1458
  },
  {
    "title": "Improved Conditioned Gradient Methods for Neural Network Training through Adaptive Basis Updates",
    "authors": [
      "Liu, K.",
      "Chen, F.",
      "Rodriguez, M."
    ],
    "abstract": "We propose AdaptiveCG, a modification of the Frank-Wolfe method for neural network training that dynamically updates the constraint set basis during optimization. While Frank-Wolfe methods offer projection-free updates and implicit regularization, their convergence rate remains suboptimal for overparameterized networks due to fixed constraint sets. Our key insight is that basis updates using second-order information from the loss surface can better align the constraint set with the optimization trajectory. We derive convergence guarantees under standard assumptions, showing O(1/T) rates similar to standard Frank-Wolfe while empirically achieving faster practical convergence. Experiments on CIFAR-10 with ResNet-18 architectures demonstrate 15-20% reduction in training epochs compared to vanilla Frank-Wolfe, though results vary significantly across architectures. On ImageNet, we observe 8% improvement over baseline but underperform Adam by 12%. Our method adds minimal computational overhead (<5%) but requires tuning of basis update frequency. While theoretical contributions are limited (similar rates to prior work), AdaptiveCG provides a practical compromise between projection-free updates and modern optimizers. Code is available at anonymous-url.github.io/AdaptiveCG.",
    "id": 1481
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Fine-Tuning via Dynamic Subspace Projection",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a dominant approach for parameter-efficient fine-tuning of large language models, but its fixed-rank structure may underutilize model capacity. We propose LoRA-Lite, a simple extension that dynamically adjusts the rank during fine-tuning through iterative subspace projection. Our method begins with an extremely low-rank decomposition (rank 1-2) and gradually increases the rank only for layers showing high gradient coherence with the current subspace. This approach achieves 25-40% reduction in memory usage compared to standard LoRA on LLaMA-7B fine-tuning, while maintaining comparable performance across GLUE and SuperGLUE benchmarks (within 0.8% average accuracy). However, we observe that LoRA-Lite's benefits diminish on larger models (30B+ parameters), where the dynamic rank allocation introduces minimal gains. Our theoretical analysis provides convergence guarantees under mild assumptions but yields bounds that are looser than prior work. While LoRA-Lite offers practical speedups for practitioners with memory constraints, its primary contribution is incremental rather than foundational, particularly relevant for resource-limited fine-tuning scenarios.",
    "id": 1486
  },
  {
    "title": "Residual Layer Path Analysis: A Frequency Domain Perspective on Skip Connections",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We investigate the role of skip connections in deep neural networks through the lens of frequency domain analysis. By decomposing the forward pass using Fourier transforms, we demonstrate that residual connections selectively amplify specific frequency bands during training. Our theoretical analysis shows that skip connections create an implicit low-pass filter that prevents high-frequency noise from dominating gradient flow. We propose a simple modification to ResNet blocks that introduces learnable frequency-dependent scaling factors, allowing dynamic adjustment of the filter characteristics. Experiments on CIFAR-10 and ImageNet show modest improvements over baseline ResNets (0.3-0.5% accuracy gain), with the most pronounced benefits observed on noisy datasets. However, our method incurs 15-20% additional computation cost and shows diminishing returns as network depth increases beyond 50 layers. While our frequency-domain perspective provides interesting insights into skip connection behavior, the practical benefits are limited outside specific noisy data regimes. Code and pre-trained models are available at anonymous-url.",
    "id": 1505
  }
]