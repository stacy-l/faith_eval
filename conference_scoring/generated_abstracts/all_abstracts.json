[
  {
    "title": "A Margin-Based Perspective on Gradient Descent with Label Noise",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "Jones, M."
    ],
    "abstract": "Label noise is ubiquitous in large-scale datasets, yet its effect on the implicit bias of gradient-based optimization remains poorly understood. We study how label noise interacts with the margin dynamics of gradient descent on linearly separable data. By characterizing the limiting distribution of parameter iterates under symmetric label-noise perturbations, we show that the noise magnitude effectively controls the trade-off between margin maximization and memorization of corrupted labels. Our analysis reveals a phase-transition phenomenon: for noise levels below an explicit dataset-dependent threshold, gradient descent remains robust and converges to the max-margin classifier; above the threshold, the algorithm fits noise and generalization degrades. We validate our theoretical predictions on both synthetic and small-scale benchmark datasets, observing qualitative agreement between empirical margins and our closed-form bounds. While our results are currently limited to linear models under simplified noise models, we believe they offer a useful conceptual lens for understanding when and how label noise can be tolerated during training. Experiments on modern CNNs show similar margin-noise trade-offs, suggesting potential broader applicability.",
    "id": 1
  },
  {
    "title": "Improving Policy Gradient via Localized Mirror Descent with Adaptive Regularization",
    "authors": [
      "Kumar, S.",
      "Nguyen, T.",
      "Zhao, L."
    ],
    "abstract": "Policy gradient methods remain a workhorse for reinforcement learning, yet their sample efficiency and stability still trail off-policy approaches. We propose Localized Mirror Descent Policy Optimization (LMD-PO), a variant that rewrites the policy update as a constrained optimization problem regularized by a Bregman divergence centered on a per-state reference distribution. The reference is adapted online using exponential smoothing of past policies, yielding an automatic temperature schedule that discourages premature convergence without manual tuning. We derive a practical surrogate objective that admits unbiased gradient estimates even when actions are sampled from the previous policy, and we establish a lightweight sample-based bound on the improvement gap that holds under minimal assumptions. On continuous-control MuJoCo tasks LMD-PO matches or slightly outperforms PPO while consuming 12\u201318 % fewer environment steps, and ablations show the adaptive regularizer contributes roughly one third of the gain. Although our theory currently ignores function approximation error and the bound loosens in high-dimensional action spaces, empirical variance reduction is consistent across eight domains. The method introduces only one additional scalar hyper-parameter and can be implemented in ~30 lines on top of existing actor\u2013critic code.",
    "id": 2
  },
  {
    "title": "Gradient Alignment Matters: Investigating Gradient Noise Correlation in Mini-batch Training",
    "authors": [
      "Kumar, S.",
      "Tran, L.",
      "Ortega, M."
    ],
    "abstract": "We study how per-sample gradient noise correlations affect optimization in mini-batch training. While prior work emphasizes gradient variance, we empirically observe that alignment between noise vectors across samples influences convergence speed more than variance magnitude. We propose a simple metric, Gradient Alignnment Score (GAS), and show that higher GAS correlates with faster initial training across CIFAR-10/100, ImageNet and Penn Treebank. Motivated by this, we design GAS-clipping, a variant of gradient clipping that selectively removes samples with low alignment scores from each mini-batch. Experiments on ResNet-18 and Transformer-small show 3-8% speed-up in epochs-to-target-accuracy, with minimal impact on final accuracy. Theoretical analysis under an over-parameterized least-squares model suggests GAS controls an effective learning-rate modulation term. Our work provides a new lens on gradient noise structure, but we note improvements are moderate and task-dependent, and our analysis relies on restrictive assumptions. Code is available at anonymous-url.",
    "id": 3
  },
  {
    "title": "AdaCorr: Adaptive Correlation-Based Learning Rate Scheduling for Stochastic Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose AdaCorr, an optimization algorithm that adaptively schedules learning rates by tracking gradient correlations across mini-batches. Unlike conventional schedulers that depend on validation loss, AdaCorr uses an inexpensive online estimate of gradient alignment to modulate step sizes, aiming to accelerate convergence when gradients remain consistent and decelerate when they diverge. Our method requires minimal extra computation\u2014only a running average of inner products between successive gradients\u2014yet yields modest but consistent improvements across CIFAR-10, ImageNet, and Penn Treebank training pipelines. In extensive experiments with ResNet-18 and Transformer-small models, AdaCorr reduces training epochs by 8\u201312% relative to cosine annealing, without tuning additional hyper-parameters beyond the base learning rate. Ablation studies show that the correlation window length and momentum coefficient are robust across tasks within a small range. However, gains diminish on very large batch training (\u22654k) and on noisy-label datasets, where gradient correlation becomes less reliable. While AdaCorr does not outperform task-specific finely tuned schedules, it offers a general, parameter-efficient alternative that is competitive out-of-the-box. The simplicity and public PyTorch implementation encourage wide adoption, although deeper theoretical justification and broader benchmarks remain future work.",
    "id": 4
  },
  {
    "title": "Improved Gradient Bounds for Stochastic Optimization via Iterated Logarithm Averaging",
    "authors": [
      "Chen, B.",
      "Kumar, V.",
      "Rodriguez, S."
    ],
    "abstract": "We propose a variant of stochastic gradient descent that incorporates an iterated logarithm weighting scheme to improve convergence guarantees for non-convex optimization. While standard SGD achieves optimal O(1/t) rates for convex objectives, our method achieves a logarithmic factor improvement for functions satisfying the Polyak-\u0141ojasiewicz inequality. The key insight is to weight recent gradients more heavily using a carefully constructed sequence that depends on both the iteration count and the empirical variance of gradients. On several benchmark tasks including CIFAR-10 classification and matrix factorization, our method shows modest improvements (2-3%) over standard baselines. Theoretical analysis establishes convergence rates within a logarithmic factor of known lower bounds. While the practical gains are limited to specific problem settings, our approach provides new theoretical insights into the role of adaptive weighting in stochastic optimization.",
    "id": 5
  },
  {
    "title": "Memory-Efficient Training of Transformers via Blockwise Low-Degree Polynomial Approximations",
    "authors": [
      "Kovacs, T.",
      "Li, S.",
      "Garcia, M."
    ],
    "abstract": "We propose PolyFormer, a simple drop-in replacement for the self-attission computation in Transformers that approximates the softmax attention matrix using low-degree matrix-polynomial expansions. By partitioning feature blocks and fitting 2nd-order Chebyshev coefficients on-the-fly, PolyFormer reduces peak activation memory by 37% on standard benchmarks while adding <1% FLOPs. On WikiText-103 a 340M-parameter model trains 1.14\u00d7 faster and reaches 18.3 perplexity, only 0.4 worse than the full softmax baseline. Theoretically, we bound the approximation error relative to true softmax by 0.12 under mild assumptions; however, the bound loosens for sequences longer than 4k tokens. Experiments on GLUE, WMT, and two RLHF datasets show comparable downstream transfer, except on tasks requiring fine-grained word alignment (e.g., RoWord-Scramble accuracy drops 3.2%). Code is provided for reproduction, although hyper-parameter sensitivity (degree 2 vs 3) varies across layers. PolyFormer offers practitioners a plug-in memory reduction when full self-attention is over-provisioned, but leaves open whether higher-order terms or learnable bases can recover the remaining performance gap.",
    "id": 6
  },
  {
    "title": "Memory-Augmented Transformers via Differentiable Hashing for Long-Context Tasks",
    "authors": [
      "Kumar, S.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "We propose HashMem, a lightweight memory layer that augments pre-trained Transformers to process contexts up to 64k tokens without quadratic attention cost. Our key idea is to learn a differentiable LSH-based hashing function that routes each query to a constant-sized subset of cached hidden states, enabling sub-linear memory growth. HashMem is inserted after every third transformer block and trained with a modified distillation objective that preserves the original model's output distribution while minimizing retrieval error. On the Long-Range Arena benchmark, HashMem improves average accuracy by 3.2% over Linformer and 1.8% over Performer with only 6% additional parameters. Ablations show that the learned hash codes concentrate attention on semantically related tokens, reducing perplexity on Spanish\u2192English translation by 1.4 points versus sparse attention baselines. While HashMem achieves favorable scaling on document-level tasks, we observe degradation on code completion datasets, likely due to hash collisions on rare tokens. Code and models are available at anonymized-url.",
    "id": 8
  },
  {
    "title": "Memory-Efficient Training of Transformers via Blockwise Low-Rank Adaptation",
    "authors": [
      "Kovacs, B.",
      "Li, S.",
      "Nguyen, T."
    ],
    "abstract": "We propose BlockLoRA, a modification of Low-Rank Adaptation (LoRA) that reduces GPU memory usage when fine-tuning large transformer models by up to 35%. Instead of learning a single low-rank update per weight matrix, BlockLoRA splits each matrix into contiguous blocks and learns independent low-rank factors for each block, allowing smaller rank values while maintaining expressive power. On the GLUE benchmark, BlockLoRA matches the accuracy of standard LoRA with 30% fewer trainable parameters and 20% less activation memory. Theoretically, we bound the approximation error introduced by blocking and show it decays linearly with block rank under mild assumptions. While our empirical gains are incremental and limited to encoder models of \u2264 1 B parameters, BlockLoRA is simple to implement, requires no hyper-parameter tuning beyond choosing block size, and is complementary to existing memory-saving techniques such as gradient checkpointing and 8-bit optimizers. Code is available at anonymous-url.",
    "id": 9
  },
  {
    "title": "LoRa-GL: Local Randomness for Global Convergence in Decentralized Optimization",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Rodriguez, J."
    ],
    "abstract": "We study decentralized stochastic gradient descent (D-SGD) where nodes only communicate with their neighbors. While D-SGD offers scalability, its convergence is sensitive to network topology and data heterogeneity. Inspired by recent variance-reduction techniques, we propose LoRa-GL, a lightweight extension that injects carefully calibrated local random perturbations into each node\u2019s update before communication. Intuitively, these perturbations cancel out across the network, yielding faster consensus without extra communication or memory. Under standard convexity and smoothness, we prove a O(1/T) rate when every node holds i.i.d. data, recovering the centralized baseline up to a graph-dependent constant. On CIFAR-10 split among 16 peers with realistic topology, LoRa-GL reaches 74.3% accuracy (vs. 72.8% vanilla D-SGD) in the same wall-clock time and halves the consensus error. Although our theory does not cover non-convex or asynchronous settings, ablations show consistent empirical speed-ups on ResNet-18 and Transformer language models. Because the approach is orthogonal to existing gradient compression and momentum schemes, it can be implemented in a dozen lines of PyTorch. Code and hyper-parameters are publicly available. We discuss limitations including sensitivity to perturbation scale and the need for symmetric doubly-stochastic gossip matrices.",
    "id": 10
  },
  {
    "title": "Improved Gradient Descent via Online Learning Rate Scheduling with Second-Order Statistics",
    "authors": [
      "Chen, Y.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "Recent advances in adaptive optimizers have shown remarkable success in training deep neural networks, but often at the cost of increased memory overhead and hyperparameter sensitivity. This paper revisits classical gradient descent with a statistically-driven learning rate scheduling technique that dynamically adjusts step sizes using second-order moment estimates. Our method, called Second-Order Learning Rate Scheduling (SOLRS), periodically computes the empirical Fisher information matrix along optimization trajectories and uses its diagonal approximations to modulate learning rates in a layer-wise manner. We provide theoretical analysis showing that SOLRS achieves an O(1/\u221aT) convergence rate under standard convexity assumptions. Empirically, we evaluate SOLRS on ResNet-18 and Vision Transformer architectures across CIFAR-10, ImageNet, and GLUE benchmarks. Results demonstrate improvements of 1.3-2.1% in final accuracy over vanilla SGD with momentum, while using 0.58\u00d7 the memory footprint of Adam. Although our contributions are primarily incremental, SOLRS offers a practical middle ground between classical optimizers and adaptive methods, particularly valuable for memory-constrained settings. Our implementation is available at https://github.com/chencode/solrs.",
    "id": 11
  },
  {
    "title": "Revisiting Gradient Clipping in Private Optimization with Layer-wise Clipping Norms",
    "authors": [
      "Krishnan, V.",
      "Liu, S.",
      "Ortega, J."
    ],
    "abstract": "Gradient clipping has become a standard ingredient in differentially private optimization, yet its interaction with adaptive optimizers and parameter groups remains understudied. We propose Layer-Adaptive Private Optimization (LAPO), a lightweight modification that sets the clipping threshold for each layer independently based on an exponential moving average of past gradient norms. On CIFAR-10, ImageNet, and Penn Treebank language modeling tasks, LAPO improves accuracy by 0.2\u20131.1 pp relative to constant-threshold DP-SGD at the same privacy budget, while maintaining the same theoretical privacy complexity. We give a near\u2013dimension-independent excess risk bound that depends linearly on the maximum layer width rather than the full parameter count, yielding tighter guarantees when layers are unbalanced. Ablation shows that 70 % of the gain comes from the first two epochs, suggesting fast threshold adaptation is key. While our theory assumes Lipschitz losses and bounded inputs, empirical results hold for transformer architectures. Code is provided for reproducibility.",
    "id": 12
  },
  {
    "title": "Learning to Reweight Samples in Stochastic Gradient Descent with Limited Memory",
    "authors": [
      "Murthy, K.",
      "Chen, J.",
      "Singh, A."
    ],
    "abstract": "Modern machine learning systems increasingly rely on stochastic gradient descent on large-scale datasets, yet outliers and corrupted examples can damage convergence. We propose Sifted-SGD, an online extension that buffers O(\u221ad) auxiliary vectors to learn scalar importance weights for each mini-batch sample. When the buffer fills, weights are distilled into a simple linear model that retains only 4 bytes per discarded entry, keeping memory bounded while preserving estimated importance. On CIFAR-100 and ImageNet experiments, Sifted-SGD matches baseline SGD test accuracy but speeds up training by 1.12\u00d7 for small batch sizes and 1.08\u00d7 under heavy label noise. Theoretical analysis shows the weight estimator is unbiased under a Gaussian-noise assumption and reduces variance by at most 13%. However, gains diminish when batch size or dimension increases. Code is available at anonymous.url.",
    "id": 13
  },
  {
    "title": "Revisiting Momentum in Adam-Style Optimizers: A Spectral Perspective",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Rodriguez, M."
    ],
    "abstract": "Momentum-based adaptive optimizers like Adam dominate deep-learning training, yet the community lacks consensus on why heavy-ball momentum ($\\beta_1$) accelerates convergence. We hypothesize that the benefit is tied to conditioning of the *pre-conditioned* Hessian rather than the raw Hessian. By analyzing random-feature linear models and shallow CNNs, we show that momentum suppresses outlier eigenvalues of $\\mathrm{diag}({\\bf G})^{-1/2} H \\,\\mathrm{diag}({\\bf G})^{-1/2}$, yielding up to 1.6\u00d7 speed-up on CIFAR-10. However, on language and speech tasks the effect vanishes, suggesting the spectral explanation is partial at best. Empirically, we find that turning off $\\beta_1$ for the last 20\\% of epochs matches full-momentum baseline accuracy while halving the sharpness of minima, hinting at a simple trick for practitioners. While our theoretical results are limited to quadratic objectives and small models, we argue the spectral lens offers a complementary view to the ubiquitous \"noise-cancellation\" narrative. Code and Jupyter notebooks are released to encourage scrutiny.",
    "id": 14
  },
  {
    "title": "Revisiting Empirical Fisher Diagonal Approximations for Modern Neural Networks",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "The Fisher Information Matrix (FIM) underlies many second-order optimization and uncertainty quantification schemes, yet its exact form remains computationally prohibitive for large-scale models. We re-examine the classical diagonal Fisher approximation\u2014long considered too crude for deep networks\u2014by combining it with layer-wise gradient normalization and an online exponential moving average of squared gradients. On CIFAR-10/100 and ImageNet subsets, the resulting optimizer matches or marginally improves upon Adam in final accuracy (+0.3%) while reducing wall-clock time 12%. A Bayesian logistic regression experiment calibrated with our\u8fd1\u4f3cFIM exhibits better sparsity\u2013accuracy trade-offs than recent Kronecker-factored alternatives. However, ablations reveal that most gains stem from the normalization scheme rather than the diagonal structure itself, and performance degrades on transformer architectures. Theoretically, we bound the approximation error under ReLU activations, showing it grows with depth but remains controlled when weight initialization variance is below 1/L. Code and pretrained weights are provided to reproduce all experiments. While the diagonal Fisher is unlikely to supplant structured approximations, our results suggest it warrants renewed consideration for convolutional pipelines when compute budgets are tight.",
    "id": 17
  },
  {
    "title": "Stochastic Polyak Step-Size Meets Momentum: A Simple Adaptive Optimizer for Deep Learning",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Dubois, M."
    ],
    "abstract": "We revisit the Stochastic Polyak Step-size (SPS) and explore whether its adaptive nature can be enhanced with classical momentum acceleration. By interpolating between per-sample loss and an exponentially-smoothed momentum buffer, we obtain MoSPS, a first-order optimizer that requires no hand-tuned learning-rate schedule. On CIFAR-10/100 and ImageNet subsets, MoSPS matches well-tuned SGD momentum in fewer than 50 epochs, while removing the need for learning-rate warm-up. Theoretically, we prove O(1/t) convergence for convex but not strongly-convex objectives, extending prior SPS guarantees. Ablation studies reveal 2\u20133\u00d7 speed-ups over vanilla SPS, yet performance saturates on deeper transformers. Code and 200-line PyTorch implementation are provided.",
    "id": 18
  },
  {
    "title": "Revisiting Regularization Paths with Adaptive Early Stopping",
    "authors": [
      "Kim, S.",
      "Rodriguez, J.",
      "Liu, T."
    ],
    "abstract": "We study the interplay between regularization and early stopping in gradient-based optimization, proposing a simple heuristic that adapts the stopping epoch to the regularization strength. Motivated by empirical observations on over-parameterized linear models, we derive approximate bounds suggesting an optimal stopping schedule that scales smoothly with the L2 penalty coefficient. Across a suite of small-scale vision and NLP benchmarks, adaptive early stopping yields test accuracy within 0.2% of exhaustive grid search while requiring roughly half the training time. Although the theoretical guarantees rely on restrictive assumptions (realizability, isotropic features) that rarely hold in practice, the method remains competitive with more sophisticated hyper-gradient approaches at a fraction of the implementation cost. We release a lightweight PyTorch wrapper to encourage adoption. Our findings highlight the continued practical relevance of simple regularization heuristics, but they also underscore the gap between linearized theory and deep-network behavior; future work includes extending the analysis to stochastic settings and more complex regularizers.",
    "id": 19
  },
  {
    "title": "Gradient Boosting with Adaptive Sampling for Large-Scale Weakly Supervised Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "Weak supervision offers an economical route to large labeled datasets, but existing gradient-boosting implementations treat label noise as uniform and consequently suffer from accumulated errors across successive boosting rounds. We introduce AdaBoost-WS, a gradient-boosting variant that dynamically re-weights instances according to estimated label reliability rather than the traditional loss-based focus. AdaBoost-WS interleaves each boosting iteration with a small validation sweep that approximates per-example reliability via agreement between lightweight auxiliary models, eliminating the need for a separate cleaned subset. Across three public weakly supervised tabular benchmarks totaling 2.4M noisy instances, AdaBoost-WS improves test AUC by 2.1\u20133.4% over CatBoost and XGBoost while using 30% fewer trees, and scales linearly on 16-core Spark clusters. Ablations show that removing either the adaptive sampling or the auxiliary agreement module degrades performance to within 0.5% of baselines, suggesting both components contribute modestly. While effective on noisy tabular data, preliminary ImageNet experiments with gradient-boosted CNN features yield only marginal gains, highlighting limits to high-dimensional regimes. Theoretically we provide a loose generalization bound that assumes bounded noise but relies on a restrictive data-purity condition unlikely to hold in practice. Code and scripts are publicly available.",
    "id": 20
  },
  {
    "title": "Sharpening the Margins: A Variance-Reduced Frank-Wolfe Variant for Structured Prediction",
    "authors": [
      "Kim, J.",
      "Singh, V.",
      "Andersson, E."
    ],
    "abstract": "Structured prediction tasks with large output spaces often rely on Frank-Wolfe (FW) algorithms for efficient optimization, yet FW's convergence slows as gradient variance increases late in optimization. We propose VR-FW, a simple plug-in variance-reduction step that re-weights the away-step direction using the empirical gradient covariance. Theoretically, we show VR-FW achieves an \ud835\udcaa(1/k\u00b2) convergence rate when the risk is linear-dependency bounded, improving over the standard \ud835\udcaa(1/k). Empirically, on two dependency-parsing datasets and a coarse-to-fine image segmentation benchmark, VR-FW reaches the same objective value 1.2\u20131.7\u00d7 faster than FW. On larger tasks the gap narrows, and runtime overhead becomes noticeable. Code and scripts are provided for reproducibility. While the speed-ups are consistent, our analysis assumes a restrictive curvature bound that may not hold beyond pairwise graphical models, and the method still requires an exact linear oracle at each iteration, limiting scalability to truly massive output spaces. Nevertheless, VR-FW is easy to implement, adds only a SVD on the batch gradient, and offers a practical boost for medium-scale structured learning problems.",
    "id": 21
  },
  {
    "title": "Improved Adam with Layer-wise Learning Rates for Fine-Tuning Large Language Models",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Rodriguez, S."
    ],
    "abstract": "We propose Layer-wise Adaptive Moments optimizer for Fine-tuning (LAMF), a simple modification to Adam that assigns independent learning rates to each transformer layer during supervised fine-tuning of pre-trained language models. Motivated by the observation that gradient statistics vary across layers, LAMF maintains separate exponential moving averages of for each layer, allowing more aggressive updates in upper layers while stabilizing early layers that exhibit larger gradient variance. On the GLUE benchmark, LAMF achieves 0.8% average improvement over Adam with default hyperparameters when fine-tuning RoBERTa-base, and 0.5% gains on RoBERTa-large. The method introduces negligible memory overhead and requires no additional hyperparameter tuning beyond standard Adam. While improvements are consistent across most tasks, gains are marginal on larger models and ablations reveal that benefits diminish with careful learning rate scheduling. Theoretically, we provide convergence guarantees for a simplified version of LAMF in the convex setting. Though incremental, our results suggest that layer-wise adaptive methods may offer a practical alternative to extensive hyperparameter search during fine-tuning, particularly for practitioners with limited computational resources.",
    "id": 22
  },
  {
    "title": "Revisiting Neural Scaling Laws with Limited-Width Ensembles",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, T."
    ],
    "abstract": "Empirical scaling laws predict that transformer performance improves smoothly with model size when compute, data, and parameters are jointly scaled. We investigate whether these trends persist when width is held constant while depth increases\u2014a regime motivated by deployment constraints on embedded GPUs. In experiments on C4 language modeling and ImageNet-1k classification, we train ensembles of up to 768-layer models whose widths are capped at 256 hidden units. Results reveal three deviation patterns: (i) downstream fine-tune accuracy plateaus beyond \u2248400 layers despite continued training loss improvements; (ii) the optimal depth-to-width ratio drifts from the power-law predicted by prior work, suggesting an implicit regularizer absent in over-parameterized networks; and (iii) a simple entropy-based diagnostic anticipates the plateau, enabling early stopping that saves 23% compute on average. Although our study is limited to standard transformer blocks and two domains, the diagnostic offers practitioners a lightweight signal for deciding when growing depth further is wasteful. We release code and up to 600-layer checkpoints to facilitate replication.",
    "id": 23
  },
  {
    "title": "Improved Momentum Schedules for Stochastic Optimization via Online Learning",
    "authors": [
      "Kovacs, S.",
      "Li, J.",
      "Nguyen, T."
    ],
    "abstract": "We revisit the classical heavy-ball momentum method for stochastic optimization and propose a data-adaptive momentum schedule that is learned online during training. Inspired by recent advances in hyperparameter optimization, our method adjusts the momentum coefficient based on the local geometry of the loss surface, approximated through exponential moving averages of gradient statistics. Unlike fixed schedules or hand-tuned baselines, our approach requires no manual intervention while remaining computationally lightweight, adding less than 1% overhead compared to standard SGD with momentum. On a diverse set of convex and non-convex problems\u2014including CIFAR-10/100 image classification and WikiText-103 language modeling\u2014our method achieves marginal but consistent improvements (0.3\u20130.8%) over tuned baselines. Theoretically, we show that under standard smoothness and convexity assumptions, our schedule converges at the same rate as optimal constant momentum up to logarithmic factors. While the empirical gains are modest and the method does not outperform carefully tuned baselines on all tasks, it provides a principled and practical alternative to manual tuning. Code is available at anonymous-repository.github.io.",
    "id": 24
  },
  {
    "title": "Adaptive Gradient Rescaling: A Simple but Modest Improvement for Deep Network Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "We propose Adaptive Gradient Re-scaling (AGR), a lightweight modification to standard optimizers that re-weights parameter updates based on an exponentially smoothed estimate of gradient curvature. While methods like Adam and RMSProp adapt learning-rates coordinate-wise, AGR re-calibrates the global step magnitude by tracking ratio of change in loss to average gradient norm over a sliding window. On CIFAR-10/100 and ImageNet, AGR+SGD yields 0.9\u20131.2% top-1 accuracy gains over tuned SGD+momentum and 0.4% over AdamW at equal wall-clock time, with 5% reduction in validation loss fluctuations. The method adds three lines of code, one extra hyper-parameter (window size w), and negligible memory cost. Ablation shows benefits vanish when curvature signals are noisy. We provide convergence analysis for quadratic objectives, but general theory is limited. Code is included, yet extensive evaluation on larger tasks and downstream transfer is deferred to future work. AGR offers consistent but incremental improvements, making it attractive for practitioners willing to tune an additional hyper-parameter, but its simplicity limits novelty.",
    "id": 25
  },
  {
    "title": "Improving Transformer Optimization via Layer-wise Learning Rate Scaling",
    "authors": [
      "Kim, J.",
      "Rodriguez, C.",
      "Nguyen, P."
    ],
    "abstract": "Training large Transformer models efficiently remains challenging due to optimization instabilities and slow convergence. We propose LayerLR, a simple approach that applies layer-specific learning rate multipliers during training. Our method is motivated by the observation that layers closer to the output exhibit larger gradient magnitudes and thus may benefit from adjusted learning rates. LayerLR scales learning rates based on a layer's depth and attention head configuration, without adding computational overhead or memory costs. We evaluate LayerLR on language modeling tasks using GPT-2 and BERT architectures. Experiments on Wikitext-103 show 2-3% perplexity improvements and 5-10% faster convergence compared to standard training with the same hyperparameters. Additionally, LayerLR slightly improves downstream task performance on GLUE benchmarks. While these gains are consistent across multiple runs, they are relatively modest and task-dependent. Our approach requires minimal code changes and can be easily integrated into existing training pipelines. However, the effectiveness of layer-wise scaling varies with model size and dataset characteristics, suggesting the need for further investigation into when and why the method works. We hope our work encourages more research into adaptive optimization strategies for deep architectures beyond standard gradient-based methods.",
    "id": 26
  },
  {
    "title": "Gradient Norm Clipping with Adaptive Momentum: A Middle-Ground Approach",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Rojas, C."
    ],
    "abstract": "Modern stochastic optimization methods rely on two core heuristics\u2014gradient clipping for stability and heavy-ball momentum for acceleration\u2014yet the interaction between them is poorly understood. We introduce clippedAdam, a simple modification to Adam that adaptively rescales momentum updates using coordinate-wise gradient norms. While prior clipping schemes use a static threshold, clippedAdam learns a separate cutoff for each parameter by online exponential smoothing, yielding automatic robustness to heavy-tailed noise without hand-tuning. On CIFAR-10/100 our method matches the accuracy of well-tuned Adam while reducing gradient variance by 18%; similar gains appear on two language-modeling tasks. Theoretically, we prove O(\u221aT T\u207b\u00b9/\u2074) convergence for smooth non-convex objectives under expected gradient noise, bridging the gap between clipping-only and momentum-only analyses. Although the technique adds one extra hyper-parameter (decay factor \u03b1), we show it is insensitive over two orders of magnitude. Experiments on very large architectures (ViT-B/16, 2.7 B params) reveal diminishing returns, suggesting scalability limits. Code is provided for full reproducibility.",
    "id": 27
  },
  {
    "title": "Revisiting Weight Averaging for Better Flat-Minima Selection",
    "authors": [
      "Nguyen, T.",
      "Klein, J.",
      "Singh, V."
    ],
    "abstract": "Recent empirical work suggests that wide, flat minima generalize better than sharp ones, prompting interest in optimizers that bias trajectories toward flat regions. We revisit the classical idea of exponentially smoothing the iterate (EMA) and uncover a surprising bias: when the damping coefficient is tuned just below the divergent limit, successive averages accumulate along an approximate eigenvector of the Hessian at the optimum, yielding a flat-minimum solution without modifying the underlying optimizer. Building on this, we propose Flat-Seeking Iterate Averaging (FSIA), a post-hoc wrapper that performs only three extra full-backward passes at the end of training to choose the flattest among the averaged candidates. On CIFAR-10/100 and ImageNet with ResNet-18/50, FSIA lowers the sharpness by 15\u201322% and improves top-1 accuracy by 0.4\u20130.7pp over vanilla SGD momentum baselines, while incurring <0.01% overhead. Theoretical analysis in a quadratic toy model characterizes when the bias emerges, but the story is less complete for deep nets. Code, code-anonymized, and checkpoints are provided.",
    "id": 28
  },
  {
    "title": "On the Role of Temperature Scaling in Self-Supervised Representation Learning",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Gonzalez, J."
    ],
    "abstract": "Temperature scaling is a common component in contrastive self-supervised learning, yet its precise role in representation quality remains poorly understood. We systematically study how the temperature parameter influences learned features across three popular contrastive methods on ImageNet and CIFAR-10. Our theoretical analysis reveals a connection between temperature and feature space uniformity, providing a lower bound on downstream linear separability. Empirically, we find that optimal temperature values correlate strongly with dataset granularity and augmentation diversity. Based on these observations, we propose an adaptive temperature schedule that adjusts based on feature cosine similarity distributions during training. Across five benchmarks, our method improves linear accuracy by 0.8-1.2% over fixed temperature baselines. While the gains are moderate, our findings suggest that temperature acts as an implicit regularizer controlling the tradeoff between feature diversity and instance discrimination. The simplicity and computational efficiency of our approach make it practical for large-scale pretraining. Our results indicate that careful temperature tuning, while not a silver bullet, can provide complementary benefits to architectural improvements in self-supervised learning.",
    "id": 29
  },
  {
    "title": "Improving Gradient Alignment in Federated Learning via Cyclic Learning Rate Scheduling",
    "authors": [
      "Liu, H.",
      "Nguyen, T.",
      "Kumar, A."
    ],
    "abstract": "Client drift remains a central challenge in federated decentralized optimization, where non-IID data and partial participation distort local updates and slow convergence. We propose FedCycle, a simple extension to FedAvg that cyclically periods the server learning rate across rounds. Intuitively, high rates accelerate initial consensus while low rates later refine the aggregate, yielding tighter gradient alignment without extra communication. On CIFAR-10, FedCycle improves final accuracy by 1.4 pp and halves the variance of client updates compared to the best constant schedule. Convergence guarantees are provided under bounded gradient dissimilarity, matching standard rates up to constants. Code is released for full reproducibility.",
    "id": 30
  },
  {
    "title": "Revisiting Gradient Clipping in Private Stochastic Optimization: When Theory Meets Practice",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Johnson, M."
    ],
    "abstract": "Gradient clipping is a standard ingredient in differentially private (DP) stochastic optimization, used to bound per-sample gradient norms and calibrate noise. While clipping is routinely motivated by worst-case Lipschitz constants, empirical distributions in vision and language tasks are often tight, suggesting much of the clipping is unnecessary. We ask: can we reduce clipping bias while preserving DP guarantees? We propose Adaptive-Clipped DP-SGD (AC-SGD), a simple variant that re-scales gradients by learned, data-dependent quantiles rather than a fixed cap. AC-SGD performs two passes: a private estimate of the q-th quantile of per-sample norms, followed by DP-SGD with a clipping threshold set to the estimated quantile plus a small slack. On CIFAR-10/100 and StackOverflow next-word prediction, AC-SGD improves test accuracy by 0.6\u20131.2 pp over tuned baselines at the same (\u03b5, \u03b4) privacy budget and incurs comparable runtime. Ablation shows most gains stem from clipping only the tail of the norm distribution. Theoretical analysis reveals a bias-variance trade-off: AC-SGD introduces a small negative bias bounded with high probability, while reducing variance from excess noise. Limitations include additional hyper-parameter q and lack of convergence rates under heavy-tailed gradients. Code and experiments are available in supplemental material, though full hardware specifications are omitted due to space.",
    "id": 31
  },
  {
    "title": "Residual Temperature Scaling: Post-Hoc Confidence Calibration for Black-Box Neural Networks",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "Well-calibrated predictive are essential for trustworthy deployment, yet modern deep networks tend to output over-confident probabilities. We revisit temperature scaling\u2014the simplest post-hoc calibration rule\u2014and reveal a systemic under-correction when the network\u2019s own embeddings drift from the target domain. Motivated by this observation, we propose Residual Temperature Scaling (RTS), a lightweight wrapper that refits a scalar temperature on held-out data, then adds a small residual correction obtained from a linear model on penultimate-layer features. On standard image and text benchmarks, RTS yields average ECE reductions of 5.3% over plain temperature scaling with no extra cost at test time, and requires less than 0.6% additional parameters. We accompany RTS with a finite-sample generalization bound that predicts calibration error within \u00b10.6% on CIFAR-100. Code and trained models are released to facilitate reproduction. While empirical gains are incremental, our analysis sheds new light on the role of feature drift in calibration and offers practitioners a drop-in upgrade that is compatible with existing checkpoints.",
    "id": 32
  },
  {
    "title": "Revisiting Sample Re-weighting in Mini-batch SGD: A Skew-Gradient Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Sample re-weighting is a classical technique for handling label noise or class imbalance, yet its interaction with modern mini-batch SGD remains poorly understood. We propose Skew-GD, a simple re-weighting rule that distorts the per-sample gradient distribution rather than the loss surface. Under the assumption that clean and noisy examples exhibit different gradient kurtosis, Skew-GD assigns weights inversely proportional to a running estimate of gradient kurtosis raised to a learnable temperature. On CIFAR-10/100 with 40% symmetric label noise, Skew-GD improves test accuracy by 2.1% over standard baselines and 0.7% over the recent state-of-the-art MAE-based filter, while adding only 3% wall-clock overhead. Ablations show the temperature parameter automatically interpolates between uniform weighting (high temperature) and hard thresholding (low temperature). Theoretically, we prove that Skew-GD converges at the same O(1/T) rate as vanilla SGD up to a constant factor that depends on the kurtosis gap. Limitations include reliance on the kurtosis assumption, sensitivity to batch size, and lack of improvement on real-world noisy datasets without synthetic corruption. Code is available at anonymous-url.",
    "id": 33
  },
  {
    "title": "Residual Gradient Correction for Variance Reduction in Off-Policy Actor-Critic Methods",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Anderson, M."
    ],
    "abstract": "Off-policy actor-critic algorithms suffer from high variance due to importance sampling ratios, leading to unstable training. We propose a simple residual gradient correction term that re-weights policy gradients based on the magnitude of value function residuals. Our method can be integrated into any existing actor-critic framework with minimal computational overhead. We provide theoretical analysis showing the correction term reduces variance under certain smoothness assumptions, though we note these assumptions may not hold for all MDPs. Experimental results on continuous control tasks show modest improvements over SAC and TD3 baselines, with 5-8% average return gains on half of the MuJoCo environments tested. However, performance gains are inconsistent across tasks, with several environments showing no improvement or slight degradation. The method adds two hyperparameters that require task-specific tuning, limiting its practical applicability. While our approach offers a theoretically grounded perspective on variance reduction, the empirical improvements are incremental and may not justify the added complexity for practitioners. Code is available at anonymized-repository-url.",
    "id": 34
  },
  {
    "title": "Gradient Noise Revisited: A Second-Order Tail-Averaging Perspective",
    "authors": [
      "Koh, J.",
      "Murthy, S.",
      "Ahmadi, A."
    ],
    "abstract": "Modern stochastic optimization relies heavily on gradient noise to escape sharp minima, yet theoretical understanding of how noise structure affects generalization remains incomplete. We revisit the classical continuous-time Langevin model and introduce a lightweight second-order tail-averaging scheme that re-weights noisy gradient updates by the inverse local Fisher information. Empirically, the procedure matches the computational cost of standard SGD while yielding marginally flatter minima on CIFAR-10/100. Theoretically, we prove that under the (restrictive) additive Gaussian gradient noise assumption, tail-averaging achieves a convergence bound of O(1/T^1.2), slightly improving the vanilla SGD rate. Ablation studies reveal sensitivity to batch size and learning-rate schedule, suggesting diminishing returns without careful tuning. While our analysis is limited to full-batch settings and the Gaussian noise assumption is rarely met in practice, we hope our reformulation sparks renewed interest in exploitingiting curvature information for improved generalization.",
    "id": 35
  },
  {
    "title": "Revisiting Dropout for Uncertainty Estimation: A Kernel Perspective",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Adams, P."
    ],
    "abstract": "Dropout is widely used both as a regularizer and as an inexpensive way to obtain model uncertainty, yet the relationship between these two roles remains poorly understood. We reinterpret dropout as an implicit kernel approximation that trades representational capacity for regularization. By deriving the corresponding kernel in closed form for single-hidden-layer networks, we show that standard Monte-Carlo dropout corresponds to a biased estimator of model variance. We propose a simple correction term that reduces this bias without additional forward passes. Empirically, on small-scale regression benchmarks (UCI and toy datasets), our adjusted dropout yields better-calibrated confidence intervals while maintaining the same computational cost. Theoretical results are confined to shallow networks, but ablations suggest the correction continues to help deeper architectures. While our contribution is incremental, it clarifies an implicit assumption in a common approximation and offers a drop-in refinement for uncertainty estimation in resource-constrained settings.",
    "id": 36
  },
  {
    "title": "Improved Gradient Noise via Weight-Shuffling for Faster SGD Convergence",
    "authors": [
      "Kim, J.",
      "Navarro, S.",
      "Thompson, L."
    ],
    "abstract": "Stochastic Gradient Descent (SGD) remains the workhorse of large-scale machine learning, yet its convergence rate often plateaus in practice due to gradient staleness and correlated noise. We propose Weight-Shuffled SGD (WS-SGD), a simple modification that periodically randomizes the internal ordering of model parameters prior to gradient computation. This shuffling acts as a lightweight gradient noise augmentation, decorrelating successive updates without altering the loss landscape or requiring additional data. Across CIFAR-10/100 and ImageNet, WS-SGD accelerates early-phase training by 1.12\u00d7 on average and yields a 0.4% top-1 boost, while keeping computational overhead below 2%. Theoretically, we show that shuffling reduces the maximum eigenvalue of the gradient covariance matrix under a block-independence assumption, implying a tighter convergence bound in non-convex settings. WS-SGD can be implemented in 7 lines of PyTorch and integrates seamlessly with momentum, Adam, and LARS. While improvements are consistent, they are modest and most pronounced in small-batch regimes; in large-batch training the gains vanish, suggesting limited scalability. We release code and hyper-parameter presets to encourage reproducibility.",
    "id": 37
  },
  {
    "title": "AdaSmooth: An Adaptive Smoothing Regularizer for Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose AdaSmooth, a lightweight regularization technique that adaptively smooths the penultimate layer activations of deep neural networks during training. Motivated by the empirical observation that label noise often correlates with abrupt changes in intermediate representations, AdaSmooth applies a data-dependent smoothing penalty whose strength scales with the local neighborhood variance in latent space. Unlike classical L2 regularization or label-smoothing, our method requires no additional hyper-parameters beyond a single temperature coefficient and adds negligible computational overhead. Experiments on CIFAR-10/100 and ImageNet-transfer tasks show consistent but modest accuracy gains (0.3-0.8%) across ResNet and Vision-Transformer architectures, particularly under 10%-20% label noise. Analyses indicate improved calibration and slightly flatter loss landscapes; however, gains vanish when training data are clean or severely corrupted (>40% noise). While AdaSmooth is simple to implement and may prove practically useful for noisy, real-world vision datasets, our theoretical motivation remains heuristic and evaluation is limited to image classification. Code is available at anonymous-url.",
    "id": 38
  },
  {
    "title": "Accelerating Transformer Training with Progressive Gradient Dropout",
    "authors": [
      "Liu, J.",
      "Banerjee, S.",
      "Nguyen, T."
    ],
    "abstract": "Large transformer models require extensive compute budgets, motivating methods that speed training without hurting final accuracy. We introduce Progressive Gradient Dropout (PGD), a simple modification to standard stochastic gradient optimizers that probabilistically drops gradient coordinates during back-propagation. PGD gradually increases the dropout rate according to a scheduled cooling function, letting early updates use dense signals while later updates focus on salient directions. Across 6 GLUE tasks, PGD speeds pre-training by 9\u201314% (wall-clock) and downstream fine-tuning by 6\u201310% relative to AdamW with no hyper-parameter retuning. Ablation experiments show the schedule shape matters more than the maximal rate. Theoretical analysis in a stylized quadratic setting shows PGD implicitly performs coordinate-wise adaptive regularization, offering partial explanation for its empirical stability. While gains are consistent, they remain modest on larger architectures (\u22651.1B parameters) and do not transfer to black-box optimizers such as LAMB. Code and configurations are released to ensure reproducibility.",
    "id": 39
  },
  {
    "title": "Improving Transformer Training with Layer-wise Gradient Dropout",
    "authors": [
      "Liu, J.",
      "Okafor, C.",
      "Kim, D."
    ],
    "id": 40,
    "abstract": "Training deeper Transformer models often suffers from gradient instability and slow convergence. We propose Layer-wise Gradient Dropout (LGD), a simple regularization technique that randomly masks gradient updates at different layers during each training step. Unlike standard dropout applied to activations, LGD operates directly on back-propagated gradients with layer-specific dropout rates. Our experiments on small-to-medium scale language modeling tasks (up to 280M parameters) show 3-5% perplexity improvements over vanilla Transformers. Theoretical analysis reveals LGD provides an implicit regularization that encourages better gradient conditioning. However, benefits diminish with models larger than 300M parameters, and we observe minimal gains on vision tasks. While LGD introduces minimal computational overhead and can be implemented with 10 lines of PyTorch code, the improvements are incremental and the theoretical analysis assumes restrictive conditions. Code and experiments will be made available upon acceptance."
  },
  {
    "title": "Gradient-Noise Annealing for Improved Optimization in Shallow Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose gradient-noise annealing (GNA), a simple technique that injects decaying isotropic Gaussian noise into stochastic gradient updates, re-purposing the classic simulated-annealing heuristic for modern mini-batch optimizers. While noise injection is known to help non-convex optimization, existing schedules are either problem-specific or require costly posterior re-weighting. GNA introduces a universal temperature schedule driven only by the norm of the exponentially averaged gradient, removing tunable hyper-parameters across tasks. On three small-scale vision and tabular benchmarks we observe 1\u20132% test-accuracy gains over vanilla SGD and 0.5\u20131% over entropy-SGLD, paired with faster initial convergence. Theoretically, we prove that GNA with appropriate decay rates converges to first-order stationary points at the standard O(1/T) rate, although the constants are slightly looser than SGD. Ablation studies show that most benefits stem from early-training exploration, after which the noise effectively switches off. While improvements on deeper architectures (ResNet-50, GPT-2) were marginal and within error bars, GNA incurs essentially zero overhead and can be implemented in fewer than ten lines of PyTorch code. Our results suggest that controlled, schedule-free noise can still benefit resource-constrained settings, but its practical impact in the high-capacity regime remains limited.",
    "id": 41
  },
  {
    "title": "Improved Gradient Noise Scaling for Large-Batch Training with a Modified Learning-Rate Warmup Schedule",
    "authors": [
      "Jiang, K.",
      "Okafor, C.",
      "M\u00fcller, H."
    ],
    "abstract": "Large-batch stochastic optimization can speed up neural-network training but often leads to accuracy loss. Existing noise-scaling rules adjust learning rates by assuming gradient noise is isotropic; however, empirical evidence shows that this assumption degrades as batch size grows. We propose Coordinated Noise Scaling (CNS), a simple modification that incorporates layer-wise gradient covariances estimated from the first handful of training steps. Coupled with a piecewise-linear warmup schedule that slows the initial learning-rate ramp, CNS preserves the total number of optimizer updates while yielding consistent loss trajectories across batch sizes. On ImageNet with ResNet-50, scaling the batch size from 256 to 4,096, CNS recovers 0.62% top-1 accuracy compared to standard linear scaling and matches small-batch performance after 90 epochs. Experiments on IWSLT14 German-English with the Transformer small model show similar, albeit smaller, gains (0.3 BLEU). We also derive a heuristic bound relating the number of covariance samples to the batch-size multiplier, offering limited theoretical support. Despite gains on canonical benchmarks, we observe negligible improvements when training longer or with stronger regularization. Our PyTorch implementation, requiring fewer than 30 lines of code, is included in the supplementary material. Overall, CNS is a lightweight extension to existing large-batch recipes that can modestly improve accuracy but appears less beneficial in regimes where stronger data augmentation or extended training schedules are employed.",
    "id": 42
  },
  {
    "title": "Gradient Noise Re-scaling: A Lightweight Alternative to BatchNorm for Small Batch Training",
    "authors": [
      "Chen, H.",
      "Agrawal, S.",
      "O'Connor, B."
    ],
    "abstract": "We propose Gradient Noise Re-scaling (GNR), a simple plug-in module that stabilizes training of deep networks with small batch sizes. Motivated by the observation that gradient noise magnitude scales inversely with batch size, GNR adaptively re-scales gradient updates based on an online estimate of gradient variance. Unlike BatchNorm, GNR does not rely on batch statistics, making it applicable to recurrent architectures and online learning settings. We show that GNR can recover much of the performance lost when reducing batch size from 128 to 4 on CIFAR-10/100, achieving within 2% of large-batch accuracy across standard architectures. However, we find that GNR interacts poorly with strong data augmentation and is less effective on high-resolution datasets. Theoretical analysis shows that GNR converges to similar stationary points as large-batch training under certain assumptions, though these assumptions are restrictive for common architectures. Code is available at https://github.com/HMChen-Fake/GNR.",
    "id": 43
  },
  {
    "title": "Improved Generalization via Layer-wise Learning Rate Warm-up for Fine-tuning Vision Transformers",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Fine-tuning pre-trained Vision Transformers (ViTs) often leads to overfitting on small datasets, limiting their applicability in data-scarce scenarios. While various regularization techniques have been proposed, the initial training phase has received limited attention. We present Layer-wise Learning Rate Warm-up (LLRW), a simple technique that applies staggered learning rate schedules across different layers of ViTs during early fine-tuning. By gradually unfreezing layers with increasing learning rates, LLRW encourages the model to first adapt high-level features before modifying low-level representations. Our experiments on CIFAR-10, CIFAR-100, and ImageNet subsets show that LLRW improves test accuracy by 1.2-2.1% over standard fine-tuning with minimal computational overhead. Despite its simplicity, our method reveals an interesting phenomenon: the order of layer unfreezing impacts feature reuse efficiency. While our results are consistent across multiple ViT variants, we acknowledge that the improvements are modest and most pronounced on smaller datasets. We provide PyTorch code and hyperparameter configurations for reproducibility. This work suggests that careful initialization strategies for fine-tuning merit further investigation, though the practical impact may be incremental compared to architectural innovations.",
    "id": 44
  },
  {
    "title": "Improved Gradient Estimators for Discrete Variational Inference via Continuous Relaxations with Learned Temperature",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Chen, Y."
    ],
    "abstract": "Discrete latent variables are fundamental to many probabilistic models, but learning remains challenging due to the high variance of gradient estimators. We propose a simple modification to continuous relaxation methods that automatically learns a temperature parameter for each discrete variable during training. Our key insight is that the optimal relaxation temperature depends on the local curvature of the loss landscape, which can be estimated efficiently using a low-rank approximation of the Hessian. Experiments on text generation and neural architecture search show modest improvements over fixed-temperature relaxations, reducing gradient variance by 15-20% on average. While our method falls short of state-of-the-art discrete optimization techniques on benchmark tasks, it offers a lightweight alternative that requires minimal code changes and adds negligible computational overhead. Theoretical analysis reveals that our adaptive temperatures converge to a stationary point under standard assumptions, though we rely on several approximations that may not hold in practice. Code is available at anonymous-link.github.io.",
    "id": 45
  },
  {
    "title": "Gradient-Noise-Averaged Adam: A Lightweight Learning-Rate Scheduler Without the Learning-Rate",
    "authors": [
      "Kumar, A.",
      "Orlov, V.",
      "Nguyen, H."
    ],
    "abstract": "Adaptive optimizers such as Adam and RMSProp rescale gradients element-wise to stabilise training, yet they still require practitioners to manually tune the base learning-rate. We revisit a long-overlooked property of mini-batch gradients\u2014sample noise\u2014to ask whether the optimal learning-rate can be predicted on-the-fly instead of grid-seched. We propose Gradient-Noise-Averaged Adam (GNA-Adam), a drop-in wrapper that maintains an exponential moving average of the squared noise-to-signal ratio across consecutive parameter updates. When the ratio shrinks, GNA-Adam automatically increases the effective step-size; when it grows the step-size is curtailed. On CIFAR-10/100 and ImageNet-1k, GNA-Adam matches a hand-tuned Adam schedule within 1.2% validation accuracy after one hour of search-free training. Theoretical analysis shows a convergence rate O(log T/\u221aT T) for smooth convex objectives. While GNA-Adam cannot provide a universal rate guarantee or remove all hyper-parameters (\u03b21,\u03b22 remain), it eliminates the single most expensive one\u2014the initial learning-rate\u2014at the cost of 2% extra memory. Experiments on GLUE language fine-tuning demonstrate reduced sensitivity to learning-rate choice, suggesting practical value for entry-level users.",
    "id": 46
  },
  {
    "title": "Revisiting Entropy Regularization in Policy Gradient Methods with Adaptive Temperature Schedules",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Entropy regularization is widely used in reinforcement learning to encourage exploration and prevent premature convergence. While fixed temperature parameters are common, we propose an adaptive entropy schedule that dynamically adjusts the regularization strength based on policy performance. Our method tracks the entropy of the policy distribution and modulates the temperature parameter using a simple proportional controller. We evaluate our approach on continuous control tasks from the MuJoCo suite and find modest improvements in sample efficiency, achieving 8-12% faster convergence on half of the environments tested. Theoretical analysis shows that our adaptive schedule preserves the original convergence guarantees under standard assumptions, though we note that the practical benefits appear limited to environments where exploration is particularly challenging. While our contributions are primarily incremental, we believe this work provides a useful ablation study on the role of entropy scheduling in modern policy optimization. Code is available online, though we acknowledge that hyperparameter sensitivity remains a significant limitation that may hinder broader adoption.",
    "id": 47
  },
  {
    "title": "Revisiting Reset Schedules for Policy Optimization: When Warm Starts Meet Periodic Restarts",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Policy restart strategies are commonly used in deep reinforcement learning to escape local optima, yet the theoretical underpinnings of when and how to restart remain poorly understood. We revisit the classic optimization idea of warm-started periodic restarts and adapt it to policy gradient methods. Our method, WR-PO, periodically resets the policy to a geometric mixture of the current iterate and an earlier checkpoint, with restart frequency and mixing coefficient chosen via a simple grid-search heuristic. On a suite of nine continuous-control tasks, WR-PO improves average return over the vanilla PPO baseline by 4.7% with similar sample complexity, and matches SAC on four of nine tasks. Ablation studies indicate that the benefit is largest in environments with sparse rewards, suggesting that controlled resets help exploration. While our approach is easy to implement and yields consistent gains, the improvements are incremental and the heuristic nature of the schedule limits generality. We provide partial convergence guarantees under strong convexity assumptions, but the general case remains open. Code and 50 random seeds are provided to ensure reproducibility.",
    "id": 48
  },
  {
    "title": "Improved Gradient Bounds for Smooth Non-Convex Games via Recursive Variance Reduction",
    "authors": [
      "Chen, L.",
      "Kim, J.",
      "Rossi, M."
    ],
    "abstract": "We study first-order algorithms for computing approximate Nash equilibria in two-player, smooth non-convex games. While gradient-descent-ascent (GDA) is widely used, its last-iterate convergence guarantees remain brittle: constant step-sizes can cycle, and decreasing step-sizes yield slow O(1/T) rates. Motivated by recent variance-reduced methods for non-convex optimization, we propose a recursive gradient estimator that reuses stale stochastic gradients with a simple trust-region-style correction. The resulting algorithm, VR-GDA, maintains the same per-iteration complexity as GDA while improving the convergence bound from O(1/T) to O(1/T^1.25) in terms of expected squared gradient norm. On three standard testbeds\u2014smooth bilinear games, Dirac-GAN, and a synthetic zero-sum quadratic\u2014we observe 15\u201330% faster reduction in gradient norms compared to tuned GDA and optimistic variants, without significant hyper-parameter tuning. Theoretically, our analysis relies on a refined Lyapunov function that couples gradient variance with iterate distance; we conjecture the rate is tight for the class of algorithms that use only stochastic gradient queries. Limitations include the need for Lipschitz-gradient oracles and a bounded-domain assumption; extensions to constrained or nonsmooth settings are deferred to future work. Code is provided for reproducibility.",
    "id": 49
  },
  {
    "title": "Improved Gradient Alignment for Meta-Learning with Noisy Labels",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Meta-learning algorithms often assume clean training data, but real-world tasks frequently contain label noise that degrades few-shot performance. We study the interaction between gradient-based meta-learning and noisy labels, revealing that corrupted support examples disproportionately bias task-specific adaptation. To address this, we propose Gradient Alignment with Noise Suppression (GAINS), a simple plug-in that re-weights support gradients according to their agreement with a small trusted clean set. On two benchmarks, GAINS improves 5-way 1-shot accuracy by 2.3% under 20% symmetric noise and by 1.8% under more realistic asymmetric noise, while adding negligible overhead. Ablations show the clean set can be as small as 5% of the support size. Although gains are incremental and experiments are limited to sinusoidal regression and mini-ImageNet classification, our work highlights an under-explored robustness issue in meta-learning and provides a lightweight mitigation strategy. Code will be made available.",
    "id": 50
  },
  {
    "title": "AdaMoPro: Adaptive Momentum Scheduling for Proximal Policy Optimization with Generalized Advantage Estimation",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Kowalski, J."
    ],
    "abstract": "Proximal Policy Optimization (PPO) remains a dominant algorithm for reinforcement learning, yet its sample efficiency is hindered by fixed momentum parameters that fail to adapt to varying gradient noise across training phases. We propose AdaMoPro, an adaptive momentum scheduling scheme that dynamically adjusts the momentum coefficient based on a running estimate of the gradient signal-to-noise ratio within the generalized advantage estimation (GAE) framework. Our method introduces a lightweight meta-controller that modifies momentum every K steps without additional environment interactions. Across 12 continuous control tasks from DeepMind Control Suite and 6 robotic manipulation environments, AdaMoPro achieves a 7.3% median improvement in sample efficiency compared to standard PPO with fixed momentum (0.9), while maintaining stable training curves. Ablations reveal that the benefits are most pronounced in tasks with sparse rewards or high-dimensional action spaces. Although our approach introduces two new hyperparameters (meta-learning rate K), we show that performance is robust to their choice within reasonable ranges. AdaMoPro can be implemented in 15 lines of code and integrates seamlessly with existing PPO implementations, providing a practical enhancement for practitioners seeking modest gains in sample efficiency without architectural changes.",
    "id": 51
  },
  {
    "title": "Revisiting Momentum with Look-ahead Model Predictions",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Nguyen, T."
    ],
    "abstract": "Momentum-based optimizers remain the de-facto standard for training deep networks, yet their theoretical benefits often vanish under practical hyper-parameter choices. We propose Look-ahead Momentum (LaM), a simple modification that computes the momentum update using a one-step model prediction rather than the current gradient. By linearly extrapolating the loss landscape, LaM adaptively mixes current and future gradient information, yielding improved convergence on sharp loss surfaces. While similar in spirit to extrapolation methods like Lookahead and extragradient optimizers, LaM requires no additional forward passes or inner-loop updates, making it essentially free at runtime. We prove O(1/T) convergence for LaM under standard convexity assumptions, matching classical momentum rates, and show empirical improvements on CIFAR-10/100 and ImageNet across ResNet and Vision Transformer architectures. However, gains are inconsistent across tasks: LaM marginally improves final accuracy (+0.3-0.7%) on vision transformers but shows negligible benefits on smaller ResNet models. Additionally, LaM introduces two new tunable hyper-parameters whose optimal values vary across datasets, complicating practical adoption. Our findings suggest that while look-ahead gradient information can modestly improve optimization, the benefits are task-dependent and may not justify the added complexity for practitioners seeking robust, plug-and-play optimizers.",
    "id": 52
  },
  {
    "title": "Momentum-Scheduled Sharpness-Aware Minimization for Better Generalization",
    "authors": [
      "Kumar, S.",
      "Nguyen, T.",
      "Garcia, L."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising optimization technique for improving generalization by seeking flat minima. However, we observe that the standard SAM update rule can be overly conservative early in training and insufficiently adaptive later on. We propose Momentum-Scheduled SAM (MS-SAM), which dynamically adjusts the perturbation radius based on the momentum history of gradients. Our key idea is to schedule the perturbation magnitude according to the stability of the optimization trajectory, allowing for more aggressive exploration initially and finer adjustments as training progresses. We provide theoretical analysis showing that MS-SAM converges at the same rate as SAM under standard assumptions. Empirically, we evaluate MS-SAM on CIFAR-10, CIFAR-100, and ImageNet with ResNet and Vision Transformer architectures, achieving 0.3-0.7% improvements over SAM. While these gains are consistent, they remain relatively modest. Additionally, our method introduces two new hyperparameters that require careful tuning. Our code is available at [anonymous link].",
    "id": 53
  },
  {
    "title": "Improving Transformer Generalization with Curriculum-Based Positional Encoding",
    "authors": [
      "Liu, K.",
      "Nguyen, T.",
      "Kowalski, M."
    ],
    "abstract": "Positional encodings are crucial for Transformer architectures, yet their role in generalization remains poorly understood. We hypothesize that abrupt exposure to long sequences hampers extrapolation to longer contexts. To address this, we introduce Curriculum Positional Encoding (CPE), a simple schedule that progressively increases the maximum sequence length during training. CPE first trains on short subsequences with truncated absolute sinusoidal encodings, then gradually lengthens them until the target length is reached. Our experiments on language modeling, image completion, and symbolic music generation show that CPE yields perplexity reductions of 1\u20132 % over standard fixed encodings on sequences up to 4 k tokens, and improves zero-shot extrapolation by \u2248 5 % when tested on 8 k tokens. Ablations reveal that gradual length progression matters more than the specific encoding variant. While our improvements are modest and diminish on very long contexts (>16 k), CPE adds no inference cost and can be implemented in < 20 lines of code. We provide PyTorch code and trained checkpoints to support reproducibility.",
    "id": 54
  },
  {
    "title": "Improved Gradient Bounds for Deep ReLU Networks via Layerwise Learning Rate Modulation",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Nguyen, T."
    ],
    "abstract": "Training stability in deep ReLU networks is often hampered by vanishing and exploding gradients, especially in architectures exceeding 100 layers. We propose Layer-Adaptive Gradient Scaling (LAGS), a simple plug-in method that modulates per-layer learning rates based on the norm of weight gradients during backpropagation. Unlike carefully-tuned warmup schedules or expensive second-order approximations, LAGS requires one extra multiply per layer and no hyper-parameter search when a default damping constant is used. On CIFAR-100 and ImageNet subsets, ResNet-152 coupled with LAGS achieves 0.8% higher top-1 accuracy and 12% lower loss fluctuation than the standard SGD baseline trained for the same epochs. Ablation studies indicate that most benefits concentrate in early layers, suggesting LAGS primarily corrects low-level gradient magnitudes. Theoretically, we bound the expected gradient norm at arbitrary depth by a factor that exponentially improves with our modulation strength; however, the bound relies on a sub-Gaussian weight initialization that is stronger than usual He initialization. Although the technique is straightforward to implement, its advantage vanishes when batch-size is scaled above 512 or when used with Adam-like optimizers that already employ second-moment normalization. Code is available at anonymized-repo.github.io/lags.",
    "id": 55
  },
  {
    "title": "Gradient Amplification for Stabilizing GAN Training with Uneven Learning Rates",
    "authors": [
      "Garcia, M.",
      "Kumar, S.",
      "Thompson, L."
    ],
    "abstract": "We propose gradient amplification, a simple plug-in technique that re-weights generator and discriminator updates in GANs when the two networks are trained with different learning rates\u2014a common practical heuristic that often leads to unstable oscillations. Starting from a local bilinear game approximation, we derive a closed-form coefficient that amplifies generator gradients when the discriminator learns faster and shrinks them in the opposite regime. On CIFAR-10, gradient amplification improves Inception score from 6.18 \u00b1 0.12 to 6.43 \u00b1 0.07 without architectural changes, while on the more challenging 128\u00d7128 ImageNet subset it reduces training FID by 8.2%. Ab but only 56% of runs still diverge, suggesting that higher-order dynamics remain unaccounted for. The method introduces one extra hyper-parameter \u03b2; while \u03b2 = 1/2 works well in all experiments, we lack a principled automatic tuning rule. Theoretical analysis is limited to a two-parameter quadratic game and does not extend to general non-convex settings. Despite these limitations, gradient amplification can be implemented in 5 lines of code and integrates seamlessly with existing optimizers, offering practitioners a lightweight stabilizer for large-scale GAN training.",
    "id": 56
  },
  {
    "title": "Residual Correction Networks: Iterative Refinement with Learned Fixed-Point Updates",
    "authors": [
      "Liu, M.",
      "Chen, S.",
      "Johnson, K."
    ],
    "abstract": "We introduce Residual Correction Networks (RCNs), a framework for improving the accuracy of pre-trained neural networks through learned iterative refinement. RCNs treat the original network\u2019s predictions as an initial estimate and train a secondary network to iteratively compute additive corrections by learning the fixed-point iteration dynamics of residual errors. This approach is motivated by classical numerical methods for solving nonlinear systems, where successive residual corrections converge to a better solution. Our method is architecture-agnostic and can be applied to any pre-trained model without modifying its parameters. We evaluate RCNs on image classification benchmarks (CIFAR-10, ImageNet) and a language modeling task (WikiText-2), demonstrating consistent but modest improvements over baseline models (0.8-1.2% accuracy gains, 2-3% perplexity reduction). While RCNs are easy to implement and training is stable, our theoretical analysis reveals limited expressivity: the correction network must approximate the inverse of the Jacobian of the original model, which is inherently constrained for deep networks. Additionally, inference cost increases linearly with the number of refinement steps. Our results suggest RCNs provide a practical but incremental improvement, with clearer benefits in low-data or noisy-label regimes where initial model bias is pronounced.",
    "id": 57
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Autoencoders via Rao-Blackwellized Control Variates",
    "authors": [
      "Liu, C.",
      "Nguyen, T.",
      "Roberts, J."
    ],
    "abstract": "Discrete latent-variable models offer interpretable representations but remain challenging to train due to high-variance gradient estimates. We propose RB-REINFORCE, a control-variate gradient estimator that Rao-Blackwellizes the REINFORCE baseline by conditioning on partial trajectory information. Our method interpolates between the fully factorized RELAX baseline and the unstructured REINFORCE estimator, yielding a bias-variance trade-off controllable via a temperature hyperparameter. Experiments on binarized MNIST and text generation show 5\u201312 % reductions in gradient variance versus RELAX with comparable computational cost, translating to modest improvements in negative log-likelihood. While our estimator does not achieve the sample efficiency of continuous relaxations, it remains unbiased and requires no auxiliary networks. Extensive ablations indicate that gains are limited to settings with sparse rewards and small latent spaces; on richer tasks such as structured VAEs with hundreds of discrete units, improvements are within error bars. Code and checkpoints are provided for reproducibility.",
    "id": 58
  },
  {
    "title": "Improved Smoothing Schedules for Diffusion Models via Learned Timestep Reweighting",
    "authors": [
      "Kim, J.",
      "Singh, P.",
      "Nguyen, T."
    ],
    "abstract": "Denoising diffusion probabilistic models (DDPMs) deliver high-quality samples but require hundreds of forward-noising steps, slowing both training and generation. While recent works tune the noise scheduler by hand, we ask whether the effective weight of each timestep can instead be learned. a novel training objective we term Timestep Importance-weighted ELBO (TIELBO), we jointly optimize the variational lower bound with respect to both network parameters and a per-timestep scalar weight. The reweighting is regularized with a simplex constraint to keep the scheduler interpretable. On CIFAR-10 and CelebA-HQ we obtain marginal log-likelihood improvements of 0.09 and 0.14 bits per dim, respectively, while reducing sampling steps by 15%. Abl analysis reveals that the learned schedule allocates additional capacity to mid-noise levels, corroborating prior heuristic designs. However, the gains plateau after 200 timesteps and the proposed objective increases training time by 30%. Theoretically, we prove that for linear diffusion the reweighted objective is a tighter bound than ELBO, yet the improvement vanishes when the true data manifold is perfectly recovered. Overall, our work offers a lightweight plug-in that mildly accelerates diffusion models, but the benefits appear dataset-specific and may not justify the added hyper-parameter burden.",
    "id": 59
  },
  {
    "title": "Improved Sharpness-Aware Minimization with Momentum-Enhanced Gradient Averaging",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Kumar, V."
    ],
    "abstract": "We propose Momentum-Enhanced Sharpness-Aware Minimization (ME-SAM), a drop-in replacement for stochastic optimizers that encourages convergence to flat minima by periodically averaging stochastic gradients over small parameter neighborhoods. ME-SAM builds on Sharpness-Aware Minimization (SAM) but replaces its two-backward-pass strategy with an efficient momentum-based gradient buffer that reuses past gradients. We derive a convergence bound for \u00b5-nonconvex objectives that improves the leading constant of \u03a9(1/\u221a) dependence in prior work to \u00d5(1/\u03b5^2) under a bounded-intermediate-gradient assumption. Empirically, ME-SAM matches SAM\u2019s test accuracy on CIFAR-10/100 and ImageNet while cutting wall-clock training time by 14\u201322%. We also show that ME-SAM provides a 0.7\u20131.1% boost over vanilla SGD on small-scale text classification with BERT-Base, suggesting robustness to domain shift. Although the theoretical analysis hinges on a locally-Lipschitz Hessian restriction that may not hold universally, extensive ablations demonstrate consistent empirical gains across vision and NLP tasks. Code and checkpoints are provided for reproduction. Future work will explore adaptive neighborhood sizes and extension to federated settings.",
    "id": 60
  },
  {
    "title": "Revisiting Entropy Regularization with Improved Exploration Bonuses for Policy Optimization",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Ortega, L."
    ],
    "abstract": "Entropy regularization is widely used in reinforcement learning to encourage exploration and prevent premature convergence to sub-optimal policies. While theoretically motivated, the practical effectiveness of entropy bonuses varies significantly across environments, often requiring problem-specific tuning. In this work, we analyze the limitations of standard entropy regularization and propose an adaptive entropy bonus that scales with the policy's uncertainty over state-action pairs. Our method, Uncertainty-Weighted Entropy Regularization (UWER), incorporates a learned state-density model to modulate entropy bonuses based on visitation frequency. We evaluate UWER on a suite of continuous control tasks and demonstrate modest improvements over baseline SAC and PPO algorithms, achieving an average 7% gain in sample efficiency on half of the tested environments. However, performance gains are inconsistent across domains, with negligible improvements in high-dimensional tasks. While our approach offers a principled way to adjust exploration bonuses, the additional computational overhead and sensitivity to hyperparameters may limit its practical impact. Our results suggest that while adaptive entropy regularization can yield small empirical gains, the broader challenge of environment-agnostic exploration remains unresolved.",
    "id": 62
  },
  {
    "title": "Rethinking Batch Normalization: A Gradient-Norm Perspective for Improved Optimization",
    "authors": [
      "Kumar, A.",
      "Jiang, S.",
      "Bennett, K."
    ],
    "abstract": "Batch Normalization (BN) has become a staple in deep learning, yet its precise mechanism for accelerating optimization remains debated. We propose a new perspective that explains BN's benefits through gradient norm equalization across layers. By analyzing the spectral properties of the Jacobian, we show that BN implicitly balances gradient magnitudes without requiring careful initialization. We introduce Layer-Adaptive Batch Normalization (LABN), a lightweight modification that adapts the normalization strength based on gradient statistics. On CIFAR-100 and ImageNet, LABN achieves modest improvements of 0.3-0.7% over standard BN while reducing training time by 5-10%. However, the gains diminish on very deep architectures like ResNet-152, suggesting our method works best for moderately deep networks. We also explore LABN's interaction with different optimizers, finding it particularly effective with SGD but showing limited benefits with Adam. While our theoretical analysis provides new insights into BN's role in optimization, it relies on simplifying assumptions that may not hold in practice. Code and pretrained models are available.",
    "id": 63
  },
  {
    "title": "Revisiting Momentum in Adam-Style Optimizers Through a Learning-Rate Curvature Lens",
    "authors": [
      "Dubois, M.",
      "Chatterjee, P.",
      "Garcia, R."
    ],
    "abstract": "Adaptive optimizers such as Adam and RMSProp dominate deep-learning practice, yet their second-order moment estimation combined with heavy-ball momentum remains poorly understood. We observe that the effective learning-rate schedule of Adam depends on the local curvature of the loss landscape and propose Curvature-Aware Momentum (CAM), a lightweight plug-in that reweights momentum updates by the empirical inverse curvature trace. On CIFAR-10/100 and ImageNet-1k, CAM yields marginal top-1 gains of 0.3\u20130.5 pp over vanilla AdamW while reducing training time by  \u22487%. Further ablations show that CAM's benefit is strongest in low-batch or high-LR regimes, where curvature noise is elevated. A regret bound for smooth non-convex objectives suggests CAM converges in O(\u03b5\u22122) iterations, matching Adam, but our proof requires a bounded curvature ratio assumption that is hard to verify in practice. Although preliminary, CAM highlights an under-explored interplay between curvature, momentum, and adaptive learning-rate schedules, offering a simple drop-in that can complement existing optimizers. Code and learning-rate curvature visualizations are provided at anonymous-url.",
    "id": 64
  },
  {
    "title": "AdaSharp: Adaptive Sharpness-Aware Minimization with Scheduled Weight Perturbations",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Recent work shows that flat minima in the loss landscape correlate with better generalization, motivating Sharpness-Aware Minimization (SAM) and variants that regularize the maximal loss within a neighborhood.  Empirical studies reveal, however, that the optimal neighborhood radius changes during training and differs across layers, yet current methods fix both hyper-parameters.  We propose AdaSharp, an extension that adaptively schedules the perturbation radius and exploits layer-wise heterogeneity.  Specifically, we maintain per-layer perturbation buffers updated via an exponentially-weighted average of the optimal ascent direction magnitudes; the buffers decay via a cosine schedule and are clipped with a trust-region inspired bound.  On CIFAR-10/100 and ImageNet, AdaSharp matches or slightly improves base SAM accuracy (e.g., +0.3% top-1 on ImageNet) while reducing gradient evaluations by 18%.  The algorithm introduces one extra hyper-parameter, the decay rate, which we find robust across datasets after grid search.  Ablation shows that layer-wise adaptation contributes roughly two-thirds of the benefit, whereas cosine scheduling supplies the remainder.  We also conduct preliminary analysis showing tighter valleys in the final loss landscape.  While the gains are moderate and theoretical properties remain open, AdaSharp offers a lightweight, plug-and-play wrapper that can be combined with any SAM variant.",
    "id": 65
  },
  {
    "title": "Improving Few-Shot Learning with Class-Aware Mixup and Adaptive Margin Loss",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "M\u00fcller, H."
    ],
    "abstract": "Few-shot classifiers struggle when base and novel classes have overlapping feature distributions. We study this problem in the 5-way, 5-shot mini-ImageNet setting and observe that cross-entropy with standard data augmentation can assign nearly identical scores to visually similar classes, leading to frequent confusions. Motivated by this observation, we propose CAMA, a lightweight training recipe that couples Class-Aware MixUp with an Adaptive Margin loss. During meta-training, CAMA interpolates features only between classes whose prototypes are closer than a learned threshold, while the margin term dynamically expands the decision boundary of each class proportionally to its observed confusion rate. On mini-ImageNet, CAMA improves the 1-shot accuracy of a ResNet-12 backbone by 2.3% (65.7% \u2192 68.0%) over a competitive baseline, and by 1.1% over the previous best result that used a deeper network and more parameters. Ablation studies indicate that each component contributes, although gains saturate when the backbone is wider or when more shots are available. Analysis of learned embeddings shows tighter intra-class clusters but also reveals increased sensitivity to the choice of mixup coefficient. Code and pretrained weights are provided to ensure reproducibility.",
    "id": 66
  },
  {
    "title": "Adaptive Gradient Clipping with Scheduled Update Frequency for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient clipping is widely used to stabilize transformer training, but fixed thresholds often require extensive tuning across tasks and model sizes. We propose Adaptive Gradient Clipping with Scheduled Update Frequency (AGC-SUF), a method that automatically adjusts clipping thresholds based on gradient statistics while periodically updating the clipping bound to balance stability and convergence speed. Our approach computes a moving average of gradient norms and scales the clipping threshold proportionally, with an update frequency that decreases during training. We evaluate AGC-SUF on language modeling tasks using GPT-2 architectures ranging from 124M to 1.5B parameters. Experiments show that AGC-SUF reduces the need for hyperparameter tuning by 60% compared to standard gradient clipping while achieving perplexity within 2% of finely-tuned baselines on WikiText-103 and OpenWebText. Additionally, we observe improved training stability in 15% of configurations where standard clipping fails. However, computational overhead increases training time by approximately 8%. While our method provides practical benefits for practitioners by reducing tuning effort, we acknowledge that the theoretical motivation remains heuristic and the improvements, though consistent, are modest in magnitude.",
    "id": 67
  },
  {
    "title": "Improving Transformer Generalization with Periodic Weight Averaging",
    "authors": [
      "Kumar, V.",
      "Osei-Bonsu, D.",
      "Liu, H."
    ],
    "abstract": "Transformer models often display unstable performance when training data are limited or shift slightly between train and test distributions. Prior work has focused on architectural changes or data augmentation to address this brittleness. We hypothesize that the issue stems from the optimizer overshooting sharp minima in the loss surface. We therefore propose Periodic Weight Averaging (PWA), a simple modification to standard Adam training that computes a uniform moving average of weights every k steps and substitutes the averaged checkpoint at evaluation time. On four small- to medium-scale language and vision tasks, PWA yields test-error reductions of 0.8\u20131.5% and lower variance across five random seeds, with no architecture or data changes. While these gains are not uniformly consistent, statistical significance (p < 0.05) is attained on two tasks. However, we observe negligible impact on larger models (>250M parameters) and marginal computational overhead that some practitioners may deem undesirable. Code and hyper-parameters are provided to ensure reproducibility. Overall, PWA offers a drop-in enhancement for resource-constrained settings, but its benefit appears to diminish with scale, suggesting that complementary techniques or scheduling strategies merit future study. We discuss limitations and potential future directions.",
    "id": 68
  },
  {
    "title": "Adaptive Gradient Clipping for Gradient-Starved Transformer Architectures",
    "authors": [
      "Liu, J.",
      "Martinez, C.",
      "Kim, H."
    ],
    "abstract": "Gradient clipping is widely used to stabilize the training of large transformer models, yet its fixed threshold often leads to under-utilization of gradient information. We investigate whether an adaptive clipping strategy can improve convergence when gradient norms are naturally small, a regime we term 'gradient-starved.' Our method, Clip-\u0394, adjusts the clipping bound based on the ratio of the current gradient norm to an exponential moving average of historical norms. Across 3 language-modeling datasets and 2 vision tasks, Clip-\u0394 yields 1.2-2.4% perplexity reductions and 0.4-0.9% top-1 accuracy gains over standard clipping, while requiring <1% overhead in wall-clock time. A convergence bound is proved under smoothness assumptions, albeit with a worse constant than SGD. Ablation studies indicate that the moving-average window size is sensitive to batch size, limiting off-the-shelf deployment. Code and pretrained weights are provided.",
    "id": 69
  },
  {
    "title": "Revisiting Weight Averaging with Cyclic Learning Rates for Better Generalization",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Goldman, J."
    ],
    "abstract": "Weight averaging techniques such as stochastic weight averaging (SWA) and exponential moving average (EMA) have become standard tools for improving generalization in deep learning. While these methods typically operate on weights collected near the end of training, we investigate whether averaging weights from earlier stages\u2014when learning rates are still high\u2014can yield comparable or better performance. We propose Cyclic Weight Averaging (CWA), a simple extension that collects snapshots during cyclic learning-rate schedules and averages them with a trainable convex combination. On CIFAR-10, CIFAR-100, and ImageNette, CWA improves top-1 accuracy by 0.4\u20130.9% over SWA without extra hyper-parameters, and by 0.2\u20130.5% over EMA while using 30% fewer parameters. Theoretical analysis in a two-layer linear network suggests that early-cycle averages can lie in flatter minima, supporting our empirical observations. Although gains are consistent, they are incremental and diminish on larger models; on ImageNet we observe only 0.1% improvement. Code is available at anonymized link.",
    "id": 70
  },
  {
    "title": "Gradient Clipping with Adaptive Momentum: A Minor Tweak or a Meaningful Fix?",
    "authors": [
      "Nguyen, T.",
      "Chen, L.",
      "Garcia, M."
    ],
    "abstract": "Gradient clipping is routinely used to stabilize training of large language models, yet its interaction with adaptive optimizers remains poorly understood. We introduce CAB-Clip, a lightweight modification that scales momentum updates by a running estimate of the clipped gradient variance. On three medium-scale language modeling tasks (up to 770 M parameters) CAB-Clip reduces perplexity by 0.4\u20130.9 points versus standard clipping, while halving the number of divergent training runs. Theoretically, we prove that CAB-Clip converges in the convex setting at the same O(1/\u221aT) rate as vanilla SGD, up to a constant factor that depends on the clipping threshold. Ablation studies show that 70 % of the gain vanishes when momentum is frozen, suggesting the key effect is a second-order bias correction rather than mere rescaling. While the method adds only four lines of code, benefits appear task-specific: gains are negligible on image classification and reinforcement-learning benchmarks. Our results indicate that gradient clipping can be slightly improved, but the improvement is incremental and may not justify a new hyper-parameter in every pipeline.",
    "id": 71
  },
  {
    "title": "Revisiting Gradient Clipping for Private-Label Training with Mixed-Batch Normalization",
    "authors": [
      "Kim, J.",
      "Singh, V.",
      "Liu, H."
    ],
    "abstract": "Modern deep nets are trained with increasingly aggressive augmentations whose gradients can explode, prompting widespread use of per-sample clipping. Prior work attributes clipping\u2019s success solely to noise stable optimization, overlooking its implicit regularization of batch-normalization (BN) statistics. We formalize this interaction and propose Mixed-Batch Normalization (MBN), a simple modification that keeps clean and augmented samples in separate BN statistics while sharing all other parameters. On ImageNet and CIFAR-10, MBN improves top-1 accuracy by 0.4\u20130.9 pp over vanilla clipping at the same compute budget; conversely, removing clipping hurts MBN by 1.1 pp, confirming a complementary regularization effect. Theoretically, we bound the Wasserstein distance between the clean and augmented feature distributions in terms of the clipping threshold, yielding a principled schedule that eliminates manual tuning on half of our tasks. Although the gains are incremental and limited to private-label setups, MBN costs one line of code and no extra parameters, making clipping-aware normalization a drop-in replacement for standard BN. Code and trained weights are provided.",
    "id": 72
  },
  {
    "title": "AdaSmooth: An Adaptive Momentum Schedule for Non-Convex Optimization That Usually Helps",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Martinez, C."
    ],
    "abstract": "We propose AdaSmooth, a simple modification to standard momentum-based optimizers that adaptively adjusts the momentum coefficient based on the gradient variance. While momentum is known to accelerate convergence in smooth regions, it can hinder progress in noisy or rapidly changing landscapes. Our method heuristically scales momentum inversely with local gradient variance, motivated by the intuition that low variance suggests a stable region where higher momentum is beneficial. We evaluate AdaSmooth on training ResNet-34/50 on CIFAR-10/100 and small-scale vision transformers on ImageNet-1k. Results show 2-5% relative improvement in final accuracy over AdamW with momentum 0.9 in 3/5 experiments, while remaining competitive in the others. We provide a limited convergence guarantee for quadratic objectives but acknowledge the theory does not extend to the general case. Code is available at anonymized-url. While the gains are modest and inconsistent, AdaSmooth introduces no additional hyper-parameters and adds negligible computational overhead, making it a practical drop-in replacement that might help in some settings.",
    "id": 73
  },
  {
    "title": "Improved Generalization via Layer-wise Learning Rate Warmup for Fine-Tuning Vision Transformers",
    "authors": [
      "Kovacs, B.",
      "Nguyen, T.",
      "Martinez, L."
    ],
    "abstract": "Fine-tuning large-scale Vision Transformers (ViTs) on downstream tasks often suffers from feature over-correlation and early layer overfitting, leading to brittle optimization landscapes and sub-optimal generalization. We hypothesize that gradual layer-wise learning-rate warmup, or LLRW, can mitigate these pathologies by stabilizing early layer representations while allowing later layers to adapt quickly. We study a simple variant where the warmup duration for each layer grows linearly with its depth. Across seven medium-scale benchmarks (0.5-4M training images), LLRW obtains a modest but consistent average gain of 0.9% (\u00b10.4) accuracy over the standard linear warmup schedule, while requiring no extra parameters or data. Additional ablations show that the benefit is mostly concentrated in low-data regimes and correlates with smaller kernel alignment drift. Although these results support the usefulness of depth-dependent warmup, we observe diminishing returns when the pre-training dataset is large and the task diversity is high. Our PyTorch implementation requires only 12 lines of code change, facilitating adoption. Code is available at github.com/anon-llrw.",
    "id": 74
  },
  {
    "title": "Asynchronous Gradient Descent with Delayed Gradient Updates: A Middle Ground Between Delay and Consistency",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Johnson, M."
    ],
    "abstract": "We study asynchronous gradient descent in distributed settings where gradient computations experience unpredictable delays. While existing methods either wait for stragglers (synchronous SGD) or use stale gradients (asynchronous SGD), we propose Delayed Gradient Synchronization (DGS), which maintains a bounded delay window while allowing partial gradient updates. Our method uses a novel gradient buffering mechanism that selectively incorporates delayed gradients based on their staleness and contribution to overall convergence. We provide theoretical analysis showing DGS converges for smooth non-convex objectives under standard assumptions, achieving a rate of O(1/\u221aT steps) where \u221a accounts for effective gradient staleness. Experimental results on CIFAR-10 and ImageNet demonstrate marginal improvements over vanilla asynchronous SGD (0.8% accuracy gain) while reducing training time by 12-15% on 8-GPU setups. Though gains are modest, our approach offers practical benefits for practitioners dealing with variable-speed workers without complex synchronization protocols. Code and implementations are available for reproducibility.",
    "id": 75
  },
  {
    "title": "Revisiting Weight Averaging with Temporal Ensembling for Small-Data Deep Learning",
    "authors": [
      "Nguyen, T.",
      "Kowalski, K.",
      "Singh, M."
    ],
    "abstract": "Weight averaging has emerged as a simple yet effective technique to improve generalization in deep learning, but its benefits diminish when labeled data are scarce. We propose Temporal Averaging with Ensembling (TAE), an algorithm that couples weight averaging with a temporal-ensembling teacher to exploit unlabeled examples. On semi-supervised image-classification benchmarks with 250\u20131000 labels, TAE yields 2\u20134% accuracy gains over the underlying backbone with no additional training cost. Theoretically, we show that TAE reduces variance in an online fashion, inheriting the convexity guarantees of stochastic weight averaging while adapting to the local curvature revealed by the ensembling target. Extensive ab1lations on CIFAR-10/100, Mini-ImageNet and three text datasets confirm that the teacher signal is the dominant factor; yet when labels are plentiful, TAE degrades slightly to vanilla averaging. Our code and checkpoints will be released upon publication.",
    "id": 76
  },
  {
    "title": "LoRa-GD: Low-Rank Gradient Descent for Parameter-Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Fine-tuning large language models (LLMs) remains computationally prohibitive for most practitioners. While Low-Rank Adaptation (LoRA) offers parameter efficiency, we observe that its training dynamics often diverge from full fine-tuning, particularly in downstream tasks requiring subtle semantic understanding. We propose LoRa-GD, which injects carefully constructed low-rank gradient corrections into LoRA updates to better approximate full fine-tuning trajectories. Our method computes these corrections using an efficient online PCA of the full gradient, requiring only 3% additional memory overhead. On GLUE tasks, LoRa-GD achieves 96.2% of full fine-tuning performance compared to 93.8% for standard LoRA, while using comparable parameters. However, we find these gains diminish on larger models (>7B parameters) and tasks with limited training data. Theoretical analysis shows LoRa-GD converges under similar conditions to standard gradient descent, though with a worse dependence on condition number. While our method provides modest improvements over LoRA with minimal overhead, we acknowledge the performance gap to full fine-tuning remains significant on complex reasoning tasks. Code and pretrained adapters will be made available.",
    "id": 77
  },
  {
    "title": "Momentum-Aided Gradient Descent with Iterate Averaging for Over-Parameterized Networks",
    "authors": [
      "Chen, Y.",
      "Rangan, V.",
      "Johnson, K."
    ],
    "abstract": "We revisit iterate averaging in stochastic gradient descent (SGD) for modern over-parameterized models. While classical theory suggests averaging can improve convergence rates, its benefits in deep learning remain unclear. We propose MAGNet (Momentum-Aided Gradient averaging), a simple modification that combines heavy-ball momentum with iterates averaged using an exponential window. On linear regression and two-layer networks with large widths, MAGNet achieves a 10-15% speedup in wall-clock time over tuned SGD. On CIFAR-10/100 and ImageNet, MAGNet matches baseline performance but shows more stable training curves with reduced variance in the final epoch test accuracy (\u00b10.2% vs. \u00b10.5%). Theoretically, we prove that for quadratic objectives MAGNet achieves the same O(1/t) rate as SGD but with a smaller leading constant when the mini-batch noise is high. Although the gains are incremental and limited to specific regimes, our work suggests that iterate averaging\u2014when properly combined with momentum\u2014can still confer modest practical benefits in large-scale training. Extensive ablations and open-source code are provided.",
    "id": 78
  },
  {
    "title": "Gradient-Enhanced Dropout: A Simple Extension with Modest Gains in Generalization",
    "authors": [
      "Kumar, S.",
      "Ostrovski, G.",
      "Liang, P."
    ],
    "abstract": "Dropout remains one of the most widely used regularization techniques in training neural networks, yet little work has explored whether second-order information can improve its effectiveness. We propose Gradient-Enhanced Dropout (GED), a method that adapts dropout probabilities for each neuron based on the magnitude of the gradient flowing through it. Intuitively, neurons that exhibit large gradient variations during training are assigned higher dropout rates to encourage robustness. Our approach requires minimal hyperparameter tuning and can be implemented in fewer than 20 lines of PyTorch code. We evaluate GED on CIFAR-10/100 and ImageNette, where it achieves 0.4-0.7% accuracy improvements over standard dropout, albeit with a 12% increase in training time. While these gains are consistent, they are relatively modest compared to more sophisticated regularization schemes. Theoretically, we prove a PAC-Bayes generalization bound that improves upon standard dropout by a constant factor, though the improvement is small. Our extensive ablations reveal that GED is most effective for medium-sized networks (1-10M parameters) but offers diminishing returns for larger models. Though not a dramatic breakthrough, GED represents a simple, practical enhancement that may benefit practitioners seeking incremental improvements.",
    "id": 79
  },
  {
    "title": "Improved Smoothing Schedules for Denoising Diffusion Models via Learnable Noise Scaling",
    "authors": [
      "Liu, H.",
      "Chen, J.",
      "Anderson, K."
    ],
    "abstract": "Denoising diffusion models have achieved impressive generative performance, but their sampling process remains computationally expensive due to the large number of required steps. We propose Learnable Noise Scaling (LNS), a simple modification to the diffusion process that adaptively adjusts the noise schedule during training. Our key insight is that the optimal noise scaling varies across different regions of the data distribution, and can be efficiently learned through an auxiliary neural network. LNS introduces minimal computational overhead during training and no additional cost during sampling. On CIFAR-10, our method achieves an FID of 2.87 with 1000 sampling steps, matching the baseline while requiring 10% fewer training iterations. With fewer sampling steps (50-100), we observe modest improvements of 3-5% in FID scores across standard benchmarks. While the theoretical analysis of LNS remains limited, we provide empirical evidence that the learned schedules better capture the local geometry of the data manifold. Our approach is complementary to existing acceleration techniques and can be combined with recent fast sampling methods for further improvements. Though the gains are incremental rather than breakthrough, LNS offers a practical and easy-to-implement enhancement to existing diffusion frameworks.",
    "id": 80
  },
  {
    "title": "Improving Transformer Generalization with Sparse Mixtures of Low-Rank Experts",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "While transformers have achieved remarkable success across domains, their parameter efficiency remains a concern for large-scale deployment. We propose SMoLRE, a sparse mixture-of-experts architecture that combines low-rank adaptation with learned routing. Each expert operates on a low-rank subspace of the input, reducing computational cost while maintaining expressivity. Our method introduces a novel routing mechanism that balances expert utilization through a combination of load-balancing losses and cosine similarity-based assignment. We evaluate SMoLRE on language modeling and vision tasks, achieving 2-3% improvements over standard transformers with 40% fewer parameters. However, we observe that performance gains diminish on smaller datasets, suggesting that our approach may require large-scale pretraining to be effective. Ablation studies reveal that the low-rank constraint sometimes limits expert capacity, particularly for tasks requiring fine-grained reasoning. While our method provides practical benefits for deployment scenarios, we acknowledge that the architectural innovations are incremental and the theoretical understanding of when sparse low-rank experts outperform dense models remains limited. Code and pretrained models will be made available upon acceptance.",
    "id": 81
  },
  {
    "title": "Gradient Noise Re-scaling Improves Adam at Large Batch Sizes",
    "authors": [
      "Liu, K.",
      "Subramanian, P.",
      "Korhonen, T."
    ],
    "abstract": "Adaptive optimizers such as Adam remain the default for training deep networks, yet their performance degrades with very large mini-batches, limiting scalability. We propose GN-Adam, a light-weight modification that re-scales per-parameter gradient noise estimates before updating second moments. The rule amplifies updates for parameters whose stochastic gradients exhibit high signal-to-noise ratio and suppresses those with noisy gradients, inspired by earlier work in adaptive learning-rate schedules. On ImageNet with a ResNet-50, GN-Adam matches Adam\u2019s small-batch accuracy up to batch size 8K without extra hyper-tuning, whereas Adam\u2019s top-1 drops by 2.3% (absolute) under the same regime. Similar gains are observed on training a medium-scale transformer for language modeling. Theoretical analysis in a quadratic setting shows that rescaling lowers the effective step-size variance, loosely bounding convergence. While the technique is simple and complementary to existing large-batch schemes (e.g., LARS or gradual warmup), gains shrink at batch sizes \u226516K and on smaller vision datasets like CIFAR-10. Code is provided for reproducibility. GN-Adam offers practitioners an easy plug-in for moderately scaling batch size, but its benefit appears task-dependent and does not resolve the broader generalization gap at extreme scale.",
    "id": 82
  },
  {
    "title": "Gradient-Noise Steering: A Lightweight Alternative to Adaptive Optimizers",
    "authors": [
      "Baranov, S.",
      "Nguyen, T.",
      "Kumar, V."
    ],
    "abstract": "Adaptive optimizers such as Adam and RMSprop accelerate training but double memory footprint and impose optimizer-state synchronization costs in distributed settings. We explore a simpler alternative: steering the per-sample gradient noise to mimic the preconditioning effect of Adam without maintaining any running statistics. Specifically, we inject scaled isotropic noise whose variance is predicted on-the-fly by a lightweight convolutional network that sees only the current gradient and loss curvature proxy. On CIFAR-10/100 and ImageNet this <i>gradient-noise steering</i> (GNS) matches 91 % of Adam\u2019s final accuracy while using the same memory as vanilla SGD+momentum. Wall-clock speed-ups reach 1.26\u2009\u00d7 on 8-GPU ImageNet training because no second-order buffers are communicated. However, we observe higher gradient variance at convergence and occasional instability when batch sizes exceed 2 k samples. Theoretical analysis shows GNS is equivalent to a diagonal preconditioner only under restrictive assumptions on the Hessian that rarely hold in practice; empirical Hessian spectra confirm the mismatch. Ablation indicates that 60 % of the benefit comes from noise scale alone, while the learned steering network contributes the remainder. Code and checkpoints are provided to reproduce all experiments. GNS offers a practical middle ground between SGD and adaptive methods when memory or communication is constrained, but does not surpass Adam in final accuracy or theoretical elegance.",
    "id": 83
  },
  {
    "title": "Revisiting Random Fourier Features with Data-Dependent Sampling Updates",
    "authors": [
      "Kumar, A.",
      "Liu, J.",
      "Schmidt, M."
    ],
    "abstract": "Random Fourier Features (RFF) provide a fast way to approximate shift-invariant kernels, but their practical performance often degrades when the data distribution deviates from stationarity assumptions. We propose a simple data-dependent re-sampling scheme that periodically refits the RFF basis to the current training distribution. Our method, termed RFF-DR, maintains the linear-time complexity of standard RFF while adapting to local curvature of the target function. On eight UCI benchmarks we observe modest but consistent improvements over standard RFF (1.2\u20132.3 % average error reduction), and on CIFAR-10 our approach closes up to 41 % of the gap between RFF and exact Gaussian kernels. Theoretical analysis shows that the refit frequency controls a bias\u2013variance trade-off, and we prove a generalization bound that grows as O(T^-1/2 log T) with T updates. While our contribution is primarily empirical, the simplicity and plug-and-play nature of RFF-DR make it an attractive baseline for future scalable kernel methods. Code and hyper-parameters are provided to ensure reproducibility.",
    "id": 84
  },
  {
    "title": "Gradient Noise Re-Scaling: A Lightweight Mechanism for Alleviating Sharp Minima in Deep Networks",
    "authors": [
      "Kumar, A.",
      "Zhou, S.",
      "Nguyen, L."
    ],
    "abstract": "The sharp minima hypothesis attributes generalisation difficulties in deep learning to converged solutions that lie in regions of high curvature. Existing counter-measures rely on explicit regularisation, large batch optimiser modifications, or computationally heavy second-order information. We propose Gradient Noise Re-Scaling (GNR), a simple, plug-in heuristic that re-weights stochastic gradients by a scalar function of their historical variance. During training GNR slightly amplifies parameter updates in high-variance directions, encouraging escape from sharp basins without changing the base optimiser or introducing hyper-parameters beyond the usual learning rate and momentum. On CIFAR-10/100 and ImageNet GNR improves top-1 accuracy by 0.2\u20130.8% over well-tuned SGD baselines and matches the performance of Strong-Weight, a recent sharpness-aware optimizer, while adding <3% wall-clock overhead. Theoretically, we prove that GNR performs implicit additive gradient perturbation whose magnitude is bounded by the gradient variance. Experiments show the method is complementary to data augmentation and label smoothing, but ablations reveal gains vanish when the network width is doubled, suggesting limited scalability. Code and 50-line PyTorch implementation are provided.",
    "id": 85
  },
  {
    "title": "Meta-Smoothing: A Simple Plug-in for Improved Semi-Supervised Generalization",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Chen, L."
    ],
    "abstract": "Semi-supervised learning (SSL) has shown remarkable empirical success, yet standard consistency-regularization methods still degrade when unlabeled data distributions drift from labeled ones. We introduce Meta-Smoothing, a lightweight wrapper that re-weights pseudo-labels by learning a small, layer-wise perturbation network on the fly. During training our module predicts per-sample confidence adjustments, which are then applied before the consistency loss without changing the base SSL objective. On CIFAR-10/100 with 250 and 4 000 labels we obtain 1\u20132% accuracy gains over FixMatch and MixMatch, while on ImageNet with 10% labels we see a 0.7% top-1 improvement. Ablations indicate that gains stem mostly from down-weighting high-entropy pseudo-labels rather than sophisticated architectural changes. Although the method adds 5% training time and introduces two new hyper-parameters, it can be implemented in under 30 lines of PyTorch. Code is provided and reproducibility is supported by fixed random seeds and detailed hyper-parameter ranges.",
    "id": 86
  },
  {
    "title": "Gradient Surgery for Partially Overlapping Client Data in Federated Learning",
    "authors": [
      "Murthy, V.",
      "Chen, S.",
      "Krishnan, R."
    ],
    "abstract": "Federated learning enables distributed training without centralizing data, but performance degrades when clients\u2019 local distributions differ. Most prior work assumes either homogeneous data or completely non-overlapping classes, whereas real deployments often lie in between: clients share a subset of classes while possessing private ones. We formalize this intermediate scenario and propose Federated Gradient Surgery (FedGS), an algorithm that identifies and down-weights conflicting gradient directions during local training. FedGS uses a lightweight proxy comparison performed on the server to split each minibatch gradient into shared and client-specific components, reducing the influence of private-class updates on the global model. On standard federated benchmarks (CIFAR-10/100, FEMNIST) we show 1.2\u20132.3% higher accuracy and 25% faster convergence relative to FedAvg and the recent SOTA FedProx, with only 2% parameter overhead. However, gains diminish under large client drift or tiny shared label sets, and theoretical justification is limited to quadratic objectives. Code and tuned hyper-parameters are publicly available.",
    "id": 87
  },
  {
    "title": "Improved Zeroth-Order Optimization via Laplace Approximated Gradients",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Garcia, L."
    ],
    "abstract": "Zeroth-order (ZO) methods are essential for optimizing black-box functions where gradients are unavailable. Existing ZO algorithms estimate gradients through finite differences, incurring O(d) function queries per step in d dimensions. We propose LA-ZO, a simple modification that replaces the standard kernel with a Laplace approximation constructed from a small random subset of past function evaluations. Under Lipschitz assumptions, we show LA-ZO reduces the expected query complexity to O(d/k) when k past points are reused, yielding a 15\u201325% speed-up on quadratic benchmarks. On CIFAR-10 robustness tasks, LA-ZO matches the accuracy of the state-of-the-art ZO-SGD while using 30% fewer model queries. Although our theoretical guarantee is limited to convex settings and the hyper-parameter sensitivity increases with dimension, empirical gains persist on moderate-scale deep network tuning. Our contribution is a lightweight augmentation, compatible with any ZO optimizer, that trades a controllable bias for a measurable reduction in query cost. Code and notebooks are provided for full reproducibility.",
    "id": 88
  },
  {
    "title": "Revisiting Entropy-Regularized Policy Search with Adaptive Temperature Scheduling",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Bouchard, G."
    ],
    "abstract": "Entropy regularization is widely used in reinforcement learning to encourage exploration and prevent premature convergence. While most prior work employs a fixed temperature coefficient, recent heuristics suggest that annealing this parameter during training can accelerate learning. We formalize this intuition by proposing adaptive entropy temperature (AET), an online schedule that adjusts the coefficient based on the agent\u2019s policy entropy relative to a moving target. Under the framework of soft policy iteration, we prove that AET achieves monotonic improvement up to a bounded error, relaxing the standard requirement of a constant temperature. We evaluate the method on continuous-control tasks from DeepMind Control Suite, where AET matches or exceeds the final returns obtained by the best fixed temperature tuned via grid search in 9/12 environments, while requiring 18% fewer environment steps on average. AET incurs only 0.3% overhead over the baseline SAC algorithm and introduces no additional hyper-parameters except for two intuitive moving-average coefficients. Our ablations indicate that the primary benefit stems from faster early exploration rather than asymptotic gains. Although limited to continuous control and requiring entropy estimation, AET offers a practical drop-in replacement for fixed-temperature RL and sheds light on principled temperature adaptation.",
    "id": 89
  },
  {
    "title": "AdaSkip: An Adaptive Curriculum for Efficient Pre-Training of Language Models by Dynamically Skipping Easy Examples",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Pre-training large language models is computationally expensive, yet many training examples are redundant for already-learned skills. While curriculum learning orders examples by hand-crafted heuristics, we propose AdaSkip, a lightweight bandit algorithm that chooses\u2014at every gradient step\u2014whether to process or skip the current mini-batch. AdaSkip maintains a small reservoir of past gradients and treats the skip decision as a contextual bandit whose reward is the validation-loss decrease per FLOP. On the 1.2B-parameter CommonCrawl corpus, AdaSkip eliminates 23 % of updates, matches the final perplexity of the standard baseline, and cuts wall-clock time by 14 %. Ablations show the gain is not due to simple filtering: the curriculum adapts online, shifting from short to longer sequences and from high- to low-frequency tokens. However, benefits diminish on smaller models (\u2264 0.3 B) and on code data, where deterministic difficulty proxies outperform the adaptive policy. The method adds < 1 % memory overhead and is implemented in 40 lines of PyTorch; code and tuned hyper-parameters are provided. AdaSkip offers a cheap complement to static optimization tricks, but its regressor is linear and the theoretical justification is limited to a coarse upper-bound, leaving room for deeper analysis.",
    "id": 90
  },
  {
    "title": "Gradient Surgery for Cyclical Data: Improving Generalization with Periodic Weight Averaging",
    "authors": [
      "Liu, Q.",
      "Thompson, J.",
      "Kumar, V."
    ],
    "abstract": "We propose Periodic Weight Averaging (PWA), a simple modification to standard stochastic gradient descent that cyclically averages model weights during training to improve generalization. Motivated by the observation that interpolating between minima can yield flatter solutions, PWA maintains a running average of weights over fixed intervals while continuing optimization. We provide theoretical analysis showing PWA converges for convex functions and demonstrate empirical improvements on CIFAR-10/100 and ImageNet, achieving 0.5-1.2% accuracy gains over SGD with momentum. While our method is easy to implement with minimal computational overhead, we find that performance gains diminish with larger architectures and longer training schedules. Our experiments suggest PWA works best for medium-scale vision tasks with limited training budgets. Code is available at anonymous-url.",
    "id": 91
  },
  {
    "title": "Momentum-Aware Gradient Descent: Combining Nesterov Acceleration with Adaptive Learning Rates",
    "authors": [
      "Jin, Y.",
      "Mukherjee, S.",
      "Kleinberg, A."
    ],
    "abstract": "We present Momentum-Aware Gradient Descent (MAGD), a simple technique that blends Nesterov accelerated momentum with per-parameter adaptive learning rates inspired by Adam. MAGD computes an auxiliary lookahead gradient, reweights it element-wise using second-moment estimates, then applies an update that interpolates between heavy-ball and adaptive schemes via a single interpolation coefficient \u03b2. On a testbed of 12 image-classification and language-modeling tasks, MAGD yields 1.4% average top-1 accuracy gain over vanilla SGD and 0.8% over Adam while preserving their respective wall-clock training times. Theoretical analysis for quadratic objectives shows MAGD converges at a rate between O(1/t\u00b2) and O(1/\u221at), depending on \u03b2. Although the improvement is incremental and the method relies on two standard ingredients, the combination is not previously studied, and practitioners may find the plug-and-drop implementation appealing. Code is available at [anonymous GitHub].",
    "id": 92
  },
  {
    "title": "Revisiting Dropout Schedules: A Frequency-Domain Perspective on Regularization Trade-offs",
    "authors": [
      "Kumar, S.",
      "Nguyen, T.",
      "Anderson, J."
    ],
    "abstract": "Dropout schedules\u2014deterministic annealing paths that interpolate between dropout rates of 0 and 1\u2014were introduced as a theoretically principled alternative to standard dropout, yet reported gains on small vision benchmarks have proven hard to reproduce at scale. We re-examine these schedules through the lens of Fourier analysis, arguing that the implicit regularization strength can be quantified by the energy retained in high-frequency components of mini-batch gradients. Under this view, existing monotone schedules correspond to low-pass filters that may over-regularize when label noise is limited. We propose SpectralDrop, a simple cosine-modulated schedule whose cutoff adapts to the observed signal-to-noise ratio. On CIFAR-10/100 we obtain 0.6\u20130.9% accuracy improvements over both standard dropout and the original dropout schedule with no additional hyper-parameters, while on ImageNet the same procedure yields only 0.2% top-1 gain and increases training time by 12%. Theoretical analysis shows the method minimizes a tighter PAC-Bayesian bound under data-dependent priors, but the bound relies on a variance term that is unidentifiable without distributional assumptions. Code and checkpoints are provided.",
    "id": 93
  },
  {
    "title": "Gradient-Noise-Scaled Learning Schedules via Online Loss-Variance Estimates",
    "authors": [
      "Kovacs, B.",
      "Nguyen, T.",
      "Singh, S."
    ],
    "abstract": "Stochastic gradient descent is sensitive to the variance of the per-sample gradients, yet most learning-rate schedules ignore this quantity and decay monotonically. We propose V-STEP, a first-order schedule that scales the learning rate inversely with an exponential moving average of the mini-batch loss variance. On convex problems, V-STEP guarantees convergence at the same rate as SGD with an optimally tuned constant step size, up to a small multiplicative factor. Empirically, training ResNet-18 on ImageNet with V-STEP reaches 69.7% top-1 accuracy, matching the hand-tuned cosine baseline (70.1%), while removing one hyper-parameter. Abllation on CIFAR-10/100 shows speed-ups of 5\u201312% over step decay without replay sweeps. A limitation is that two new hyper-parameters (momentum coefficient and damping constant) are introduced; although they transfer across architectures, they require grid search on small proxy sets. Code and configs are provided for full reproducibility.",
    "id": 94
  },
  {
    "title": "Improved Margin-Based Generalization Bounds for SGD Using Iterate-Curvature Alignment",
    "authors": [
      "Chen, Y.",
      "Garcia, L.",
      "Thompson, K."
    ],
    "abstract": "We revisit uniform-convergence generalization bounds for stochastic gradient descent (SGD) through the lens of the recently proposed iterate-curvature alignment (ICA) measure. While margin-based bounds provide intuitive explanations for SGD\u2019s generalization, their practical looseness limits utility in model selection. We first prove that the ICA between parameter trajectories and the Hessian spectrum correlates strongly with the compressibility of the gradient sequence on benchmark tasks. Leveraging this observation, we derive a data-dependent PAC-Bayesian bound that scales as O(\u221a(ICA \u00b7 Tr(H)/n)), where H is the empirical Hessian and n is the sample size. Experiments on CIFAR-10/100 and ImageNet subsets show our bound yields non-vacutan 15\u201330 % tighter estimates than the state-of-the-art comparator, yet remains 2\u20135\u00d7 looser than naive union bounds. Ablation studies indicate the improvement vanishes when labels-information is destroyed via label permutation, suggesting the bound captures nontrivial structure. Although our theoretical improvement is incremental rather than transformative, the methodology opens a new observable\u2014ICA\u2014that can be computed in a single backward pass, offering practitioners a lightweight diagnostic for overfitting without held-out data. Code is provided for reproducibility.",
    "id": 95
  },
  {
    "title": "Improved Gradient Noise for Faster Convergence: A Modified Learning Rate Schedule with Theoretical and Empirical Insights",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "We propose a simple yet effective learning-rate scheduling technique, called GradNoise, that injects carefully-scaled gradient noise to accelerate stochastic optimization. Inspired by recent work linking gradient noise to generalization, we derive a schedule that increases the variance of stochastic gradients during early training while ensuring convergence. Our method requires only one additional hyper-parameter and can be wrapped around any existing optimizer. On small-scale vision tasks (CIFAR-10/100) and language modeling (WikiText-2), GradNoise improves test accuracy by 0.4\u20130.8 pp over strong cosine-annealing baselines with comparable wall-clock time. Theoretical analysis in the convex quadratic setting shows that our schedule reduces the iterate error constant by a factor that scales inversely with the batch size. While our experiments are currently limited to moderate-scale settings and the theory does not extend to deep networks, the consistent gains across tasks suggest that controlled gradient noise may be an under-explored axis for improving optimization. Code is publicly available.",
    "id": 96
  },
  {
    "title": "Adaptive Gradient Rescaling for Improved Optimization in Noisy, Non-Stationary Environments",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Anderson, M."
    ],
    "abstract": "Modern adaptive optimizers like Adam and RMSProp excel in stationary settings but can stagnate when gradients evolve over time, a common scenario in continual learning or online recommendations. We propose Adaptive Gradient Rescaling (AGR), a lightweight wrapper that re-weights gradient updates by a running estimate of their non-stationarity, measured via a discounted coefficient of variation across rolling windows. AGR requires no additional hyper-parameters beyond the discount factor and integrates seamlessly with existing optimizers. On standard image and text benchmarks corrupted with synthetic distribution shift, AGR improves validation AUC by 1.8\u20132.4% over AdamW at comparable wall-clock time. Qualitatively, optimization trajectories exhibit reduced oscillation and faster recovery from sudden covariate shift. While gains vanish under clean, i.i.d. data, experiments suggest complementary benefits in noisy, partially observable regimes. The method is implemented in <30 lines of PyTorch and reproducible with provided random seeds. Limitations include marginal overhead on small architectures and sensitivity to the discount factor when shift time-scales are misspecified. We release code and environments to facilitate further study.",
    "id": 97
  },
  {
    "title": "Improving Generalization in Meta-Learning with Task-Agnostic Noise Injection",
    "authors": [
      "Nguyen, T.",
      "Kowalski, J.",
      "Singh, A."
    ],
    "abstract": "Recent gradient-based meta-learning algorithms have shown strong performance in few-shot learning, yet they remain vulnerable to overfitting on task-specific noise. We propose Task-Agnostic Noise Injection (TANI), a simple regularizer that adds calibrated isotropic Gaussian noise to inner-loop gradients before the meta-parameter update. Unlike prior noise-based regularizers, TANI requires no knowledge of task semantics and adds negligible computational overhead. Across five few-shot image classification benchmarks, TANI yields 1.3%\u20132.8% absolute gains in 5-shot accuracy over MAML with identical architectures and training budgets, while also lowering meta-gradient variance. Ablation studies show that TANI\u2019s benefit persists when the outer-loop optimizer is Adam or proximal regularization is added, but vanishes when the inner-loop learning rate is above 0.08. Theoretical analysis reveals that TANI implicitly constrains the Lipschitz constant of the meta-loss surface, providing a loose bound on generalization error that grows as O(\u221aK/\u221aN) where K is the number of inner steps and N is the number of meta-training tasks. Our results suggest that lightweight, task-agnostic perturbations can improve meta-generalization; however, gains are incremental and diminish on larger backbones or when the base algorithm already employs second-order MAML variants.",
    "id": 98
  },
  {
    "title": "Improving Transformer Generalization with Periodic Weight Re-initialization During Training",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer models often exhibit brittle generalization when evaluated outside their training distribution. We propose Periodic Weight Re-initialization (PWR), a simple regularization technique that periodically re-initializes a random subset of weights during training while retaining the remaining parameters. This approach aims to encourage the learning of more robust features by preventing over-reliance on specific weight configurations. We conduct experiments on text classification and machine translation tasks using standard Transformer architectures. Results show that PWR achieves modest improvements in out-of-distribution performance compared to standard training, with gains of 1-3% on robustness benchmarks. However, we observe a slight degradation in in-distribution performance (-0.5 to -1.2 BLEU). Theoretical analysis suggests PWR acts as an implicit ensemble method, though we acknowledge that the gains are relatively small and the computational overhead increases training time by ~20%. While PWR is easy to implement and may inspire future work, its practical benefits appear limited given the computational cost.",
    "id": 99
  },
  {
    "title": "Improving Transformer Efficiency via Head-Shared Attention with Learned Routing",
    "authors": [
      "Kumar, A.",
      "Nguyen, T.",
      "Roberts, J."
    ],
    "abstract": "Multi-head self-attention is a key component of Transformer models, but its computational cost grows quadratically with sequence length and linearly with the number of heads. We propose Head-Shared Attention (HSA), a method that reduces the number of distinct attention computations by sharing a subset of attention heads across different representation subspaces. A lightweight routing module learns to assign input-dependent combinations of shared heads to each subspace. On WikiText-103 and WMT'14 En-De, HSA achieves 92% of the baseline BLEU with 30% fewer attention parameters and 1.2\u00d7 decoding speed-up. Ablation studies show that learned routing outperforms fixed patterns, and that sharing 60% of heads preserves most performance. While HSA yields modest gains on long sequences, perplexity degrades on tasks requiring fine-grained token-level alignment. Our code is publicly available.",
    "id": 100
  },
  {
    "title": "Revisiting Dropout Through the Lens of Adaptive Regularization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Dropout remains one of the most widely used regularization techniques in deep learning, yet its theoretical understanding is still incomplete. We propose Adaptive Dropout (AD), a simple modification that adjusts dropout rates based on the magnitude of activations during training. Our key insight is that neurons with larger activations contribute more to the network's output variance and thus should be dropped with higher probability. We derive the regularization effect of AD and show it approximates a data-adaptive form of L2 regularization, with stronger regularization on features with higher variance. Experiments on CIFAR-10, CIFAR-100, and ImageNet show AD achieves 0.2-0.8% accuracy improvements over standard dropout on ResNet-18 and VGG-16 architectures. However, we find these gains diminish on larger models and are task-dependent. While our theoretical analysis provides some insight into when AD might help, the assumptions required limit its generality. We provide an efficient implementation with minimal computational overhead. Though the improvements are modest, our work suggests that re-examining classical techniques through modern theoretical tools can yield practical benefits and highlights the importance of adaptive regularization in deep learning.",
    "id": 101
  },
  {
    "title": "Revisiting Entropy-Regularized Policy Optimization with Mirror Extragradient Updates",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Nguyen, T.",
      "White, S."
    ],
    "abstract": "Entropy-regularized reinforcement learning (RL) algorithms have shown strong empirical success, yet their convergence behavior remains poorly understood when combined with modern policy parameterizations. We revisit the classical entropy-regularized policy optimization framework and introduce a simple modification using mirror extragradient updates that improves stability without sacrificing convergence guarantees. Our method, called MEPPO, applies a predictor-corrector scheme to the natural policy gradient direction, requiring only minor computational overhead. We prove that MEPPO converges to the optimal regularized policy at a O(1/T) rate for tabular MDPs, matching the best-known bounds while operating under weaker assumptions about the behavior policy. In deep RL experiments on continuous control tasks, MEPPO demonstrates modest improvements over PPO and SAC on 6 out of 12 MuJoCo environments, though gains diminish with larger network architectures. While our theoretical results are limited to tabular settings and our empirical evaluation shows variance across seeds, this work suggests extragradient methods may offer a practical path toward more stable policy optimization. Code is available at anonymous-url.github.io.",
    "id": 102
  },
  {
    "title": "Gradient Silence in Transformer Language Models: A Post-Hoc Pruning Perspective",
    "authors": [
      "Kumar, V.",
      "Chen, L.",
      "Nguyen, T."
    ],
    "abstract": "As transformer language models scale, pruning redundant parameters after training has emerged as a practical compression strategy. We observe that magnitude-based pruning often removes weights whose gradients vanished late in training, suggesting these parameters were already effectively 'silent'. Motivated by this, we propose Gradient-Silence Weight Attribution (GSWA), a simple post-hoc criterion that ranks weights by the cumulative  \u21132 norm of their gradients over the final 5% of training steps. On the Pythia-410M model and three downstream tasks, GSWA retains 98.2% of zero-shot accuracy when 30% of weights are pruned, slightly outperforming standard magnitude pruning (97.6%). Theoretically, we bound the error introduced by removing low-GSWA weights under a sparsity-promoting regularizer, though our assumptions (diagonal Fisher approximation, bounded token covariance) limit generality. Ablations show that using only the final 1% of steps hurts performance, indicating that a short horizon is necessary but insufficient. While GSWA is cheap to compute and complementary to existing methods, its gains are incremental, and we do not establish whether the identified silence is causal or merely correlated with redundancy. We release code and sparse checkpoints to facilitate further study.",
    "id": 103
  },
  {
    "title": "Improving Generalization in Meta-Learning via Task-Distribution Sharpness Minimization",
    "authors": [
      "Kumar, A.",
      "Nguyen, T.",
      "Klein, S."
    ],
    "abstract": "Model-agnostic meta-learning (MAML) algorithms adapt quickly to new tasks by learning an initialization that performs well across the meta-training distribution. We investigate why standard MAML solutions can over-fit to frequently sampled tasks under minor covariate shift, and propose Task-distribution Sharpness Minimization (TSM): a simple plug-in that augments gradients with an approximate Hessian penalty to favor flatter minima. On toy regression and Omniglot benchmarks, TSM lowers within-task overfitting but introduces a small computational overhead (<7% wall-clock). Theory for over-parameterized linear models shows TSM reduces excess risk under covariate shift at the price of an O(\u221an) higher bias. Experiments on Mini-ImageNet give +1.8% accuracy and better worst-class recall over MAML, yet gains vanish on the more diverse tieredImageNet split. Ablations indicate that curvature damping and batch-size interact non-trivially, suggesting that hyper-parameter sensitivity may limit practical impact. Code is attached; we discuss failure modes when task distributions are scarcely explored. Contributions are (i) identifying sharpness as a driver of meta-overfitting and (ii) a lightweight sharpness-aware optimizer that yields modest improvements on popular benchmarks. We hope these findings encourage further work on geometry-based regularization in meta-learning.",
    "id": 104
  },
  {
    "title": "GradNorm-Lite: Layer-Wise Gradient Scaling for Faster Convergence in Over-Parameterized Networks",
    "authors": [
      "Nguyen, T.",
      "Kumar, V.",
      "Zhou, L."
    ],
    "abstract": "We propose GradNorm-Lite, a lightweight extension of the popular GradNorm algorithm that adaptively scales layer-wise gradient magnitudes without requiring auxiliary loss terms or computational overhead at inference. By re-parameterizing gradient norms through an online moment-matching objective that only tracks first-order statistics, GradNorm-Lite achieves 10\u201315 % faster convergence on CIFAR-10/100 and ImageNet while using 32 % fewer gradient norm computations compared to the original GradNorm. Although our method does not consistently outperform carefully-tuned baseline schedules, it reduces sensitivity to the initial learning rate choice across six standard architectures (ResNet, DenseNet, ConvNeXt, Swin, ViT-Tiny, and GPT-2 124 M). Ablation studies show the layer-wise scaling coefficients stabilize after roughly one quarter of training epochs, suggesting the technique mainly accelerates early optimization. Theoretically, we bound the additional noise introduced by the approximate statistics, but our bound is looser than comparable adaptive optimizers and assumes L-smoothness constants are known. Code and tuned hyper-parameters are provided to ensure reproducibility.",
    "id": 105
  },
  {
    "title": "LoRa-C: A Lipschitz-Regularized Curriculum for Improving Generalization in Reinforcement Learning",
    "authors": [
      "Kumar, V.",
      "Chen, J.",
      "Nguyen, T."
    ],
    "abstract": "Deep reinforcement learning agents often struggle with unstable training and poor generalization to unseen environments. We propose LoRa-C, a lightweight curriculum-based method that applies a learned Lipschitz regularizer to policy updates, encouraging smoother value functions without heavy computational overhead. Our approach alternates between phases of aggressive policy improvement and conservative regularization, guided by a simple schedule derived from the agent\u2019s average TD-error. Across five continuous-control tasks in MuJoCo, LoRa-C achieves a 7\u201312% improvement in average return over standard PPO when transferring to environments with perturbed dynamics, while introducing only a 3% increase in wall-clock training time. Ablation studies indicate that the regularizer alone contributes roughly half the gain, with the curriculum schedule providing the remainder. Although our method is limited to low-dimensional state spaces and does not consistently outperform strong baselines such as SAC or DrQ on the original training domains, it offers a computationally cheap way to boost robustness for practitioners who face domain-shift at deployment. Our code and hyper-parameter configurations are publicly available.",
    "id": 106
  },
  {
    "title": "Improving Transformer Generalization with Iterative Re-Attention",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Rodriguez, L."
    ],
    "abstract": "While self-attention is central to Transformer success, recent work suggests standard single-pass attention may limit model flexibility. We propose Iterative Re-Attention (IRA), a training technique that applies K cascaded attention modules over the same input, allowing the model to progressively refine its focus. IRA can be integrated with any pre-trained Transformer without architectural changes; at inference the modules are merged into one attention layer, preserving original efficiency. Experiments on GLUE, WMT\u201914 En-De and CIFAR-10 (treated as sequence modeling) show IRA yields average gains of 0.9 GLUE points and 0.4 BLEU over strong baselines, with K=2 already beneficial. Analysis reveals IRA encourages feature diversity and reduces error concentration on out-of-distribution subsets. Limitations include additional 30% pre-training cost for K=2 and diminishing returns beyond K=3. Code and models are provided to ensure reproducibility.",
    "id": 107
  },
  {
    "title": "Improving Transformer Sequence Length by Factorized Position Encoding",
    "authors": [
      "Kumar, V.",
      "Chen, L.",
      "Nguyen, T."
    ],
    "abstract": "The quadratic complexity of self-attention limits the sequence length that Transformers can process efficiently. While many efficient attention mechanisms have been proposed, they often require significant architectural changes or complex implementations. We propose Factorized Position Encoding (FPE), a simple modification to standard attention that jointly learns sequence position and content information in a factorized manner. Our method decomposes the attention matrix into position-position, content-content, and cross interaction terms, allowing linear complexity scaling while maintaining the original attention formulation. We evaluate FPE on long-context language modeling tasks and document-level machine translation. Results show that FPE achieves comparable perplexity to full attention on sequences up to 8K tokens while reducing computational cost by 60%. However, we observe performance degradation on longer sequences and tasks requiring fine-grained positional reasoning. Our analysis reveals that the factorization introduces an implicit bias toward capturing local dependencies over long-range interactions. While FPE provides a lightweight alternative to more complex efficient attention mechanisms, its benefits are most pronounced in specific sequence length regimes. Code and pretrained models are available at [anonymous link].",
    "id": 108
  },
  {
    "title": "Re-weighted Gradient Descent: A Simple Tweak for Modest Acceleration in Smooth Convex Optimization",
    "authors": [
      "Nguyen, T.",
      "Kim, H.",
      "Anderson, C."
    ],
    "abstract": "We present Re-weighted Gradient Descent (RGD), a lightweight modification to standard gradient descent that re-weights the iterate update using a diagonal approximation of the local curvature. Motivated by the observation that curvature information is often under-utilized in first-order methods, RGD scales the gradient by a running average of the inverse diagonal of the empirical Fisher information matrix, computed with minimal overhead from batch gradients. On convex logistic regression and shallow auto-encoder benchmarks, RGD converges 8\u201315 % faster in wall-clock time than Adam and SGD+momentum while requiring only 4 % extra memory and no additional hyper-parameters beyond the learning rate. Theoretically, we prove that RGD achieves an O(1/t) rate for smooth, strongly-convex objectives\u2014matching gradient descent but with a smaller effective constant when the iterates enter a low-curvature subspace. Although the speed-ups are incremental and the method does not improve the worst-case rate, RGD is trivial to implement in existing codebases and consistently outperforms tuned baselines on medium-scale problems with redundant features. We release a 20-line PyTorch implementation and discuss limitations in the stochastic, non-convex regime where diagonal curvature estimates are noisy.",
    "id": 109
  },
  {
    "title": "Gradient Alignment Improves Transfer in Fine-Tuned Language Models, Sometimes",
    "authors": [
      "Kumar, V.",
      "Ouyang, T.",
      "Fern\u00e1ndez, L."
    ],
    "abstract": "Fine-tuning large pre-trained language models has become the default paradigm for downstream NLP tasks, yet our understanding of when and why this yields better generalization remains limited. We study the role of gradient alignment\u2014the cosine similarity between gradients computed on the source (pre-training) and target (downstream) objectives\u2014as a predictor of transfer success. Using 12 GLUE tasks and three model sizes (110M, 340M, 1.5B parameters), we show that higher gradient alignment during early fine-tuning correlates moderately with final transfer accuracy (Spearman \u03c1 = 0.42). Motivated by this, we propose AlignReg, a lightweight regularizer that encourages alignment without requiring source-task data. On six tasks, AlignReg yields average gains of 1.3 F1 over standard fine-tuning, but degrades performance by 0.8 F1 on the remaining six, with larger degradation on syntactic tasks. Analysis reveals that alignment helps when target data size \u2264 5k examples, but can over-constrain optimization on larger datasets. Our findings suggest gradient alignment is a useful but insufficient indicator of transfer, and that explicit regularization based on it should be applied selectively.",
    "id": 110
  },
  {
    "title": "Revisiting Entropy Regularization with Amortized Tempering for Improved Off-Policy Reinforcement Learning",
    "authors": [
      "Kumar, A.",
      "Nguyen, T.",
      "Robinson, S."
    ],
    "abstract": "Entropy regularization is widely used to encourage exploration in reinforcement learning, but choosing the right temperature parameter remains challenging. We propose Amortized Tempering for Entropy Regularization (ATER), a simple method that adaptively adjusts the temperature parameter during training through a learned amortization network. Our approach maintains a distribution over temperatures and updates it based on policy improvement signals, eliminating the need for manual tuning. We evaluate ATER on continuous control tasks from the DeepMind Control Suite and find that it achieves competitive performance with the best fixed temperature in hindsight on 6 out of 10 environments, while requiring 30% fewer hyperparameter sweeps. Additionally, we provide theoretical analysis showing that ATER converges to a near-optimal temperature under certain regularity conditions. While our method shows promise for reducing hyperparameter sensitivity, we observe that the performance gains are modest compared to recent adaptive regularization techniques, and the computational overhead may not justify deployment in all scenarios. Our code is available at [anonymous link].",
    "id": 111
  },
  {
    "title": "Improved Hessian-Based Generalization Bounds via Second-Order Sharpness-Aware Minimization",
    "authors": [
      "Liu, H.",
      "Kumar, S.",
      "Vasileva, D."
    ],
    "abstract": "We propose Second-Order Sharpness-Aware Minimization (SOSAM), a simple extension to SAM that additionally penalizes the trace of the empirical Hessian diagonal. In experiments on CIFAR-10, CIFAR-100, and ImageNette, SOSAM outperforms SAM by 0.3\u20130.7% test accuracy with negligible overhead (<3% wall-clock). We further show that SOSAM tightens existing PAC-Bayesian bounds by an average factor of 1.8\u00d7 for ResNet-18 and WRN-28-10, suggesting improved generalization. Ablation studies indicate that the Hessian term is most beneficial for label-noise settings >20%. While our theoretical analysis relies on the diagonal approximation and bounded loss assumptions, empirical evidence aligns well with predicted improvements. Code and 200 random seeds are publicly available.",
    "id": 112
  },
  {
    "title": "Adaptive Gradient Clipping with Layer-wise Curvature Estimates for Faster Transformer Training",
    "authors": [
      "Kumar, S.",
      "Lee, J.",
      "Zhou, H."
    ],
    "abstract": "Gradient clipping is routinely used when training large Transformer models, yet its global threshold is typically set via hand-tuned constants that ignore the varying curvature across layers. We propose a per-layer adaptive clipping strategy that re-scales gradients according to an online estimate of the local Lipschitz constant derived from the Hutchinson estimator. On three medium-scale language-modeling benchmarks (\u2264 1.3 B parameters) our method reduces optimizer steps to target validation loss by 8\u201314 % versus standard clipping, at the cost of a 5 % per-step overhead for the curvature estimator. Ablation studies show that most gains come from the two deepest blocks, corroborating the hypothesis that gradient magnitudes are heteroscedastic across depth. While the speed-up is consistent, we observe no improvement in final perplexity after sufficient training, and wall-clock benefits shrink on systems with stronger compute budgets. Theoretically, we prove that the clipping operation is equivalent to an implicit trust-region step only when the curvature bound holds uniformly; our estimator meets this assumption with high probability for sufficiently large batch sizes (\u2265 2k). Code is provided for reproduction.",
    "id": 113
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Inference via Quadratic Extrapolation",
    "authors": [
      "Kim, S.",
      "Rodriguez, L.",
      "Chen, T."
    ],
    "abstract": "Gradient-based optimization of discrete latent variable models remains challenging due to the high variance of score-function estimators. We propose Quadratic Extrapolation for Gradient Estimation (QEGA), a control-variate technique that constructs quadratic baselines from previous iterations to reduce variance. Unlike moving-average baselines, QEGA adaptively extrapolates parameters based on the optimization trajectory, yielding lower variance when the loss landscape is locally smooth. Across five benchmark tasks\u2014including discrete VAE training and structured prediction\u2014QEGA achieves 5-12 % lower gradient variance and marginal improvements in final ELBO compared to REINFORCE with optimized moving-average baselines. Although our theoretical analysis is limited to strongly log-concave exponential families, empirical results suggest broader applicability. Code and hyper-parameters are provided for reproducibility.",
    "id": 114
  },
  {
    "title": "Revisiting Dropout for Uncertainty Estimation in Small-Scale Neural Networks",
    "authors": [
      "Chen, L.",
      "Vasquez, J.",
      "Kumar, A."
    ],
    "abstract": "Bayesian neural networks promise calibrated uncertainty but remain prohibitively expensive for modest-sized applications. We reconsider standard dropout training as a lightweight alternative and ask whether simple post-hoc corrections can turn arbitrary networks into reliable uncertainty estimators. Starting with the observation that usual Monte-Carlo dropout severely under-estimates epistemic variance, we derive a data-dependent scaling term that re-calibrates predictive variance without model changes. On UCI regression benchmarks our procedure yields 5-15% lower NLL than vanilla dropout and is competitive with Deep Ensembles while using a single model. However, calibration gains vanish on high-dimensional inputs (CIFAR-10), and the method still lags behind gold-standard Hamiltonian Monte-Carlo. Theoretical analysis shows the proposed adjustment is equivalent to a misspecified prior, explaining limited robustness under covariate shift. Code and trained weights are provided, but the calibration procedure requires held-out data, raising fairness concerns in production regimes. Our results suggest semi-Bayesian tricks can trade a small accuracy drop for moderate uncertainty gains, yet fall short of full Bayesian validity.",
    "id": 115
  },
  {
    "title": "Momentum-Guided Learning Rate Scheduling via Local Curvature Estimates",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Fernandez, A."
    ],
    "abstract": "Modern deep learning relies heavily on adaptive optimizers, yet practical learning-rate schedules remain largely hand-tuned. We propose MoCurv, a plug-in schedule that adaptively adjusts the global learning-rate by monitoring the alignment between stochastic momentum updates and local curvature approximated by the per-sample Hessian diagonal. When this alignment exceeds a threshold, the step size is increased, and vice-versa, eliminating the need for manual decay plans. On CIFAR-10/100 and ImageNet, MoCurv matches the final accuracy of tuned cosine annealing while cutting tuning effort. Theoretically, we show that MoCurv converges in O(1/T) for smooth non-convex objectives, matching standard SGD+momentum. Ablation studies indicate sensitivity to the alignment threshold and to the Hessian approximation noise, suggesting room for refinement. Code and checkpoints are provided.",
    "id": 116
  },
  {
    "title": "Boosting by Example: Iterative Refinement with Contrastive Pseudo-Labels",
    "authors": [
      "Garcia, M.",
      "Chen, Y.",
      "Kumar, A."
    ],
    "abstract": "Self-training relies on pseudo-labeling unlabeled data, yet mistakes in these labels can accumulate and degrade performance. We propose Iterative Boosting Refinement (IBR), a simple wrapper that periodically re-weights unlabeled examples according to per-sample uncertainty. IBR alternates between (i) training a model, (ii) generating soft labels on a moving subset of \u2018hard\u2019 examples whose predictions disagree with a momentum teacher, and (iii) re-fitting on the union of gold labels and the current pseudo-label set. Experiments on CIFAR-100 and ImageNet-50 show 1\u20132% top-1 gains over FixMatch and a 20% parameter reduction relative to VAT, while requiring no new loss terms or architectural changes. Although gains saturate on larger models and the method is sensitive to the threshold schedule, IBR offers a lightweight pathway to recycle existing checkpoints without extra data. Code and checkpoints are provided.",
    "id": 117
  },
  {
    "title": "Improving Transformer Generalization with Frequency-Sensitive Positional Encodings",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "While transformers have shown impressive performance across sequence modeling tasks, they often struggle to generalize to sequences longer than those seen during training. We hypothesize that standard positional encodings fail to capture frequency-based patterns that are crucial for length extrapolation. To address this, we introduce Frequency-Sensitive Positional Encodings (FSPE), which incorporate learnable sinusoidal components that adapt to the frequency content of the training data. Our method augments existing relative positional encodings by learning optimal frequency parameters through a lightweight auxiliary objective that predicts local sequence periodicities. We evaluate FSPE on synthetic algorithmic tasks, language modeling, and image sequence prediction. Results show modest improvements: 8-12% better length extrapolation on algorithmic tasks and 1.5-2.1% lower perplexity when generalizing beyond training lengths. Ablation studies reveal that the frequency learning component contributes most benefits for sequences with periodic structure, while providing minimal gains on natural language data. While our approach offers a simple plug-in enhancement for existing transformer architectures, the improvements are task-specific and require additional hyperparameter tuning. These mixed results suggest that frequency-aware positional representations may help particular extrapolation scenarios but are not a universal solution to transformer length generalization.",
    "id": 118
  },
  {
    "title": "Gradient-Aware Kernel Tuning for Scalable Gaussian Processes",
    "authors": [
      "Nguyen, T.",
      "Kwon, J.",
      "Singh, P."
    ],
    "abstract": "Gaussian processes (GPs) provide well-calibrated uncertainty estimates, but their O(N\u00b3) complexity limits application to large datasets. While sparse variational approximations mitigate cost, kernel hyperparameters are typically selected by maximizing the marginal likelihood on a small subset, ignoring gradient information from the full data. We propose gradient-aware kernel tuning (GrAKT), a one-shot procedure that adjusts length-scales by matching gradient covariances computed on random Fourier features with those estimated from mini-batch gradients of a pretrained neural network. On UCI benchmarks with up to 200K points, GrAKT reduces negative log-likelihood by 6\u201312% compared to subset optimization at identical wall-clock time, and matches the accuracy of the more costly full-data approach in 70% of tasks. Theoretically, we bound the KL divergence between the GrAKT and exact GP posteriors by \u00d5(1/\u221aM) where M is the number of features. Although our bound depends on a strong data-subspace assumption that may not hold for heavy-tailed spectra, ablations show the gradient alignment term contributes 60% of the observed gain. Code is available at anonymous.url/GrAKT.",
    "id": 119
  },
  {
    "title": "AdaQR: Adaptive Query Reduction for Efficient Transformer Inference on Long Sequences",
    "authors": [
      "Kumar, A.",
      "Santos, L.",
      "Zhou, J."
    ],
    "abstract": "Transformer models achieve state-of-the-art performance across many tasks, but their quadratic complexity in sequence length limits deployment for long inputs. Prior work speeds up inference by pruning queries in self-attention, yet relies on fixed heuristics that ignore layer-wise redundancy and content structure, often degrading accuracy. We introduce AdaQR, a lightweight plug-in that learns to adaptively drop queries at runtime. Our method trains a tiny recurrent policy on a proxy task that predicts attention entropy from pooled key statistics. During inference, the policy emits a keep/drop action per query and head, trimming the attention matrix before the softmax. On the Long-Range Arena benchmark, AdaQR reduces FLOPs by 32% and wall-clock latency by 23% relative to vanilla transformers with only 0.8% drop in task accuracy. Ablations reveal that entropy-based gating outperforms uniform sparsity and random baselines. While gains are consistent, they plateau on sequences beyond 4K tokens, and fine-tuning the backbone restores only half of the lost accuracy, suggesting room for improvement. Code and checkpoints are provided to reproduce all experiments.",
    "id": 120
  },
  {
    "title": "RevisitingMomentum: A Lightweight Momentum Variant for Faster Stochastic Optimization",
    "authors": [
      "Kumar, A.",
      "Liu, S.",
      "Roberts, J."
    ],
    "abstract": "We present RevisitingMomentum (RM), a simple modification to classical momentum that adapts the momentum coefficient using a quadratic surrogate of the local loss landscape. Unlike adaptive methods such as Adam or RMSprop, RM retains the low memory footprint of vanilla SGD while offering modest speed-ups on deep network training. Our key insight is that the optimal momentum coefficient can be approximated by the relative change in gradient norm over a short horizon, yielding an update rule with negligible overhead. On CIFAR-10/100 and ImageNet, RM improves convergence by 5\u201312 % over tuned SGD+momentum at the same learning rate schedule, but gains diminish when the baseline is heavily optimized. Theoretically, we prove O(1/T) convergence for convex smooth objectives, matching standard momentum. Ablation studies show that the surrogate horizon hyperparameter is stable across architectures, although extreme batch sizes require retuning. Code is provided for reproducibility. While RM does not surpass carefully tuned schedules or second-order methods, its simplicity and consistent small gains may benefit practitioners with limited compute budgets.",
    "id": 121
  },
  {
    "title": "LoRA-NTK: A Parameter-Efficient Initialization for Low-Rank Adaptation of Large Language Models",
    "authors": [
      "Chen, L.",
      "Krishnan, A.",
      "Roberts, S."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, but its performance depends heavily on the random initialization of the low-rank matrices. We propose LoRA-NTK, a simple initialization scheme inspired by the Neural Tangent Kernel (NTK) theory that sets the initial scale of adaptation matrices based on the spectrum of pretrained weights. Our method requires no additional hyperparameters beyond standard LoRA and adds negligible computational overhead. We evaluate LoRA-NTK on instruction tuning and domain adaptation tasks using LLaMA-7B and OPT-13B models. Experiments across 8 datasets show modest but consistent improvements over standard LoRA initialization, with average gains of 1.2% on downstream tasks. Ablation studies reveal that our initialization particularly helps in low-data regimes (\u22641K examples), suggesting better utilization of the pretrained representation. While the improvements are incremental and task-dependent, our work provides theoretical insights into the role of initialization in parameter-efficient fine-tuning and offers practitioners a drop-in replacement that costs nothing to implement. Code is available at anonymous-github.url.",
    "id": 122
  },
  {
    "title": "Improved Gradient Descent via Passive Momentum Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We revisit momentum-based acceleration for stochastic gradient descent (SGD) through the lens of passive hyper-parameter scheduling. Rather than tuning momentum coefficients manually or using adaptive optimizers, we propose Passive Momentum Scheduling (PMS), which anneals momentum following a fixed quasi-periodic trajectory derived from a second-order differential equation. Our key insight is that this deterministic schedule implicitly regularizes the effective learning rate without introducing additional learnable parameters. On CIFAR-10/100 and ImageNet, PMS coupled with vanilla SGD matches the convergence speed of AdamW while maintaining generalization within 0.6% of hand-tuned baselines. Theoretical analysis in the convex quadratic setting shows that PMS achieves an accelerated O(1/t\u00b2) rate up to a logarithmic factor, although the proof relies on an idealized full-batch assumption that does not carry over to the mini-batch regime. Ablation studies indicate that the specific frequency of the schedule matters less than adhering to any consistent annealing pattern. Code is available, but requires a custom CUDA kernel for efficiency. Overall, PMS offers a lightweight drop-in replacement for manual momentum tuning, yet its advantage diminishes when combined with modern learning-rate schedulers.",
    "id": 123
  },
  {
    "title": "Gradient-Noise Re-scaling for Improved Semi-Supervised Learning under Memory Constraints",
    "authors": [
      "Chen, L.",
      "Bose, J.",
      "Murthy, K."
    ],
    "abstract": "Semi-supervised learning (SSL) frequently relies on large, unlabeled batches to regularize feature representations, but storing these batches often exceeds limited GPU memory. We propose Gradient-Noise Re-scaling (GNR), a lightweight modification that re-weights parameter updates according to a learned noise model rather than keeping full batches in memory. During each iteration, GNR approximates the gradient signal that would have been obtained from the unseen examples by perturbing the current mini-batch gradients with calibrated noise estimated from previous steps. We evaluate GNR on CIFAR-10, SVHN, and a 10-class ImageNet subset, using 4 GB and 8 GB memory caps. Test accuracy improves by 0.9\u20131.6% over the strongest memory-bounded baselines (MixMatch, FixMatch) while using 30\u201345% less memory; the gains vanish when unlimited memory is available. Theoretically, we prove a PAC-Bayesian bound showing that GNR reduces generalization error when label noise is moderate, but the bound becomes vacuous under high noise. Experiments confirm that performance degrades when the labeled set drops below 1% of the data. Code and checkpoints will be released upon acceptance.",
    "id": 124
  },
  {
    "title": "Rethinking Batch-Normalization-Aware Quantization via Second-Order Sensitivity Scores",
    "authors": [
      "Kovacs, B.",
      "Nguyen, T.",
      "Zhou, S."
    ],
    "abstract": "Batch-normalization-aware (BNA) quantization has emerged as a practical way to train low-bit networks without recalibrating BN statistics post-training. Yet the field lacks a principled measure of how sensitive different BN layers are to quantization noise, leading to heuristics that either freeze or re-estimate all BNs. We propose a lightweight second-order sensitivity score that estimates the Hessian trace of each BN layer with respect to the quantized weights, without back-propagating through the full model. Layers with small scores are frozen, while those with large scores are re-estimated using a small calibration set. Experiments on ImageNet show 1.3\u20131.7\u00d7 speed-ups over full re-estimation pipelines with <0.4% top-1 loss compared to 8-bit baselines for ResNet-50/18 and MobileNet-V2. Ablation studies indicate the metric correlates reasonably with brute-force sensitivity sweeps (Spearman \u03c1=0.72), but gains diminish on architectures with synchronized BN or depthwise separable layers. While the method is simple to implement in existing quantization tool-chains, we acknowledge limitations: our theoretical justification assumes a local quadratic loss landscape and the calibration set size must be tuned per architecture. Code and quantized models are provided to ensure reproducibility.",
    "id": 125
  },
  {
    "title": "Revisiting Momentum Schedules for Practical Stochastic Optimization",
    "authors": [
      "Kovacs, L.",
      "Singh, P.",
      "Matsumoto, K."
    ],
    "abstract": "We present an empirical study of momentum scheduling strategies for stochastic gradient descent (SGD) in modern deep learning settings. While constant momentum parameters remain the default choice across most applications, recent theoretical work suggests time-varying schedules could improve convergence. We systematically evaluate polynomial, exponential, and cyclical momentum schedules on standard vision and language benchmarks. Our experiments reveal that certain schedules provide modest improvements over constant momentum (averaging 0.8% test accuracy gain across CIFAR-10, CIFAR-100, and ImageNet), particularly in low-noise training regimes. However, we find these gains are inconsistent across architectures and disappear when standard regularization techniques are applied. We provide theoretical justification for these observations by extending classical momentum analysis to stochastic settings with label noise. While our work offers practical guidance for practitioners seeking marginal improvements, we conclude that the computational overhead of scheduling may outweigh benefits in many scenarios. Our code and hyperparameter configurations are available online.",
    "id": 126
  },
  {
    "title": "Improved Block-Diagonal Approximations for Distributed Second-Order Optimization",
    "authors": [
      "Kim, J.",
      "Singh, V.",
      "Chen, H."
    ],
    "abstract": "Second-order methods promise faster convergence than stochastic gradient descent, but their distributed implementations are bottlenecked by costly communication and inversion of the full Hessian. We propose a simple block-diagonal approximation that partitions parameters by layer and applies a Kronecker-factored damping correction to each block. The scheme is implemented in PyTorch with gradient all-reduce only once per epoch, matching the communication cost of first-order baselines. On CIFAR-10/100 and ImageNet training of ResNet-18/50, the method reduces epochs-to-target by 15\u201325 % relative to SGD with momentum while keeping the peak memory per worker unchanged. A proximal variant is derived for federated setups with intermittent client availability; experiments on non-IID versions of FEMNIST show 10 % higher final accuracy than FedAvg at the same communication budget. Although the convergence proof is limited to convex quadratic problems under a strict diagonal dominance assumption, empirical evidence suggests the algorithm stabilizes training in highly non-convex settings. Code and scripts are provided for full reproducibility. Our study indicates that moderately accurate curvature information, when carefully localized, can yield practical speed-ups without the infrastructure burden of standard second-order techniques.",
    "id": 127
  },
  {
    "title": "Revisiting Dropout as an Implicit Regularizer for Modern Convolutional Architectures",
    "authors": [
      "Johnson, K.",
      "Das, P.",
      "Nakamura, H."
    ],
    "abstract": "Dropout remains a widely adopted regularization technique, yet recent evidence suggests its effectiveness may diminish when training data and model capacity scale. We conduct a large-scale empirical re-evaluation of standard and refined dropout variants across 16 ImageNet-scale convolutional architectures. By coupling structured dropout masks with adaptive drop-rate schedules, we obtain consistent 0.8\u20131.2% top-1 accuracy improvements compared to weight-decay-only baselines, with comparable wall-clock time. Our ablations indicate that most gains concentrate in mid-depth layers and vanish when residual connections are removed. Theoretically, we cast dropout as an instance of stochastic perturbation and derive new generalization bounds that depend on both mask sparsity and network width. While these bounds tighten as the drop probability increases, experimental trade-offs reveal that aggressive rates harm final performance, suggesting that current theory under-estimates practical sampling variance. The work provides timely evidence that dropout can still benefit modern pipelines, but highlights the need for architecture-specific calibration. Code and trained checkpoints will be released upon publication.",
    "id": 128
  },
  {
    "title": "Improving Transformer Generalization with Iterative Self-Distillation",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Chen, Y."
    ],
    "abstract": "While transformers have achieved strong performance across many domains, they often struggle with out-of-distribution generalization. We propose Iterative Self-Distillation (ISD), a simple technique that improves generalization by having the model teach itself through successive distillation steps. Our method works by periodically updating a teacher model as the exponential moving average of the student, then distilling knowledge from this teacher to refine the student's predictions. We evaluate ISD on image classification and natural language understanding tasks, showing 2-4% improvements in accuracy on corrupted test sets compared to standard training. Theoretical analysis suggests ISD acts as a form of regularization that encourages smoother decision boundaries. While our empirical results are encouraging, we acknowledge that the improvements are modest and the theoretical understanding remains incomplete. We hope this work sparks further investigation into self-distillation as a practical regularization technique for improving robustness in neural networks.",
    "id": 129
  },
  {
    "title": "Randomized Block-Coordinate Adam: Improving Convergence via Subspace Momentum",
    "authors": [
      "Navarro, E.",
      "Zhao, H.",
      "Chen, T."
    ],
    "abstract": "Momentum-based adaptive optimizers such as Adam accelerate training in many deep-learning tasks but suffer from slower final-phase convergence relative to SGD. We present Randomized Block-Coordinate Adam (rBC-A), a simple modification that decouples the moment estimates across randomly selected parameter blocks. By intermittently freezing subsets of the model, rBC-A reduces gradient noise variance while preserving second-order moment adaptation in the active block, yielding better iterate stability without computing full-batch statistics. On convex toy problems, rBC-A is provably within a constant factor of the optimal convergence rate established for SGD with momentum and matches the worst-case bound of Adam. In empirical evaluations on CIFAR-10/100 and WikiText-2 with ResNet-20 and a 6-layer Transformer, rBC-A achieves comparable or slightly better final accuracy (+0.3 % avg) and 6-12 % faster wall-clock time than vanilla Adam, at the cost of two tunable hyper-parameters (block size p and freeze probability \u03b1). Ablation studies suggest that gains diminish as model width grows beyond 50 M parameters, indicating that the method is most useful for moderate-scale regimes. Code and 20 random seeds are available in the supplementary ZIP.",
    "id": 130
  },
  {
    "title": "Combining Momentum with Adam: A Gentle Push for Better Generalization",
    "authors": [
      "Kumar, V.",
      "Li, S.",
      "Oliveira, T."
    ],
    "abstract": "Adaptive optimizers such as Adam are widely adopted for training deep networks, yet they sometimes lag behind SGD+momentum in final test accuracy. We revisit the role of classical momentum in adaptive schemes and propose \u201cPAdam\u201d, a lightweight modification that injects a momentum buffer into Adam's update only when the gradient signal-to-noise ratio falls below a learned threshold. The key idea is to let the optimizer borrow SGD-like stability in low-curvature regions while preserving Adam's fast early progress. On CIFAR-10/100 and ImageNet, PAdam yields 0.3\u20130.7% accuracy gains over vanilla Adam at no extra cost, and matches or slightly outperforms finely-tuned SGD schedules. We further show that PAdam adapts more smoothly to aggressive learning-rate warmup, leading to a 5-8% reduction in validation loss variance across five runs. A regret bound is provided for convex Lipschitz losses, although our analysis relies on a bounded-gradient assumption that may not hold in practice. Code is available at anonymized-url.",
    "id": 131
  },
  {
    "title": "Lookahead Learning: Shaping Representations with Future-Step Predictions",
    "authors": [
      "Kim, J.",
      "Santos, L.",
      "M\u00fcller, H."
    ],
    "abstract": "We propose Lookahead Learning, a self-supervised auxiliary objective that trains encoders to predict features several optimization steps into thefuture. By minimizing the distance between current and future embeddings, the encoder is encouraged to evolve representations smoothly along the loss surface. On CIFAR-10/100 and ImageNet, adding the objective to standard cross-entropy training yields 0.4\u20131.2 % accuracy gains and faster initial convergence, with <3 % overhead. Ablations show the effect holds for lookahead horizons of 3\u201310 steps; longer horizons degrade performance. Although gains are modest and task-specific, the method requires no architectural changes and is complementary to existing regularizers. Theoretical analysis in a two-layer linear network suggests the objective encourages alignment between the principal eigenspaces of consecutive layers, but tight generalization bounds remain open. Code and checkpoints are provided.",
    "id": 132
  },
  {
    "title": "Residual-Mixup: A Lightweight Data Augmentation Baseline for Vision Transformers",
    "authors": [
      "Kim, S.",
      "Rodriguez, M.",
      "Liu, J."
    ],
    "abstract": "Data augmentation improves generalization, yet the community has not converged on an augmentation recipe for Vision Transformers (ViTs) comparable to the established \"medium-strength\" policy used for CNNs. We propose Residual-Mixup, a trivial modification that linearly interpolates image patches after the patch-projection layer while simultaneously blending labels. By mixing in patch space rather than pixel space, the augmentation is applied at the same granularity as the transformer\u2019s internal tokens, inducing smoother attention landscapes without extra parameters. On ImageNet-1k with four common ViT variants (Ti/16, S/16, B/16, L/16) trained from scratch for 300 epochs, Residual-Mixup yields consistent +0.6-0.9 % top-1 gains over the standard augmentation baseline, and is complementary to RandAugment. Ablations show the effect persists across different patch sizes and training schedules, and that it marginally reduces attention entropy. The method is easy to implement (five lines of code), introduces no measurable overhead, and supports arbitrary \u03b1\u2208[0,1] without tuning. Code is provided. While the contribution is incremental and limited to supervised image classification, the technique offers a practical, reproducible baseline that may ease future ViT research.",
    "id": 133
  },
  {
    "title": "Learning with Gradient-Noise Scaling Improves Online Quantile Estimation",
    "authors": [
      "Nguyen, L.",
      "Ghosh, D.",
      "Ahmed, S."
    ],
    "abstract": "Recent work reveals correlations between stochastic gradient variances and local curvature, prompting us to re-examine explicit noise modeling in online learning. We propose Gradient-Noise Scaling (GNS), a simple update rule that rescales each stochastic gradient by the inverse of an exponentially-smoothed estimate of its recent variance. Unlike adaptive optimizers such as Adam, GNS adds no momentum, performs no per-parameter learning-rate tuning, and keeps running variances solely for the rescaling coefficient. In controlled experiments on CIFAR-10, ImageNet, and several reinforcement-learning environments, GNS accelerates early training by roughly 10\u201315% and yields mildly lower final losses, but convergence improvements plateau after the initial epochs. Theoretical analysis shows GNS is equivalent to a time-varying, diagonal metric preconditioner under quadratic interpolation errors, though this view is restricted to local neighborhoods and constant noise statistics. Our primary contribution is empirical verification that lightweight noise tracking can improve online quantile estimation of gradients, providing robustness against heavy-tailed gradient distributions. The method adds only one extra hyper-parameter and halves the computational overhead of existing variance-based optimizers. Code and pre-trained models are provided for reproduction. While the gains are consistent, they are incremental; larger architectures and long-training regimes show diminishing returns, questioning the method's significance beyond warm-up regimes.",
    "id": 134
  },
  {
    "title": "Probable Fix: A Posterior Adjustment to Improve Calibration of Deep Ensembles",
    "authors": [
      "Kim, J.",
      "Rossi, S.",
      "Zhao, L."
    ],
    "abstract": "Ensemble methods remain a popular approach for estimating predictive uncertainty, yet even deep ensembles tend to yield systematically over-confident probabilities as network depth grows. We propose Probable Fix (PF), a lightweight, temperature-free post-training step that improves calibration while preserving accuracy. Given a held-out validation set, PF estimates per-sample Laplace approximations of the ensemble logits and applies a data-dependent correction that effectively shrinks confident predictions toward the uniform distribution. On CIFAR-10/100, ImageNet and three NLP classification tasks, PF yields average calibration error reductions of 8\u201315% with no additional training or hyper-parameter tuning. Theoretically, we bound the KL-divergence between PF and the Bayes posterior under a Gaussian homoscedastic assumption; experiments on synthetic data confirm the bound is tight when the assumption holds. Although PF consistently outperforms temperature scaling across our benchmarks, the gains are incremental and rely on a small validation set whose distribution must match test data; performance degrades under moderate covariate shift. Our code is publicly available. We see PF as a simple drop-in module for practitioners seeking modest uncertainty improvements without retraining, but acknowledge its scope is limited to classification and its theoretical guarantees do not extend to modern architectures with skip connections.",
    "id": 135
  },
  {
    "title": "Improved Generalization via Iterative Sample Reweighting",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Empirical risk minimization can over-fit training distributions that differ from deployment data. We propose Iterative Sample Reweighting (ISR), a simple algorithm that re-ights each mini-batch according to estimated generalization gaps accumulated from prior epochs. ISR alternates between learning a predictor and updating per-sample weights via a held-out validation loss, without requiring domain knowledge or extra models. On CIFAR-10-C and three text-classification benchmarks, ISR improves test accuracy by 0.8\u20131.4 pp over ERM and 0.2\u20130.5 pp over recent reweighting schemes, while adding <5% training time. Theoretically, we bound the expected risk under covariate shift by the weighted empirical risk plus a term that decays with effective sample size, offering limited but novel justification. Ablation studies reveal that gains diminish with heavy data augmentation and that hyper-parameter sensitivity is non-trivial. Code will be released upon acceptance.",
    "id": 136
  },
  {
    "title": "Improved Margin-Based Generalization Bounds via Sample-Dependent Priors",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "We revisit margin-based generalization bounds for deep neural networks, proposing a refined PAC-Bayesian framework that incorporates sample-dependent priors. While existing bounds often rely on worst-case analyses over parameter space, our approach leverages data-dependent information to construct tighter priors, yielding non-vacuous guarantees on standard benchmarks like CIFAR-10 and ImageNet. Our method combines a modified KL divergence term with an empirically-estimated margin distribution, achieving a 30% improvement over state-of-the-art bounds in terms of the effective complexity measure. However, the improvements diminish for networks with batch normalization or residual connections, suggesting fundamental limitations in our current analysis. Theoretically, we establish a new concentration inequality for sample-dependent priors under mild assumptions, though our results require bounded loss functions and do not extend directly to cross-entropy losses commonly used in practice. Experimental validation on small-scale vision tasks demonstrates consistent improvements, but computational overhead scales poorly with network depth, limiting practical applicability to modern architectures. Our work suggests that incorporating data-dependent information into generalization bounds remains promising, though significant theoretical and computational challenges persist for scaling these insights to contemporary deep learning paradigms.",
    "id": 137
  },
  {
    "title": "Gradient Surgery with Cyclic Momentum: A Cheap Fix for Sharpness-Aware Minimization",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Roberts, S."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) improves generalization by penalizing sharp local minima, but the required two-forward, two-backward design doubles the computational cost. We revisit the gradient-accuracy trade-off and propose Cyclic Momentum SAM (CMSAM), a plug-in wrapper that replaces one of SAM\u2019s gradient queries with a momentum buffer updated in a cyclical schedule. Experiments on CIFAR-10/100 and ImageNette show CMSAM reaches 98.4 % of SAM\u2019s accuracy while using 33 % fewer gradient calls, and removes the need for a second forward pass when the momentum buffer is warm. Theoretically, we bound the additional regret introduced by the approximate sharpness term under convex quadratic objectives, revealing a schedule-dependent slack that shrinks as O(1/\u221aT). Ablation studies indicate the cyclic coefficient, rather than momentum per-se, controls the stability\u2013sharpness frontier. Although CMSAM lags behind full SAM on larger-scale tasks without heavy augmentation, it provides a lightweight option for practitioners and highlights the largely unexplored role of momentum in sharpness regularization.",
    "id": 138
  },
  {
    "title": "Scheduled Temperature Averaging for Improved Semi-Supervised Learning",
    "authors": [
      "Garcia, L.",
      "Nguyen, P.",
      "Dubois, M."
    ],
    "abstract": "Self-training and consistency regularization have proven effective for semi-supervised learning yet remain sensitive to the quality of the teacher model. While prior work relies on exponential moving average (EMA) of weights with a fixed decay, we observe that a carefully scheduled temperature parameter integrated into the averaging process improves pseudo-label accuracy, especially when labeled examples are extremely scarce. We introduce Scheduled Temperature Averaging (STA), a simple wrapper that modulates temporal ensembling through a cyclical temperature schedule during training. On CIFAR-10 with 250 labels, STA boosts the baseline by 1.8% accuracy; similar gains appear on ImageNet-1k with 10% labels. Theoretical insights demonstrate that STA implicitly performs moment-matched distillation, which stabilizes early training iterations. Although gains diminish when label noise or domain shift is large, STA incurs minimal overhead and integrates seamlessly with existing frameworks. Code is available at anonymous-link.",
    "id": 139
  },
  {
    "title": "Gradient Norm Regularization for Improved Deep Network Generalization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Norm Regularization (GNR), a simple penalization of the parameter-gradient \u211322 norm during training, to improve generalization in deep networks. Motivated by the observation that the gradient magnitude correlates with the effective capacity of theomial models, GNR encourages flatter minima without requiring higher-order derivative information. We provide theoretical analysis in two-layer convex setting where the regularization term is equivalent to a data-dependent weight decay that reduces the Rademacher complexity by a factor of O(1/\u221am). Extensive experiments on CIFAR-10/100 and ImageNet subsets show that adding GNR to standard cross-entropy training improves test accuracy by 0.8\u20131.5% over strong baselines, with negligible computational overhead. Ablation studies reveal that the optimal coefficient is robust across architectures but sensitive to batch size and learning-rate schedules, suggesting practical tuning guidelines. While the gains diminish on datasets with heavy regularization already in place, GNR offers a drop-in enhancement that can complement existing techniques such as label smoothing and MixUp. Code is available at anonymized link.",
    "id": 140
  },
  {
    "title": "AdaMix: Adaptive Initialization for Mixup Training via Reinforcement Learning",
    "authors": [
      "Liu, C.",
      "Kumar, S.",
      "Zhao, J."
    ],
    "abstract": "Mixup, a simple data augmentation technique that trains on convex combinations of training examples, has demonstrated empirical success across vision and language tasks. However, its reliance on uniform sampling of interpolation weights has been noted to under-utilize the geometric structure learned by the model during training. We present AdaMix, a method that uses reinforcement learning (RL) to dynamically schedule the mixup coefficient at every iteration. A lightweight policy network, trained on meta-features extracted from the current mini-batch, predicts the optimal mixing weight without additional forward passes on held-out data. We evaluate AdaMix on ResNet-18/50 and ViT-B/16 classifiers trained on CIFAR-10, CIFAR-100 and ImageNet-1k, observing average test accuracy gains of 1.2\u00b10.2% across datasets and models. Notably, AdaMix yields larger improvements (+2.1%) when training from a poor random initialization produced by reduced hyper-parameter tuning budgets. Ablation studies indicate that the RL component contributes roughly 60% of the gain, while schedule annealing supplies the remainder. Although gains are dataset-dependent and the policy network adds 1.4% FLOPs per iteration, AdaMix introduces no overhead at inference and can be integrated into existing pipelines via three lines of code. Our findings suggest that RL-guided augmentation policies can provide modest but consistent improvements for mixup-based training with minimal disruption to existing workflows. Code and checkpoints are released.",
    "id": 141
  },
  {
    "title": "Gradient Surgery Revisited: A Reproducibility Check on Multi-Task Learning Optimization",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Thomas, L."
    ],
    "abstract": "Recent gradient manipulation techniques such as Gradient Surgery (GS) have shown promise for improving multi-task learning by selectively dropping conflicting gradient components. While GS has been adopted in several downstream applications, its empirical benefits have not been systematically verified across common benchmark domains. We revisit GS and examine whether its reported gains transfer to established multi-task settings in vision and NLP. Through extensive re-implementation and a controlled ablation study, we find that GS offers only marginal improvements over a properly tuned baseline that simply scales gradient norms. Specifically, on three Vision-Language datasets and two multi-objective reinforcement learning environments, GS increases average accuracy by 0.7% \u00b1 0.4% while incurring 18% higher wall-clock time. We trace the limited impact to two factors: (1) natural gradient alignment already exceeds 80% in the targeted tasks, and (2) the surgical step shrinks isometric components that assist optimization. Our results re-emphasize the need for rigorous ablations when proposing optimizer-level interventions. While this work does not introduce a new algorithm, it provides the first large-scale reproducibility report on GS and releases a modular code base that decouples gradient surgery from task-specific boilerplate.",
    "id": 142
  },
  {
    "title": "Improved Gradient Penalties for Wasserstein GANs via Adaptive Sampling",
    "authors": [
      "Chen, J.",
      "Nair, A.",
      "Schmidt, L."
    ],
    "abstract": "Training Generative Adversarial Networks (GANs) with Wasserstein distances has shown promise in producing stable and high-quality samples, yet selecting appropriate gradient-penalty weights remains empirically intensive. We propose an adaptive sampling strategy that adjusts the weight of the gradient penalty on a per-sample basis according to local discrepancies between the generator and discriminator distributions. Our method introduces a lightweight controller network trained online to predict penalty weights that keep gradient norms close to the theoretical constraint without heavy tuning. On CIFAR-10 and CelebA, the proposed technique achieves an FID of 15.3 and 8.7 respectively\u2014comparable to standard baselines with carefully tuned fixed weights\u2014while reducing sensitivity to initial learning rates by 40% across three optimizer choices. Theoretically, we provide a partial showing that the controller\u2019s objective upper-bounds local Wasserstein distances, although the global guarantee holds only under restrictive smoothness assumptions which may not apply to practical architectures. Experiments demonstrate moderate improvements in sample diversity metrics, yet the method incurs a 14% computational overhead and still exhibits mode collapse in limited-data regimes. Code and trained models are available to reproduce all experiments.",
    "id": 143
  },
  {
    "title": "Memory-Constrained Curriculum Learning via Adaptive Parameter Freezing",
    "authors": [
      "Huang, Y.",
      "Kumar, S.",
      "Johnson, A."
    ],
    "abstract": "Curriculum learning has shown promise in improving generalization by gradually increasing example difficulty, yet its practical deployment is hindered by growing memory demands as the model expands. We propose Adaptive Parameter Freezing (APF), a simple strategy that periodically freezes a fraction of parameters during curriculum stages, trading accuracy for memory efficiency. Our intuition is that early curriculum phases focus on low-level features, so freezing later-layer weights should minimally impact performance while reducing activation memory. On CIFAR-10 and ImageNet subsets, APF cuts peak GPU memory by 28% with only a 2.1% drop in final accuracy relative to standard curriculum baselines. Further ablations reveal that a dynamic schedule outperforming uniform freezing, but gains vanish when the full dataset is already small. Theoretically, we bound the error introduced by freezing using Rademacher complexity under covariate shift; however, our bound is loose for deep networks. While APF offers a practical trade-off, its benefits shrink on modern architectures with residual connections and sophisticated optimizers. Code and checkpoints are publicly available.",
    "id": 144
  },
  {
    "title": "Revisiting Gradient Clipping Thresholds for Transformer Pre-Training with Layer-Wise Learning Rate Tuning",
    "authors": [
      "Kovacs, B.",
      "Das, P.",
      "Nguyen, T."
    ],
    "abstract": "Gradient clipping is routinely applied to stabilize large-scale transformer training, yet the clipping threshold is usually chosen by ad-hoc rules. We introduce a simple re-scaling rule that sets layer-specific clipping bounds proportional to the square root of their initialization variances, combined with a schedule that anneals the global factor by 0.1% each update. On 117M-parameter language models trained with Adam we obtain 1\u20132% lower validation perplexity versus the standard global threshold of 1.0. The same rule accelerates convergence in masked-language-modeling by \u223c8% wall-clock time on the One Billion Word benchmark. Theoretical analysis for two-layer linear networks shows clipped SGD with our bound yields an O(1\u221aT) convergence rate, identical to unclipped SGD but with tighter constant factors when gradients are heavy-tailed. Ablation studies indicate the schedule contributes about two-thirds of the gain, while the layer-wise component adds the remainder. Code is provided for fair comparison.",
    "id": 145
  },
  {
    "title": "Lookahead-Q: Improving Deep Q-Learning with One-Step Rollouts",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "Deep Q-Networks (DQN) remain a foundational algorithm for reinforcement learning, yet sample efficiency remains a challenge in complex environments. We present Lookahead-Q, a simple extension to DQN that incorporates one-step model rollouts to improve value estimation. Our method trains a learned dynamics model alongside the Q-network and uses it to simulate the immediate next state for each transition, incorporating the resulting Q-value estimates through a convex combination with the standard temporal difference target. This approach requires minimal additional compute and no architectural changes to the Q-network. Across 15 Atari games, Lookahead-Q achieves a 12% median improvement in sample efficiency compared to Double DQN, with particularly strong gains on games requiring planning-like behavior. Theoretical analysis shows our update rule reduces variance under mild assumptions about model accuracy. While the improvements are consistent, they are incremental and the method adds hyperparameter sensitivity. Our code and trained models are available online.",
    "id": 146
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum Compensation for Improved Transformer Training",
    "authors": [
      "Liu, K.",
      "Chen, T.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient clipping is widely used to stabilize transformer training, but standard clipping thresholds are often set heuristically and can impede convergence. We propose Adaptive Gradient Clipping with Momentum Compensation (AGCM), a method that dynamically adjusts clipping thresholds based on gradient history while compensating for the bias introduced to momentum-based optimizers. Our approach tracks the ratio of gradient norm to parameter norm across mini-batches and adapts clipping thresholds using a exponential moving average with learnable decay. We theoretically show that AGCM reduces the upper bound on convergence rate by O(1/T) compared to fixed clipping under standard smoothness assumptions. On the WMT'14 English-German translation task, AGCM achieves 0.4 BLEU improvement over standard clipping with AdamW, while requiring minimal hyperparameter tuning. Experiments on vision transformers (ViT-B/16) demonstrate 1.2% accuracy gain on ImageNet with 15% reduction in training time. Though improvements are consistent, they are modest compared to recent architectural innovations. Our method adds computational overhead of ~5% during training and introduces one additional hyperparameter. Code is available at anonymous-url.github.io/agcm.",
    "id": 147
  },
  {
    "title": "Gradient Noise Re-scaling: A Lightweight Alternative to BatchNorm for Small-Batch Training",
    "authors": [
      "Kim, J.",
      "Garcia, L.",
      "Thompson, S."
    ],
    "abstract": "Normalization layers like BatchNorm improve optimization but rely on large batch sizes, degrading performance when memory constraints force small batches. We propose Gradient Noise Re-scaling (GNR), a simple alternative that re-scales each parameter\u2019s gradient by the inverse of its historical signal-to-noise ratio estimated from a small exponential window. GNR introduces no extra parameters, is applied only during training, and requires one additional line of code. On CIFAR-10/100 and ImageNet, ResNet-18/50 trained with GNR and batch size 2\u20134 reaches 0.3\u20131.2 % lower accuracy than large-batch BatchNorm baselines, outperforming GroupNorm, LayerNorm, and prior small-batch stabilizers in the same compute budget. Ablations show that the re-scaling term reduces gradient noise by 18 % on average and smooths the loss landscape as measured by the Hessian trace. However, gains diminish with ample regularization or very deep networks (>100 layers). Theoretically, we prove that GNR converges in O(1/\u221aT) for smooth non-convex objectives under standard assumptions. Code is provided for 1-GPU reproduction in \u226412 hours. While GNR advances lightweight small-batch training, its benefit is task-specific and does not fully close the generalization gap of large-batch training.",
    "id": 149
  },
  {
    "title": "Improved Gradient Noise Scaling for Large-Batch Training with a Modified Learning Rate Schedule",
    "authors": [
      "Kumar, V.",
      "Liu, Z.",
      "Mishra, A."
    ],
    "abstract": "Large-batch training can reduce wall-clock time but often hurts generalization due to sharp minima. We propose a simple modification to the popular linear scaling rule for SGD by introducing a batch-size dependent dampening factor that adapts the learning rate schedule during training. Our method combines a carefully chosen noise injection scheme with momentum correction to maintain optimization stability. Experiments on CIFAR-10 and ImageNet show our approach achieves comparable accuracy to small-batch baselines while using batch sizes up to 16\u00d7 larger, reducing training time by 35% on 8 GPUs. While our theoretical analysis is limited to convex quadratic objectives, empirical results suggest benefits for deep networks. Our method adds minimal computational overhead and can be easily integrated into existing frameworks. However, we observe diminishing returns beyond a certain batch size and some hyperparameter sensitivity. Code and pretrained models are available online.",
    "id": 150
  },
  {
    "title": "Gradient Norm Dropout: A Lightweight Regularizer for Deep Networks via Random Gradient Scaling",
    "authors": [
      "Liu, T.",
      "Nguyen, K.",
      "Roberts, J."
    ],
    "abstract": "We introduce Gradient Norm Dropout (GND), a simple regularization technique that randomly scales gradient norms during back-propagation. Inspired by the success of stochastic regularization methods like Dropout and DropConnect, GND perturbs gradient magnitudes with a learned noise distribution to prevent overfitting in deep neural networks. Unlike more complex regularizers that require additional model components or multi-stage training, GND can be implemented with two lines of code and adds no overhead at inference. We theoretically analyze GND in the over-parameterized regime, showing it approximately minimizes a data-adaptive robust loss. Experiments on CIFAR-10/100 and ImageNet show modest improvements (0.3-0.7% top-1 accuracy) over standard baselines, with slightly larger gains on smaller networks. While the gains are not dramatic, GND offers a practical, easy-to-implement option for practitioners seeking lightweight regularization. Our results suggest GND may be most beneficial when computational budget or memory constraints preclude heavier techniques like MixUp or Cutout. Code is available at [anonymous link].",
    "id": 151
  },
  {
    "title": "Improved Regularization via Iterative Gradient Surgery",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Ahmed, S."
    ],
    "abstract": "We propose an iterative first-order optimization procedure that periodically \"resets\" gradient components deemed uninformative according to a simple magnitude-based criterion. Applied to deep neural networks trained on standard image-classification datasets, our method yields marginal but consistent test-accuracy gains (0.5\u20131.2%) over standard SGD with momentum while reducing weight norm. Although the mechanism resembles both learning-rate annealing and weight-decay schedules, we argue it acts as a complementary regularizer by implicitly shrinking less-used parameter directions. Extensive ablations show that the gains evaporate without the proposed reset step and that performance is sensitive to the reset frequency and layer-wise learning-rate ratios. Experiments on CIFAR-10/100 and ImageNet-subset demonstrate our approach is competitive with stronger baselines at equal training epochs, yet we observe no improvement on small tabular data or language tasks. Theoretically, we connect the algorithm to a relaxed online gradient-descent bound; however, the bound is looser than classical guarantees and does not clearly explain empirical gains. Code is available anonymously on GitHub.",
    "id": 152
  },
  {
    "title": "Improved Smoothing Schedules for Score-Based Diffusion Sampling",
    "authors": [
      "Keller, J.",
      "Ouyang, L.",
      "Murali, V."
    ],
    "abstract": "Score-based diffusion models generate high-quality samples but suffer from slow MCMC iterations when discretizing the reverse stochastic differential equation (SDE). We revisit the smoothing schedule\u2014the schedule that controls the signal-to-noise ratio during training\u2014and show that a simple quadratic decay can reduce FID by 3\u20137% on CIFAR-10 and ImageNet-64 while maintaining the same wall-clock time. The proposed schedule is derived by minimizing an analytically tractable upper bound on the reverse-SDE discretization error; the bound is tightest for images with near-Laplacian spectra, a common characteristic of natural images. Although our theoretical result relies on a Gaussian assumption that can be violated in practice, we demonstrate empirically that the quadratic schedule yields consistent gains across deterministic and stochastic samplers. When combined with standard variance-exploding LDMs, we obtain a FID of 2.41 on CIFAR-10 without classifier-free guidance or distillation. Finally, we show that our schedule is complementary to existing acceleration techniques such as DDIM and DPM-Solver, suggesting a drop-in replacement for practitioners. The code and pre-trained weights are available anonymously on GitHub.",
    "id": 153
  },
  {
    "title": "LoCA: Localized Clustering Adjustment for Partial-Label Learning with Limited Annotations",
    "authors": [
      "Nguyen, T.",
      "Iyer, V.",
      "Kumar, S."
    ],
    "abstract": "Partial-label learning (PLL) addresses classification where each training instance is associated with a candidate set of labels, only one of which is valid. While existing PLL methods assume abundant partial annotations, many real-world scenarios provide only a handful of labeled candidates per class. We introduce Localized Clustering Adjustment (LoCA), a two-stage framework that first identifies class-prototype regions via unsupervised density peaks and subsequently refines per-example candidate sets through local label propagation. By jointly optimizing a cluster compactness objective with a standard PLL loss, LoCA encourages the model to down-weight ambiguous labels within each localized neighborhood. Experiments on CIFAR-100, ImageNet-127, and three text benchmarks show consistent but modest improvements (1.5-2.3% accuracy) over state-of-the-art PLL baselines when only 10% of the training data carry partial annotations. Ablation studies reveal that most gains come from the clustering step, whereas the label-propagation refinement adds marginal benefit. Although LoCA is straightforward to implement, its theoretical justification is limited to a simplified two-class setting, and hyper-parameter sensitivity can affect stability. Code is publicly available.",
    "id": 154
  },
  {
    "title": "LoRA-Flow: Memory-Efficient Adaptation of Transformer Language Models via Dynamic Low-Rank Paths",
    "authors": [
      "Nguyen, T.",
      "Kovacs, D.",
      "Zhao, L."
    ],
    "abstract": "We present LoRA-Flow, a simple extension of Low-Rank Adaptation (LoRA) that conditions rank-specific matrices on a lightweight routing network instead of learning a single static adapter. Motivated by empirical evidence that optimal intrinsic dimension varies across layers, we allocate a bank of rank-1 to rank-16 matrices per layer and learn a sparse mixture of them. On eight GLUE tasks and two domain-shift datasets, LoRA-Flow matches full fine-tuning within 0.3 F1 of LoRA while reducing trainable parameters by 35-50%. Furthermore, our ablations show that routing stabilizes optimization when LoRA rank is misspecified. Although we observe consistent gains on moderate-sized models (\u2264 1.3 B parameters), improvements vanish on larger 7 B checkpoints checkpoints, suggesting scalability limits. Theoretically, we bound the excess risk introduced by dynamic rank selection under Gaussian data assumptions; however, the bound scales unfavorably with model width. Code and hyper-parameters are provided to ensure reproducibility. While LoRA-Flow offers practical memory savings for downstream practitioners, its contribution is largely incremental and the scalability challenges raise open questions about rank-adaptive fine-tuning at scale.",
    "id": 155
  },
  {
    "title": "Gradient Norm Clipping Improves Generalization in Over-Parameterized Models: A Large-Scale Empirical Study",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We conduct the largest-to-date empirical investigation into how gradient norm clipping affects generalization in over-parameterized neural networks across vision, language, and tabular domains (2,400 trained models). While most practitioners set clipping thresholds heuristically, we systematically sweep 15 thresholds per task and propose a simple data-driven rule that predicts a near-optimal clipping value from gradient statistics computed during the first epoch. On average, our rule matches or outperforms hand-tuned baselines on 68% of tasks, yielding a statistically significant 1.3% improvement in test accuracy (p<0.01). Ablation studies reveal that moderate clipping reduces the spectral norm of layerwise Jacobians, hinting at a regularization effect complementary to batch normalization and weight decay. Nevertheless, benefits vanish when models are trained with strong data augmentation or near the interpolation threshold, suggesting practical limits to the technique\u2019s applicability. We release all logs and checkpoints to facilitate future meta-learning research on hyperparameter prediction. Although our theoretical exposition is limited, the scale of evidence and lightweight protocol may aid practitioners seeking reliable generalization boosts without extensive re-tuning.",
    "id": 157
  },
  {
    "title": "Towards Faster Neural Network Training via Cyclical Block-Diagonal Approximations",
    "authors": [
      "Kovacs, P.",
      "Wang, L.",
      "Garcia, J."
    ],
    "abstract": "Second-order optimization methods can accelerate neural network training, but their high per-iteration cost and memory complexity limit practical adoption. We propose Cyclical Block-Diagonal Adam (CBDA), a hybrid optimizer that periodically approximates the Fisher information matrix with a dynamically sized block-diagonal structure updated only every m steps. On CIFAR-10/100 and ImageNet subsets, CBDA reaches target accuracies 11\u201317% sooner in wall-clock time than AdamW on ResNet-18 and Vision Transformer models, while using 38% less memory than K-FAC. Theoretically, we show that CBDA converges at aO(1/\u221aT) rate under standard smoothness assumptions and that the block-diagonal approximation error remains bounded by a quadratic function of the learning rate. Although our ImageNet gains diminish on larger models (EfficientNet-B3), extensive ablations indicate that the procedure is most effective when combined with small weight decay and cosine annealing. Code and hyper-parameter configs are made publicly available for straightforward reproduction. The method\u2019s simplicity and drop-in PyTorch compatibility make it attractive for practitioners, yet the approximation\u2019s reliance on layer ordering and heuristic block schedules raises questions about broader applicability.",
    "id": 158
  },
  {
    "title": "Revisiting Weight Averaging with Cyclic Learning Rates for Better Generalization",
    "authors": [
      "Kumar, A.",
      "Liu, S.",
      "Thompson, J."
    ],
    "abstract": "Weight averaging has emerged as a simple yet effective technique to improve generalization in deep learning, typically applied at the end of training using an exponential moving average (EMA). In this work, we explore whether more aggressive weight averaging during training can yield additional benefits. We propose Cyclic Weight Averaging (CWA), which performs weight averaging at regular intervals throughout training, synchronized with cyclical learning rate schedules. Our approach maintains multiple averaged copies of model parameters and selects the best performing snapshot on a hold-out validation set. We conduct experiments on CIFAR-10, CIFAR-100, and ImageNet using ResNet architectures. Results show that CWA achieves modest improvements of 0.3-0.7% in test accuracy over standard EMA and baseline training, with particularly consistent gains on smaller datasets. While these improvements are statistically significant across five runs, the effect size is relatively small and computational overhead increases by approximately 20% due to maintaining multiple model copies. Theoretical analysis reveals that CWA approximates a form of ensemble learning in weight space, though the practical benefits diminish with increased model size. Our findings suggest that while cyclic weight averaging can provide marginal improvements, careful hyperparameter tuning is required to realize these gains, limiting its practical applicability.",
    "id": 159
  },
  {
    "title": "Improved Generalization via Iterative Gradient Aggregation in Federated Learning",
    "authors": [
      "Kim, J.",
      "Rodriguez, L.",
      "Singh, A."
    ],
    "abstract": "Federated learning faces challenges from client drift caused by data heterogeneity across devices. We present Iterative Gradient Aggregation (IGA), a simple modification to standard federated optimization that reduces client drift without requiring additional communication rounds. Unlike previous approaches that rely on regularization or complex control variates, IGA periodically aggregates gradients from a small subset of clients before completing local updates, effectively creating a form of implicit regularization. Our experiments on CIFAR-10, FEMNIST, and StackOverflow demonstrate that IGA improves generalization by 2-4% over FedAvg while maintaining the same communication complexity. However, we observe diminishing returns on more heterogeneous datasets like CIFAR-100, suggesting limitations in highly skewed settings. Theoretical analysis shows IGA converges for strongly convex objectives, though our bounds are not tighter than existing work for general federated learning. While IGA is straightforward to implement, it introduces an additional hyperparameter for the aggregation frequency, which may be dataset-dependent. Our code is available online, though full hyperparameter sweeps for all benchmarks were not completed due to computational constraints. These results suggest IGA is a promising but specialized technique that may be most useful in moderately heterogeneous federated settings.",
    "id": 160
  },
  {
    "title": "Momentum Rescaling: A Lightweight Alternative to Adaptive Optimizers",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Ortiz, J."
    ],
    "abstract": "Adaptive optimizers such as Adam and RMSProp accelerate training by per-parameter learning-rate adjustment, but they double memory use and can impair generalization. We propose Momentum Rescaling (MoRe), a drop-in modification to vanilla stochastic momentum SGD that re-normalizes each parameter\u2019s update by the inverse of its recent gradient variance estimated from a short, fixed-length history. MoRe retains the single-state memory footprint of SGD while adaptively rescaling updates similar to second-order moment methods. On CIFAR-10/100 and ImageNet, MoRe matches the convergence speed of Adam on ResNet and Transformer models, but achieves 0.2-0.6% higher test accuracy. We prove that MoRe converges at rate O(1/T) for smooth, non-convex objectives, matching standard SGD guarantees. Ablation studies show that the rescaling horizon (8-16 steps) and exponential decay factor (0.95) are robust across tasks. The method is implemented in <10 lines of PyTorch code and incurs <1% overhead compared to SGD. While MoRe does not surpass tuned SGD+momentum on every benchmark, it consistently outperforms adaptive optimizers in generalization, offering a middle ground between simplicity and speed. Code is available at anonymous URL.",
    "id": 161
  },
  {
    "title": "LoRa-GD: Layer-wise Randomized Gradients for Memory-efficient Large-batch Training",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Garcia, M."
    ],
    "abstract": "Large-batch training can accelerate neural-network optimization, but memory budgets often limit the maximum usable batch size. We propose LoRa-GD, a simple scheme that decouples storage from the effective batch by randomly projecting each layer\u2019s gradients onto a low-dimensional subspace before aggregation. Empirically, LoRa-GD halves memory overhead on ResNet-50 and BERT pre-training while matching baseline accuracies on ImageNet and GLUE. Theoretically, we show that layer-wise random projections approximately preserve gradient variance under standard smoothness assumptions, yielding a convergence rate of O(1/\u221aT) for non-convex objectives. When the projection dimension is tuned jointly with learning-rate warmup, LoRa-GD scales gracefully to batches of 32k on 8 V100s with only 2% final accuracy drop. Though limited to fully connected and convolutional layers (attention projections remain dense) and requiring a second-order moment estimator, LoRa-GD offers practitioners a plug-and-play wrapper for existing optimizers at the cost of one extra hyper-parameter. Code and checkpoints are provided.",
    "id": 162
  },
  {
    "title": "Improving Transformer Generalization with Gradient-Based Context Compression",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Transformer models often struggle with long-context tasks due to quadratic attention complexity and difficulty attending to relevant information. We propose Gradient-based Context Compression (GCC), a training-free method that compresses input sequences by pruning tokens with minimal gradient contributions. Our approach computes token importance through gradients of a proxy loss, then iteratively removes up to 50% of tokens while maintaining task performance. We extend this idea in a learned variant that trains a lightweight compression network to predict gradient-based importance scores, achieving 2.3\u00d7 speedup during inference. Experiments on document classification, question answering, and summarization show GCC maintains 94-98% of full-context performance while reducing computational cost by 30-60%. While our method demonstrates practical efficiency gains, we observe performance degradation on tasks requiring fine-grained reasoning over long documents. GCC provides a simple plug-and-play technique for accelerating transformer inference, though careful hyperparameter tuning is required for optimal compression ratios.",
    "id": 163
  },
  {
    "title": "Improved Gradient Noise Scaling for Distributed Training with Delayed Gradients",
    "authors": [
      "Kim, J.",
      "Rodriguez, L.",
      "Singh, P."
    ],
    "abstract": "As machine learning models grow larger, distributed training across many workers becomes essential, but synchronization overhead and communication delays often create bottlenecks. We revisit the problem of delayed gradient updates in asynchronous distributed optimization, where stale gradients can degrade convergence. While prior work has proposed various gradient weighting schemes, these methods typically require strong assumptions about delay distributions or convexity. We introduce Delay-Aware Noise Scaling (DANS), a simple technique that reweights gradients based on their staleness using an estimated noise-variance relationship. Our key insight is that delayed gradients can be viewed as noisier versions of current gradients, allowing us to adapt existing noise-scaling techniques from the robust optimization literature. We provide theoretical analysis showing convergence guarantees for convex objectives and conduct extensive experiments on CIFAR-10 and ImageNet training. Results show 5-12% improvement in final accuracy over standard asynchronous methods with minimal computational overhead. However, our method shows limited benefits when delays are small or when using adaptive optimizers like Adam. While DANS offers a practical improvement to distributed training, we acknowledge that the gains are incremental and the theoretical results are restricted to convex settings. Code is available at [anonymous link].",
    "id": 164
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Small-Batch Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose AdaNoise, a simple modification to standard stochastic gradient descent that adaptively injects Gaussian noise during optimization. While previous work has shown noise injection can improve generalization, existing methods rely on fixed schedules or require additional hyper-parameters. AdaNoise estimates the gradient noise scale online and adjusts the injection variance proportionally. Our theoretical analysis shows this approach approximately preserves the stationary distribution of SGD while encouraging exploration in sharp minima. On CIFAR-10 and ImageNet, AdaNoise improves test accuracy by 0.5-1.2% compared to strong baselines when training ResNet-18 with batch sizes 32-128. However, benefits diminish with larger batches and deeper networks. Ablations reveal the method is sensitive to the noise estimation window size, and performance varies significantly across architectures. While AdaNoise provides consistent improvements for small-batch scenarios common in resource-constrained settings, its impact appears limited for standard large-batch training regimes. Code is available at anonymized-url.",
    "id": 165
  },
  {
    "title": "AdaSVRG: Adaptive Momentum Scheduling for Stochastic Variance-Reduced Gradient Methods",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "We propose AdaSVRB, a simple modification to the stochastic variance-reduced gradient (SVRG) family that replaces the inner-loop learning-rate schedule with an automatically chosen momentum coefficient. Rather than tuning a fixed epoch length, AdaSVRB monitors the running gradient variance and restarts the inner loop whenever an empirical upper bound is exceeded. On three standard convex benchmarks (protein, covtype, rcv1) the method converges in 2\u201310% fewer effective passes than Katyusha, and matches the best-tuned SVRG step-decay. The algorithm introduces only one extra scalar hyper-parameter, requires no warm-up, and keeps the memory footprint of vanilla SVRG. We give a short, non-asymptotic convergence proof for quadratics that predicts the observed restarts, but a full theoretical justification is still open. Experiments on two-layer ReLU auto-encoders show similar trends; however, the gains shrink when the batch is larger than 512. Code is available and results are reproducible in under 30 GPU-minutes.",
    "id": 166
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum Scheduling for Language Model Fine-Tuning",
    "authors": [
      "Kumar, S.",
      "Obinwa, C.",
      "Liu, H."
    ],
    "abstract": "Fine-tuning large language models (LLMs) often suffers from unstable gradients that hinder convergence and degrade downstream performance. We propose Adaptive Gradient Clipping with Momentum Scheduling (AGCMS), a simple extension to AdamW that rescales gradients based on a rolling percentile estimate and couples the clipping threshold to a cyclical momentum schedule. On the GLUE benchmark, AGCMS improves average scores by 0.7 % over AdamW while reducing gradient norm variance by 23 %. Ablation studies indicate that the momentum schedule accounts for most gains, whereas the clipping component primarily stabilizes early training. Although the method is straightforward to implement and requires negligible extra memory, we observe only marginal improvements on larger models (>7 B parameters) and limited transfer across domains. Theoretically, we provide a convex convergence guarantee that matches standard bounds up to a constant factor. Code and hyper-parameters are publicly available.",
    "id": 167
  },
  {
    "title": "Gradient Noise Revisited: A Modified Schedule for Improved Optimization",
    "authors": [
      "Kim, J.",
      "Singh, V.",
      "Anderson, L."
    ],
    "abstract": "We revisit the role of gradient noise in stochastic optimization and propose a simple modification to standard schedules that can improve convergence in practice. Motivated by recent work linking noise scale to generalization, we introduce Adaptive Gradient Noise Injection (AGNI), which adjusts the variance of stochastic gradients according to a principled heuristic based on gradient norms and learning rate history. Our method requires minimal hyperparameter tuning and can be applied on top of existing optimizers with negligible overhead. We conduct experiments on image classification and language modeling benchmarks, showing up to 2% accuracy improvement over strong baselines in some settings. While the gains are not universal, our approach highlights the underexplored potential of controlled noise injection beyond standard regularization. Theoretical analysis is provided to motivate the schedule, though we acknowledge that the assumptions do not fully capture the complexity of deep network landscapes. Code and hyperparameters are publicly available. We believe this work opens interesting questions about the interaction between optimization dynamics and noise, though the empirical benefits are modest and further validation is needed on larger-scale tasks.",
    "id": 168
  },
  {
    "title": "LoRa-SGD: Locally-Randomized SGD with Theoretical Guarantees for Heterogeneous Federated Learning",
    "authors": [
      "Kim, J.",
      "Rodriguez, C.",
      "Singh, A."
    ],
    "abstract": "Federated learning (FL) struggles when client data distributions are highly heterogeneous, even with popular personalization techniques. We propose LoRa-SGD, a lightweight modification that injects client-specific random projections into local SGD updates. Using a new matrix-Bernstein concentration bound, we prove that LoRa-SGD converges at a rate of O(1/\u221aT) under (\u03b1,\u03b2)-dissimilarity, matching the best-known rates for homogeneous FL while requiring only projection matrices to be shared. On standard benchmarks (CIFAR-10 non-IID, Shakespeare) LoRa-SGD improves test accuracy by 2.3% over FedAvg and 1.1% over FedProx while adding <0.5% communication overhead. However, gains diminish on larger-scale datasets (ImageNet), and hyper-parameter sensitivity increases with client drift. Our code is available at anonymized-url.",
    "id": 169
  },
  {
    "title": "Boosting Gradient Descent with Adaptive Look-Ahead Momentum",
    "authors": [
      "Kovacs, B.",
      "Nguyen, T.",
      "Mehta, V."
    ],
    "abstract": "Momentum-based optimizers are widely used to accelerate gradient descent, yet their performance is sensitive to the choice of momentum coefficient and can degrade near sharp minima. We propose Look-Ahead Momentum (LAM), a lightweight modification that lets the optimizer extrapolate gradients one step into the future before applying the usual momentum update. LAM uses a learned scalar \u03b1, updated online via a simple gradient-based meta-rule, to control the look-ahead horizon. On CIFAR-10/100 and ImageNet experiments with ResNet and Vision Transformer architectures, LAM yields 3\u20137% faster convergence and 0.2\u20130.4% accuracy improvements over AdamW and SGD with momentum at the same wall-clock time, with only 1% extra memory and no additional hyper-parameters to tune. Theoretical analysis in the quadratic case shows LAM can reduce the condition-number dependence of the effective Hessian, although the improvement saturates when the spectrum is tightly clustered. While the gains are consistent, they remain modest on large-scale tasks and depend heavily on the batch size and learning-rate schedule. Code and pretrained weights are provided to ensure full reproducibility.",
    "id": 170
  },
  {
    "title": "Gradient Alignment for Transfer: A Simple Baseline Revisited",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Singh, P."
    ],
    "abstract": "Fine-tuning large pre-trained models has become the default paradigm for transfer learning, yet the community still debates which components actually matter for downstream performance. We revisit the classic linear-probe-plus-fine-tune protocol and isolate the role of gradient alignment\u2014cosine similarity between the initial gradient and the target-task objective\u2014as a predictor of transfer success. Using 12 vision and 9 language tasks, we show that a single alignment measurement explains up to 68 % of the variance in final accuracy, outperforming more elaborate metrics such as H-score and NCE. To leverage this insight, we propose Aligned Initialization (AI): a lightweight, optimizer-agnostic procedure that re-weights the pre-trained parameters before fine-tuning begins. AI raises average accuracy by 1.8 % over standard fine-tuning while adding < 30 s to training on ImageNet-scale models, and behaves consistently across optimizers and learning-rate schedules. Although alignment is conceptually simple, our large-scale empirical study reveals non-trivial interactions with batch size, regularization, and layer-wise learning rates that prior literature has overlooked. Our findings suggest that gradient alignment is a useful, but not sufficient, diagnostic for transfer; performance plateaus when alignment exceeds 0.9, indicating other factors dominate at the high-accuracy regime. Code and pre-computed alignments are provided to ensure reproducibility.",
    "id": 171
  },
  {
    "title": "Improving Transformer Generalization with Periodic Weight Re-initialization",
    "authors": [
      "Liu, M.",
      "Kovacs, B.",
      "Johnson, T."
    ],
    "abstract": "We propose Periodic Weight Re-initialization (PWR), a simple technique that improves generalization in transformer models by periodically resetting select weights during training. Motivated by the observation that transformers suffer from representational drift in later training phases, PWR selectively re-initializes 20% of feed-forward weights every k steps while maintaining learned attention patterns. Our experiments on Wikitext-103 and GLUE benchmarks show modest improvements: 1.2% average perplexity reduction on language modeling and 0.8% accuracy gain on classification tasks compared to standard training. PWR requires minimal computational overhead and integrates seamlessly with existing training pipelines. While the improvements are consistent across tasks, they remain small in magnitude and vary significantly based on hyperparameter choices. Theoretical analysis reveals PWR acts as a form of implicit regularization, though the exact mechanisms remain unclear. Our work suggests periodic re-initialization as a promising but not fully resolved approach to improving transformer training dynamics.",
    "id": 172
  },
  {
    "title": "LoRa-Flow: Lightweight Low-Rank Normalizing Flows for Fast Density Estimation",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Liu, Y."
    ],
    "abstract": "Normalizing flows provide flexible probabilistic models by learning invertible transformations, but their computational cost limits deployment on edge devices. We propose LoRa-Flow, which injects low-rank constraints into the weight matrices of coupling layers to reduce parameters without redesigning the network. On five UCI benchmarks, LoRa-Flow achieves 1.5\u00d7 speed-up and 60 % fewer parameters at the cost of <2 % bits-per-dimension increase versus unconstrained RealNVP. Theoretical analysis shows the low-rank bottleneck still preserves universal approximation in the affine coupling class. However, ablations reveal degradation on high-dimensional images, suggesting the low-rank prior is too restrictive for modalities with complex spatial correlations. Code is available.",
    "id": 173
  },
  {
    "title": "Gradient-free Optimization of Attention Temperatures via Quotient Manifold Search",
    "authors": [
      "Kumar, S.",
      "Okafor, C.",
      "Hu, Q."
    ],
    "abstract": "Transformer self-attention distributions are controlled by a single temperature parameter that is typically fixed or set via heuristics. We propose a gradient-free optimization scheme that tunes layer-wise temperatures by treating the attention cone as a point on a quotient manifold of positive-definite matrices. Our algorithm performs zeroth-order search along geodesics identified through Grassmannian embeddings, requiring only black-box access to downstream validation loss. On WMT'14 En-De and ImageNet-1k we obtain consistent BLEU/Top-1 gains of 0.3-0.5 points over learned-by-default baselines at comparable compute budgets. Theoretically, we bound the regret of the search procedure under bounded variance of the stochastic ruler estimator, yielding a convergence rate of \u00d5(T^{-2/3}). While the improvements are reliable, they are modest; ablation shows 60% of the gain can be recovered by a learnable scalar bias. Code will be made available.",
    "id": 174
  },
  {
    "title": "Improving Gradient Noise Scale Estimation with Adaptive Batch Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient noise scale (GNS) quantifies the signal-to-noise ratio of stochastic gradients and is widely used to predict large-batch training regimes. Current GNS estimators rely on fixed batch schedules that can misrepresent the true noise scale when learning rates or data ordering change. We propose Adaptive Batch GNS (AB-GNS), a simple modification that dynamically adjusts batch sizes during estimation while keeping the total sample budget constant. AB-GNS computes local noise estimates over nested mini-batches and aggregates them with an exponential moving average. On CIFAR-10/100 and ImageNet pre-training tasks, AB-GNS reduces the prediction error of critical batch size by 12\u201318% over static baselines and remains within 8% of the oracle value without requiring prior knowledge of model architecture. While our method introduces negligible overhead, we observe that gains diminish on heavily augmented or class-imbalanced data, suggesting the estimator still relies on approximate gradient norm homogeneity. Our code and 100-epoch noise traces are publicly available. AB-GNS offers practitioners a drop-in refinement to existing large-batch heuristics at minimal cost.",
    "id": 175
  },
  {
    "title": "Gradient Descent with Restart: A Simple Provable Escape from Sharp Minima",
    "authors": [
      "Kumar, S.",
      "Lopez, J.",
      "Nguyen, T."
    ],
    "abstract": "Stochastic gradient descent (SGD) is widely observed to escape poor local minima when training over-parameterized neural networks, yet sharp minima\u2014associated with poor generalization\u2014can trap optimizers for exponentially long times. We propose Gradient Descent with Restart (GDR), a minimal, hyperparameter-free variant that restarts from a uniformly random previous iterate whenever the training loss plateaus. On a class of d-dimensional parity-learning problems with planted sharp minima, we prove that GDR reaches an \u03b5-neighborhood of a flat minimum in O(1/\u03b5^2) steps with high probability, improving the \u03a9(1/\u03b5^d) persistence time of standard SGD. Empirically, training a 6-layer convolutional network on CIFAR-10 with GDR yields the same final accuracy (92.3%) as tuned SGD, yet consistently selects solutions whose eigenvalue sharpness is 18% lower without any test-time augmentation. Although our theoretical results are limited to parity tasks with Gaussian covariates and the CIFAR-10 gains are modest relative to recent sharpness-aware methods, GDR introduces no additional backward passes or meta-parameters, making it an attractive drop-in module for practitioners seeking lightweight generalization improvements.",
    "id": 176
  },
  {
    "title": "Lipschitz-Guided Initialization for Scalable Training of Deep ReLU Networks",
    "authors": [
      "Huang, T.",
      "Martinez, S.",
      "Rodriguez, K."
    ],
    "abstract": "We propose a lightweight initialization scheme that leverages local Lipschitz estimates to stabilize early-phase training of deep ReLU networks. By rescaling initial weights using cheap, layer-wise upper bounds on the Lipschitz constant, our method encourages activations to remain in the smooth, nearly-linear region of ReLU without architectural changes or auxiliary losses. On CIFAR-10/100 and ImageNet subsets, models initialized with our scheme converge 14\u201328 % faster and reach comparable or slightly improved accuracy (\u00b10.3 %), while requiring <0.5 % extra compute. Theoretically, we show that the rescaling reduces the worst-case Lipschitz constant of the initial network by a factor exponential in depth, yielding tighter margin-based generalization bounds. Experiments on 50-layer vanilla CNNs indicate reduced gradient variance and lower sharpness at initialization, correlating with stable optimization. However, gains diminish when strong regularization or batch normalization is present, and we observe negligible improvement on very small networks (<5 layers). Our code is publicly available and consists of <100 lines of PyTorch. While the contribution is incremental and the scope limited to ReLU architectures, the approach is practical, reproducible, and may complement existing initialization or pre-training strategies.",
    "id": 177
  },
  {
    "title": "Block-Diagonal Adaptation: A Structured Approach to Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Dubois, Y."
    ],
    "abstract": "We present Block-Diagonal Adaptation (BDA), a lightweight fine-tuning method that learns task-specific block-diagonal perturbations of pre-trained weights. Unlike prior adapters that introduce bottlenecks or sparse masks that can fragment gradient flow, BDA factorizes the adaptation matrix into a small set of dense, non-overlapping blocks, balancing expressivity and parameter efficiency. On GLUE and three vision datasets, BDA matches full fine-tuning with 14\u201322% of the parameters, outperforming LoRA and (IA)$^3$ by 0.8\u20131.3 pp average accuracy while using comparable FLOPs. Ablations indicate that block size acts as an implicit regularizer: larger blocks improve transfer on high-resource tasks but may over-fit on low-resource ones. Theoretically, we bound the generalization error under sub-Gaussian covariates, showing a favorable trade-off that depends on the total number of blocks rather than the ambient dimension. Code is provided, and full results can be reproduced in <24 GPU-hours on RTX-4090 hardware. Though competitive, gains over strong baselines are modest, and extending BDA to billion-scale models remains future work.",
    "id": 178
  },
  {
    "title": "Improved Generalization via Parameter-Averaging with Cyclical Step Sizes",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Roberts, J."
    ],
    "abstract": "We propose Cyclical Weight Averaging (CWA), a simple variant of stochastic weight averaging that cycles learning-rate schedules and keeps an exponentially-decayed running mean of parameter checkpoints. By synchronizing the averaging window to learning-rate valleys, CWA yields flatter minima and marginally lower test error on CIFAR-10/100 and ImageNet without additional hyper-parameters. Theoretically, we bound the deviation between CWA iterate and (S)GD trajectory by O(\u03b7^{1/2}T^{3/4}), suggesting the average remains close to the optimization path. On ResNet-18 and WRN-28-10, CWA improves baseline generalization by 0.3\u20130.7% while adding <1% compute overhead, and is complementary to strong augmentations. However, gains diminish on larger models (EfficientNet-B3) and are insignificant when training already uses aggressive regularization. Compared to SWA, CWA converges 15% faster but yields similar final accuracy; hence the primary benefit is training speed rather than ultimate performance. We release code and checkpoints to facilitate future work.",
    "id": 179
  },
  {
    "title": "Improving Few-Shot Classification via Iterative Meta-Augmentation with Confidence-Based Filtering",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Johnson, L."
    ],
    "abstract": "Few-shot learning remains challenging due to the limited availability of labeled examples. While data augmentation techniques have shown promise in this setting, existing approaches often generate noisy samples that degrade performance. We propose Iterative Meta-Augmentation (IMA), a method that progressively augments the support set by leveraging model confidence to filter synthetic examples. Our approach alternates between training a classifier on the current support set and generating new samples using a conditional generative model. Generated samples are retained only if the classifier's prediction confidence exceeds an adaptive threshold. Through experiments on miniImageNet and tieredImageNet, IMA achieves 62.3% and 70.1% 5-way 1-shot accuracy respectively, representing a 2-3% improvement over strong baselines. We provide theoretical analysis showing that our confidence-based filtering reduces label noise in the augmented set. While our results are positive, we acknowledge several limitations: the method requires careful hyperparameter tuning for each dataset, and the computational cost is higher than simpler augmentation strategies. Additionally, we observe that performance gains diminish when moving from 1-shot to 5-shot scenarios. Our code is available at [anonymous repository].",
    "id": 180
  },
  {
    "title": "Revisiting Gradient Clipping in AdamW: A Simpler Adaptive Rule for Language Model Pre-training",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Nguyen, T."
    ],
    "abstract": "Gradient clipping is routinely used when pre-training large language models, yet its interaction with adaptive optimizers like AdamW remains poorly understood. We propose ClipScale, a lightweight modification that rescales the clipping threshold as a function of the exponential moving average of gradient norms. Unlike constant or scheduled clipping, ClipScale adapts to the evolving landscape without extra hyper-parameters. On a 1.3 B-parameter transformer trained on the Pile, ClipScale yields a 4.7% perplexity reduction versus standard clipping while matching the final downstream GLUE score. Ablation shows the gain is concentrated in early training, where exploding gradients are most severe; removing ClipScale after 20k steps marginally hurts convergence. Theoretically, we prove that ClipScale preserves the affine-invariance of AdamW in convex settings and derive a O(1/\u221aT) regret bound identical to clipped SGD. However, experiments on smaller (100 M) models reveal only marginal improvements, and ClipScale adds 2% wall-clock overhead. Code is provided, but full reproducibility requires unreleased web-crawled data.",
    "id": 181
  },
  {
    "title": "Gradient Alignment Improves Transfer Learning in Limited-Data Regimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Fine-tuning large pre-trained models has become the dominant approach for transfer learning, yet its performance degrades sharply when target data are scarce. We propose Gradient Alignment Transfer (GAT), a simple regularizer that encourages the fine-tuning gradients to align with the pre-trained feature directions. By penalizing gradient components orthogonal to the principal subspace of pre-training updates, GAT constrains optimization to parameters that matter most for the source domain. On 8 few-shot vision and NLP benchmarks, GAT yields 2\u20134% absolute gains over standard fine-tuning while adding <1% overhead. Theoretical analysis shows GAT is equivalent to a soft constraint on the distance between source and target feature covariances, clarifying when the method helps. Although the improvements are consistent, they are modest and diminish as data increase; above 5k examples per class, GAT matches vanilla fine-tuning. Our code, tuned for stability rather than speed, requires 30% more memory because of second-order moment tracking. Overall, GAT offers a lightweight, interpretable way to nudge transfer learning, but its narrow scope and incremental gains may limit broader impact.",
    "id": 182
  },
  {
    "title": "Revisiting Gradient Clipping: A Spectral Perspective on Training Stability",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Liu, M."
    ],
    "abstract": "Gradient clipping is widely used to stabilize neural network training, yet its theoretical underpinnings remain poorly understood. We propose SpectralClipping, a variant that adaptively clips gradients based on the spectral norm of layer-wise Jacobians. By constraining the Lipschitz constant of each layer, our method seeks to balance optimization speed with training stability. We prove a one-sided bound showing SpectralClipping reduces worst-case sharpness growth compared to standard clipping, though the bound depends on data-dependent constants that may scale poorly with depth. Empirically, on CIFAR-10 and ImageNette, SpectralClipping achieves marginally better (\u22480.5%) final accuracy than standard clipping when tuned meticulously, but shows no consistent gain on larger architectures without careful re-tuning. Ablations reveal that 60% of the benefit vanishes when the spectral estimate is approximated efficiently, suggesting limited scalability. While our analysis offers new insights into clipping as a form of implicit regularization, the practical gains remain modest and require overhead that may not justify deployment. Code is available at anonymous link.",
    "id": 183
  },
  {
    "title": "Layerwise Learning Rate Warmup for Transformers: A Simple but Effective Trick?",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Anderson, K."
    ],
    "abstract": "Training large transformer models often requires extensive hyperparameter tuning, particularly for learning rate schedules. We propose Layerwise Learning Rate Warmup (LLRW), a simple modification to standard warmup that applies increasing learning rates to different layers sequentially during early training epochs. Our method is motivated by the observation that lower layers in transformers tend to converge faster than higher layers when training on NLP tasks. We evaluate LLRW on BERT-base and GPT-2 small models across GLUE tasks and WikiText-103, showing 1.2-2.3% average improvements in downstream metrics compared to standard linear warmup, with minimal computational overhead. Theoretically, we provide convergence guarantees for two-layer linear networks under simplifying assumptions. However, our experiments are limited to relatively small models due to compute constraints, and the improvements, while consistent, are modest in magnitude. We also find that LLRW's benefits diminish with larger models and may interact negatively with advanced optimizers like AdamW. Our code is available at [anonymous link]. While LLRW offers a practical training trick for certain settings, its theoretical justification and broader applicability require further investigation.",
    "id": 184
  },
  {
    "title": "Improved Convergence of Gradient Descent via Learning-Rate Scheduling with Bounded Memory",
    "authors": [
      "Kovacs, L.",
      "Choudhury, A.",
      "Steiner, B."
    ],
    "abstract": "We revisit plain gradient descent for smooth, convex optimization and propose SimpleStep, a lightweight scheduling rule that uses only the last two iterates to adapt the learning rate online. Unlike line-search or adaptive methods, SimpleStep maintains O(1) memory and avoids additional gradient evaluations or function calls. Our key idea is to shrink the step size when successive iterates are poorly aligned with the gradient direction, a condition we show is sufficient to guarantee non-increasing potential in a Lyapunov sense. For L-smooth objectives we prove that SimpleStep achieves an O(1/\u221aT) convergence rate\u2014matching the worst-case lower bound\u2014while empirical traces on common logistic-regression benchmarks exhibit up to 30% faster reduction in gradient norm compared to tuned constant steps. Extending the schedule to stochastic training yields modest gains on CIFAR-10 with ResNet-20 when the baseline learning rate is slightly misspecified. Although our theoretical contribution is incremental and the method does not outperform carefully tuned Adam or cosine-annealing baselines, SimpleStep offers practitioners a drop-in replacement that requires no hyper-parameter grid search, making it attractive for resource-constrained environments. Code is provided for full reproducibility.",
    "id": 186
  },
  {
    "title": "Improving Adaptive Learning Rates via Block-Diagonal Curvature Estimates",
    "authors": [
      "Delgado, M.",
      "Chen, H.",
      "Kumar, S."
    ],
    "abstract": "We propose AdaBlock, a memory-efficient variant of adaptive optimizers that approximates the Fisher information matrix using non-overlapping parameter blocks. By retaining only diagonal elements within each block, AdaBlock achieves a smoother learning-rate schedule than Adam while using 50% less second-order memory. Experiments on CIFAR-10 and WikiText-103 show comparable convergence to full-matrix AdaGrad at one-third the cost. Although wall-clock speed-ups are modest (5\u20138%), block-diagonal estimates yield higher test accuracy on small-scale vision and language tasks (+0.9% on ResNet-20, +0.7 BLEU on Transformer-base). Theoretical analysis proves O(1/t) regret in the convex case when block size is constant, but relies on an approximate Hessian that ignores inter-block interactions. Code and hyper-parameters are provided to ensure reproducibility.",
    "id": 187
  },
  {
    "title": "Improving Sample Efficiency of Variational Actor-Critic with Partially-Updated Critics",
    "authors": [
      "Kumar, S.",
      "Li, Y.",
      "Johnson, M."
    ],
    "abstract": "Off-policy deep reinforcement learning algorithms often suffer from high sample complexity and unstable training due to distribution shift. We propose Partially-Updated Actor-Critic (PUAC), an algorithm that periodically freezes critic network updates while still performing policy improvement. Our key insight is that full critic updates in off-policy actor-critic methods may cause overfitting, leading to premature convergence. By withholding critic gradient updates for a controllable number of steps, we enable the actor to explore regions of the state-action space that would otherwise be deemed sub-optimal by an overfitted critic. We evaluate PUAC on a suite of continuous control tasks from MuJoCo and benchmark it against state-of-the-art algorithms such as SAC and TD3. Our experiments demonstrate that PUAC achieves comparable performance to these baselines while requiring modestly fewer gradient updates. Ablation studies show that PUAC's benefits are most pronounced in tasks with sparse rewards and in high-dimensional action spaces. Theoretically, we show that PUAC corresponds to an approximate trust-region update under certain assumptions. Our empirical results suggest that partial critic updates can be a simple yet effective strategy for stabilizing off-policy deep RL training. Code and hyper-parameters will be released upon publication.",
    "id": 188
  },
  {
    "title": "LoRA-FA: Memory-Efficient Low-Rank Adaptation with Frozen A-Matrices",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Zhao, L."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method for large language models, but its memory footprint still scales with the product of rank and hidden dimension for both decomposition matrices. We propose LoRA-FA, a simple variant that freezes the down-projection matrix (A) to its random initialization while only training the up-projection matrix (B). This reduces activation memory by 50% while maintaining the same number of trainable parameters. We provide theoretical analysis showing that freezing A is equivalent to performing gradient descent in a randomly projected subspace, and prove convergence under standard assumptions. Experiments on GLUE and instruction-following benchmarks show that LoRA-FA achieves 95-98% of full LoRA performance with ranks 16-64, while requiring 30-40% less GPU memory during training. However, we observe performance degradation on tasks requiring fine-grained reasoning (e.g., DROP dataset). Ablation studies reveal that trainable A matrices are crucial when the pretrained model lacks relevant features for downstream tasks. While LoRA-FA offers practical memory savings for deployment scenarios, its theoretical contributions are incremental and the approach may be viewed as a special case of existing randomized adaptation methods.",
    "id": 189
  },
  {
    "title": "Gradient Surgery for Improved Low-Precision Training: Combining Clipping and Scaling Heuristics",
    "authors": [
      "Kumar, V.",
      "Zhou, T.",
      "Fern\u00e1ndez, S."
    ],
    "abstract": "Training deep networks in low-precision arithmetic promises faster, cheaper models, but standard gradient descent suffers from instability and accuracy degradation as bit-width shrinks. While prior work has separately proposed gradient clipping to bound extreme values and learnable scaling factors to preserve relative magnitudes, the interaction between these two techniques is poorly understood. We introduce GradSnip, a simple procedure that interleaves per-tensor clipping limits with element-wise scaling updates computed from local curvature estimates. On CIFAR-10/100 and ImageNet, GradSnip attains within 0.8% of full-precision accuracy at 4-bit activations without modifying the forward graph, outperforming vanilla clipping by 1.3% on average. However, on Transformer language models the gains shrink to 0.3%, and below 4-bit our method still collapses. Theoretical analysis shows GradSnip is equivalent to projected gradient descent on a constrained optimization problem whose feasible set depends on the clipping threshold, but the projection lacks a closed form, complicating convergence proofs. Code and checkpoints are provided.",
    "id": 190
  },
  {
    "title": "Momentum-Aided Gradient Descent: A Simple Twist on Momentum for Faster Non-Convex Optimization",
    "authors": [
      "Kim, J.",
      "Singh, A.",
      "Gonzalez, M."
    ],
    "abstract": "Stochastic gradient descent with momentum (SGDM) remains the de-facto optimizer for training deep networks, yet its theoretical acceleration guarantees are limited to convex or nearly-convex regimes. We propose Momentum-Aided Gradient Descent (MAGD), a lightweight modification that periodically amplifies the momentum buffer by a data-dependent factor. The amplification schedule is derived from a simple online estimate of the gradient variance; when variance is high, momentum is boosted to curb oscillations, whereas in low-variance regions the update resembles standard SGDM. On three standard image-classification benchmarks (CIFAR-10/100 and ImageNet) and two language modeling tasks (WikiText-2, Penn Treebank), MAGD achieves 1.05\u20131.18\u00d7 speed-up in epochs-to-target-accuracy compared to tuned SGDM, without hyper-parameter re-tuning. Averaged across seven diverse architectures (ResNet, DenseNet, Vision-Transformer, etc.), the wall-clock improvement is 6.8%. Theoretically, we prove that MAGD enjoys the same O(1/T) convergence rate as SGDM on smooth non-convex objectives, but with a slightly smaller constant of proportionality. Ablation shows the periodic amplification contributes the majority of the empirical gain, while the variance-based schedule is only marginally better than a fixed heuristic. Code and 50-line PyTorch implementation are attached. While further study is needed to understand the variance-estimation mechanism on very large batch or distributed settings, MAGD offers practitioners a plug-and-drop replacement for SGDM with minimal overhead.",
    "id": 191
  },
  {
    "title": "LoRA-Flow: Adaptive Low-Rank Decomposition with Gradient Flow Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods like LoRA have become popular for adapting large language models, but their performance often degrades when the rank of the decomposition is too small or when the model is fine-tuned for too long. We propose LoRA-Flow, a simple extension to LoRA that adaptively adjusts the rank of the low-rank matrices during training using a gradient-based criterion. Our method monitors the magnitude of gradient updates in LoRA modules and gradually increases the rank of components whose gradients persist above a threshold, allowing the model to expand capacity where needed without overfitting elsewhere. We evaluate LoRA-Flow on instruction-following benchmarks and find modest improvements over standard LoRA across model sizes (though gains are inconsistent across tasks). Ablation studies suggest that the scheduling mechanism is most beneficial for long fine-tuning runs or sparse datasets. While LoRA-Flow introduces minimal overhead, its gains remain marginal and are sensitive to the threshold selection. This work highlights the challenge of balancing representational capacity and parameter efficiency in PEFT, and suggests that dynamic rank adaptation may be a promising but nuanced direction. Code and experimental configurations are provided for reproducibility.",
    "id": 192
  },
  {
    "title": "LoRAMA: Low-Rank Adaptation with Memory Attention for Efficient LLM Fine-Tuning",
    "authors": [
      "Kumar, S.",
      "Jones, L.",
      "Nguyen, P."
    ],
    "abstract": "We propose LoRAMA, a that augments Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning of large language models by incorporating a lightweight memory attention mechanism. While LoRA has shown strong empirical results, its fixed low-rank decomposition may limit adaptation capacity for more complex downstream tasks. LoRAMA introduces a small memory bank of trainable vectors that are dynamically attended to during forward passes, allowing the low-rank matrices to vary in a task-dependent manner. Our method adds only 0.3M parameters beyond standard LoRA while enabling more expressive adaptation. We evaluate LoRAMA on instruction tuning and domain adaptation benchmarks using Llama-2 7B and 13B models. Results show modest improvements of 1-2% accuracy over LoRA baselines across most tasks, with some degradation on simpler datasets. Abprisingly, LoRAMA reduces training time by 12% compared to standard LoRA due to our memory attention implementation. Theoretical analysis suggests our method interpolates between LoRA and full fine-tuning, though convergence guarantees remain limited. ablation studies reveal performance is sensitive to memory bank size and initialization strategy. LoRAMA provides a simple yet effective extension to LoRA that may be particularly useful for practitioners seeking slightly improved adaptation capacity without substantial computational overhead.",
    "id": 193
  },
  {
    "title": "Gradient Confusion-Aware Learning Rates for Stochastic Optimization",
    "authors": [
      "Chen, L.",
      "Martinez, J.",
      "Singh, P."
    ],
    "abstract": "Mini-batch stochastic gradient descent remains the workhorse for large-scale machine learning, yet practical tuning of learning rates still relies on costly grid searches or hand-crafted schedules. We propose Gradient Confusion-Aware Learning Rate (GCALR), a heuristic that monitors the pairwise cosine similarity among per-sample gradients within a mini-batch to adjust the step size on the fly. Intuitively, high disagreement (confusion) suggests the current batch contains noisy or conflicting signals; GCALR reduces the learning rate to stabilize updates. Convergence is proved for smooth, strongly convex objectives under standard assumptions, although the derived bound is no better than classical SGD with the best constant step. Extensive experiments on CIFAR-10/100, ImageNet and Penn Treebank show that GCALR matches hand-tuned SGD performance in 60% of tasks while eliminating tuning budgets. However, gains vanish when large-batch training exceeds 1,024 samples, and competing adaptive optimizers (Adam, Adagrad) remain superior on language modeling benchmarks. Code is publicly available with full hyper-parameter tables. GCALR offers a tuning-free alternative to constant-step SGD with minimal computational overhead, but theoretical insight into its empirical benefits is still incomplete.",
    "id": 194
  },
  {
    "title": "Improved Gradient Bounds for Plug-and-Play Priors via Approximate Jacobian Regularization",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Nguyen, T."
    ],
    "abstract": "Plug-and-play (PnP) methods combine physical forward models with learned denoisers to solve imaging inverse problems without end-to-end training. While practical, these approaches lack theoretical guarantees on convergence because the denoiser\u2019s Jacobian is unavailable. We introduce a lightweight surrogate network that predicts the trace of the denoiser Jacobian, enabling an approximate gradient descent scheme with an explicit regularization term that encourages fixed-point consistency. Under mild assumptions on the denoiser\u2019s Lipschitz constant, we derive a loose but non-vacuous upper bound on the spectral norm of the iteration operator, yielding iterate stability in the low-noise regime. On two compressed-sensing MRI datasets we obtain 1.3 dB PSNR improvement over standard PnP at 4\u00d7 acceleration while requiring only 7 % extra parameters and no additional training data. Experiments on natural-image super-resolution reveal a residual gap between theory and practice: our bound holds numerically but is 4\u20135\u00d7 larger than observed convergence thresholds. Ablation studies show that most gains come from the trace-regularizer rather than the spectral bound, suggesting that tighter theoretical certificates may need stronger assumptions. Code and trained networks are provided.",
    "id": 195
  },
  {
    "title": "Revisiting Gradient Clipping for Private SGD: A Second-Order Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is a standard ingredient when training deep networks with Differentiallyial Privacy (DP), yet its interplay with curvature information remains under-explored. We propose Adaptive Curvature Clipping (ACC), a lightweight modification that rescales per-sample gradients by the inverse of an empirical Fisher diagonal before the clipping operation. On CIFAR-10/100 and StackOverflow NLP tasks, ACC reaches target accuracy with 8\u201315 % fewer gradient queries relative to vanilla DP-SGD, while introducing no additional hyper-parameters beyond the usual clipping threshold. Theoretically, we show that ACC reduces the Lipschitz constant of the loss landscape, yielding a tighter utility bound that scales with the effective rank rather than the dimension. Comprehensive ablations indicate, however, that gains vanish under aggressive noise multipliers (\u03c3\u22654) and for very deep architectures (>100 layers). Our results suggest that curvature-aware clipping is a practical, orthogonal axis for improving privacy\u2013utility trade-offs, but also highlight scenarios where first-order clipping suffices.",
    "id": 196
  },
  {
    "title": "Revisiting Dropout with Block-Diagonal Noise: A Cheap Fix for Overconfident Bayesian NNs?",
    "authors": [
      "Liu, S.",
      "Kumar, V.",
      "Nguyen, T.",
      "Cai, H."
    ],
    "abstract": "We revisit Monte-Carlo dropout as a Bayesian approximation and observe that the standard Bernoulli noise correlates poorly with the true epistemic uncertainty at the end of training, resulting in overconfident predictive distributions on image, speech and tabular benchmarks. We propose a plug-in replacement that injects block-diagonal Gaussian noise whose covariance is estimated from the empirical Fisher of the last epoch. The extra cost is one gradient and a small Cholesky decomposition per layer, i.e. <5% overhead versus standard dropout. On CIFAR-10/100, ImageNet-1k subsets and five UCI datasets our network achieves 1\u20133% better accuracy and ~15% lower ECE with the same forward passes at test time. A PyTorch implementation is under 40 lines of code. While the theoretical justification is limited to the high-dimensional regime and we do not yet scale to billion-parameter models, the consistent improvements across small- and medium-sized tasks suggest that the community\u2019s de-facto regulariser can be marginally but reliably upgraded with minimal effort.",
    "id": 197
  },
  {
    "title": "Improving Transformer Generalization with Adaptive Positional Interpolation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Okafor, K."
    ],
    "abstract": "While Transformers have achieved remarkable success across domains, their ability to generalize to sequences longer than those seen during training remains limited. We propose Adaptive Positional Interpolation (API), a simple yet effective method that dynamically adjusts positional encodings based on the input sequence length. Our approach interpolates between learned positional embeddings using a learned temperature parameter that controls the smoothness of the interpolation. Unlike recent work that requires architectural modifications, API can be applied as a plug-and-play module to pre-trained models. We evaluate API on language modeling, machine translation, and document classification tasks. Results show modest improvements of 2-3% in perplexity for sequences 2x longer than training data, with some tasks showing no improvement. Ablations reveal performance is sensitive to the initialization of the temperature parameter. While our method provides a lightweight solution to length extrapolation, we acknowledge it falls short of more complex approaches. Theoretically, we provide limited analysis of when interpolation might succeed or fail. Our code is available.",
    "id": 198
  },
  {
    "title": "Improving Gradient Flow in Residual Networks with Learnable Skip-Scaling",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rodriguez, A."
    ],
    "abstract": "Deep residual networks have shown remarkable success in training very deep architectures, yet the choice of how skip connections are weighted remains largely heuristic. We propose a simple method for learning per-layer, data-independent scalar coefficients that dynamically rescale the residual branch during training. Our approach introduces only one additional parameter per residual block and is optimized jointly with the network weights via standard back-propagation. We demonstrate that learnable rescaling can accelerate convergence on CIFAR-10 and ImageNet by up to 18% in wall-clock time and yield modest accuracy gains of 0.3\u20130.5% over the original ResNet baseline. Although the technique generalizes across common vision benchmarks, gains diminish when strong regularization or modern architectural refinements are present. Extensive ablations indicate that improvements are most pronounced in mid-depth networks (20\u201350 layers), while deeper models benefit less. The method requires no architectural redesign and is easily integrated into existing frameworks. Our code and trained models are publicly available.",
    "id": 199
  },
  {
    "title": "Revisiting Weight Averaging with Layer-wise Learning Rate Modulation",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Liu, J."
    ],
    "abstract": "Weight averaging has emerged as a simple yet effective technique for improving generalization in deep neural networks. While existing methods typically apply uniform averaging schedules, we hypothesize that different layers converge at varying rates and thus benefit from layer-specific treatment. We propose Layer-wise Averaging with Modulated Learning rates (LAML), a plug-in scheme that assigns learnable coefficients to each layer when computing the exponential moving average. These coefficients are optimized on a small held-out validation set using differentiable perturbations. Across image classification benchmarks (CIFAR-10/100, ImageNet-1K), LAML yields 0.3\u20130.7% top-1 gains over standard EMA with comparable compute. Ablation studies indicate that earlier layers require slower averaging rates, aligning with recent observations on feature evolution. However, gains diminish on larger models (>200M params) and on robustness benchmarks, suggesting limited scalability. Theoretical analysis shows LAML converges to a stationary point under standard convexity assumptions, although the layer-wise coefficients introduce additional hyper-parameters that require careful tuning. Code and pretrained weights are provided.",
    "id": 200
  },
  {
    "title": "Revisiting Mixup for Semi-Supervised Learning: When Do Interpolated Labels Improve Generalization?",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "While Mixup has been widely adopted as a data-augmentation technique in supervised learning, its application to semi-supervised learning (SSL) remains under-explored. We empirically investigate whether Mixup improves SSL performance when combined with consistency-regularization methods such as FixMatch and FlexMatch. Across CIFAR-10/100 and Mini-ImageNet, we observe that na\u00efve Mixup can actually degrade accuracy by up to 2.3% when the labeled set is extremely small (<5%), but yields marginal gains (+0.5%) when more labels are available. Motivated by this non-monotonic trend, we propose Curriculum-Mixup (CM), a simple schedule that gradually increases both Mixup strength and interpolation probability as the model's prediction confidence rises. CM recovers the lost performance at low-label regimes and achieves state-of-the-art results on CIFAR-10 with only 40 labels (89.1%). Ablation studies reveal that CM's benefit is primarily due to calibrated pseudo-labels rather than input diversity. Although our method is intuitive and implementation requires only 20 additional lines of code, our theoretical analysis is limited to linear models and does not explain non-monotonicity in deep nets. Code and checkpoints are publicly available.",
    "id": 201
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Autoencoders via Continuous Relaxations with Learned Temperature",
    "authors": [
      "Klein, S.",
      "Murali, V.",
      "Andersson, J."
    ],
    "abstract": "Discrete latent variables offer interpretable representations but complicate training of variational autoencoders (VAEs) because reparameterization gradients are unavailable. While continuous relaxations such as the Gumbel-softmax trick yield low-bias gradient estimates, they require a fixed temperature schedule that can lead to high variance or over-regularized latent codes. We propose L-Temp-VAE, a simple extension that learns a free parameter per latent dimension to control relaxation temperature jointly with the generative and inference networks. In experiments on text generation, molecule generation, and semi-supvervised MNIST classification, L-Temp-VAE lowers gradient variance by 15\u201330% compared with annealed baselines and improves ELBO by 0.5\u20131.6 nats without additional computational cost. Ablation studies reveal that learned temperatures automatically soften during early training and stiffen toward convergence, behaviour difficult to replicate with hand-designed schedules. Although the idea is conceptually incremental, our results suggest that learnable relaxation parameters can be integrated into existing discrete-VAE pipelines with minimal effort. Code is available at anonymous.url.",
    "id": 202
  },
  {
    "title": "Improving Transformer Generalization with Spectral Mixture Positional Codes",
    "authors": [
      "Kim, S.",
      "Okafor, C.",
      "Nguyen, T."
    ],
    "abstract": "Positional encoding is crucial for Transformers to model sequence order, yet standard sinusoidal or learned embeddings struggle with length extrapolation and can exhibit pathological attention patterns. Inspired by recent advances in neural temporal modeling, we propose SpectralMix, a family of positional codes derived from spectral mixture kernels that blend periodic and aperiodic components. By treating position as a continuous pseudo-temporal input, SpectralMix yields translation-sensitive attention that remains stable far beyond the training horizon. Experiments on character-level language modeling, algorithmic tasks, and a modest vision transformer show 5\u201312% lower perplexity and improved length extrapolation when training on sequences up to 2k tokens and testing on 8k. Ablations reveal that removing either the mixture weights or the learned phase offsets eliminates most gains, corroborating the value of both inductive biases. While our derivation is grounded in Gaussian-process regression, we provide PyTorch code that drops into existing models with <15 extra lines. Limitations include additional hyper-parameters and a small computational overhead; nonetheless, SpectralMix offers a plug-and-play alternative to sinusoidal encodings at minimal implementation cost.",
    "id": 203
  },
  {
    "title": "Adaptive Step-Size Scheduling via Online Learning: A Gradient Descent Variant Approach",
    "authors": [
      "Kim, J.",
      "Singh, P.",
      "Chen, L."
    ],
    "abstract": "We revisit the classical problem of step-size selection in gradient-based optimization, motivated by the observation that popular heuristics such as cosine annealing can be brittle when hyper-parameters are misspecified. Motivated by online learning, we propose AdaStep, a lightweight wrapper that adaptively tunes the step size on the fly by reducing the problem to an expert-tracking subroutine over a discrete grid of learning rates. Under standard convexity and smoothness assumptions, we show that AdaStep achieves a worst-case rate of O(\u221aT) relative to the best post-hoc grid expert, matching the exponent of SGD while eliminating the need for a pre-specified horizon T. On CIFAR-10/100 and ImageNet, AdaStep improves only marginally over competitive baselines (\u223c0.3% absolute) but displays reduced sensitivity to the initial learning rate and avoids the final sharp drop in accuracy often observed with cosine schedules. Linear-probe experiments on Vision Transformers further suggest mild robustness benefits under label noise. While AdaStep\u2019s practical gains are modest, we believe its interpretability and minimal overhead (five extra lines of code) make it a useful addition to the optimizer toolbox. Code will be released upon publication.",
    "id": 204
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Inference via Continuous Relaxations with Control Variates",
    "authors": [
      "Liu, M.",
      "Johnson, K.",
      "Chen, S."
    ],
    "abstract": "Variational inference for discrete latent variables remains challenging due to the high variance of gradient estimators. While continuous relaxations such as the Gumbel-Softmax enable reparameterization, they introduce bias that can degrade performance. We propose a simple modification that incorporates a Taylor-based control variate into the relaxed gradient estimator, yielding lower variance without additional model evaluations. Applied to variational autoencoders with categorical latents, our method reduces gradient variance by 35% on average across three benchmark datasets, translating to modest improvements in held-out likelihood. Theoretically, we bound the bias introduced by the relaxation and show that the control variate does not asymptotically increase it. Empirically, we observe faster convergence in training and slightly better sample quality. While the gains are incremental and the approach is limited to moderate-sized vocabularies, the technique is easy to implement and may complement existing variance-reduction strategies. Code is available at anonymized-url.",
    "id": 205
  },
  {
    "title": "Re-eval: Improving Model Reuse via Task-Agnostic Weight Re-initialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Singh, K."
    ],
    "abstract": "Transfer learning typically begins from weights trained on ImageNet or large language corpora, but little guidance exists on which layers should be kept, fine-tuned, or randomly re-initialized for a new task. We propose Re-eval, a simple heuristic that re-initializes the final half of network parameters before fine-tuning. Across 12 vision and 8 NLP benchmarks, Re-eval yields an average 1.3% improvement over standard fine-tuning while reducing trainable parameter count. Theoretically, we show that layerwise re-initialization can be viewed as an implicit form of curriculum learning. Although the gains are consistent, they rarely exceed 2%, and performance drops when the source and target tasks are dissimilar. Code and scripts are publicly available.",
    "id": 206
  },
  {
    "title": "Improved Generalization via Periodic Weight Averaging with Cyclical Learning Rates",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Cyclical Weight Averaging (CWA), a simple extension to Stochastic Weight Averaging (SWA) that leverages cyclical learning rate schedules to improve generalization in deep neural networks. While SWA averages weights along the optimization trajectory after the learning rate has converged, CWA performs averaging throughout training by identifying stable points during cyclical learning rate annealing. Our method requires minimal hyperparameter tuning and adds negligible computational overhead compared to standard training. We evaluate CWA on CIFAR-10/100 and ImageNet with ResNet and Vision Transformer architectures, achieving 0.3-0.8% accuracy improvements over SWA and 1.2-1.9% over standard training. Theoretical analysis suggests CWA converges to a wider optimum by exploring multiple modes during the cyclical schedule. However, we find that the benefits diminish with stronger data augmentation and regularization techniques. While our empirical results are consistent across architectures and datasets, the theoretical justification remains incomplete, and computational overhead increases for very large models. Code and pre-trained models are available at anonymous-url.",
    "id": 207
  },
  {
    "title": "Improving Transformer Generalization with Iterative Input Refinement",
    "authors": [
      "Kovacs, B.",
      "Zhao, L.",
      "Murthy, A."
    ],
    "abstract": "Despite their empirical success, Transformer models often struggle with compositional generalization on structured prediction tasks. We propose Iterative Input Refinement (IIR), a lightweight training-time augmentation that exposes Transformers to progressively perturbed versions of each training example, encouraging the model to learn input-robust representations. IIR is implemented as a secondary transformer that generates small, task-preserving perturbations, which are then mixed into the original inputs at varying strengths. Across three semantic parsing datasets and two arithmetic reasoning benchmarks, IIR yields average gains of 2.1 BLEU over strong baselines, with larger improvements on out-of-distribution splits. Analyses suggest that the technique encourages attention heads to specialize in syntactic structure rather than lexical shortcuts. While the method requires a second forward pass and introduces one additional hyper-parameter, the computational overhead at inference is zero. Code and checkpoints are publicly available.",
    "id": 208
  },
  {
    "title": "Gradient Alignment Improves Few-Shot Generalization in Meta-Learning",
    "authors": [
      "Chen, Y.",
      "Kumar, S.",
      "Nguyen, L."
    ],
    "abstract": "Meta-learning algorithms aim to learn models that can quickly adapt to new tasks with limited data. While recent approaches have shown promise, they often struggle with task heterogeneity, leading to poor generalization in few-shot scenarios. We propose a simple yet effective method called Gradient Alignment (GradAlign) that encourages task-specific gradients to point in similar directions during meta-training. Our approach adds a regularization term that penalizes the cosine distance between gradients computed on different support sets within each meta-batch. This encourages the meta-learner to find representations that are robust to task variation without requiring explicit task similarity metrics. We evaluate GradAlign on standard few-shot classification benchmarks including mini-ImageNet, tiered-ImageNet, and CIFAR-FS. Our method achieves modest improvements of 1-2% over strong baselines like MAML and ProtoNets, with particularly notable gains in 5-shot scenarios (e.g., 68.3% vs 66.1% on mini-ImageNet). While the improvements are not dramatic, GradAlign introduces minimal computational overhead and can be easily integrated into existing meta-learning frameworks. However, we find that the benefits diminish as the number of shots increases, suggesting limitations in handling more complex tasks. Our work highlights both the potential and challenges of gradient-based regularization in meta-learning settings.",
    "id": 209
  },
  {
    "title": "Self-Attention with Learnable Temperature for Improved Generalization in Low-Data Regimes",
    "authors": [
      "Chandran, K.",
      "Osei, E.",
      "Li, M."
    ],
    "abstract": "We introduce learnable temperature scaling into the self-attention mechanism of Transformer architectures, aiming to improve generalization when training data are scarce. By re-parameterizing the softmax temperature as a light-weight, input-dependent function, we obtain a flexible attention module that can interpolate between peaky and diffuse attention patterns. On standard NLP and vision benchmarks, our method yields 1.2\u20132.4 % accuracy gains over baseline Transformers when fewer than 5 k labeled examples are available, while matching performance in the large-data regime. A Bayesian interpretation reveals that the learned temperature performs an implicit form of posterior tempering, trading off data likelihood against model complexity. Extensive ablations show that gains diminish when pre-training is provided and that results are sensitive to initialization. Code and pre-trained weights are provided.",
    "id": 210
  },
  {
    "title": "Kernelized Stein Control Variates: Practical Non-Parametric Correction of Variational Inference Bias",
    "authors": [
      "Chen, L.",
      "Thompson, A.",
      "Sridharan, K."
    ],
    "abstract": "Variational inference (VI) trades statistical accuracy for computational speed, often under-estimating posterior spread. While existing control-variate post-processing techniques can theoretically remove this bias, they rely on parametric families that struggle in high dimensions. We introduce Kernelized Stein Control Variates (KSCV), a non-parametric plug-in method that augments any pre-trained VI approximation with a reproducing-kernel correction trained on the same samples. KSCV minimizes a closed-form Stein discrepancy objective, requires no additional model evaluations, and converges at \u00d5(1/n) rate under standard regularity. On eight UCI regression tasks and a 256-dim Bayesian neural net, KSCV reduces VI mean-field test log-likelihood error by 10\u201323% with under 5% runtime overhead versus the original run. Nonetheless, our theoretical guarantee is limited to log-Sobolev targets, and kernel hyper-parameters are chosen by heuristic grid search. The method is easy to implement and may serve as a practical complement to richer\u2014but costlier\u2014VI diagnostics.",
    "id": 211
  },
  {
    "title": "Revisiting Momentum SGD with Layer-wise Learning-rate Tuning for Modern CNN Training",
    "authors": [
      "Kumar, A.",
      "Chen, S.",
      "Nguyen, L."
    ],
    "abstract": "Recent advances in adaptive optimizers have overshadowed the classical momentum SGD algorithm, yet SGD with momentum remains widely used in practice. We revisit this optimizer through a layer-wise learning-rate scheduling perspective. Our method, called L-SGD, assigns independent learning rates to each layer of a neural network and updates them adaptively based on gradient statistics collected during training. While this idea is not new, our approach introduces a simple yet effective re-scaling rule that reduces sensitivity to hyper-parameter choices. We provide empirical evidence on CIFAR-10, CIFAR-100, and ImageNet-1k that L-SGD achieves comparable or better test accuracy than AdamW and LAMB on several standard CNN architectures while using fewer optimizer-specific hyper-parameters. However, we observe that the computational overhead from per-layer learning-rate tracking can be non-negligible in practice. Additionally, L-SGD does not consistently improve performance on Transformers or smaller models, suggesting limited generality. Our code is publicly available and reproducible. This work highlights that with careful per-layer tuning, momentum SGD can be competitive with adaptive methods, though at the cost of increased engineering effort.",
    "id": 212
  },
  {
    "title": "Revisiting Weight Averaging for Cold-Start Recommendation with Limited Interactions",
    "authors": [
      "Kumar, S.",
      "Okafor, C.",
      "Zhao, J."
    ],
    "abstract": "Weight averaging has shown promise in vision and NLP domains for improving generalization, yet its utility for collaborative filtering\u2014especially under extreme sparsity\u2014remains under-explored. We conduct a large-scale empirical study of exponential moving average (EMA) and stochastic weight averaging (SWA) when only 5\u201320 user-item interactions are available at inference time. Across three standard benchmarks (MovieLens-1M, Amazon-Books, and a new dataset we collected from a mobile news app), we show that simple last-iterate EMA consistently boosts Recall@20 by 2\u20134% over vanilla matrix-factorization baselines, while SWA yields smaller but stable gains on deeper models. Ablation indicates that the benefit is not attributable solely to regularization; rather, averaged weights appear to find flatter minima that transfer better to cold-start users. Nevertheless, gains vanish when interaction histories exceed 30 events or when side features are incorporated, suggesting limited applicability to warm-start scenarios. We provide open-source code and all hyper-parameters necessary for reproduction. While our contribution is primarily empirical, we believe systematic evidence that weight averaging can matter in collaborative filtering\u2014without architectural change\u2014is valuable to practitioners coping with data-paucity.",
    "id": 213
  },
  {
    "title": "Self-Adjusting Step-Size Schedules for Stochastic Gradient Descent: A Kernel Regression Approach",
    "authors": [
      "Lee, J.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "While carefully tuned step-size schedules significantly improve SGD performance, existing adaptive methods rely on heuristics with limited theoretical backing. We propose KISS, a kernel regression approach that predicts near-optimal step sizes from the recent gradient history. On convex problems, KISS achieves a 1.9\u00d7 speed-up over tuned polynomial decay schedules and matches Adam's final accuracy without momentum or second-order statistics. The method requires only one extra hyper-parameter (window size) and adds O(w) memory and compute per iteration. Experiments on CIFAR-10/100 and WikiText-2 show consistent gains, but improvements diminish on very large-batch or Transformer training. Theoretically, we bound expected progress for quadratics; however, general convex and non-convex analyses remain incomplete. Code and tuned schedules are provided to ensure reproducibility.",
    "id": 214
  },
  {
    "title": "Recycling Gradients: Improved Memory Efficiency by Recomputing Back-Propagation on the Fly",
    "authors": [
      "Krishnan, S.",
      "Orlova, T.",
      "Nguyen, V."
    ],
    "abstract": "As neural models grow deeper, GPU memory becomes a bottleneck for large-batch training. Gradient checkpointing reduces memory by re-computing forward activations during back-propagation, but introduces a fixed 33%-50% runtime overhead. We propose ReCycle-Grad, a simple modification that stores only a probabilistic sketch of activations and reconstructs the rest from parameters and inputs on demand. Theoretically, we prove that when layer Jacobians are (\u025b,\u03b4)-stable, reconstruction error is bounded by O(\u221a\u025b log 1/\u03b4) and converges under standard smoothness assumptions. Empirically, ReCycle-Grad cuts peak memory by 22% with a 12% slowdown on ImageNet ResNet-152 compared to vanilla checkpointing. On a 12-layer Transformer language model, we fit 30% longer sequences at the same memory footprint. Because the method re-uses existing auto-grad graphs, implementation requires \u226450 lines of PyTorch. While our bounds rely on strong stability conditions that may not hold for all architectures, ablations show consistent memory\u2013time trade-offs across ResNets, Transformers and graph networks. Code is provided for reproducibility.",
    "id": 215
  },
  {
    "title": "Gradient Norm Clipping with Layer-wise Adaptive Thresholds for Deep Network Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Johnson, M."
    ],
    "abstract": "We propose a simple modification to standard gradient norm clipping that adapts the clipping threshold for each layer based on the historical gradient statistics collected during training. While gradient clipping is widely used to stabilize training and avoid exploding gradients, we argue that a single global threshold across all layers may be sub-optimal due to heterogeneous gradient magnitudes across layers. Our method, Layer-Adaptive Gradient Clipping (LAGC), maintains a moving average of gradient norms for each layer, and sets the clipping threshold as a fixed quantile of this running estimate. Our experiments on Transformer language models and ResNet vision models demonstrate that LAGC can match the training stability of standard clipping while leading to marginally better validation loss, and in some cases, a modest improvement in final accuracy (0.2-0.5%). Theoretical analysis shows that LAGC preserves the convergence guarantees of standard clipping under smoothness assumptions. While the gains are not dramatic, the method introduces minimal overhead and can be easily integrated into existing training pipelines. We believe this work provides a useful empirical insight into the dynamics of per-layer gradient magnitudes, though its broader impact impact is incremental. Code is available in supplementary materials.",
    "id": 216
  },
  {
    "title": "Improved Confidence Bounds for Linear Bandits via Root-Penalized Least Squares",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "We revisit the standard confidence ellipsoid used in linear stochastic bandits and propose a simple modification to the least-squares estimator. By adding a square-root regularization term to the ridge penalty, we obtain tighter confidence widths that scale with the empirical standard deviation of the observed rewards rather than the worst-case range. The resulting algorithm, RootLinUCB, achieves a gap-dependent regret bound of  \u00d5(d\u221a(Sn) + d\u00b2) where S is the empirical reward variance, improving on the classical  \u00d5(d\u221an) when rewards exhibit low variance. Experiments on synthetic data with heteroscedastic noise show 12\u201318% regret reduction compared to OFUL, while performance on standard benchmarks (e.g., Jester dataset) is comparable. Although the regret improvement is incremental and the analysis relies on sub-Gaussian noise, the approach is easy to implement and requires no additional hyper-parameters beyond standard ridge regression. Our results suggest that data-dependent confidence widths can yield modest practical gains without sacrificing computational efficiency.",
    "id": 217
  },
  {
    "title": "Revisiting Entropy-Regularized Policy Optimization with Adaptive Temperature Scheduling",
    "authors": [
      "Kim, J.",
      "Singh, V.",
      "Ortega, C."
    ],
    "abstract": "Entropy-regularized reinforcement learning has become a standard tool for improving exploration and policy stability. Yet the choice of temperature\u2014the coefficient weighting the entropy bonus\u2014remains largely heuristic: fixed schedules or hand-tuned constants that can yield brittle performance. We propose ALoE, an approximate Levenberg-Marquardt style online procedure that adapts the temperature parameter on-the-fly by treating it as a learnable regularization weight. ALoE maintains running estimates of both policy entropy and return variance, and adjusts the temperature to keep the effective entropy within a desired relative range. Experiments on continuous-control MuJoCo tasks show modest but consistent gains over SAC with fixed temperature (average +3.2% return), while requiring only 7% additional compute. Ablations indicate that the benefit is larger in sparse-reward environments; on dense-reward variants the margin shrinks to +0.9%. Theoretically, we derive a one-step contraction bound that relates the adaptive temperature to the policy improvement guarantee; however, the bound depends on quantities that are difficult to compute in practice, limiting its prescriptive value. Code is provided and results are reproducible across three random seeds.",
    "id": 218
  },
  {
    "title": "Revisiting Momentum in Stochastic Optimization with Cyclical Step-Size Schedules",
    "authors": [
      "Chen, Y.",
      "Kumar, A.",
      "Nguyen, T.",
      "Roberts, S."
    ],
    "abstract": "We investigate whether classical momentum acceleration remains beneficial when training modern neural networks with cyclical learning-rate schedules. While momentum is widely adopted with constant or decaying step sizes, its interaction with recently popularized cyclical schedules is poorly understood. We derive a simplified quadratic model suggesting that momentum can destabilize cyclical updates when the cycle length is shorter than the effective horizon of the momentum buffer. Extensive experiments on CIFAR-10/100 and ImageNet show that, on average, removing momentum from SGD with a triangular schedule yields a 0.4\u00b10.2% top-1 drop yet halves memory overhead and eliminates one hyper-parameter. On smaller architectures (ResNet-18/-34) the difference is within standard error, whereas larger models (ResNet-50, EfficientNet-B0) still benefit slightly from momentum. We further propose Cyclical-Momentum-SGD (CM-SGD) that modulates the momentum coefficient in phase with the learning rate; CM-SGD matches baseline momentum accuracy on ImageNet with 30% fewer epochs. Our results indicate that momentum\u2019s value is task- and schedule-dependent, and we release a lightweight PyTorch hook that automatically disables momentum when its estimated contribution falls below a tunable threshold.",
    "id": 219
  },
  {
    "title": "Towards Lightweight Gradient Compression with Learned Quantization Schedules",
    "authors": [
      "Chen, Y.",
      "Natarajan, S.",
      "Klein, M."
    ],
    "abstract": "Gradient compression reduces communication overhead in distributed training, but existing heuristics like Top-K or fixed quantization levels often hurt convergence when applied aggressively. We propose AdaQs, a scheduler that adaptively picks one of four quantization levels (2\u20138 bits) per layer according to a lightweight LSTM-based policy trained on the fly. The policy receives summary statistics of each layer\u2019s gradient history and outputs a probability vector over quantization levels; the chosen level is used for that step\u2019s all-reduce. Training the scheduler with vanilla RL is unstable, so we introduce a variance-similarity reward that balances compression ratio with an auxiliary term that approximates the true gradient variance. On CIFAR-10/100 and WikiText-103 using ResNet-20 and a 6-layer transformer, AdaQs achieves 3.9\u20135.1\u00d7 higher compression than the best static baseline while matching final accuracy within 0.3%. Analysis shows the scheduler automatically assigns higher precision to the first and last layers, corroborating empirical observations but without hand-tuning. Limitations include a 12% overhead on a single V100 due to the small LSTM and a lack of theory; convergence guarantees remain an open question. Code is provided for reproducibility.",
    "id": 220
  },
  {
    "title": "Boosting Gradient Descent with Periodic Weight Averaging",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "We propose Periodic Weight Averaging (PWA), a simple plug-in technique that averages model parameters every k iterations during gradient descent training. Motivated by the observation that individual gradient steps can oscillate around sharp minima, PWA aims to stabilize optimization and improve generalization. Unlike stochastic weight averaging which requires a separate learning rate schedule, PWA integrates the averaging step directly into the standard training loop. Our theoretical analysis shows that PWA achieves a convergence rate of O(1/T) for smooth convex objectives, matching standard gradient descent while potentially finding flatter minima. Empirically, we evaluate PWA on CIFAR-10 and CIFAR-100 with ResNet-18 and WideResNet-28-10 architectures, showing modest improvements of 0.3-0.5% accuracy over standard SGD with momentum. While these gains are consistent, they are smaller than those achieved by recent adaptive optimizers. Additionally, our hyperparameter sensitivity analysis reveals that PWA performance depends heavily on the averaging frequency k and learning rate tuning. Though limited in scope, our results suggest that periodic averaging could serve as a complementary technique to existing optimization methods, particularly in resource-constrained settings where additional computational overhead must be minimized.",
    "id": 221
  },
  {
    "title": "Revisiting Momentum with Adaptive Look-ahead: A Small Constant Helps",
    "authors": [
      "Chawla, A.",
      "Nguyen, P.",
      "Klein, S."
    ],
    "abstract": "Momentum-based optimizers remain the default choice for training deep networks, yet their hyper-parameter sensitivity introduces non-trivial tuning overhead. We propose ALM-SGD (Adaptive ALM momentum), a light-weight modification that multiplies the classical momentum coefficient by a small, learnable scalar (\u22480.1) updated online via gradient descent on the one-step validation loss. On CIFAR-10/100 and ImageNet, ALM-SGD matches the final accuracy of hand-tuned SGD-m at 90% confidence while requiring 30% fewer parameter sweeps. Theoretically, we characterize convergence on quadratic objectives, showing the adaptive constant yields an O(1/t^1.1) rate versus O(1/t) for plain momentum. Ablation studies reveal the scalar typically stabilizes near 0.07-0.12 across architectures. Although benefits shrink on very small models or short schedules, ALM-SGD introduces only three extra lines of code and negligible overhead, offering practitioners a drop-in tuning heuristic. Code is available anonymously.",
    "id": 222
  },
  {
    "title": "Revisiting Regularization Trajectories: A Kernel Perspective on Early Stopping",
    "authors": [
      "Anderson, C.",
      "Mukherjee, S.",
      "Li, H."
    ],
    "abstract": "Early stopping is a simple yet effective regularization technique, but its theoretical understanding remains incomplete, especially for modern over-parameterized models. In this work, we revisit early stopping through the lens of kernel regression, examining how the stopping time affects the bias-variance trade-off in high-dimensional settings. We provide a refined analysis showing that the optimal stopping epoch depends on the effective dimension of the data rather than the ambient dimension, leading to improved finite-sample bounds. Our experiments on synthetic and small-scale vision datasets demonstrate that our stopping rule can reduce test error by up to 5% compared to standard validation-based approaches. While our results offer new insights into early stopping behavior, the practical gains are modest, and the assumptions\u2014particularly the sub-Gaussian design and well-specified model\u2014may limit broader applicability. Nonetheless, this work contributes a clearer theoretical characterization of an ubiquitous heuristic and suggests avenues for adaptive stopping criteria in deep learning.",
    "id": 223
  },
  {
    "title": "Improved Gradient Noise Scaling for Large-Batch Training with a Modified Learning Rate Schedule",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Ortiz, J."
    ],
    "abstract": "Large-batch training can accelerate neural network optimization, but often leads to degraded generalization. We revisit gradient noise scaling (GNS) as a principled approach to maintain performance when scaling batch sizes. While prior work suggests a simple power-law relationship, we propose a modified GNS based on an empirically observed curvature-adaptive correction term. Our key insight is that the effective noise scale depends not only on batch size and learning rate, but also on the local Hessian trace, which we approximate efficiently using a Hutchinson-style estimator. We introduce a corrected learning rate schedule that adaptively adjusts the noise scale during training. Extensive experiments on CIFAR-10/100 and ImageNet show that our method allows 2--4\u00d7 larger batch sizes compared to baseline GNS, while maintaining test accuracy within 0.5% of small-batch training. However, we observe diminishing benefits at the largest batch sizes and on deeper architectures. Theoretically, we provide a convergence guarantee under standard smoothness assumptions, albeit with a more restrictive learning rate bound than prior analyses. Code is available at anonymized-link.github.io.",
    "id": 225
  },
  {
    "title": "Improved Gradient Norm Estimation for Adaptive Learning Rates in Deep Networks",
    "authors": [
      "Chen, L.",
      "Kothari, P.",
      "Nguyen, T."
    ],
    "abstract": "Adaptive learning rates are crucial for training deep neural networks efficiently. While Adam and its variants dominate practice, they rely on biased gradient moment estimates, potentially hindering convergence. We propose GN-Adam, a lightweight modification that re-scales Adam's update by an online estimate of the true gradient L2-norm obtained via a momentum-based EMA of per-sample gradients. Theoretically, we show GN-Adam achieves a  \u00d5(1/\u221aT) regret bound in the convex setting, matching Adam but under weaker assumptions. Empirically, GN-Adam yields 1\u20133% top-1 accuracy gains over AdamW on CIFAR-10/100 and ImageNet when training ResNet-18/50 and Vision-Transformers, with comparable wall-clock time. Ablation studies indicate the improvement comes chiefly from better handling of gradient noise early in training. While our bound does not extend to the non-convex case and the additional memory footprint equals the size of one gradient tensor, GN-Adam is easy to implement and may offer practical benefits for large-scale vision and language tasks.",
    "id": 226
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Neural Networks",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Johnson, M."
    ],
    "abstract": "We propose AdaGN, an optimization technique that adaptively injects gradient noise during neural network training to improve generalization. While previous work has shown that carefully tuned noise schedules can help escape sharp minima, determining optimal noise levels remains challenging. AdaGN addresses this by dynamically adjusting noise magnitude based on gradient history and local curvature estimates. Our method adds zero-mean Gaussian noise to gradients each iteration, with variance computed using a running average of gradient norms and a novel curvature-sensitive scaling factor. On CIFAR-10/100 and ImageNet experiments with ResNet-18 and VGG-16, AdaGN shows modest improvements over SGD+momentum (0.3-0.7% accuracy gains) and competitive performance compared to strong baselines like SAM and AdamW. Theoretical analysis suggests AdaGN encourages flatter minima under simplified quadratic assumptions. While intuitive and easy to implement, our approach requires careful tuning of two additional hyperparameters and may not scale well to very large models due to computational overhead. Code available at anonymous-url.github.io/adagn.",
    "id": 227
  },
  {
    "title": "Adaptive Gradient Rescaling for Improved Training Stability in Deep Neural Networks",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Training stability remains a critical challenge in deep learning, particularly for architectures with varying layer scales and activation magnitudes. We propose Adaptive Gradient Rescaling (AGR), a simple yet effective method that dynamically adjusts gradient magnitudes during training based on layer-wise gradient statistics. Unlike existing adaptive optimizers that modify learning rates through momentum-based estimates, AGR directly rescales gradients using a running estimate of gradient norms, normalized by the exponential moving average of historical gradient magnitudes across layers. Our method requires minimal hyperparameter tuning and can be seamlessly integrated with standard optimizers like SGD and Adam. We evaluate AGR on CIFAR-10/100 and ImageNet classification tasks, as well as on a variety of architectures including ResNet, Vision Transformer, and deep MLPs. Results show that AGR reduces training loss oscillations by 15-20% compared to vanilla optimizers and leads to marginally improved final accuracy (0.3-0.5% on average). While the performance gains are modest, AGR demonstrates consistent improvements across settings without introducing significant computational overhead. Theoretical analysis suggests AGR implicitly bounds the gradient Lipschitz constant, potentially explaining its stabilizing effect. Code and experiments will be made publicly available.",
    "id": 228
  },
  {
    "title": "LoRA-Lite: Structured Low-Rank Adaptation with Gradient-Free Hyper-Network Tuning",
    "authors": [
      "Kim, J.",
      "Liu, S.",
      "Martinez, C."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method, yet its reliance on back-propagation through the full network still incurs non-negligible memory and compute overhead. We propose LoRA-Lite, a plug-in extension that eliminates gradient flow through frozen weights by learning a compact hyper-network to generate the low-rank adapters. A factorized tensor-train compression of the hyper-network reduces trainable parameters by 38% on 7B-scale language models while maintaining the same inference path as standard LoRA. Empirically, LoRA-Lite recovers 96\u201398% of full fine-tune performance on GLUE when the base model is held fixed, and matches baseline LoRA scores on five downstream summarization tasks. Ablations show that the choice of rank scheduling and the hyper-network bottleneck width are critical; removing either component drops average scores by 2\u20133 points. Although our method introduces extra hyper-parameters and a small pre-training cost for the meta-weights, it enables true weight-free optimization and opens the door to on-device adapter personalization. Code and adapters are provided for reproducibility.",
    "id": 229
  },
  {
    "title": "On the Importance of Normalization in Transductive Few-Shot Learning",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transductive few-shot learning (TFSL) leverages test-time query statistics to adapt class prototypes, yet performance varies dramatically across benchmark splits. We hypothesize that batch normalization layers, fixed after meta-training, distort feature scales under domain shift. We propose Adaptive Transductive Normalization (ATN), a lightweight wrapper that re-estimates batch statistics using support and query features, coupled with a temperature-scaled cross-entropy loss. Extensive experiments on mini-ImageNet, tiered-ImageNet and CUB-200 show gains of 1.3\u20132.7% over state-of-the-art TFSL methods, while adding only 0.4 ms per task. Ablation reveals that half of the improvement comes from temperature scaling alone, and benefits shrink when pre-training on larger corpora. Analysis indicates that ATN chiefly corrects over-confident logits in low-shot regimes, leaving inter-class margins largely unchanged. Although our contribution is modular and easy to plug into existing pipelines, its theoretical justification is limited: we provide only intuitive arguments and empirical correlation rather than generalization bounds. Code and trained checkpoints are publicly available.",
    "id": 230
  },
  {
    "title": "Improved Margin Bounds for Generalization in Neural Networks via Layer-wise Spectral Shifts",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Roberts, J."
    ],
    "abstract": "We present a margin-based general on generalization bounds for deep feed-forward networks by tracking how feature-space margins evolve across layers. Building on PAC-Bayesian tools, we define a \u201cspectral shift\u201d quantity that measures the change in the principal angles between successive weight matrices. By upper-bounding the Rademacher complexity of the class of networks whose spectral shifts lie below a data-dependent threshold, we derive a generalization bound that scales as O(\u221a(\u2211_\u2113 \u03c1_\u2113\u00b2 \u2016W_\u2113\u2016\u2082\u00b2/n)), where \u03c1_\u2113 is our layer-wise shift and n is the sample size. On CIFAR-10 and ImageNet-subsets our bound is 7\u201322% tighter than the best published margin bounds yet remains 1.6\u00d7 looser than the true error, indicating room for improvement. Ablation shows the bound is sensitive to the choice of temperature in a novel \u201cspectral straight-through\u201d estimator we propose for efficient computation during training. While the estimator introduces a small bias (\u22480.8% drop in accuracy), it allows online monitoring of the bound without a second forward pass, making regularization practical. Our results suggest margin stories that focus on single layers may be insufficient, but coupling them per-layer yields measurable, if modest, gains.",
    "id": 231
  },
  {
    "title": "Lookahead Learning: A Simple Momentum-Based Correction to SGD with Polyak Step-Size",
    "authors": [
      "Kwon, S.",
      "Rodriguez, A.",
      "Nguyen, T."
    ],
    "abstract": "We propose Lookahead-Learning, a lightweight wrapper that augments stochastic gradient descent with Polyak step-size (SGD-PS) using a momentum-guided correction. While SGD-PS adaptively chooses step-size via function sub-optimality, its practical performance degrades when the estimated optimal value is misspecified. Our method maintains two sets of weights: a fast \u2018inner\u2019 SGD-PS iterate and a slow \u2018outer\u2019 iterate updated by an exponential moving average. At every k steps the outer weights are replaced by the inner ones, effectively injecting a Polyak-calibrated momentum. On convex problems Lookahead-Learning inherits the O(1/t) rate of SGD-PS without requiring knowledge of the optimal value; on over-parameterized neural networks it yields 5\u201312 % faster convergence in wall-clock time compared to vanilla SGD-PS on CIFAR-10/100 and ImageNette. The wrapper adds two lines of code and introduces a single extra hyper-parameter, the synchronization period k, which we show is stable across tasks when set between 5 and 10. Theoretically, we prove that the method preserves convergence under standard smoothness assumptions, but the analysis does not demonstrate strict improvement over SGD-PS. Although the empirical gains are consistent, they are modest and appear limited to small- and medium-scale vision benchmarks. We release PyTorch code to encourage reproducibility.",
    "id": 232
  },
  {
    "title": "Residual Learning for Improved Few-Shot Adaptation in Prototypical Networks",
    "authors": [
      "Nguyen, T.",
      "Chen, L.",
      "Kumar, V."
    ],
    "abstract": "Metric-based meta-learning methods such as Prototypical Networks achieve strong performance on few-shot classification, but require large meta-training sets and struggle when the query-support shift is large. We propose Residual Prototypical Networks (ResPN), which inject residual connections inside the embedding backbone while keeping metric space unchanged. ResPN enables deeper backbones without overfitting the limited support sets, yielding 2-3% accuracy gains on minimal and tieredImageNet. A second contribution is a simple support-set augmentation scheme that linearly interpolates support features, acting as a cheap form of hallucination that further improves robustness. Extensive ablation shows that both components are necessary; removing either reverts gains. Although the residual trick is simple and the augmentation resembles prior work in self-supervised learning, the combination consistently outperforms baseline PN and Meta-Baseline under the same training budget. Limitations include marginal gains on cross-domain transfer and no theoretical justification for why residuals help in the few-shot regime. Code and checkpoints will be released upon acceptance.",
    "id": 233
  },
  {
    "title": "LoRAMA: Low-Rank Adaptation with Momentum Averaging for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Kim, J."
    ],
    "abstract": "We present LoRAMA, a simple extension to the Low-Rank Adaptation (LoRA) framework that incorporates momentum-based weight averaging during fine-tuning. Despite the widespread adoption of LoRA for parameter-efficient adaptation of large language models, the original method lacks explicit mechanisms for stabilizing training dynamics when adapting to domains with limited data. LoRAMA addresses this by maintaining an exponential moving average of the low-rank matrices, motivated by recent observations that weight averaging improves generalization in full fine-tuning. Our experiments on GLUE and three downstream classification tasks show that LoRAMA achieves comparable or slightly better performance than standard LoRA while maintaining the same parameter efficiency (0.1% of total parameters). On average, LoRAMA improves accuracy by 0.7% over vanilla LoRA across tasks, with gains concentrated in low-data regimes (<1k examples). The method requires minimal code changes and no additional hyperparameters beyond the momentum coefficient. While the improvements are modest, LoRAMA represents a computationally cheap enhancement to an already popular technique. Limitations include marginal gains on high-resource tasks and lack of theoretical justification for the momentum mechanism. We release our code and checkpoints to facilitate reproducibility.",
    "id": 234
  },
  {
    "title": "Gradient Norm Annealing: A Simple Trick for Better Generalization in Stochastic Optimization",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Roberts, C."
    ],
    "abstract": "We propose Gradient Norm Annealing (GNA), a lightweight modification to standard stochastic optimizers that periodically rescales gradients based on their historical norms. Motivated by the observation that flat minima often correlate with improved generalization, GNA adjusts the effective learning rate for each parameter proportionally to the exponential moving average of past gradient norms. Unlike adaptive methods such as Adam or RMSProp, GNA retains the sign and direction of stochastic gradients while only modulating their magnitude, leading to minimal overhead and compatibility with existing architectures. On CIFAR-10/100 and ImageNet, GNA improves test accuracy by 0.4\u20130.9 pp over vanilla SGD with momentum at no additional compute cost, and matches or slightly outperforms strong baselines including SAM and SWA. Averaged across eight diverse tasks\u2014from language modeling on WikiText-2 to reinforcement learning on Atari\u2014the gains are smaller (+0.2 pp) but consistent. Theoretical analysis in a quadratic model suggests GNA encourages convergence to wider valleys, yet the effect vanishes under high label noise. Code is publicly available. While the contribution is incremental, extensive ablations and a simple implementation may prove useful to practitioners.",
    "id": 235
  },
  {
    "title": "LoRa-\u0394: Partial Low-Rank Adaptation for More Parameter-Efficient Fine-Tuning",
    "authors": [
      "Nguyen, T.",
      "Kumar, V.",
      "Chen, S.",
      "Garcia, J."
    ],
    "abstract": "We study whether the rank of LoRA adapters can be reduced further without hurting downstream performance. Motivated by the observation that only a subset of layers undergo large distributional shift during fine-tuning, we propose LoRa-\u0394, a simple variant that applies low-rank updates to a data-dependent fraction of transformer blocks. A lightweight importance score, computed from the Fisher information of each layer on a small validation split, selects the target blocks; the remaining layers are frozen. On GLUE and NLG benchmarks, LoRa-\u0394 retains 96-99 % of full-LoRA quality while training up to 27 % fewer parameters. Ablation shows that layer selection matters more than rank allocation, and that a single shared rank is often sufficient. Although our gains are incremental, the method is implementation-trivial, introduces no hyper-parameters beyond a sparsity threshold, and is orthogonal to other efficiency techniques. We release PyTorch code and adapters to facilitate reproduction.",
    "id": 236
  },
  {
    "title": "LoRa-SGD: Low-Rank Stochastic Gradient Descent for Communication-Efficient Distributed Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose LoRa-SGD, a simple modification to distributed SGD that compresses gradients using low-rank matrix factorization. Motivated by the observation that gradient matrices in deep networks often exhibit approximately low-rank structure, our method decomposes each gradient matrix into the product of two smaller matrices, reducing communication costs by 60-80% on standard vision and language tasks. Theoretically, we prove convergence guarantees under standard convexity assumptions, though our bounds depend on potentially restrictive spectral gap conditions that may not hold in practice. Empirically, LoRa-SGD achieves comparable test accuracy to full-rank training on CIFAR-10 and WikiText-2, but shows degradation on ImageNet-scale models. While our approach is straightforward to implement and requires minimal hyperparameter tuning, the compression introduces additional computational overhead during decomposition, and our theoretical analysis does not extend to non-convex settings. Code is available at: [anonymous repo link].",
    "id": 237
  },
  {
    "title": "Towards Calibrated Uncertainty via Test-Time Adaptation of Batch-Normalization Moments",
    "authors": [
      "Klein, L.",
      "Liu, M.",
      "Rodr\u00edguez, C."
    ],
    "abstract": "Accurate epistemic uncertainty is crucial for safe deployment of deep networks, yet modern CNNs remain poorly calibrated under distribution shift. We propose a lightweight test-time adaptation scheme that improves calibration by refining only the batch-normalization (BN) statistics. Our method, TANGO, computes entropy-guided gradient steps on running mean and variance stored in BN layers, requiring neither additional parameters nor model retraining. Across CIFAR-10-C, ImageNet-C and three clinical X-ray benchmarks, TANGO lowers expected calibration error (ECE) by 8\u201315 % relative to the pretrained backbone while maintaining accuracy. Theoretically, we show that our update rule minimizes an upper bound on the calibration error under label shift. Although gains are smaller on naturally occurring drifts and computational overhead scales linearly with batch size, TANGO offers a plug-and-play alternative to full-model fine-tuning that can be implemented in five lines of PyTorch. Code is publicly available.",
    "id": 238
  },
  {
    "title": "Gradient Norm Clipping with Root Mean Square Leads to More Stable Training",
    "authors": [
      "Nguyen, K.",
      "Ravi, S.",
      "Johnson, T."
    ],
    "abstract": "Gradient norm clipping is a widely used heuristic for stabilizing neural network training, yet its theoretical understanding remains limited. We study how the choice of clipping statistic affects convergence and propose a simple modification: root-mean-square (RMS) clipping. By analyzing a heavy-tailed stochastic optimization model that captures gradient noise observed in practice, we prove that RMS clipping yields a tighter trade-off between bias and variance than the standard max-norm variant. Empirically, switching the clipping statistic in Adam and LAMB improves validation perplexity on WikiText-103 by 0.4-0.7 without additional hyper-parameter search, and reduces training instability in 6/9 large-scale vision and language experiments. While our theoretical bounds require a bounded gradient assumption that may not hold for very deep networks, the consistent empirical gains suggest RMS clipping can serve as a drop-in replacement. Code and 50 random seeds are provided for reproducibility.",
    "id": 239
  },
  {
    "title": "Momentum Without the Mass: Memory-Free Weight Interpolation for Online Continual Learning",
    "authors": [
      "Fernandez, L.",
      "Zhao, K.",
      "Nguyen, P."
    ],
    "abstract": "Continual learning methods typically rely on replay buffers or regularization to mitigate catastrophic forgetting, both of which incur memory or compute overheads that scale with task sequence length. We propose Streaming Weight Averaging (SWA), an embarrassingly simple alternative that keeps only a single exponentially-weighted copy of the parameters and uses no replay data. SWA interpolates between current and past weights with a scalar momentum coefficient that is annealed according to a schedule we derive from a stochastic linearization of the loss surface. On standard benchmarks such as Split-CIFAR-100 and CORe50, SWA matches or marginally outperforms rehearsal-based baselines while storing 0\u00d7 examples; on larger-scale ImageNet-1k sequences it lags the best buffer approaches by 2\u20134 % accuracy but still outperforms regularization-only methods. Theoretically, we bound the forgetting gap under convexity assumptions and relate the schedule to the task switch rate. Although our analysis relies on approximate quadratic loss surfaces and does not yet extend to modern deep architectures, SWA offers practitioners a parameter-only baseline that can be implemented in 5 lines of code. Code is available at anonymous URL.",
    "id": 240
  },
  {
    "title": "Gradient Confusion-Aware Label Smoothing for Improved Generalization in Over-parameterized Models",
    "authors": [
      "Kumar, A.",
      "Li, S.",
      "Johnson, M."
    ],
    "abstract": "Label smoothing is widely used to prevent overconfident predictions, yet its interaction with the high-dimensional, non-convex loss surfaces of modern deep networks remains poorly understood. We observe that when gradient confusion\u2014the inconsistency of per-example gradients\u2014is high, standard label smoothing can exacerbate optimization instability and hurt generalization. To address this, we propose Gradient Confusion-Aware Label Smoothing (GCA-LS), which dynamically adjusts the smoothing parameter as a function of the empirical gradient confusion measured in a sliding window during training. On CIFAR-100 and ImageNet subsets, GCA-LS yields average top-1 gains of 0.3\u20130.7 % over vanilla label smoothing with the same architecture, and 1.1 % when label smoothing alone degrades accuracy. A simple PyTorch extension making two extra scalar operations per step keeps overhead under 1 %. While our empirical study is limited to image classification and small-scale NLP tasks, GCA-LS introduces a lightweight mechanism linking gradient geometry to regularization strength, providing a new angle on why and when label smoothing helps. Code is available anonymously.",
    "id": 241
  },
  {
    "title": "Improved Convolutional Filters via Learnable Pixel Permutations",
    "authors": [
      "Kumar, S.",
      "Ortega, M.",
      "Zhou, J."
    ],
    "abstract": "Convolutional Neural Networks (CNNs) achieve translation equivariance by sharing filters across spatial locations, yet this very weight sharing prevents them from adapting to non-stationary image statistics such as slowly varying lighting or texture gradients. We propose PermuteConv, a lightweight module that learns a smooth, input-dependent pixel permutation to re-order spatial locations before standard convolution. A small auxiliary network predicts permutation parameters from local neighborhoods; these parameters are constrained to form a diffeomorphic warp so that gradients flow reliably and no costly permutation matrix is stored. On CIFAR-100 and ImageNet our ResNet-50 equipped with PermuteConv yields +0.7% and +0.4% top-1 accuracy with <1% parameter overhead. Ablation shows gains concentrate on images with non-uniform illumination. While the improvement is incremental and wall-clock time rises by 8%, PermuteConv is the first module that allows convolutions to adapt spatial sampling without abandoning weight sharing, providing a plug-and-drop replacement for standard layers. Code is available at anonymous-url.",
    "id": 242
  },
  {
    "title": "Improved Generalization via Gradient Norm Clipping with Momentum-Based Adaptive Thresholds",
    "authors": [
      "Liu, M.",
      "Kumar, S.",
      "Anderson, B."
    ],
    "abstract": "Deep neural networks often achieve low training error but struggle with generalization, particularly in small-data regimes. Motivated by empirical observations that the distribution of gradient norms correlates with generalization performance, we propose Momentum-GNC, an enhancement to standard gradient norm clipping that adaptively sets clipping thresholds based on a running estimate of gradient norm statistics. While gradient clipping is traditionally used for training stability, our method exploits it as a regularizer by modulating the effective learning rate per iteration. We derive a PAC-Bayesian generalization bound that incorporates the proposed adaptive mechanism, showing theoretical improvement when gradient norms follow a sub-gaussian tail. On CIFAR-10/100 and ImageNet-1K, Momentum-GNC yields 0.8\u20131.2% top-1 accuracy gains for ResNet-50 and 0.5% for Vision Transformers relative to vanilla SGD with momentum, while converging 5\u201310% faster in wall-clock time. Nonetheless, gains diminish on larger datasets and heavily tuned baselines, limiting broad applicability. The method requires one additional hyper-parameter that interacts non-trivially with learning-rate schedules, complicating adoption. Code and checkpoints are provided for reproducibility. Overall, Momentum-GNC offers a lightweight modification that can slightly boost generalization and training speed, but its benefits may be incremental for practitioners already employing strong regularization protocols.",
    "id": 243
  },
  {
    "title": "Boosting Batch RL with Imperfect Online Queries",
    "authors": [
      "Chen, L.",
      "Kumar, V.",
      "Roberts, J."
    ],
    "abstract": "We study whether a handful of inexpensive but potentially sub-optimal online interactions can improve batch reinforcement learning when the logged data are poorly aligned with the task at hand. Starting from a fixed offline dataset, we interleave policy-optimized queries that may be executed by a fallible supervisor (e.g., a heuristic controller or even a human typing actions with delay). Rather than trusting these queries outright, we treat them as a biased data source and re-weight their transitions through a simple importance-ratio clipping scheme derived from a KL-constrained trust region. Across 12 continuous-control benchmarks, our hybrid protocol yields 5-15% gains over pure offline RL at the cost of fewer than 1k environment steps, and remains competitive with fully online fine-tuning that uses 20\u00d7 more experience. Although our asymptotic performance plateaus early and the clipping threshold is sensitive to domain noise, the procedure is easy to implement and requires no additional models beyond standard off-policy value methods. The results suggest that even imperfect online interactions can be recycled effectively when handled with appropriate caution.",
    "id": 244
  },
  {
    "title": "Momentum-Scheduled Sharpness Minimization for Improved Generalization",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Ortiz, J."
    ],
    "abstract": "Recent work has shown that minimizing the sharpness of the loss landscape improves generalization in neural networks. While several algorithms achieve this by biasing optimization toward flat minima, they often introduce sensitive hyperparameters that require costly tuning. We propose Momentum-Scheduled Sharpness Minimization (MSSM), a simple extension that adaptively schedules the momentum coefficient based on local curvature estimates. Our method uses an efficient approximation of the sharpness measure via gradient covariances, avoiding the computational overhead of prior approaches. On CIFAR-10 and CIFAR-100, MSSM achieves competitive test accuracy compared to standard sharpness-aware minimization, while reducing hyperparameter sensitivity. Theoretical analysis in a simplified quadratic setting suggests that adaptive momentum can guide optimization toward flatter regions, though the assumptions are restrictive. Ablation studies indicate that the scheduling mechanism contributes most to performance gains. While our empirical results are promising, we acknowledge limitations in scalability to larger architectures and the lack of convergence guarantees under general conditions. Code is available at anonymized-url.",
    "id": 245
  },
  {
    "title": "Towards Adaptive Momentum via Online Learning Rate Derivatives",
    "authors": [
      "Kumar, A.",
      "Huang, S.",
      "Ramos, J."
    ],
    "abstract": "Momentum-based optimizers are widely used in training deep networks, yet their hyperparameters are typically fixed or scheduled heuristically. We propose Adaptive Momentum via Online Learning Rate Derivatives (AMOL), a method that continuously tunes the momentum coefficient as a function of the local curvature revealed by finite-difference estimates of the learning-rate gradient. AMOL requires only scalar operations per step and introduces no additional memory overhead. On standard image-classification benchmarks (CIFAR-10/100, ImageNette) AMOL matches the final accuracy of tuned SGD-momentum and Adam while reducing manual tuning by roughly 50%. Theoretically, we show that under L-smoothness AMOL converges at a rate of O(1/T) for convex objectives, matching known bounds for fixed-momentum methods. However, ablations reveal that gains diminish on larger architectures and that hyper-sensitivity to the finite-difference step-size can hurt stability. Code and 50 seed logs are provided for reproducibility.",
    "id": 246
  },
  {
    "title": "Improving Transformer Generalization with Iterative Attention Sharpening",
    "authors": [
      "Kim, J.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "While transformer models have achieved remarkable success across many NLP tasks, they often struggle with compositional generalization and systematic extrapolation. We propose Iterative Attention Sharpening (IAS), a simple yet effective training augmentation that encourages sparse, peakier attention distributions without architectural changes. IAS periodically raises the temperature of the softmax in self-attention layers, then slowly anneals it back, encouraging the model to progressively focus on smaller subsets of relevant tokens. We evaluate IAS on synthetic algorithmic tasks, natural language inference, and low-resource translation. Results show modest but consistent improvements: 3.2% average accuracy gain on SCAN compositional instructions, 1.1 BLEU improvement on IWSLT\u201915 English-Vietnamese, and 0.7% higher accuracy on HANS challenge set, compared with standard transformers of equal size. Ablation studies indicate that two sharpening cycles during the last 30% of training yield the best trade-off between bias and variance. Although the gains are incremental and task-dependent, IAS introduces only one hyper-parameter, requires no extra parameters or inference cost, and can be easily integrated into existing training pipelines. Code is available at anonymous-url.",
    "id": 247
  },
  {
    "title": "Revisiting Gradient Clipping in Private SGD: A Second-Order Perspective",
    "authors": [
      "Koh, J.",
      "Singh, A.",
      "Morrison, D."
    ],
    "abstract": "Gradient clipping is a standard ingredient in differentially-private stochastic optimization, yet its interaction with curvature information remains poorly understood. We introduce Curvature-Aware Clipped SGD (CAC-SGD), a variant that adapts clipping thresholds using local second-order statistics estimated by the Hutchinson method. On logistic regression and small-scale vision tasks, CAC-SGD yields privacy\u2013utility curves within 1.5% of the unclipped baseline while spending 20\u201335% less privacy budget. Theoretically, we bound excess empirical risk as  \u00d5(\u221arank(H)/\u03b5n) under (\u03b5,\u03b4)-DP, where H is the averaged Hessian. Although our bound does not improve the worst-case rate, empirical Hessian spectra on real datasets show that rank(H) can be an order of magnitude smaller than dimension, suggesting tighter instance-dependent guarantees. A distributed implementation in JAX requires only a constant-factor overhead in wall-clock time. Limitations include scalability to very large models (>200M parameters) and reliance on subsampling-based curvature estimates that may amplify privacy noise. We release code and anonymized checkpoints.",
    "id": 248
  },
  {
    "title": "Revisiting Batch-Normalization with Learnable Scaling Factors for Improved Generalization",
    "authors": [
      "Kovacs, B.",
      "Nguyen, T.",
      "Schmidt, L."
    ],
    "abstract": "Batch normalization (BN) has become a standard component of deep neural networks, yet its theoretical understanding and practical variants continue to evolve. We propose L-BN, a simple extension that multiplies the normalized activations by a learned channel-wise scaling vector before the standard affine transform. This minor change allows the network to softly interpolate between normalized and unnormalized representations, effectively adding one extra degree of freedom per channel. Empirically, L-BN improves top-1 accuracy on ImageNet by 0.3-0.7% for ResNet-50 and EfficientNet-B0 with no additional inference cost, and yields modest gains on CIFAR-10/100 across four architectures. Ablation studies indicate that benefits are larger when the base BN is less stable (small batch sizes, large learning rates). Theoretical analysis shows that L-BN can be viewed as a regularizer whose strength diminishes as training progresses, offering a possible explanation for the observed improvements. While the contribution is incremental, our work highlights that small, carefully chosen tweaks to well-established modules can still yield measurable gains, and we hope our findings encourage deeper audits of commonplace components.",
    "id": 249
  },
  {
    "title": "Improved Gradient Bounds for Wasserstein GANs with Relaxed Lipschitz Constraints",
    "authors": [
      "Chen, T.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "The Lipschitz constraint is central to Wasserstein GANs, yet enforcing it via gradient penalty leads to training instabilities and mode dropping. We propose a relaxed Lipschitz condition that only requires the discriminator to be locally Lipschitz within a data-dependent neighborhood, eliminating the need for explicit regularization. Our approach uses a novel margin-based loss that encourages bounded gradients only in regions supported by the data manifold. Theoretically, we establish approximation bounds showing that Wasserstein distance estimation remains accurate under our relaxation. Empirically, we demonstrate improved sample quality on CIFAR-10 (FID 3.21 vs 3.87 for baseline) and reduced training time on CelebA-HQ. While our method achieves competitive results on standard benchmarks, we observe increased sensitivity to hyperparameter choices in low-data regimes. Our work provides a principled alternative to gradient penalties, though the theoretical guarantees hold only for distributions with compact support.",
    "id": 250
  },
  {
    "title": "Revisiting Momentum with Signed Gradient Noise: Improved Convergence via Gradient Sign Asymmetry",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Nguyen, A."
    ],
    "abstract": "We propose Signed Momentum (SignMo), a simple modification to standard momentum-based optimizers that incorporates sign information of the gradient noise. By tracking the asymmetry between positive and negative gradient signs across iterations, SignMo adaptively re-scales the momentum coefficient without additional hyper-parameters. Our method is motivated by recent empirical observations of sign imbalances in gradients when training over-parameterized models. We prove convergence rates for SignMo in convex and non-convex settings that match or improve upon standard momentum under a bounded sign-asymmetry assumption. Experiments on ResNet-18/50 across CIFAR-10/100 and ImageNet show modest improvements in final accuracy (+0.3-0.7%) and 5-10% faster convergence compared to SGD with momentum. However, gains diminish on larger models and are less consistent than adaptive methods like Adam. Ablation studies indicate performance is sensitive to batch size and learning rate schedules. While SignMo introduces minimal overhead and achieves measurable speedups on some benchmarks, its theoretical assumptions may not hold broadly, and improvements are task-specific. Code is available at anonymized-repo.github.io/SignMo.",
    "id": 251
  },
  {
    "title": "AdaSmooth: Adaptive Label Smoothing with Gradient Feedback for Calibration",
    "authors": [
      "Nguyen, K.",
      "Chen, L.",
      "Garcia, M."
    ],
    "abstract": "Label smoothing improves calibration and generalization by softening one-hot targets, yet the optimal smoothing rate remains data- and model-dependent. We propose AdaSmooth, a plug-in method that adjusts per-sample smoothing on-the-fly using gradient feedback from a held-out calibration mini-batch. During training, a light-weight meta-network predicts sample-wise smoothing magnitudes; the meta-parameters are updated by minimizing the expected calibration error on the calibration set, propagated through the base model via implicit differentiation. Experiments on CIFAR-100 and ImageNet with ResNet-50/101 show that AdaSmooth yields modest gains in top-1 accuracy (+0.3%) and reduces expected calibration error by 5\u20138% relative to uniform label smoothing. Ablations indicate that the calibration set can be as small as 5% of the training data without hurting performance. While the approach is applicable to arbitrary architectures, computational overhead amounts to \u224815% longer training time. Theoretically, we bound the generalization gap under adaptive smoothing by extending uniform-stability arguments; however, the bound scales with the calibration set size and is loose in practice. Code is made available.",
    "id": 252
  },
  {
    "title": "Gradient Flow Regularization: A Light-Weight Technique for Improving Generalization in Neural Networks",
    "authors": [
      "Kim, J.",
      "Rodriguez, S.",
      "Tanaka, H."
    ],
    "abstract": "We propose Gradient Flow Regularization (GFR), a simple training augmentation that encourages parameters to follow flatter trajectories in loss landscapes. Motivated by the empirical observation that mini-batch gradients oscillate more in regions of high curvature, GFR adds a quadratic penalty on the difference between successive gradient estimates. Unlike prior curvature-smoothing schemes, GFR introduces no additional forward or backward passes and incurs <1% overhead. On CIFAR-10/100 and ImageNet, GFR improves the average test error of ResNet-18/50 and Vision-Transformer-Small by 0.6\u20131.2% compared to vanilla training and 0.2\u20130.4% compared to strong baselines such as SAM and ShakeDrop. Ablation studies confirm the effect is complementary to data augmentation and label smoothing. Theoretical analysis shows GFR upper-bounds the trace of the Hessian under standard assumptions, suggesting a connection to flat-minima regularization. While the gains are consistent, they are modest and appear largest on medium-scale architectures. Code and hyper-parameters will be provided for reproducibility.",
    "id": 253
  },
  {
    "title": "Momentum-Scheduled Sharpness-Aware Minimization for Improved Generalization",
    "authors": [
      "Liu, K.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Recent sharpness-aware minimization (SAM) methods improve generalization by seeking parameters in neighborhoods of uniformly low loss. However, SAM treats all examples and training epochs equally, potentially over-regularizing at early stages and under-regularizing at later ones. We propose Momentum-Scheduled SAM (MS-SAM), a simple variant that anneals the neighborhood radius according to a momentum-corrected estimate of the sharpness magnitude. Intuitively, MS-SAM allows larger updates early in training while maintaining robustness near convergence. On CIFAR-10/100 and ImageNet, MS-SAM improves top-1 accuracy over SAM by 0.3\u20130.7%, with gains most pronounced on architectures prone to sharp minima. Ablation studies show the schedule derived from momentum tracking is more stable than hand-tuned polynomial decay. We provide uniform-stability bounds suggesting the adaptive radius yields a tighter generalization gap, though the improvement is constant-factor rather than asymptotic. Code and checkpoints are available.",
    "id": 254
  },
  {
    "title": "Improving Gradient Estimation in Score-Based Models Through Adaptive Noise Scaling",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Ortiz, J."
    ],
    "abstract": "Score-based generative models have achieved impressive results on image synthesis, yet their training remains unstable for high-resolution data. We observe that the commonly-used fixed noise schedule produces gradients with high variance during early training, leading to slow convergence and occasional divergence. We propose Adaptive Noise Scaling (ANS), a simple modification that adjusts the noise magnitude based on the current gradient statistics. Our method tracks an exponential moving average of gradient norms and rescales the noise terms to maintain a target signal-to-noise ratio. On CIFAR-10 and ImageNet 64\u00d764, ANS reduces the gradient variance by 37% at the start of training and improves FID scores modestly (2.1\u21921.9 on CIFAR-10). While the approach is straightforward and introduces minimal overhead, we show it leads to more stable training across multiple architectures. Theoretically, we provide a convergence guarantee for a simplified Langevin dynamics setting, though the bound is looser than prior work. Our results suggest that adaptive scheduling can benefit score-based training, but the gains are incremental and limited to early training phases. Code is available anonymously.",
    "id": 255
  },
  {
    "title": "Improved SAM Requires Only Half the Sharpness: A Modified Minimax Optimization",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Garcia, M."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a popular optimizer for improving generalization by seeking flat minima. However, SAM requires two forward-backward passes per step, doubling the computational overhead. We propose Half-SAM (HSAM), a simple modification that approximates the worst-case perturbation using a first-order Taylor expansion around the current parameters rather than an explicit ascent step. This reduces the per-step cost to roughly that of standard SGD while preserving the core sharpness-seeking behavior. We prove that HSAM converges to an  \u03f5-stationary point in O(1/\u03f5\u00b2) iterations for smooth non-convex objectives, matching the rate of SAM. Empirically, HSAM achieves comparable or slightly worse accuracy compared to SAM on CIFAR-10/100 and ImageNet while reducing training time by 30\u201340%. Ablations suggest the first-order approximation is sufficient when the loss surface is locally quadratic, but degrades on highly non-convex landscapes. While HSAM offers a practical speed-up, the generalization gains over well-tuned SGD+Momentum are marginal in some settings. Code is available at anonymized-url.",
    "id": 256
  },
  {
    "title": "Revisiting Weight Averaging: A Simple Baseline for Semi-Supervised Deep Regression",
    "authors": [
      "Kumar, V.",
      "Zhou, L.",
      "Anderson, J."
    ],
    "abstract": "Weight averaging has shown promise for improving generalization in supervised learning, but its role in semi-supervised regression remains under-explored. We study the effectiveness of exponential moving average (EMA) weights across different confidence-based pseudo-labeling schemes for regression tasks. Specifically, we propose an adaptive EMA decay schedule that adjusts the momentum based on the prediction uncertainty of unlabeled samples. Our experiments on UCI benchmarks and two synthetic datasets show that the proposed schedule achieves 3-5% lower RMSE compared to standard EMA with fixed decay, while requiring no additional hyper-parameters at test time. Although the gains are consistent, they are modest and diminish as the labeled set size increases. Furthermore, our method under-performs recent consistency-regularization approaches when abundant unlabeled data is available. Our contribution is primarily empirical: we provide a simple, implementation-ready baseline and release code to encourage future comparisons. Theoretical analysis of the adaptive averaging scheme is left for future work.",
    "id": 257
  },
  {
    "title": "Adaptive Gradient Clipping with Local Lipschitz Estimates Can Improve GAN Training",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Nguyen, T."
    ],
    "abstract": "Training GANs remains challenging due to instability from unbounded gradients and rapidly changing discriminators. We propose a simple modification to Adam\u2014termed LiGradClip\u2014that clips gradients using online estimates of the local Lipschitz constant of the loss surface. Empirically, LiGradClip reduces catastrophic gradient spikes by 34% on CIFAR-10 and lowers FID on CelebA-64 from 13.2 to 11.8 versus standard Adam with fixed clipping. Although our method adds two hyper-parameters (window size and percentile), we provide a heuristic grid-search routine that completes in <20% of total training time. Theoretically, we show LiGradClip converges at the same O(1/\u221aT) rate as Adam for smooth non-convex objectives. Ablation studies indicate that local estimation is crucial; fixed clipping actually degrades FID when the discriminator capacity is high. While our gains are incremental and mostly demonstrated on smaller datasets, LiGradClip requires minimal code changes and could be complementary to recent regularization techniques. We release PyTorch code to reproduce all experiments.",
    "id": 259
  },
  {
    "title": "Revisiting Dropout through the Lens of Shattering Regularization",
    "authors": [
      "Morales, L.",
      "Jawahar, S.",
      "Hans, A."
    ],
    "abstract": "Dropout remains one of the most widely used regularizers in deep learning, yet its theoretical underpinnings are still debated. We propose an alternative view that interprets dropout as approximately maximizing the shattering coefficient of the learned representation, thereby encouraging linear separability at training time. By connecting dropout masks to random hyperplane arrangements, we derive a PAC-Bayesian bound that depends on the expected number of dichotomies realizable by the stochastic network. Our bound suggests that higher dropout rates monotonically increase the effective VC-dimension up to a data-dependent saturation point, offering a principled guideline for choosing p. Empirically, we verify that the bound is non-vacuous on small-scale vision tasks when combined with a tight union-bound technique. On CIFAR-10 and ImageNet subsets, we show that schedules derived from our theory modestly outperform standard dropout tuning, reducing test error by 0.3\u20130.7%. While our results provide new intuition about dropout\u2019s implicit bias, the improvements are incremental and the theory currently applies only to fully-connected architectures. We discuss limitations regarding convolutional layers, batch normalization, and modern architectures, and outline directions for extending the framework to residual connections and attention mechanisms.",
    "id": 260
  },
  {
    "title": "Improved Margin-Based Bounds via Data-Dependent Partitioning",
    "authors": [
      "Nguyen, T.",
      "Kumar, A.",
      "Chen, Y."
    ],
    "abstract": "We revisit margin-based generalization bounds for multiclass classifiers and propose a simple data-dependent partition scheme that tightens classical bounds without introducing extra hyper-parameters. By clustering training features and separately bounding the complexity within each cluster, we obtain a bound that scales with the margin of the worst cluster rather than the entire dataset. The approach applies to any margin-loss and yields closed-form expressions for linear and kernel machines. On twelve UCI benchmarks we achieve $3$\u2013$7\\%$ tighter bounds over the uniform-margin baseline; on CIFAR-10 and ImageNet subsets the improvement reaches $12\\%$ at moderate network width. Although the gain vanishes when the feature space is nearly homogeneous, the method runs in $<5\\%$ of the training time and provides inexpensive side information during architecture search. While our bound is still looser than state-of-the-art PAC-Bayesian bounds, it requires no posterior optimization and may offer practitioners a lightweight sanity check before committing to expensive retraining.",
    "id": 261
  },
  {
    "title": "Re-thinking Label Smoothing with Adaptive Confidence Calibration",
    "authors": [
      "Kumar, S.",
      "Garcia, A.",
      "Zhou, L."
    ],
    "abstract": "Label smoothing is widely adopted to improve generalization in deep learning, yet its uniform confidence adjustment ignores the varying difficulty of individual training examples. We propose Adaptive Label Smoothing (AdaSmooth), which selects the smoothing factor from a small discrete set on a per-example basis by minimizing a proxy for the validation loss. The algorithm uses a lightweight importance-weighted moment-mroring scheme that requires only one additional forward pass every five epochs and negligible memory overhead. Across six image-classification tasks and two Transformer language models, AdaSmooth yields mean accuracy gains of 0.22% over tuned baseline label smoothing. While the absolute improvement is modest, the method is consistent: it improves on eight of the ten settings and reduces over-fitting in the low-sample regime. Ablations show that choosing smoothing factors adaptively matters more than the precise choice of hyper-parameters. Limitations include the discrete search grid and dependence on a held-out validation split; theoretical justification is left for future work. Code is available at [anonymized URL].",
    "id": 262
  },
  {
    "title": "Re-thinking Batch-Normalization: A Kernel-Driven View with Limited Improvements on CIFAR-10",
    "authors": [
      "Nguyen, T.",
      "Kovacs, B."
    ],
    "abstract": "Batch normalization (BN) remains a staple component in modern neural-network training, yet its theoretical underpinnings are still debated. We propose Kernel-Normalization (KN), a plug-in replacement that re-weights activations through a data-dependent kernel similarity matrix instead of the standard moment-matching used by BN. By modeling intra-batch statistics as graphs, KN is able to learn instance-specific rescaling parameters that are updated only once per epoch, reducing computational overhead. Extensive experiments on CIFAR-10, CIFAR-100 and a 10-class subset of ImageNet show that KN achieves comparable accuracy to BN while offering a modest 5-7 % reduction in gradient-conflict variance. Ablation studies reveal that most gains vanish when the batch size exceeds 256, indicating that KN primarily benefits small-batch regimes. Although KN does not outperform newer norm-free architectures or state-of-the-art group normalization variants, our work provides an alternative lens that connects batch normalization to kernelized similarity spaces, opening avenues for future investigation. Code is attached as a single Jupyter notebook.",
    "id": 263
  },
  {
    "title": "AdaBoost-S: Stabilizing AdaBoost via Sample Reweighting with Self-Consistency",
    "authors": [
      "Nguyen, T.",
      "Kim, J.",
      "Singh, A."
    ],
    "abstract": "AdaBoost remains one of the most widely used ensemble algorithms, yet its tendency to overfit noisy labels and data outliers limits its modern applicability. We propose AdaBoost-S, a lightweight modification that stabilizes training by re-computing sample weights using a self-consistency loop before each boosting round. Specifically, we adjust the exponential weights by comparing each sample's current margin distribution with its historical average across previous iterations, down-weighting inconsistent updates that deviate from the sample's typical behavior. Under standard assumptions, we prove that the method preserves AdaBoost's original margin-based generalization bound up to a logarithmic factor. On 12 UCI benchmarks, AdaBoost-S matches or marginally improves test accuracy while reducing variance across noisy-label settings (5\u201330% label noise) by 6\u201314%. Ablation studies indicate that only two historical buffer steps are required for stable performance, keeping runtime overhead below 3%. Notably, AdaBoost-S can be implemented in fewer than 20 lines of code and integrates transparently with off-the-shelf decision-tree learners. Although the proposed reweighting heuristic is intuitive rather than provably optimal, empirical results suggest it offers a practical trade-off between robustness and simplicity. We release Python and Julia implementations to facilitate adoption.",
    "id": 264
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Routing",
    "authors": [
      "Chen, L.",
      "Krishnan, S.",
      "M\u00fcller, J."
    ],
    "abstract": "The quadratic complexity of self-attention in Transformers limits their application to long sequences. We propose Token Routing Networks (TRNs), a modification that learns to route only informative tokens through the full self-attention mechanism while processing others with cheaper operations. Our approach uses a lightweight routing module trained with Gumbel-Softmax to make discrete routing decisions, reducing computation while preserving model expressivity. We evaluate TRNs on language modeling and machine translation tasks, achieving 15-25% speedup with minimal perplexity increase on WikiText-103 and comparable BLEU scores on WMT'14 English-German. Theoretical analysis shows our method preserves universal approximation properties under mild assumptions. While our results demonstrate practical efficiency gains, we acknowledge that speedup saturates on shorter sequences and some architectural hyperparameters require task-specific tuning.",
    "id": 265
  },
  {
    "title": "Improved Gradient Bounds for Noisy Curriculum Learning via Sample-Dependent Smoothing",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Gao, Y."
    ],
    "abstract": "Training deep networks on datasets with gradually increasing difficulty has empirically been shown to improve final accuracy. Despite recent work, theoretical justification is scarce: existing analyses either assume bounded gradient noise\u2014a condition regularly violated in practice\u2014or invoke growth conditions that make generalization bounds vacuous for early curriculum rounds. We propose a new sample-dependent smoothing scheme that adaptively re-weights the contribution of each example based on its current estimated hardness. By coupling this re-weighting with a time-varying step-size schedule, we derive the first convergence guarantee of stochasticastic curriculum learning under a convex relaxation of deep networks. On ImageNette and three synthetic hard-positive tasks, our method reduces training error by 4\u20137% and yields marginally better calibration. While the gradient bounds remain looser than those available for vanilla SGD on clean datasets, our analysis avoids the strong Lipschitz assumptions typically required in curriculum theory. The results suggest that adaptive smoothing can partially explain curriculum gains, yet scalability to deeper architectures and non-convex landscapes requires additional assumptions. Code is available at https://github.com/nguyent/noisy-curriculum.",
    "id": 266
  },
  {
    "title": "Gradient Alignment in Pre-Training: A Lightweight Alternative to Full Fine-Tuning for Small-Scale Tasks",
    "authors": [
      "Nguyen, T.",
      "Schmidt, L.",
      "Kumar, A."
    ],
    "abstract": "Fine-tuning large pre-trained models on downstream tasks remains computationally prohibitive for practitioners with limited resources. While parameter-efficient methods such as adapters or LoRA reduce the trainable footprint, they still require a non-trivial backward pass through the entire network. We propose Gradient Alignment (GradAlign), a procedure that freezes the pre-trained weights and learns only a single shared scaling vector per layer by aligning the proxy gradients of the scaling parameters to the true fine-tuning gradient directions obtained on a small hold-out set. Theoretically, we show that when the layer-wise Jacobians exhibit near rank-1 structure\u2014a condition we verify empirically on vision and language transformers\u2014GradAlign yields a convex quadratic approximation whose closed-form solution has generalization guarantees under standard kernel regimes. On eight GLUE tasks and CIFAR-10/100 with ImageNet pre-trained ResNets, GradAlign reaches, on average, 96.8 % of full fine-tuning accuracy while updating only 0.04 % of the parameters and requiring 1.3 \u00d7 the memory footprint of inference. Nevertheless, the gains diminish on harder domain-shift benchmarks, and our analysis suggests the rank-1 assumption degrades on deeper architectures. Code and trained vectors are released to ensure reproducibility.",
    "id": 267
  },
  {
    "title": "Revisiting Momentum with Half-Space Projections: A Modest Gain for Smooth Convex Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Ortega, M."
    ],
    "abstract": "We combine Polyak momentum with a lightweight half-space projection step to obtain a slight but consistent speed-up over standard gradient descent on smooth convex objectives. Given access to a stochastic first-order oracle, the proposed method, HopMo, performs a momentum update followed by a projection onto the half-space induced by the most recent gradient direction. This projection requires only vector addition and a dot product, adding negligible cost. On quadratic losses we prove an O(\u03ba^0.87 log(1/\u03b5)) iteration complexity versus O(\u03ba log(1/\u03b5)) for tuned gradient descent; the proof relies on a refined Chebyshev polynomial bound that may be of independent interest. Empirically, HopMo reduces iterations by 5\u201312% on logistic regression and 2\u20134% on shallow auto-encoders without hyper-parameter retuning. Extensions to mini-batch and non-convex settings preserve the same overhead profile. While the gains are small and the analysis is limited to smooth problems, the method is trivial to implement and requires no extra hyper-parameters beyond the usual momentum factor, making it attractive for practitioners seeking drop-in acceleration.",
    "id": 268
  },
  {
    "title": "Boosting Sample Efficiency in Off-Policy RL via Randomized Fourier-Feature Critics",
    "authors": [
      "Kim, J.",
      "Rao, S.",
      "Ortega, L."
    ],
    "abstract": "Deep off-policy reinforcement learning excels in high-dimensional domains, yet sample efficiency remains limited by unstable temporal-difference optimization. We propose RFF-Critic, a simple augmentation that replaces the last hidden layer of the Q-network with random Fourier features. By enforcing a shift-invariant kernel on value-function updates, the critic enjoys reduced variance while retaining the expressiveness of deep networks. On a suite of continuous-control benchmarks, RFF-Critic improves sample efficiency by 18\u201325% over SAC and TD3 baselines at 1M environment steps, and converges to comparable asymptotic returns. Ablations show the gain persists across kernel bandwidths and feature counts, and theoretical analysis reveals a tighter Bellmanman-error bound under mild regularity assumptions. While the method introduces an extra hyper-parameter and modest computational overhead, it can be retrofitted to existing implementations with <10 lines of code. Our results suggest that lightweight kernelization, often overlooked in favor of complex architectural innovations, deserves renewed attention in deep RL. Code is available at anonymized-url.",
    "id": 269
  },
  {
    "title": "Gradient Descent without a Learning Rate: A Self-Tuning Approach via Implicit Regularization",
    "authors": [
      "Nguyen, T.",
      "Kovacs, A.",
      "Zhou, S."
    ],
    "abstract": "Adaptive gradient methods are widely used for training neural networks, yet selecting a good learning rate schedule remains more art than science. We propose a simple modification to standard gradient descent that removes the need for an explicit learning rate. Our key idea is to treat the learning rate as an implicit regularization parameter that is automatically adjusted via a surrogate ridge penalty on the weights. Under convexity assumptions, we show the resulting iterates converge at a rate competitive with optimally tuned SGD, and we bound the excess risk by O(1/\u221aT). On CIFAR-10 and ImageNet subset experiments, our automatic schedule reaches 91% and 74% top-1 accuracy without any hyper-parameter search, matching performance of carefully tuned baselines. However, we observe instability on deeper architectures, motivating a heuristic clipping rule. Theoretically, our analysis relies on assumptions that may be restrictive for modern over-parameterized networks, and empirical gains over strong baselines are modest. Nevertheless, our method offers a practical option for practitioners seeking robustness to learning-rate misspecification, and it opens new questions regarding implicit control of optimization trajectories.",
    "id": 270
  },
  {
    "title": "Gradient Noise Re-scaling: A Lightweight Alternative to Batch Normalization for Small-batch Training",
    "authors": [
      "Chen, L.",
      "Vasudevan, A.",
      "Kumar, S."
    ],
    "abstract": "Batch Normalization (BN) accelerates deep network training but suffers from degraded performance when batch sizes are small, a common scenario in memory-intensive tasks. We propose Gradient Noise Re-scaling (GNR), a plug-in layer that re-scales per-parameter gradient noise online to stabilize training without normalizing activations. GNR maintains running statistics of gradient variances and applies an element-wise multiplicative correction that encourages consistent update magnitudes across layers. On CIFAR-10/100 and ImageNet-1k, GNR matches BN with large batches and outperforms GroupNorm and LayerNorm when batches contain fewer than 8 samples, yielding 0.8\u20131.2% accuracy gains while adding negligible overhead (\u22483% increase in wall-clock time). Although GNR does not explicitly reduce internal covariate shift, ablations show that the adaptive noise correction implicitly regularizes the loss landscape, yielding flatter minima. The method is straightforward to implement and requires no hyper-parameter tuning beyond the learning rate. Limitations include a slight accuracy drop relative to BN on very large-batch training and a modest increase in memory due to gradient statistics buffers. Code is available at anonymous-github.com/gnr.",
    "id": 271
  },
  {
    "title": "Lipschitz-Regularized PCA for Improved Generalization in Noise-Robust Representation Learning",
    "authors": [
      "Chen, Y.",
      "Rao, P.",
      "Nguyen, K."
    ],
    "abstract": "Principal Component Analysis (PCA) remains a cornerstone of unsupervised representation learning, yet its sensitivity to input perturbations and limited robustness to noise are well-documented. In this paper, we revisit classical PCA through the lens of generalization theory and propose a Lipschitz-regularized variant (L-PCA) that explicitly controls the Lipschitz constant of the projection operator. We present an efficient alternating minimization scheme that jointly optimizes the principal subspace and a data-dependent Lipschitz bound, yielding closed-form updates with only 5% overhead over standard PCA. Theoretically, we derive a new generalization bound that trades reconstruction error against expected worst-case perturbation, and demonstrate that L-PCA achieves a tighter trade-off than the vanilla method. Empirically, L-PCA improves test accuracy by 1.2\u20132.1% on downstream linear classification tasks across four vision and tabular benchmarks when 5% label noise is injected. Ablation studies reveal that the regularizer also reduces eigengap overfitting, suggesting improved stability. Although the gains are incremental on clean data, our results indicate that explicit Lipschitz control offers a lightweight, drop-in enhancement for PCA-based pipelines when robustness is desired. Code is available at anonymized-url.",
    "id": 272
  },
  {
    "title": "Revisiting Momentum with Adaptive Restart: A Practical Acceleration Scheme for Stochastic Optimization",
    "authors": [
      "Liu, J.",
      "Kumar, A.",
      "Chen, S."
    ],
    "abstract": "We propose AR-SGD, a simple variant of stochastic gradient descent that adaptively restarts momentum when the loss plateaus. Motivated by the observation that momentum can overshoot near sharp minima in stochastic settings, AR-SGD monitors the angle between consecutive gradient estimates and resets momentum buffers when this angle exceeds a learned threshold. We provide convergence guarantees for quadratic objectives and demonstrate empirically that AR-SGD accelerates training on CIFAR-10 and ImageNet by 1.1-1.3\u00d7 compared to standard momentum SGD with minimal hyperparameter tuning. While our theoretical analysis is limited to convex problems and does not capture the full benefits observed in deep networks, AR-SGD requires only 3 lines of code change to existing optimizers and introduces negligible overhead (<0.5%). Experiments on language modeling show mixed results, suggesting the technique may be less effective when gradient noise is dominated by batch statistics rather than local curvature. Our method offers a practical middle ground between adaptive methods like Adam and classical momentum, though we acknowledge that the improvement margins are modest and may not justify adoption in all settings.",
    "id": 273
  },
  {
    "title": "Revisiting Momentum with Quasi-Hyperbolic Update Schemes for Adaptive Learning Rates",
    "authors": [
      "Kumar, V.",
      "Johnson, T.",
      "M\u00fcller, K."
    ],
    "abstract": "We propose Quasi-Hyperbolic Momentum with Adaptive Restart (QHMAR), a lightweight modification to Adam that averages fast-moving averages only when the gradient signal is noisy. Motivated by convex relaxations of hyperbolic descent, QHMAR introduces a restart-conditioned momentum coefficient that interpolates between Nesterov acceleration and plain momentum on a per-coordinate basis. On CIFAR-10/100 and Penn Treebank our best single model improves Adam\u2019s top-1 accuracy by 0.8 % and reduces perplexity by 0.6, while adding just two lines of code and no extra memory. Ablations show the restart schedule matters more than the hyperbolic correction, and wall-clock speed matches base optimizers. Theoretically, we prove a O(1/T) regret bound for online convex problems that degrades gracefully with gradient noise variance. However, gains shrink on ImageNet and larger language models, and hyper-parameters remain sensitive to batch size. Code is public, yet full ImageNet curves require 8\u00d7 V100 days. Overall, QHMAR is a simple, empirically helpful tweak, but its advantage appears dataset-specific and the restart mechanism lacks clear interpretability, leaving open whether the idea scales or merely decorrelates with existing heuristics.",
    "id": 274
  },
  {
    "title": "Variance-Reduced Q-Learning with Periodic Policy Updates",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Johnson, T."
    ],
    "abstract": "We propose VRQ-Update, a variance-reduced variant of Q-learning that periodically freezes target policies to stabilize off-policy updates. Our method maintains two Q-networks: a fast online network updated every step and a slowly evolving target network updated only when a variance threshold is exceeded. We prove convergence to a neighborhood of the optimal Q-function under standard assumptions, with convergence rate depending on the threshold parameter. On a suite of 6 Gym environments, VRQ-Update shows 12-18% faster convergence compared to standard Q-learning, though gains diminish with longer training. The method introduces two additional hyper-parameters that require environment-specific tuning. While our theoretical analysis assumes finite state spaces, we demonstrate empirical performance in continuous control tasks by combining with neural function approximation. Our approach provides a middle ground between fully offline and fully online target updates, potentially useful for applications with limited hyper-parameter tuning budgets.",
    "id": 275
  },
  {
    "title": "Improved Gradient Estimation for Discrete Variational Autoencoders via Continuous Relaxations with Learned Temperature",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Orozco, J."
    ],
    "abstract": "Training discrete latent variable models remains challenging due to the high variance of gradient estimators. While continuous relaxations such as the Gumbel-softmax trick enable back-propagation, they introduce bias that can impede optimization. We propose Learnable Temperature Relaxation (LTR), a simple modification that adaptively sets the temperature parameter during training via a learned neural network. Our method yields gradients with lower variance while maintaining reasonable bias. We evaluate LTR on structured prediction tasks, including synthetic tree generation and semi-supervised text classification. Experiments show modest improvements over fixed-temperature relaxations, reducing gradient variance by 18% and improving test log-likelihood by 0.6 nats on average across datasets. Theoretically, we provide a bound on the bias introduced by LTR, although the bound is loose for practical temperatures. While LTR does not fully close the gap with more sophisticated score-function estimators, it offers a lightweight drop-in enhancement that requires minimal code changes. Our results suggest that adaptive temperature control can serve as a complementary technique to other variance-reduction strategies, though its benefits diminish when combined with strong control variates.",
    "id": 276
  },
  {
    "title": "Improved Generalization via Layer-wise Learning-rate Decay for Fine-tuning Pre-trained Transformers",
    "authors": [
      "Nguyen, T.",
      "Gupta, V.",
      "Brown, S."
    ],
    "abstract": "Fine-tuning large pre-trained models has become the dominant paradigm in NLP, yet the optimization process remains poorly understood. We observe that different layers converge at heterogeneous rates, leading to sub-optimal final performance. To address this, we propose Layer-wise Learning-rate Decay (LLRD), a simple schedule that assigns monotonically decreasing learning rates to deeper layers. LLRD requires no additional hyper-parameters beyond standard baselines and incurs negligible overhead. On GLUE, our method improves average dev-set accuracy by 0.8% over AdamW with a standard linear decay schedule. Ablations on 5 BERT-sized architectures show consistent but modest gains; however, improvements vanish when training exceeds 5 epochs or when stronger regularization (e.g., 0.3 dropout) is applied. Theoretical analysis reveals that LLRD acts as an implicit \u21132 penalty on deeper parameters, marginally reducing Rademacher complexity. While the contribution is incremental and the scope limited to classification tasks with \u2264 250k training examples, LLRD is practical, easy to implement, and may complement other fine-tuning tricks. Code and configs are provided for reproducibility.",
    "id": 277
  },
  {
    "title": "Improved Memory-Efficient Training via Blockwise Gradient Approximation with Momentum Recycling",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Martinez, D."
    ],
    "abstract": "Training large neural networks under memory constraints is increasingly important for democratizing deep learning, but existing memory-efficient methods either compromise accuracy or impose prohibitive computational overhead. We propose MemBA, a memory-efficient optimizer that approximates gradients using blockwise updates with momentum recycling. Our key insight is that historical momentum terms can be adaptively reused across parameter blocks to reduce memory footprint without storing full gradient history. Compared to gradient checkpointing, MemBA reduces activation memory by 42% while adding only 15% training time overhead on CIFAR-10 and ImageNet. On GPT-2 medium, we achieve comparable perplexity to standard training with 30% less memory. However, we observe a 1.2% accuracy drop on ImageNet and increased hyperparameter sensitivity compared to baseline methods. Theoretical analysis shows our method converges for smooth convex objectives, though our assumptions may not fully capture modern architectures. Code is available, though hyperparameter tuning details remain incomplete. While our approach offers practical memory savings for resource-constrained settings, the accuracy trade-offs and limited theoretical guarantees suggest MemBA is best suited for scenarios where memory is the primary bottleneck rather than achieving state-of-the-art performance.",
    "id": 278
  },
  {
    "title": "Improved Margin-Based Generalization Bounds via Data-Dependent Priors",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Zhou, J."
    ],
    "abstract": "We revisit margin-based generalization bounds for deep neural networks, aiming to tighten the gap between theoretical guarantees and empirical performance. While existing PAC-Bayesian bounds provide non-vacuous guarantees, they often rely on data-independent priors that can be loose in practice. We propose a family of data-dependent priors constructed via a two-stage training procedure: first, we train a reference network on a subset of data to obtain a prior distribution; second, we train the final network on the remaining data and compute margin-based bounds using our data-dependent prior. Our theoretical analysis shows that this approach can yield improvements of up to \u221an in the generalization bound under certain conditions. Empirically, we evaluate our bounds on small-scale vision tasks (CIFAR-10, CIFAR-100) and demonstrate improvements over existing PAC-Bayesian bounds by a factor of 2-3\u00d7 while remaining computationally tractable. However, our bounds still exhibit a significant gap with test error on larger datasets, and the computational overhead of the two-stage procedure limits scalability. Our work suggests that incorporating data-dependent information into generalization bounds holds promise, though fundamental challenges remain in extending these results to state-of-the-art architectures and large-scale settings.",
    "id": 279
  },
  {
    "title": "Revisiting Dropout as an Uncertainty Proxy: Better Calibration via Multiplicative Gamma Noise",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Monte-Carlo dropout is widely used to obtain epistemic uncertainty estimates from deep networks, yet its approximations are often poorly calibrated. We propose a simple change: replacing the traditional Bernoulli mask with samples from a learned Gamma distribution whose shape parameter is conditioned on the layer\u2019s activations. This multiplicative noise generalizes dropout and, under modest assumptions, yields a closed-form KL regularizer that encourages smooth predictive distributions. On CIFAR-10/100 and ImageNet subsets, a ResNet-50 equipped with our GammaDrop matches standard MC dropout accuracy while reducing expected calibration error by 8\u201311%. Ablations show that 78% of the gain comes from the shape adaptation, not heavier tails. Theoretically, we relate the Gamma shape to the PAC-Bayesian margin, giving a calibration bound that scales as O(1/\u221ashape). Because Gamma noise is reparameterizable, the modification adds <2% training overhead and no test-time sampling. While improvements are consistent, they remain moderate on large-scale data, and our bound is looser when the label noise is non-uniform. Code and pretrained weights are provided for reproducibility.",
    "id": 280
  },
  {
    "title": "Revisiting Weight Averaging with Layer-wise Learning Rate Scaling for Fine-tuning Transformers",
    "authors": [
      "Chen, K.",
      "Rodriguez, J.",
      "Singh, A."
    ],
    "abstract": "Weight averaging has emerged as a simple yet effective technique for improving generalization in neural network training. In this work, we investigate whether layer-specific learning rate schedules can enhance the benefits of weight averaging when fine-tuning pre-trained language models. We propose Layer-wise Averaged Weight Updates (LAWU), a method that applies different learning rate scaling factors to different layers during fine-tuning, combined with exponential moving average (EMA) of weights. Our approach is motivated by the observation that lower layers in pre-trained models require smaller updates compared to higher layers. We evaluate LAWU on GLUE benchmarks using BERT-base and RoBERTa-base models. Experimental results show that LAWU achieves an average GLUE score of 82.3, compared to 81.8 for standard EMA and 81.5 for vanilla fine-tuning. While the improvements are modest, we find that LAWU provides more stable training and slightly better calibration on out-of-distribution data. However, our method requires additional hyperparameter tuning for the layer-wise scaling factors, and the computational overhead increases by approximately 15%. Our code is available at [anonymous link].",
    "id": 281
  },
  {
    "title": "Gradient Noise Revisited: Rethinking Large-Batch Scaling with Modified Learning Rate Schedules",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Orozco, J."
    ],
    "abstract": "The common belief that large-batch training degrades generalization is often blamed on the lack of gradient noise. We revisit this explanation and argue that the dominant factor is actually the scheduling of the optimizer. By re-parametrizing Adam with a batch-size-scaled quadratic warm-up and a polynomial decay, we recover competitive test accuracies on CIFAR-10 and ImageNet with batches up to 8\u00d7 larger than the baseline. Our method, which introduces no new hyper-parameters beyond the standard schedule, yields a 1.34\u00d7 wall-clock speed-up on 8 GPUs compared to the small-batch reference. While our theoretical analysis (a stability bound on the stationary distribution of SGD under relaxed assumptions) provides insights consistent with the empirical trend, it only applies to quadratic loss surfaces and does not capture the sharpness/curvature dynamics observed in very deep networks. Experiments are fully reproducible with 150 lines of PyTorch and require \u2264 16 GPU-hours. Although the practical utility is limited to data-center scale training and the gains saturate beyond 8\u00d7 batch size, our work challenges the prevailing narrative about gradient noise and offers practitioners a simple, optimizer-level fix that partially mitigates the accuracy drop of large-batch training.",
    "id": 282
  },
  {
    "title": "Improving Gradient Alignment in Self-Supervised Vision Transformers via Partially Shared Projectors",
    "authors": [
      "Kim, J.",
      "Rodriguez, M.",
      "Singh, P.",
      "Liu, S."
    ],
    "abstract": "Self-supervised learning with Vision Transformers (ViTs) has achieved impressive performance on visual representation tasks, but suffers from gradient conflicts between the contrastive loss and the patch-level consistency regularizer. We propose PartialShare, a simple architectural modification that shares the first half of the projector MLP across branches while keeping the remaining layers branch-specific. Our theoretical analysis suggests that sharing the lower projector layers reduces the angle between task gradients by approximately 25%, leading to faster convergence. When combined with existing self-supervised frameworks such as SimCLR and BYOL, PartialShare improves ImageNet linear evaluation accuracy by 0.9-1.3% on ViT-B/16 models with no additional parameters or compute. Extensive ablations show the first-half sharing pattern consistently outperforms alternatives, though gains diminish for smaller ViT variants. While the improvement is incremental and our analysis is limited to vision tasks, PartialShare can be implemented in a few lines of code and integrates seamlessly with existing frameworks. Code and pretrained weights are provided for reproducibility.",
    "id": 283
  },
  {
    "title": "Improved Gradient Bounds for Unrolling with Learned Optimization Step Sizes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient-based meta-learning algorithms often rely on unrolling optimization procedures to compute higher-order derivatives, but this can lead to exploding or vanishing gradients when many steps are used. We propose a simple modification to existing unrolled optimizers where the step sizes are learned as parameterized functions of the current gradient history, allowing the optimizer to adaptively shrink gradients in unstable regions while preserving informative updates elsewhere. Our approach only adds 5% overhead in compute time compared to standard unrolling and can be implemented in 20 lines of PyTorch code. Experiments on few-shot image classification benchmarks show modest improvements of 1-2% over baseline unrolling methods on 5-way 5-shot tasks, with more pronounced gains in 20-shot settings. We provide theoretical analysis showing that our learned step sizes effectively bound gradient norms under standard smoothness assumptions, though our bounds are looser than recent work that uses carefully engineered clipping schemes. While our method does not achieve state-of-the-art performance compared to more sophisticated meta-learning approaches, it offers a practical drop-in replacement for unrolling that improves stability without requiring extensive hyperparameter tuning. Code is available at anonymized-url.github.io.",
    "id": 284
  },
  {
    "title": "Memory-Efficient Training of Transformers via Gradient Checkpointing with Learned Activation Recomputation",
    "authors": [
      "Chen, K.",
      "Alvarez, M.",
      "Singh, P."
    ],
    "abstract": "We present a method for reducing the memory footprint of transformer training while maintaining computational efficiency. Our approach combines gradient checkpointing with a learned recomputation policy that selectively caches intermediate activations based on their estimated recomputation cost. The key insight is that certain activation tensors in transformer blocks exhibit predictable structure, allowing accurate approximation during the backward pass. We train a lightweight neural network to predict which activations to cache, minimizing a cost function that balances memory savings against extra computation. Experiments on language modeling and machine translation tasks show 25-40% memory reduction with <15% training slowdown compared to standard training. While our method underperforms the strongest baseline system (activation compression) in some settings, it requires no architectural modifications and generalizes across model sizes. Code is provided, though full reproduction of large-scale experiments requires substantial compute resources. Our work suggests that learned policies can improve memory efficiency, though the overhead may limit benefits for smaller models.",
    "id": 285
  },
  {
    "title": "Adaptive Gradient Noise Injection for Improved Generalization in Deep Neural Networks",
    "authors": [
      "Kumar, V.",
      "Chen, S.",
      "Murphy, J."
    ],
    "abstract": "We propose Adaptive Gradient Noise Injection (AGNI), a simple yet effective method for improving generalization in deep neural networks by adding carefully calibrated noise to gradient updates during training. Unlike standard noise injection techniques that apply fixed noise schedules, AGNI dynamically adjusts noise magnitude based on the gradient history and current loss landscape curvature. Our method requires minimal hyperparameter tuning and can be easily integrated into existing optimizers with only a few lines of code. We provide theoretical analysis showing that AGNI acts as an implicit regularizer, biasing optimization toward flatter minima. Experiments on CIFAR-10/100 and ImageNet show modest improvements of 0.3-0.8% over strong baselines, with more pronounced gains on smaller datasets. While our empirical improvements are incremental and the theoretical analysis relies on several assumptions, we believe AGNI offers a practical and computationally cheap way to boost performance. Code will be made publicly available upon acceptance.",
    "id": 286
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum-Based Curvature Estimates",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Adaptive Gradient Clipping with Momentum-based Curvature Estimates (AGCM), a simple modification to existing adaptive optimizers that adjusts per-parameter clipping thresholds using exponential moving averages of gradient curvature. While gradient clipping is widely used to stabilize training of large neural networks, the choice of clipping threshold often requires extensive tuning and remains fixed throughout training. AGCM automatically adjusts clipping thresholds by estimating local smoothness constants via gradient variations, requiring no additional hyperparameters beyond standard optimizers. We provide theoretical analysis showing AGCM achieves comparable convergence rates to clipped SGD for non-convex problems while adapting to local Lipschitz constants. Empirically, AGCM shows modest improvements over tuned baselines on language modeling tasks, achieving 2-3% better perplexity on Wikitext-103 when training Transformer-XL models. However, gains are less consistent on vision tasks, with improvements of 0.5-1% on ImageNet. While AGCM provides some automation benefit, the computational overhead of 15% and limited performance gains on well-tuned baselines raise questions about its practical utility. Our code is available at [anonymized link].",
    "id": 287
  },
  {
    "title": "Gradient Alignment for Improved Generalization in Over-parameterized Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Gradient Alignment (GradAlign), a simple regularizer that encourages the gradients of individual training examples to align with the average gradient direction during mini-batch training. Motivated by recent observations that poorly-generalizing solutions often exhibit high gradient variance across data points, we hypothesize that gradient alignment can serve as a proxy for flatness in the loss landscape and thus improve generalization. GradAlign adds minimal computational overhead and can be integrated into any gradient-based optimizer. We provide theoretical analysis showing that gradient alignment reduces the trace of the Hessian in a simplified two-layer linear model. Empirically, we demonstrate improvements over standard training on CIFAR-10 and CIFAR-100 with ResNet-18 and VGG-16, achieving 0.8-1.2% accuracy gains with minimal hyperparameter tuning. However, our experiments on ImageNet show more modest improvements (0.3% top-1 accuracy), and we find that GradAlign's benefits diminish with stronger baseline regularization techniques such as Mixup or CutMix. While our method is straightforward to implement and shows consistent gains in specific settings, we acknowledge that the theoretical connection between gradient alignment and generalization remains incomplete, particularly for deep non-linear networks. Code is available at [anonymized].",
    "id": 288
  },
  {
    "title": "Improving Gradient Alignment in Federated Learning via Cyclical Server Momentum",
    "authors": [
      "Kostina, I.",
      "Garcia, J.",
      "Zhao, T."
    ],
    "abstract": "Federated learning systems often suffer from client drift when data are heterogeneous. We propose Cyclical Server Momentum (CSM), a simple extension to FedAvg that modulates the server optimizer\u2019s momentum coefficient along a sinusoidal schedule. Intuitively, low-momentum phases tolerate conflicting client updates, while high-momentum phases rapidly push the global model toward currently aligned gradients. On CIFAR-10 partitioned by Dirichlet(0.1) among 100 clients, CSM improves absolute accuracy by 1.3 pp over momentum-free FedAvg and 0.7 pp over the best constant-momentum baseline, using the same number of communication rounds. Linear-probe CKA similarity between central and client models rises by 8%. Theorem 1 bounds the one-round drift under CSM, yielding an O(1/\u221aT) convergence guarantee in the convex setting, matching vanilla FedAvg up to constants. Ablation shows the schedule\u2019s amplitude matters more than its period, and that benefits vanish when client data are IID. While gains are incremental and limited to small-scale vision benchmarks, CSM is plug-and-play, adds no client memory, and can be combined with compression or differential privacy. Code is provided to reproduce experiments with under 24 GPU-hours on a single RTX-2080.",
    "id": 289
  },
  {
    "title": "Momentum Without Memory: A Streaming Variational Approach to Adaptive Gradient Descent",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Ortega, J."
    ],
    "abstract": "Adaptive optimizers like Adam and RMSProp maintain running second-moment estimates that grow unbounded in streaming or lifelong-learning settings. We propose Streaming Variational Momentum (SVM), a family of update rules that cast momentum estimation as an online variational inference problem over a latent drift process. By placing a conjugate inverse-Gamma prior on unknown squared gradients, we obtain closed-form recursive updates that (i) adapt the effective momentum coefficient to local gradient noise, (ii) provably converge in convex landscapes at the same O(1/T) rate as SGD, and (iii) require only constant memory. On CIFAR-10/100 and ImageNet transfer tasks our PyTorch implementation matches Adam\u2019s final accuracy while using 30\u201345% fewer bits per parameter. Theoretically, we bound regret in the online convex case and characterize the bias\u2013variance trade-off introduced by the prior. While SVM does not improve upon strong baselines on large-scale language modeling, it eliminates the need for gradient clipping and halves catastrophic forgetting in a class-incremental setup. Code is available at anon-url.github.io/svm-optimizer.",
    "id": 290
  },
  {
    "title": "Gradient Descent with Lookahead Meets Online Mirror Descent: A Modular Analysis",
    "authors": [
      "Chen, L.",
      "Krishnan, V.",
      "Gomez, S."
    ],
    "abstract": "We reinterpret the recently proposed Lookahead optimizer as a two-time-scale online learning procedure, and use this lens to derive convergence guarantees for smooth non-convex objectives. By casting the fast inner loop as an online mirror descent player that best-responds to a slow-moving predictor, we obtain a regret bound of order O(\u221aT) that translates, via standard online-to-batch conversion, to a 1/\u221aT stationarity rate. We then show that inserting a fixed, constant-length lookahead window inside Nesterov-type acceleration preserves this rate while reducing the provable dependence on from 1/(1-\u03b2) to 1/(1-\u03b2\u00b2). Experiments on CIFAR-10/100 and small-scale ImageNet reveal speed-ups of 3\u20136% over vanilla momentum and 1.5% over AdamW, with comparable final accuracy. While the gains are modest, our framework unifies several existing acceleration tricks under a single regret bound and offers a simple Python wrapper that works with any base optimizer. We discuss limitations\u2014including increased memory footprint and sensitivity to window size\u2014and provide full source code and hyper-parameter sweeps to encourage reproducibility.",
    "id": 291
  },
  {
    "title": "Improved Convergence of Gradient Descent via Periodic Learning Rate Re-winding",
    "authors": [
      "Kovacs, P.",
      "Singh, V.",
      "Okafor, C."
    ],
    "abstract": "We propose Learning Rate Re-winding (LRR), a simple modification to standard gradient descent that periodically resets the learning rate to an earlier value before continuing its scheduled decay. Inspired by cyclical learning rate methods and recent observations on the benefits of parameter re-initialization, LRR aims to escape sharp local minima by repeatedly injecting gradient noise. On CIFAR-10/100 and ImageNet, LRR yields consistent but modest improvements (0.2\u20130.4%) over cosine annealing when training ResNet-18/50 from scratch; similar gains are observed for WideResNet and DenseNet architectures. A theoretical analysis in the strongly-convex quadratic setting shows that LRR converges to a neighborhood of the optimum at a rate no worse than standard SGD, yet can reduce stationary variance by up to 12%. Ablation studies indicate that the reset frequency matters more than the exact restart value, and that benefits diminish when training is already well-tuned with warm restarts or heavy data augmentation. While LRR is easy to implement and requires no additional hyper-parameters beyond the reset schedule, the gains are incremental and limited to medium-scale vision tasks; we observed no improvement on large-scale language modeling or reinforcement learning benchmarks. Code and pre-trained models are provided to ensure reproducibility.",
    "id": 292
  },
  {
    "title": "Improving Transformer Generalization with Cyclical Annealed Label Smoothing",
    "authors": [
      "Chen, L.",
      "Subramanian, K.",
      "Gomez, A."
    ],
    "abstract": "Label smoothing is widely used to prevent overconfidence in neural networks, yet its benefits remain inconsistent across tasks and model scales. We propose Cyclical Annealed Label Smoothing (CALS), a simple schedule that varies the smoothing factor during training, inspired by cyclical learning rate methods. Our approach periodically transitions between high and low label confidence, allowing the model to explore and then refine its predictions. We evaluate CALS on image classification, machine translation, and speech recognition, showing modest but consistent improvements over standard label smoothing (average +0.3% accuracy/ImageNet, +0.4 BLEU/WMT'14). Theoretical analysis suggests CALS acts as a form of implicit regularization, reducing the kernel norm bound in certain linearized models. While our gains are smaller than recent architectural advances, CALS incurs no extra parameters or inference cost, making it attractive for practitioners. Extensive ablations reveal sensitivity to cycle length and peak smoothing values, indicating that optimal schedules are task-dependent. Code is available at anonymized-repo.github.io/CALS.",
    "id": 293
  },
  {
    "title": "LoRa-Drop: Structured Sparsification of Low Swiss-Army Knife",
    "authors": [
      "Kulkarni, S.",
      "Chen, L.",
      "Rodriguez, J.",
      "Nguyen, T."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods such as LoRA have become the default for adapting large language models, yet they still impose non-ne toolbox for multi-task deployments. We propose LoRa-Drop, a simple structured-sparsity wrapper that randomly deactivates entire LoRA modules during training. By treating each low-cube as a Bernoulli random mask, we obtain a single base model whose residual branches can be selectively re-activated at inference time, effectively yielding an implicit ensemble of specialized sub-models without additional storage. On GLUE and SuperGLUE, LoRa-Drop matches the average score of independently trained LoRA adapters while cutting the trainable footprint by 60%. Ablations show that sparsity schedules, initialization heuristics, and a lightweight importance-based sampling strategy are all critical to prevent collapse. Although our empirical gains are restricted to classification tasks under 3B parameters and we rely on the existing transformer architecture without theoretical guarantees, LoRa-Drop offers a practically appealing compromise between the modularity of full adapters and the compactness of prompt tuning. Code is available at anonymized-link.",
    "id": 294
  },
  {
    "title": "Improving Transformer Generalization with Gradient-Norm Warm-Start Regularization",
    "authors": [
      "Nguyen, T.",
      "Kumar, S.",
      "Jones, L."
    ],
    "abstract": "While Transformers dominate many sequence-modeling benchmarks, their capacity to overfit small and mid-sized datasets remains a practical concern. We propose Gradient-Norm Warm-Start Regularization (GNWR), a simple yet effective variant of weight decay that modulates the regularization strength for each parameter on the basis of its historical gradient norm. GNWR begins training with near-zero penalty, then gradually recovers standard decay, allowing the optimizer to select which weights should be strongly regularized. On five text-classification and two image-patch tasks (\u2264250k training examples), GNWR yields 1.3% average accuracy improvement over vanilla weight decay and 0.7% over the recent SAM optimizer, while requiring no additional hyper-parameters beyond the base decay factor. Theoretically, we show that GNWR converges to a stationary equivalent empirical risk minimizer in the convex setting, although our bound depends on a weight trajectory assumption that may not hold for deep networks. Ablation studies suggest that the warm-start period is the primary driver of gains, with diminishing returns when extended beyond the first 30% of training. Code and pretrained checkpoints are provided for reproduction.",
    "id": 295
  },
  {
    "title": "LoCo-ViT: Low-Rank Compression of Vision Transformers with Learnable Token Routing",
    "authors": [
      "Chen, L.",
      "Vaswani, A.",
      "M\u00fcller, H."
    ],
    "abstract": "Vision Transformers (ViTs) deliver impressive accuracy yet remain bottlenecked by quadratic self-attention complexity and large parameter counts. We propose LoCo-ViT, a two-stage compression pipeline that jointly learns (i) low-rank projections of attention weights and (ii) a lightweight routing module that skips entire blocks for uninformative tokens. Starting from a pre-trained ViT, we interleave each attention layer with rank-r bottleneck projections and train a differentiable gating network to drop up to 30 % of tokens per block while maintaining task-specific rewards. On ImageNet-1k, LoCo-ViT yields 1.8\u00d7 FLOPs reduction and 2.3\u00d7 throughput gain versus the base DeiT-Small with only 0.7 % top-1 accuracy loss. Ablations show that low-rank constraints account for two-thirds of the savings, while learned routing provides the rest. Although our method scales to larger models, gains saturate beyond 224\u00d7224 inputs, and downstream transfer to COCO detection underperforms prior static pruning baselines by 1.1 mAP. Nevertheless, LoCo-ViT offers a simple, training-efficient recipe for on-device deployment of vision transformers when extreme compression is more critical than final accuracy.",
    "id": 296
  },
  {
    "title": "Revisiting Regularization Paths with Adaptive Shrinkage for Over-parameterized Models",
    "authors": [
      "Kim, H.",
      "Rojas, C.",
      "Singh, V."
    ],
    "abstract": "We study the trajectory of iterative weight decay in over-parameterized networks and propose Adaptive Shrinkage (\u03b1-shrink), a simple modification that rescales the penalty by the current parameter norm. Empirically, \u03b1-shrink accelerates early-phase convergence and yields 1\u20132% test-error reductions on CIFAR-10/100 and ImageNet compared to standard weight decay when training ResNet and Vision Transformer families. Theoretically, we show that \u03b1-shrink is equivalent to a time-varying L2 penalty whose limit coincides with ridge regression in a tractable two-layer linear network; the derived closed-form solution predicts when adaptive shrinkage outperforms fixed regularization. Ablation studies reveal most benefits appear when labels are noisy or when data are few, suggesting \u03b1-shrink operates as a principled form of robust regularization. Code is available at anonymized-link.",
    "id": 297
  },
  {
    "title": "Improving Transformer Generalization with Block-Sparse Re-Attention",
    "authors": [
      "Kumar, S.",
      "Okafor, C.",
      "Nguyen, T."
    ],
    "abstract": "Recent studies suggest that the quadratic attention bottleneck in Transformers can be relaxed without sacrificing accuracy by introducing learned sparsity patterns. We propose Block-Sparse Re-Attention (BSRA), a drop-in replacement for softmax attention that learns to attend over fixed, non-overlapping blocks of the query-key matrix. BSRA couples a lightweight routing network\u2014trained with straight-through Gumbel estimation\u2014to select active blocks, with a re-attention step that allows selected blocks to share information through a low-rank adapter. On WikiText-103 and C4 language modeling, a 340M-parameter BSRA Transformer matches the perplexity of a dense 350M-parameter baseline while using 37% fewer FLOPs. We further demonstrate 0.8 BLEU improvement on the WMT\u201914 English-German task at the same compute budget. Although wall-clock speed-ups are currently limited by our CUDA kernel implementation, ablations indicate that the sparsity pattern generalizes across lengths up to 4\u00d7 the training sequence, suggesting a path to efficient long-context modeling. Code and checkpoints are available.",
    "id": 298
  },
  {
    "title": "Improved Generalization via Gradient Norm Clipping with Scheduled Thresholds",
    "authors": [
      "Liu, J.",
      "Martinez, C.",
      "Vaser, R."
    ],
    "abstract": "We revisit gradient-norm clipping as a simple yet under-explored technique for enhancing generalization in over-parameterized networks. Motivated by recent empirical observations that stochastic-gradient noise correlates with the relative change in test error, we propose an adaptive clipping schedule that monotonically reduces the clipping threshold during training. Using a synthetic noisy-quadratic benchmark, we demonstrate that the schedule can recover the generalization benefits of full-batch gradient descent while retaining the optimization speed of mini-batch SGD. On CIFAR-10/100 and ImageNet-sub, our method matches the accuracy of strong data-augmentation baselines with 20\u201335% fewer parameter updates, ablating competitively against a well-tuned cosine-learning-rate schedule. Theoretical insight is provided via a stability bound that depends on the cumulative clipped gradient norm. Although our bound does not yet explain the full empirical gains, we believe the results highlight scheduled clipping as a lightweight alternative to more complex optimizers. Code is available at anon-not-provailed.github.io/adaclip.",
    "id": 299
  },
  {
    "title": "Revisiting Entropy Regularization in Policy Gradient with Mirror Descent",
    "authors": [
      "Liu, Q.",
      "Kumar, A.",
      "Chen, B.",
      "Ozturkler, B."
    ],
    "abstract": "Entropy regularization is widely used in policy gradient algorithms to encourage exploration and prevent premature convergence. We present a novel view in which standard entropy-penalized policy iteration is re-interpreted as an approximate mirror-descent step on the Bregman divergence induced by the negative entropy. Leveraging this perspective, we propose clipped-entropy mirror descent (CEMD), an algorithm that dynamically adjusts the regularization weight using a simple, online heuristic. While the theory we develop applies only to tabular MDPs, we test CEMD on several continuous-control tasks in MuJoCo and observe modest gains (\u22485% median) in sample efficiency over PPO and SAC when hyper-parameters are tuned for the latter. Ablation studies indicate that the heuristic itself, rather than the mirror-descent re-framing, accounts for most improvement. We further prove convergence to a near-optimal policy with O(1/\u221aT) rate, matching known bounds for entropy-regularized algorithms, but under the assumption of exact gradients and uniform exploration\u2014conditions unattainable in complex domains. Finally, we open-source a compact JAX implementation to facilitate reproduction. Our work offers a complementary understanding of entropy regularization, but the empirical benefits remain incremental and the theoretical guarantees are limited to idealized settings.",
    "id": 300
  },
  {
    "title": "Residual Connection Dropout: When Skip Connections Hurt More Than Help",
    "authors": [
      "Liu, Q.",
      "Muller, S.",
      "Santos, J."
    ],
    "abstract": "Residual connections have become ubiquitous in deep learning architectures, yet their interaction with modern regularization techniques remains poorly understood. We identify a common failure mode where residual connections amplify gradient noise when combined with aggressive dropout in very deep networks (>100 layers). Through theoretical analysis of residual networks as dynamical systems, we show that high dropout rates can destabilize the identity mapping, leading to exploding gradients. We propose Residual Connection Dropout (RCD), a simple modification that selectively applies dropout to residual branches based on layer depth, with probability increasing from 0.1 to 0.5 across 50-150 layers. On CIFAR-10/100 and ImageNet, RCD improves test accuracy by 0.8-1.2% for 100+ layer networks without increasing training time. While our findings are specific to extremely deep architectures with heavy dropout, they provide insight into the brittle interaction between architectural priors and regularization. Experiments on smaller networks (\u226450 layers) show no consistent benefit, suggesting minimal impact on typical use cases.",
    "id": 301
  },
  {
    "title": "Gradient Descent with Variance-Dependent Step-Sizes for Non-Convex Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, K.",
      "Kim, S."
    ],
    "abstract": "Adaptive step-size selection remains a central challenge in non-convex optimization, with existing approaches like Adam and RMSprop relying primarily on gradient magnitudes. We propose Variance-Adapted Gradient Descent (VAGD), which incorporates per-coordinate gradient variance estimates to dynamically adjust step-sizes. Theoretically, we prove that VAGD achieves O(1/\u221aT) convergence for smooth non-convex objectives when variance is bounded, matching standard SGD rates. Empirically, we evaluate VAGD on CIFAR-10/100 image classification and Penn Treebank language modeling. Results show 3-5% improvements over tuned SGD baselines in final accuracy, though gains diminish with extensive hyperparameter tuning. Ablation studies reveal that variance estimation contributes most benefits early in training, while gradient clipping becomes dominant later. While our theoretical analysis assumes bounded variance and Lipschitz gradients\u2014restrictions not fully met in practice\u2014the simplicity and minimal overhead of VAGD make it practical for large-scale training. Limitations include sensitivity to batch size selection and marginal improvements over well-tuned baselines. Code will be released upon acceptance.",
    "id": 302
  },
  {
    "title": "Gradient Alignment for Improved Transfer Learning in Limited-Label Settings",
    "authors": [
      "Kumar, S.",
      "Chen, L.",
      "Rosenfeld, A."
    ],
    "abstract": "Transfer learning has achieved remarkable success across domains, yet its effectiveness diminishes when only a handful of target labels are available. We propose Gradient Alignment Transfer (GAT), a lightweight method that reweights source-domain gradients during fine-tuning to better match the geometry of the target loss landscape. GAT computes gradient similarity scores via an efficient approximation to the Fisher Information Matrix, requiring no additional forward passes. On five standard vision benchmarks with 5\u201320 labels per class, GAT improves average accuracy by 2.3% over standard linear probing and 1.1% over full fine-tuning, while adding under 5% computational overhead. Although the gains are modest and tasks are limited to small-scale vision datasets, GAT introduces no new parameters and can be dropped into existing pipelines. Extensive ablations reveal that alignment toward the final layers matters most, and that the method degrades gracefully when source and target domains are poorly matched. Our code and checkpoints are publicly available.",
    "id": 303
  },
  {
    "title": "Revisiting Momentum Schedules for Practical SGD: Smooth Warmup Improves Both Stability and Final Accuracy",
    "authors": [
      "Morales, L.",
      "Chaudhary, A.",
      "Kim, S."
    ],
    "abstract": "Stochastic gradient descent with momentum remains the de-facto optimizer for large-scale deep learning, yet the community has not converged on a principled way to schedule the momentum coefficient during training. We propose SWARM, a Simple WArmup-inspired momentum Rescheduling Method that linearly increases momentum from 0.5 to 0.9 over the first five epochs while simultaneously decaying the learning rate. Our experiments on CIFAR-10/100, ImageNet-1k and a subset of GLUE tasks show that SWARM yields a 0.2\u20130.4 % absolute improvement over the standard constant-momentum baseline with no extra hyper-parameters and negligible overhead. Ablations reveal that smoothing the momentum transition reduces early-training gradient noise, which correlates with better generalization. While the gains are consistent across architectures, they diminish on problems where the baseline already trains for hundreds of epochs, suggesting limited scalability. Theoretical justification is limited: we bound the extra regret compared to constant momentum in convex quadratic settings, but the bound does not clearly predict the empirical improvements. Code is provided for reproducibility.",
    "id": 304
  },
  {
    "title": "Gradient Surgery with Adaptive Dropout for Improved Generalization in Over-parameterized Networks",
    "authors": [
      "Kumar, S.",
      "Nguyen, T.",
      "Anderson, J."
    ],
    "abstract": "We propose GradDrop, a simple yet effective regularization technique that combines gradient pruning with adaptive dropout to improve generalization in over-parameterized neural networks. Our method identifies and temporarily drops gradient components that exhibit high conflict with the general gradient direction during training, while simultaneously adjusting dropout rates based on layer-wise gradient statistics. This dual mechanism encourages the network to learn more robust representations by preventing over-reliance on specific pathways. We evaluate GradDrop on image classification benchmarks including CIFAR-10/100 and ImageNet subsets, achieving 1-2% accuracy improvements over baseline SGD with standard dropout. Theoretical analysis suggests our method reduces sharpness of minima, though we acknowledge our proofs currently hold only for simplified two-layer networks. While experiments show consistent gains across architectures, the improvements are modest and diminish with heavy data augmentation. Code and pretrained models are available, though hyperparameter sensitivity requires careful tuning for each dataset. This work provides a complementary regularization approach that integrates seamlessly with existing techniques, offering practitioners another tool for improving generalization, though we recognize the contribution is incremental rather than transformative.",
    "id": 305
  },
  {
    "title": "Adaptive Gradient Clipping with Curvature-Aware Warm-up for Transformer Training",
    "authors": [
      "Chen, L.",
      "Vasileva, N.",
      "Kumar, V."
    ],
    "abstract": "Transformer models often suffer from unstable training dynamics, particularly in the early phases. We propose Adaptive Gradient Clipping with Curvature-Aware Warm-up (AGC-CAW), a simple modification to standard training procedures that adjusts gradient clipping thresholds based on local curvature estimates and training iteration. Our method computes diagonal Hessian approximations using a lightweight Hutchinson estimator and combines these estimates with a monotonically decreasing clipping schedule. AGC-CAW requires no additional hyperparameters beyond those already present in standard optimizers. We validate our approach on language modeling (Wikitext-103) and machine translation (WMT16 EN-DE), showing 2-3% improvements in perplexity and 0.1-0.2 BLEU score gains over baseline configurations. While the improvements are consistent, they remain modest and appear to diminish with larger models (>1B parameters). Theoretical analysis demonstrates convergence guarantees under standard assumptions, though these results rely on Lipschitz continuity conditions that may not hold for deep transformer architectures. Our implementation adds ~15% training overhead compared to standard AdamW. We provide open-source code and configuration files to ensure reproducibility.",
    "id": 306
  },
  {
    "title": "Amortized Learning Rate Scheduling via Meta-Gradient Descent",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Chen, L."
    ],
    "abstract": "Learning rate scheduling is critical for training neural networks, yet most approaches rely on hand-tuned schedules or domain-specific heuristics. We propose Meta-LR, a gradient-based method to learn adaptive learning rate schedules for SGD without manual intervention. Our approach treats the learning rate as a function of training statistics, represented by a small neural network whose parameters are optimized through meta-gradients computed on a validation set. Unlike full-blown meta-learning methods, Meta-LR requires only a single meta-update per batch, making it computationally efficient. We evaluate on CIFAR-10/100 and ImageNet training, demonstrating 0.5-1.2% accuracy improvements over cosine decay baselines with similar compute budget. However, gains are inconsistent across architectures and datasets, with negative results on language modeling tasks. We provide theoretical analysis showing Meta-LR converges to stationary points under restricted assumptions. While not a universal solution, our method offers a practical compromise between manual tuning and expensive meta-learning approaches.",
    "id": 307
  },
  {
    "title": "Gradient Surgery with Momentum: A Simple Fix for Multi-Task Learning in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that can impede optimization. While recent works have proposed sophisticated gradient manipulation techniques, we show that a simple modification to standard SGD with momentum can achieve comparable performance. Our method, Momentum-Adjusted Gradient Surgery (MAGS), applies a lightweight projection step that leverages momentum history to resolve gradient conflicts without requiring explicit gradient decomposition. We evaluate MAGS on three standard multi-task benchmarks: NYUv2 semantic segmentation/depth estimation, PASCAL face landmark detection/age estimation, and SURREAL human pose estimation. Results show MAGS achieves competitive performance (\u00b11.2% of state-of-the-art) while reducing computational overhead by 37% compared to PCGrad and 52% compared to CAGrad. However, we observe that MAGS struggles in settings with high task imbalance and provides diminishing returns for tasks with aligned gradients. Our extensive ablations reveal that momentum history primarily helps in early training, with benefits tapering off after 25-30 epochs. While our solution is simple and effective for moderate-scale multi-task problems, we acknowledge it may not address fundamental limitations of gradient-based multi-task optimization. We provide PyTorch code for reproducibility.",
    "id": 308
  },
  {
    "title": "Gradient Surgery with Memory: Mitigating Gradient Conflicts in Multi-Task Learning via Differentiable Gradient Filtering",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Chen, A."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts between tasks, leading to degraded performance compared to single-task baselines. While recent gradient surgery methods like PCGrad project conflicting gradients onto non-interfering subspaces, they fundamentally lose information by dropping gradient components. We propose Gradient Surgery with Memory (GSM), which instead uses a learnable filtering mechanism to softly downweight conflicting gradient directions while preserving useful information. Our method introduces a small meta-network that takes task gradients as input and outputs element-wise scaling factors, trained to maximize a surrogate objective of solution quality. We evaluate GSM on standard multi-task benchmarks including CityScapes and NYUv2, achieving modest improvements over PCGrad (0.8% mean IoU improvement) while using 15% fewer parameters than comparable methods. Analysis reveals that GSM learns to preserve low-frequency gradient components that capture shared features, providing insight into how gradient conflicts manifest. While our improvements are incremental and the method adds computational overhead during training, the framework offers a principled alternative to hard gradient projection.",
    "id": 309
  },
  {
    "title": "Improved Generalization Bounds for Transformer Attention via Kernel-based Capacity Measures",
    "authors": [
      "Chen, L.",
      "Ramos, J.M.",
      "Kumar, S."
    ],
    "abstract": "We revisit generalization bounds for self-attention mechanisms in transformers through the lens of kernel-based complexity measures. While recent theoretical work has focused on Rademacher complexity bounds, we propose a simpler analysis based on the kernel associated with the attention mechanism that yields tighter data-dependent bounds in low-rank settings. Our approach leverages the fact that attention can be viewed as a kernel smoother with specific structural properties, allowing us to derive margin-based generalization guarantees that depend on the intrinsic dimension of the learned representations rather than the full parameter count. We empirically validate our bounds on synthetic datasets and small-scale BERT models, showing improvements over naive VC-dimension based bounds by up to 2\u00d7 in some configurations. However, we find that our theoretical results still significantly overestimate the true generalization error on larger models. Our work provides a stepping stone toward bridging the gap between theoretical understanding and empirical observations in transformer architectures, though we acknowledge limitations in scaling our analysis to state-of-the-art models.",
    "id": 310
  },
  {
    "title": "Gradient Surgery for Stale Momentum: A Lightweight Framework for Asynchronous Federated Optimization",
    "authors": [
      "Chen, L.",
      "Gonzalez, J.",
      "Krishnan, S."
    ],
    "abstract": "Federated learning faces significant performance degradation under asynchronous updates when client updates arrive with varying staleness. We propose StaleMomentum, a gradient clipping technique that reweights momentum terms based on staleness intervals without requiring synchronized clocks or additional communication rounds. Our method introduces an inexpensive correction term derived from a second-order Taylor approximation of the staleness error, allowing clients to locally compensate for outdated momentum states. On benchmark datasets (CIFAR-10, FEMNIST), StaleMomentum improves convergence by 12-18% over standard asynchronous FedAvg while adding negligible computational overhead (<3%). However, gains diminish under extremely heterogeneous data partitions (>80% non-IID), suggesting the technique is most beneficial in moderate staleness regimes. Theoretical analysis provides convergence guarantees with weakened assumptions compared to prior work, though our bounds remain looser than synchronous counterparts. Code is available at [link].",
    "id": 311
  },
  {
    "title": "Towards Moderate Overparameterization: A Structured Pruning Approach for Transformer Efficiency",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "We propose Progressive Structure Sparsification (PSS), a novel pruning method for transformers that maintains performance while reducing computational costs in the moderate overparameterization regime. PSS introduces a coarse-to-fine structured pruning strategy that first identifies redundant attention heads and feed-forward network neurons at the granularity of modules, then progressively refines sparsity patterns during training. Unlike existing work focusing on extreme sparsity ratios that degrade accuracy, we demonstrate that carefully structured pruning at 40-60% sparsity preserves 95-97% of original performance on standard benchmarks (GLUE, SuperGLUE). Our theoretical analysis shows PSS implicitly regularizes the model by constraining the effective parameter count, though our bounds rely on assumptions about layer-wise Lipschitz constants that may not hold sharply. Empirically, PSS achieves 1.7\u00d7 speedup on inference with minimal accuracy loss compared to existing magnitude-based pruning baselines, particularly on smaller models. On larger architectures (>1B parameters), benefits diminish and become dataset-dependent. While our method provides consistent improvements over naive pruning, it underperforms recent lottery ticket-style approaches that leverage rewinding. Experimental results are reproducible, though ablation studies reveal sensitivity to initialization seeds in the structured sparsity mask.",
    "id": 312
  },
  {
    "title": "Gradient Surgery Revisited: Do Adaptive Momentum Methods Really Need Gradient Projection?",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Adaptive optimization methods like Adam and RMSprop have become ubiquitous in deep learning, yet their performance often degrades when gradients from different objectives conflict\u2014particularly in multi-task and GAN training. Previous work proposed gradient surgery techniques to address this by projecting conflicting gradients onto each other's orthogonal complement. However, these methods introduce significant computational overhead and require storing full gradient history. We observe that the momentum terms in adaptive optimizers already encode historical gradient information, suggesting that explicit projection may be redundant. We propose Momentum-Aware Gradient Projection (MAGP), which leverages Adam's momentum buffers to approximate gradient projections without additional memory or compute. Our theoretical analysis shows that MAGP recovers standard Adam under certain smoothness assumptions, while providing controlled deviation bounds in non-convex settings. Experimental evaluation on standard multi-task benchmarks (PubMed, NYU-v2) and GAN training shows 1.2-1.8\u00d7 speedup with comparable final performance to full gradient surgery, though we note variance across seeds (std ~0.3 in multi-task metrics). While MAGP's benefits are most pronounced in memory-constrained settings, our findings suggest the conventional wisdom about gradient alignment in adaptive methods may be overly pessimistic.",
    "id": 313
  },
  {
    "title": "LoRA-Drop: Structured Pruning of Low-Rank Adaptation Parameters via Gradient Flow Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its memory footprint can still be prohibitive when deploying multiple adapted models. We propose LoRA-Drop, a structured pruning approach that identifies and removes redundant LoRA parameters during fine-tuning while maintaining task performance. Our method uses a simple gradient flow criterion to prune entire LoRA blocks based on their contribution to the loss gradient, eliminating the need for expensive iterative magnitude pruning. Experiments on GLUE and SuperGLUE benchmarks show that LoRA-Drop can remove 30-50% of LoRA parameters with minimal performance degradation (\u22641% accuracy drop) across BERT-base and RoBERTa-large models. While our approach provides modest memory savings compared to full LoRA fine-tuning, we observe that the pruning decisions heavily depend on initialization and learning rate schedules, suggesting limited generalizability across tasks. Code to reproduce our experiments is provided in the supplementary material.",
    "id": 314
  },
  {
    "title": "Adaptive Gradient Clipping with Learnable Threshold Schedules",
    "authors": [
      "Chen, J.",
      "Liu, S.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient clipping is widely used to stabilize neural network training, yet its effectiveness heavily depends on the clipping threshold\u2014a hyperparameter typically fixed through heuristics or extensive tuning. We propose Adaptive Gradient Clipping with Learnable Threshold Schedules (AGC-LTS), a method that dynamically adjusts clipping thresholds using a lightweight neural network trained jointly with the main model. Our approach introduces auxiliary loss terms that balance gradient norm stability with convergence speed, eliminating the need for manual threshold selection across different architectures and datasets. We evaluate AGC-LTS on transformer language models, CNNs on ImageNet, and GANs, showing consistent improvements over fixed-threshold clipping in training stability (10-20% reduction in gradient explosions) and modest improvements in final performance (0.5-1.2% accuracy gains). While our method demonstrates clear benefits over naive baselines, gains over well-tuned manual clipping are incremental and may not justify the additional complexity. Code will be made available upon acceptance.",
    "id": 315
  },
  {
    "title": "Revisiting Momentum Methods in Non-Convex Optimization: When 0.9 Works and Why",
    "authors": [
      "Liu, J.",
      "Chen, M.",
      "Thompson, K."
    ],
    "abstract": "Momentum-based optimizers like Adam and SGD+Momentum dominate deep learning training, yet their interaction with non-convex loss surfaces remains poorly understood. We empirically investigate momentum behavior across 150 neural network training runs spanning vision and language tasks. Our key finding reveals that momentum coefficients near 0.9 consistently provide near-optimal convergence when (1) the gradient norm exhibits heavy-tailed behavior and (2) batch sizes exceed 64. However, these benefits disappear when these conditions fail. Through extensive ablations on CIFAR-10/100 and WikiText-2, we demonstrate that adaptive momentum schedules can recover 80% of Adam's performance on transformers while using minimal memory overhead (<2% increase). Theoretical analysis in simplified quadratic settings provides partial justification for these observations, though extending to general non-convex cases remains challenging. While our results do not explain all momentum phenomena, they offer actionable insights for practitioners tuning optimizers. Code and detailed logs are provided at [redacted].",
    "id": 316
  },
  {
    "title": "Adaptive Gradient Clipping with Dynamic Perturbation Bounds for Non-Convex Optimization",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient clipping is widely used in training neural networks to stabilize optimization, but its effectiveness depends heavily on the clipping threshold. We propose Adaptive Gradient Clipping with Dynamic Perturbation (AGCDP), a method that automatically adjusts clipping thresholds based on the local curvature of the loss landscape. Our approach estimates curvature using Hessian-vector products and adjusts clipping bounds proportionally to the gradient variance within mini-batches. We prove convergence guarantees for non-convex smooth objectives, achieving a rate of O(1/\u221aT) matching standard SGD with carefully tuned clipping. Experiments on CIFAR-10, ImageNet, and transformer language models show AGCDP reduces gradient norm outliers by 35-60% compared to fixed clipping, while achieving competitive accuracy (within 0.5% of tuned baselines). However, computational overhead from curvature estimation increases training time by 15-20%. While AGCDP provides practical benefits when clipping thresholds are poorly initialized, its advantages diminish with careful hyperparameter tuning. Code and hyperparameters are provided for reproducibility.",
    "id": 317
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: A Partial Fix with Theoretical Caveats",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Garcia, M."
    ],
    "abstract": "We propose Layer-Adaptive Gradient Surgery (LAGS), a simple modification to existing gradient-based optimizers that addresses gradient interference in overparameterized networks. Building on recent observations that gradients across different layers exhibit conflicting directions, we introduce layer-wise projection operators that remove components harmful to overall optimization progress. Our method requires minimal hyperparameter tuning and adds negligible computational overhead. While we provide theoretical analysis showing LAGS converges in idealized settings under restrictive assumptions, we acknowledge these conditions rarely hold in practice. Experimental evaluation on CIFAR-10, CIFAR-100, and ImageNet shows 1.2-2.3% accuracy improvements over AdamW on ResNet architectures, but similar gains vanish on vision transformers and NLP tasks. Despite moderate empirical success, we identify scenarios where LAGS actively hurts performance - particularly when layer gradients contain beneficial complementary information. Our results suggest caution when applying gradient surgery techniques and highlight the gap between theoretical convergence guarantees and practical effectiveness.",
    "id": 318
  },
  {
    "title": "Momentum with Directional Parameter Scaling for Training Attention Architectures",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Standard momentum-based optimizers treat all parameters uniformly when updating neural networks. We observe that attention layers and feed-forward layers exhibit qualitatively different gradient patterns during training, suggesting that adaptive scaling within the same network could improve convergence. We propose Directionally-Scaled Momentum (DSM), a simple modification to SGD with momentum that applies layer-wise scaling factors based on gradient norm statistics collected during a brief warm-up phase. DSM requires no additional hyperparameters beyond learning rate and momentum, and adds minimal computational overhead. Our experiments on language modeling tasks show 3-8% perplexity improvements over standard SGD on GPT-2 architectures when training from scratch on WikiText-103. However, gains diminish when using AdamW or on larger models (GPT-3 1.3B), limiting practical impact. While the directional scaling idea is intuitive and the method is straightforward to implement, our theoretical analysis reveals DSM can be viewed as a diagonal preconditioner under specific assumptions, raising questions about novelty. The approach may benefit practitioners constrained to basic optimizers, but the contribution remains incremental.",
    "id": 319
  },
  {
    "title": "Gradient Descent with Periodic Momentum Reset: A Simple Trick for Better Generalization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Thompson, K."
    ],
    "abstract": "We propose a simple modification to standard momentum-based gradient descent that periodically resets the momentum buffer to zero during training. While momentum acceleration is widely used to speed up optimization, we observe that it can accumulate overly aggressive updates that potentially harm final generalization. Our method, Periodic Momentum Reset (PMR), requires only a single hyperparameter - the reset frequency - and can be implemented in a few lines of code. Through experiments on CIFAR-10/100 and ImageNet, we show that PMR achieves comparable or slightly better test accuracy than standard SGD with momentum across ResNet and EfficientNet architectures, with particularly notable improvements (up to 1.2%) when training with small batch sizes or noisy labels. Theoretical analysis in a simplified quadratic setting suggests that momentum reset prevents overshooting in poorly conditioned directions, though we acknowledge this analysis does not fully capture the deep learning regime. While our approach shows consistent improvements in specific settings, the gains are modest and highly dependent on the reset schedule. We anticipate this work might be most useful as a practical trick for practitioners facing generalization challenges, rather than as a fundamental advance in optimization theory.",
    "id": 320
  },
  {
    "title": "Gradient Dropout: Improving Stochastic Optimization via Randomized Gradient Subsampling",
    "authors": [
      "Chen, Y.",
      "Kumar, V.",
      "Zhang, L.",
      "M\u00fcller, K."
    ],
    "abstract": "We propose Gradient Dropout, a simple modification to standard stochastic gradient descent that randomly drops out a subset of gradients during each optimization step. Inspired by dropout's success in preventing overfitting and signal propagation bottlenecks, we show that randomly discarding gradient components can lead to improved training dynamics and final generalization. Our method introduces a single hyperparameter controlling the fraction of gradients retained, which we show can be automatically tuned using a validation set. Through extensive experiments on CIFAR-10/100 and ImageNet with ResNet architectures, we demonstrate that Gradient Dropout achieves modest but consistent improvements over standard SGD (0.5-1.2% accuracy gains) with minimal computational overhead. While our theoretical analysis reveals some limitations\u2014particularly in convex settings where the method can degrade convergence\u2014we empirically validate its effectiveness in deep learning contexts. However, we acknowledge that the gains saturate with larger models and sophisticated optimizers like AdamW, suggesting the technique may be most valuable in resource-constrained scenarios. Code is available at [URL omitted for anonymity].",
    "id": 321
  },
  {
    "title": "Efficient Gradient Compression via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient compression is essential for distributed training at scale, but existing methods rely on hand-crafted schedules that may not adapt to varying training dynamics. We propose Adaptive Quantized SGD (AQ-SGD), which learns compression parameters jointly with model parameters using a meta-learning approach. By framing quantization levels as continuous variables that are periodically projected onto discrete values, our method discovers instance-specific compression schedules without communicating additional metadata. Experiments on CIFAR-10 and ImageNet with ResNet-50 show AQ-SGD achieves 40\u00d7 gradient compression while maintaining within 1.2% of baseline accuracy, matching state-of-the-art static methods on most tasks. However, we observe instabilities on Transformer architectures and convergence issues when batch sizes are small, suggesting our current formulation may not capture all necessary dynamics. Theoretical analysis proves convergence to stationary points under bounded gradient assumptions, but requires stronger conditions than uncompressed SGD. Our method shows promise for adaptive distributed training, though practical deployment may require additional stabilization techniques for certain architectures.",
    "id": 322
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Simple Yet Effective Pruning Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradDrop show promise, they introduce computational overhead and require careful hyperparameter tuning. We propose Gradient Pruning Surgery (GPS), a lightweight alternative that selectively drops conflicting gradient entries based on their alignment with the average gradient direction. Our method requires only a single additional hyperparameter (pruning threshold) and minimal computational cost. Experiments on three standard multi-task benchmarks\u2014including NYUv2, CityScapes, and a synthetic regression suite\u2014show GPS achieves comparable performance to existing methods while reducing training time by 15-25%. However, we find performance is sensitive to the pruning threshold and degrades on tasks with highly imbalanced gradient magnitudes. Our theoretical analysis provides convergence guarantees under simplified assumptions that may not hold in practice. While GPS offers a practical middle ground between sophistication and simplicity, its benefits are most pronounced in scenarios with moderate task conflicts. Code will be made available upon acceptance.",
    "id": 323
  },
  {
    "title": "Gradient Surgery Revisited: Do Current Methods Actually Improve Multi-Task Learning?",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Chen, L."
    ],
    "abstract": "Multi-task learning (MTL) often faces optimization challenges due to conflicting gradients between tasks. Recent work has proposed various gradient surgery techniques\u2014such as projecting gradients onto conflict-free subspaces or applying task-specific scaling\u2014that claim to improve MTL performance. In this paper, we systematically reevaluate these methods across a broader set of tasks and architectures than previously considered. We find that while gradient surgery provides consistent optimization benefits (reducing gradient noise by 15-20%), these gains do not reliably translate to improved downstream performance. Through careful ablation studies on five MTL benchmarks spanning NLP and vision domains, we show that simpler baselines with tuned loss weighting can match or exceed the performance of sophisticated gradient surgery methods in 65% of cases. Our theoretical analysis reveals that gradient conflicts are often transient during training, and that the benefits of surgery may be explained by implicit regularization rather than conflict resolution. These findings suggest that the recent push toward more complex gradient manipulation techniques may be misguided, and that renewed focus on loss function design and task weighting strategies could be more impactful for MTL.",
    "id": 324
  },
  {
    "title": "Gradient-Norm-Aware Sharpness Minimization for Improved Generalization in Language Models",
    "authors": [
      "Chen, L.",
      "Garcia, A.",
      "Thompson, S."
    ],
    "abstract": "We propose Gradient-Norm-Aware Sharpness Minimization (GNASM), a novel regularization technique that adapts the sharpness penalty based on gradient norms during training. While existing sharpness-aware minimization methods apply uniform penalties across all parameters, GNASM dynamically adjusts the regularization strength based on per-parameter gradient magnitudes, theoretically motivated by connections to PAC-Bayesian bounds. We evaluate GNASM on both vision and language tasks, showing modest improvements over SAM and its variants. On ImageNet, GNASM improves top-1 accuracy by 0.3-0.7% across ResNet-50 and EfficientNet architectures. In language modeling, we observe consistent but small gains on Wikitext-103 (0.8 perplexity reduction) and modest improvements on downstream GLUE tasks (average +0.5 points), though results vary substantially across tasks. Our analysis reveals that GNASM primarily helps during early training but shows diminishing benefits as training progresses. The computational overhead is 1.3x compared to standard training, limiting practical deployment. While our theoretical analysis provides some justification for the approach, we acknowledge the improvements are incremental and not consistently significant across all experimental settings.",
    "id": 325
  },
  {
    "title": "Gradient Surgery for Transformer Attention: Towards More Stable Training with Layer-wise Learning Rate Modulation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, D."
    ],
    "abstract": "We propose AttentionGrad-Surgery, a simple modification to transformer training that applies layer-specific gradient scaling based on attention pattern statistics. Motivated by the observation that attention weights in lower layers exhibit higher gradient variance during training, we introduce a novel gradient modulation scheme that down-weights updates from layers with unstable attention entropy. Our method requires only a single additional hyperparameter and minimal computational overhead. Experiments on IWSLT'14 De-En translation and GLUE show 0.5-1.2 BLEU/accuracy improvements over strong baselines, particularly in low-resource settings. We also demonstrate reduced training variance across 5 seeds compared to standard AdamW. While our approach shows promise for stabilizing transformer optimization, we acknowledge limitations: improvements are modest on larger datasets, and the theoretical justification remains heuristic. Code will be made available upon publication.",
    "id": 326
  },
  {
    "title": "Learning to Optimize with Compressed Gradients: A Variational Filtering Approach",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Chen, B."
    ],
    "abstract": "Gradient compression is widely used in distributed deep learning to reduce communication overhead, but existing methods suffer from performance degradation when compression ratios are aggressive. We propose a novel framework that treats gradient compression as a variational filtering problem, learning to reconstruct useful optimization directions from severely compressed gradients. Our key insight is to model the optimization trajectory as a latent state sequence, where compressed gradients provide noisy observations. We train an auxiliary neural network to perform Bayesian filtering, yielding reconstructed gradients that maintain convergence properties while achieving up to 200\u00d7 compression ratios. Experiments on ResNet-50 and Transformer models show that our method matches uncompressed training accuracy at 50-100\u00d7 compression on standard benchmarks, outperforming prior gradient compression techniques by 2-5% in final accuracy. However, we observe increased sensitivity to hyperparameter choices and some computational overhead from the filtering network. This work provides a new perspective on the fundamental trade-off between communication efficiency and optimization quality, though it may be most practical for specific resource-constrained deployment scenarios rather than general-purpose training.",
    "id": 327
  },
  {
    "title": "LoSparse: Structured Sparsity for Low-Rank Adaptation via Learned Thresholding",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "We present LoSparse, a method for reducing inference costs in low-rank adaptation (LoRA) by introducing structured sparsity patterns. While LoRA enables efficient fine-tuning of large language models, its memory and computational benefits are limited during inference when the low-rank adapters must still be processed. LoSparse addresses this by learning element-wise thresholds to zero out 30-60% of adapter parameters while maintaining performance within 2% of full LoRA on GLUE and summarization tasks. Our key insight is that the rank decomposition in LoRA creates natural groupings where sparsity patterns can be learned jointly across the low-rank matrices. We formulate this as a bilevel optimization problem with a differentiable sparsity regularizer, avoiding the need for expensive retraining. Experiments across 7 transformer architectures (140M-7B parameters) show 1.3-1.8x speedups during inference and 25-45% memory reduction, though we observe degradation beyond 60% sparsity. While our method provides practical speedups, we acknowledge theoretical gaps in understanding optimal sparsity patterns and scaling limitations when rank sizes are small. Code is available at our anonymous repository.",
    "id": 328
  },
  {
    "title": "Gradient Descent with Memory-Efficient Momentum via Taylor Approximation",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Liu, S."
    ],
    "abstract": "We propose a memory-efficient variant of momentum-based gradient descent that reduces storage requirements by approximately 50% for large neural networks without significant performance degradation. Our key insight is to approximate the historical gradient information using a second-order Taylor expansion around an exponentially-decaying basis, eliminating the need to store full gradient histories. We theoretically analyze the convergence properties under smoothness assumptions similar to standard momentum, showing an O(1/\u221aT) convergence rate. Experiments on ResNet-50 and ViT-B/16 across CIFAR-10, ImageNet-1k, and GLUE benchmarks demonstrate comparable accuracy to traditional momentum (within 0.5-1.2% on average) while halving memory usage. However, we observe increased sensitivity to hyperparameter selection and occasional instability on deeper networks. While the method provides practical memory benefits for resource-constrained training, particularly for larger batch sizes, the theoretical approximation introduces limitations on general convergence guarantees. Code will be made available.",
    "id": 329
  },
  {
    "title": "Gradient Amplification Through Layer-wise Learning Rate Modulation in Deep Networks",
    "authors": [
      "Smith, J.",
      "Chen, L.",
      "Park, K.",
      "Garcia, M."
    ],
    "abstract": "We propose Layer-wise Learning Rate Amplification (LLRA), a simple modification to standard SGD that applies different learning rate scaling factors to network layers based on their relative gradient magnitudes. Motivated by the observation that gradient norms vary significantly across layers in deep networks, LLRA amplifies updates for layers with smaller gradients while dampening those with larger gradients, using a theoretically-motivated ratio derived from the Lipschitz constants of each layer. Our method requires minimal hyperparameter tuning and can be implemented with a few lines of PyTorch code. Experiments on ResNet-50 and Vision Transformer architectures show modest improvements in convergence speed (8-12% reduction in training epochs) and final accuracy gains of 0.3-0.7% on ImageNet, CIFAR-10, and CIFAR-100. While these improvements are consistent across different architectures and datasets, we acknowledge they are relatively small compared to recent advances in optimizer design. Our theoretical analysis reveals that LLRA can be viewed as a diagonal preconditioner with convergence guarantees comparable to Adam, though with tighter bounds for certain network configurations. The method is most beneficial for very deep networks (>50 layers) and less effective for smaller models. Code will be made available upon publication.",
    "id": 330
  },
  {
    "title": "Momentum Residual Networks: Combining Implicit Depth with Explicit Optimization for Faster Convergence",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We propose Momentum Residual Networks (MoResNet), an architectural modification that injects momentum terms into residual connections without changing the forward pass. Our key insight is that standard residual networks can be viewed as explicit Euler discretizations, and adding momentum transforms them into a hybrid implicit-explicit system. This allows networks to leverage faster convergence properties of momentum methods while maintaining interpretability. We demonstrate empirically that MoResNet achieves 1.2-1.5\u00d7 speedup in training time on ImageNet compared to standard ResNet baselines, with comparable accuracy. Theoretical analysis shows our modification is equivalent to preconditioning the optimization landscape with a learned positive definite matrix. While our method shows consistent improvements on image classification, we observe diminishing returns on deeper architectures and mixed results on object detection tasks. Our code is available at [link removed for review].",
    "id": 331
  },
  {
    "title": "Improved Gradient Equivariance via Learned Coordinate Transformations",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Equivariant neural networks have demonstrated strong performance on tasks with inherent symmetries, but their gradient computation remains expensive for high-dimensional inputs. We propose Learned Coordinate Transformations (LCT), a lightweight module that approximates equivariant gradients by transforming to symmetry-reduced coordinates before backpropagation. Our theoretical analysis shows LCT achieves (\u03b5,\u03b4)-equivariant gradients with probability 1-\u03b4 under mild assumptions. On molecular dynamics prediction tasks, LCT reduces gradient computation time by 38% compared to standard equivariant networks while maintaining 94% of their accuracy. On image classification with rotations, LCT achieves comparable performance to full equivariant baselines with 2.1\u00d7 fewer parameters. However, we observe performance degradation on tasks requiring exact equivariance to discrete symmetries. Our results suggest LCT provides a practical speed-accuracy trade-off for applications where approximate equivariance is acceptable. Code and trained models are available at anonymized-url.",
    "id": 332
  },
  {
    "title": "MAML++: Yet Another Look at Model-Agnostic Meta-Learning with Adaptive Learning Rates",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Model-Agnostic Meta-Learning (MAML) has become a popular framework for few-shot learning, but its reliance on hand-tuned hyperparameters and sensitive gradient optimization limits practical applicability. We propose MAML++, a simple extension that introduces adaptive learning rates and a modified meta-objective. Our method uses per-parameter learning rates derived from a lightweight hypernetwork conditioned on task embeddings, allowing automatic adaptation across different few-shot tasks. Additionally, we propose skipping the computationally expensive second-order derivatives in favor of a first-order approximation with a novel correction term. We evaluate MAML++ on standard few-shot image classification benchmarks (Mini-ImageNet, CIFAR-FS) and achieve modest improvements over vanilla MAML (1-2% absolute accuracy gains). While our approach provides marginally better performance, we find the gains are inconsistent across task domains and sensitive to initialization. The method introduces minimal additional parameters (under 0.1% increase) but increases training time by 30% due to the hypernetwork overhead. Our results suggest the benefits of MAML++ may be limited to specific task distributions, raising questions about the brittleness of current meta-learning evaluation protocols. Code will be made available upon acceptance.",
    "id": 333
  },
  {
    "title": "Gradient Surgery in the Wild: A Post-Hoc Analysis of Existing Optimizers",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Recent work on gradient surgery has shown promise for improving multi-task learning by selectively dropping conflicting gradient components. However, these methods require explicit task labels and architectural modifications that limit applicability to standard pre-trained models. We ask: can gradient surgery principles be applied post-hoc to existing trained networks? We propose Gradient Residual Analysis (GRA), a framework that identifies and suppresses gradient conflicts by analyzing the per-example gradient covariance structure. Our method operates as a lightweight wrapper around any optimizer without retraining. On ImageNet and GLUE benchmarks, GRA provides modest but consistent improvements (0.8-1.2% accuracy/F1) over AdamW and SGD baselines for dense prediction and language understanding tasks. While GRA avoids the computational overhead of full gradient surgery, our gains are smaller than those achieved by training-time interventions. Theoretical analysis reveals limitations when applied to networks with batch normalization or attention layers. Our results suggest that gradient conflicts persist beyond training, but post-hoc correction faces fundamental constraints. Code is available at anonymous-link.",
    "id": 334
  },
  {
    "title": "Improved Gradient Estimation for Discrete Latent Variables via Learnable Quasi-Gumbel Noise",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Tanaka, H."
    ],
    "abstract": "We propose a simple modification to the Gumbel-Softmax reparameterization trick for learning models with discrete latent variables. While the Gumbel-Softmax estimator is widely used, its bias-variance trade-off depends critically on the temperature parameter, which is typically fixed or annealed heuristically. We introduce a learnable transformation of the Gumbel noise distribution that adapts during training to minimize gradient estimation variance while controlling bias. Our method adds minimal computational overhead and can be implemented in under 20 lines of PyTorch code. On three benchmark tasks\u2014structured attention, neural architecture search, and categorical VAEs\u2014we observe modest improvements over standard baselines (\u22481-2% absolute gains) with particularly strong benefits in low-temperature regimes. However, we find the improvements are not consistent across all datasets and sometimes degrade performance due to overfitting the noise transformation. We provide partial theoretical analysis showing the estimator remains consistent under mild assumptions, though a complete characterization remains elusive. While the results are encouraging, we acknowledge the gains may be limited to specific problem settings and the method introduces additional hyperparameters that require careful tuning.",
    "id": 335
  },
  {
    "title": "Sequential Gradient Sampling: A Memory-Efficient Alternative to Experience Replay in Deep Q-Learning",
    "authors": [
      "Mitchell, S.",
      "Kumar, P.",
      "Zhou, L."
    ],
    "abstract": "Experience replay has become a standard component in deep reinforcement learning, enabling stable training by decorrelating samples. However, its memory requirements scale poorly with increasing replay buffer sizes, particularly in resource-constrained environments. We propose Sequential Gradient Sampling (SGS), a simple alternative that maintains gradient estimates using only recent mini-batches without storing explicit transitions. SGS adaptively weights recent gradients based on their temporal distance and gradient magnitude similarity, theoretically providing an approximation to replay-based updates under mild assumptions. While our analysis shows SGS achieves similar convergence properties to random sampling for linear function approximators, empirical evaluation on Atari 2600 games demonstrates mixed results: SGS matches DQN performance on 7 out of 10 tested environments while failing to stabilize learning in the remaining 3. Additionally, we observe 2-4x reduction in memory usage and 15-20% faster wall-clock training time. Our method provides a practical trade-off between memory efficiency and performance stability, particularly relevant for edge deployment scenarios where storage constraints are stringent.",
    "id": 336
  },
  {
    "title": "Gradient Surgery in Federated Learning: When Does Averaging Help?",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "Federated learning faces a fundamental tension between local model updates and global coordination. While FedAvg and its variants typically average local gradients to update a shared model, the theoretical justification for this approach remains limited when client data distributions diverge significantly. We propose FedSurgeon, a method that selectively applies gradient averaging based on estimated alignment between local and global objectives. Our approach employs a lightweight cosine similarity check to identify when averaging is beneficial versus harmful for convergence. On federated CIFAR-10 with non-IID splits (\u03b1=0.1), FedSurgeon achieves 2.3% higher accuracy than FedAvg while reducing communication rounds by 15%. However, our improvements diminish as data heterogeneity decreases, suggesting limited value in near-IID settings. Theoretical analysis under a quadratic loss framework proves convergence but with bounds that depend strongly on impractical assumptions about gradient dissimilarity. While our empirical results demonstrate modest gains in specific regimes, the overhead of additional similarity computations raises questions about practicality in resource-constrained environments. Our work thus provides a nuanced view of when gradient surgery is worthwhile in federated settings, though significant gaps remain between theory and practice.",
    "id": 337
  },
  {
    "title": "Improved Convergence Rates for SGD with Time-Varying Step Sizes via Loss-Dependent Bounds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We provide tighter convergence guarantees for stochastic gradient descent (SGD) when the loss function exhibits certain regularity properties. Motivated by the empirical success of cosine annealing schedules in deep learning, we derive non-asymptotic bounds for general time-varying step sizes that depend on the accumulated gradient noise rather than worst-case quantities. Our analysis combines traditional martingale techniques with a novel loss-dependent decomposition of the update rule, yielding rates that can be significantly better than the standard O(1/\u221aT) when the training loss decreases quickly. For overparameterized linear regression, we obtain an explicit O(1/T\u00b2) rate under mild assumptions on the data. While our theoretical results are restricted to convex losses, experiments on ResNet-18 training with CIFAR-10 demonstrate 5-10% faster convergence compared to standard schedules when using our theoretically-motivated decay parameters. However, our bounds become vacuous for highly non-convex settings typical in modern deep learning, and our theoretical contributions are incremental rather than transformative.",
    "id": 338
  },
  {
    "title": "Gradient Tempering: A Simple Modification to Stochastic Optimization with Modest Generalization Benefits",
    "authors": [
      "Liu, H.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "We propose gradient tempering, a lightweight modification to standard stochastic gradient descent that rescales gradient updates by a learnable temperature parameter. While temperature scaling is well-studied in calibration, we empirically demonstrate that its application during training\u2014rather than post-hoc\u2014influences generalization dynamics. Our theoretical analysis shows that under restricted Lipschitz smoothness conditions, gradient tempering induces a tighter generalization bound by controlling the effective Lipschitz constant of the optimization trajectory. Experiments on ImageNet, CIFAR-10, and GLUE benchmarks reveal consistent but modest improvements over baseline SGD, with an average test accuracy gain of 0.7% across tasks. Notably, the benefits diminish with advanced optimizers (AdamW, Lion) and larger models (ViT-L/16). We open-source our PyTorch implementation, which adds only 4 lines of code to existing training loops. While the relative simplicity of our approach makes it immediately deployable, the marginal improvements and limited theoretical scope suggest extensions may be needed for broader impact.",
    "id": 339
  },
  {
    "title": "Improved Convergence Rates for Stochastic Gradient Descent with Adaptive Polyak Step-Size",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We revisit the classical Polyak step-size for stochastic gradient descent (SGD) and propose a simple adaptive variant that achieves improved convergence rates under standard assumptions. While the original Polyak step-size requires knowledge of the optimal function value, our method estimates this quantity online using a running average of past losses. We establish O(1/T) convergence for convex Lipschitz functions and O(log T/T) for strongly convex cases, improving upon prior adaptive methods by logarithmic factors. Experimental results on logistic regression and neural network training demonstrate modest but consistent improvements over Adam and SGD with cosine annealing. However, empirical gains diminish in overparameterized settings, suggesting the theoretical analysis may not fully capture the practical benefits. The proposed method introduces negligible computational overhead and is particularly effective for ill-conditioned problems. While our contribution is largely incremental, it bridges an important gap between theoretical guarantees and practical step-size selection. Code is available at anonymous-url.",
    "id": 340
  },
  {
    "title": "Gradient Descent with Momentum in the Presence of Sparse Outliers: A Perturbation Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We analyze the behavior of gradient descent with momentum (GDM) when training overparameterized neural networks on datasets containing sparse, high-magnitude outliers. While recent work has established convergence guarantees for GDM in idealized settings, practical datasets often contain contaminated samples that violate standard assumptions. Through a perturbation analysis of the discrete-time momentum dynamics, we show that sparse outliers create non-trivial implicit bias in the learned representations, leading to systematic deviations from clean-data solutions. Our key finding is that the momentum parameter exhibits a phase transition: below a dataset-dependent threshold, outliers have minimal impact, while above it, they can dominate the optimization trajectory. We validate our theoretical predictions on both synthetic linear models and small-scale CNNs trained on CIFAR-10 with synthetically injected outliers. Experiments show our analysis accurately predicts the failure threshold across different architectures, though we observe a non-trivial gap in the high-momentum regime that may require more sophisticated analytical tools. While focused on simplified settings, our work provides the first theoretical characterization of how robust momentum-based optimizers are to realistic data corruption patterns.",
    "id": 341
  },
  {
    "title": "Revisiting Batch Normalization through the Lens of Gradient Orthogonality",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Batch Normalization (BN) has become a de facto standard for training deep neural networks, yet the precise mechanisms underlying its effectiveness remain debated. We propose a novel perspective that connects BN's empirical benefits to properties of orthogonal gradients. Our method, OrthoNorm, modifies the standard BN layer by explicitly encouraging gradient orthogonality across mini-batch statistics. Through theoretical analysis on simplified linear models, we show that OrthoNorm can reduce gradient interference by a factor dependent on batch size. Empirically, we evaluate OrthoNorm on CIFAR-10/100 and ImageNet classification tasks using ResNet-18 and WideResNet-28-10 architectures, achieving 0.3-0.7% improvements over standard BN on most settings, with comparable computational overhead. Additionally, we demonstrate improved training stability on a small subset of ImageNet-C corruption benchmarks. While our theoretical framework provides partial justification for these gains, we acknowledge limitations in generalizing beyond specific architectures and the need for future work on larger-scale experiments. Our code and pre-trained models are available at [URL].",
    "id": 342
  },
  {
    "title": "Adaptive Gradient Clipping with Memory-Efficient Second-Order Estimates for Transformer Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, S."
    ],
    "abstract": "Gradient clipping is widely used to stabilize transformer training, but standard approaches use fixed thresholds that may hinder convergence. We propose Adaptive Second-Order Gradient Clipping (ASGC), which adjusts clipping thresholds using a low-rank approximation of the Hessian spectrum. Our method combines a streaming PCA algorithm to track dominant Hessian eigenvalues with a lightweight clipping schedule that adapts to local curvature. ASGC requires only 3% additional memory compared to standard Adam training. On the GPT-2 medium architecture trained on OpenWebText, ASGC achieves 4.2% faster convergence versus baseline clipping. We also demonstrate improved stability on ViT-L/16 training, reducing gradient norm spikes by 31%. While ASGC shows consistent improvements over naive clipping, our gains are modest (1-5% across tasks) and require careful hyperparameter tuning for different architectures. Theoretical analysis reveals ASGC converges with high probability under standard smoothness assumptions, though our guarantees are weaker than prior work due to the approximate nature of our Hessian estimates. We release an efficient PyTorch implementation compatible with existing optimizer interfaces.",
    "id": 343
  },
  {
    "title": "Gradient Surgery for Neural Networks: A Simple Pruning Criterion via Weight-Saliency Trajectories",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Recent work in neural network pruning has focused on magnitude-based approaches that remove weights with small absolute values. We propose Gradient Surgery, a pruning criterion that tracks the accumulated gradient flow through each weight over training. Our method computes weight-saliency trajectories by integrating gradient signals during fine-tuning, allowing identification of weights that contribute minimally to the loss landscape despite having large magnitudes. We evaluate our approach on ResNet-18 and Vision Transformer architectures across CIFAR-10/100 and ImageNet-subsets. Results show 2-3% accuracy improvements over magnitude pruning at 80-90% sparsity levels, with particularly strong performance on smaller datasets. However, performance gains diminish at extreme sparsities (>95%) and scale limitations emerge on full ImageNet. While our method introduces minimal computational overhead (<5% training time increase), the theoretical motivation connecting trajectory dynamics to pruning efficacy remains partially heuristic. Code and pretrained models are available at anonymized.github.io/grad-surgery.",
    "id": 344
  },
  {
    "title": "LoRA-V: Memory-Efficient Fine-Tuning via Vectorized Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "We present LoRA-V, a vectorized extension to Low-Rank Adaptation (LoRA) that reduces memory requirements for fine-tuning large language models by 15-25% while maintaining comparable performance. Our key insight is that the low-rank matrices in LoRA can be further compressed by vectorizing them along the hidden dimension, leveraging structured sparsity patterns that emerge during training. We introduce a learnable mask mechanism that dynamically selects which vector groups to retain, achieving compression ratios of 8-12x with minimal accuracy loss on GLUE tasks. On Llama-2-7B fine-tuning, LoRA-V achieves 94% of LoRA's performance while using 1.8GB less GPU memory. However, we observe that performance degradation becomes significant (>5% drop) on tasks requiring precise few-shot adaptation. Our ablations reveal that the effectiveness of vectorization depends heavily on initialization strategies, and we provide theoretical analysis showing LoRA-V preserves gradient flow under mild assumptions. While LoRA-V offers clear practical benefits for memory-constrained scenarios, its advantages diminish on larger models (>30B parameters), suggesting the technique may be most applicable during early scaling phases.",
    "id": 345
  },
  {
    "title": "Improving Transformer Efficiency Through Block-Diagonal Attention with Learnable Sparse Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Johnson, K."
    ],
    "abstract": "Self-attention mechanisms in Transformers suffer from quadratic complexity in sequence length, limiting their applicability to long sequences. While prior work has proposed sparse attention patterns to reduce this cost, these patterns are typically hand-designed and static across layers. We propose LASPA (Learnable Sparse Attention), which learns layer-specific block-diagonal attention patterns with task-adaptive sparsity levels. Our method combines structured sparsity constraints with a differentiable relaxation of the sparsity mask, enabling end-to-end training. We evaluate LASPA on language modeling (WikiText-103) and long-range tasks (LRA). Results show 2.1-2.8\u00d7 FLOP reduction during inference with <1% performance drop compared to standard attention for sequences up to 4K tokens. Analysis reveals that lower layers utilize denser patterns while upper layers converge to highly sparse (90%) diagonal patterns. Despite improvements over fixed sparse baselines, we observe instabilities during training for sequences >8K tokens and performance plateaus on longer-context tasks. While LASPA provides a practical trade-off between efficiency and accuracy for moderate sequence lengths, its benefits diminish for extreme sparsity ratios or very long sequences.",
    "id": 346
  },
  {
    "title": "Efficient Low-Rank Adaptation via Structured Kronecker Products",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a popular approach for parameter-efficient fine-tuning of large language models. We propose Kronecker-LoRA, which replaces the standard low-rank decomposition with a structured Kronecker product to reduce memory footprint and speed up training. Our method decomposes the adaptation matrix into a product of smaller Kronecker factors, achieving a compression ratio of up to 5\u00d7 compared to standard LoRA while maintaining competitive performance. On a benchmark of 5 diverse NLP tasks, Kronecker-LoRA achieves within 1.2% of LoRA's downstream performance while using 40% fewer parameters during training. However, we find the approach is sensitive to the choice of Kronecker factor shapes and exhibits instability on smaller models when the intrinsic dimension becomes too low. Theoretical analysis shows our decomposition reduces the effective rank bound, which may explain the observed limitations. Despite these challenges, Kronecker-LoRA offers a practical alternative for deployment scenarios where memory constraints are severe.",
    "id": 347
  },
  {
    "title": "ReLoRA: Low-Rank Adaptation with Reversible Projections for Memory-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We present ReLoRA, a parameter-efficient fine-tuning method that combines low-rank adaptation with reversible projections to achieve memory savings during large language model adaptation. While LoRA reduces trainable parameters by learning low-rank updates, we observe that the intermediate activations still dominate memory usage during fine-tuning. ReLoRA addresses this by introducing reversible down-projection matrices that compress hidden states before gradient computation, allowing in-memory reconstruction during the backward pass. Our method maintains LoRA's parameter efficiency while reducing peak memory usage by 25-40% on 7B parameter models. Experiments on instruction tuning and downstream tasks show ReLoRA achieves comparable performance to LoRA (within 1-2% on MMLU), though we note a slight degradation on tasks requiring precise numerical reasoning. We provide theoretical analysis showing our approach preserves gradient signal under mild assumptions about model Lipschitz constants. While our computational overhead remains modest (15% increase in training time), we acknowledge limitations in scaling to extremely large models (>30B parameters) where memory savings diminish due to reconstruction costs.",
    "id": 348
  },
  {
    "title": "Self-Refining without Self-Critique: Improving Language Model Alignment through Iterative Constrained Decoding",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Thompson, K."
    ],
    "abstract": "We propose an alternative to reinforcement learning from human feedback (RLHF) for aligning large language models that leverages constrained decoding during inference-time refinement. Our method, Iterative Constrained Alignment (ICA), generates candidate responses through beam search with dynamically adjusted constrained sampling, eliminating the need for reward model training or rejection sampling. We demonstrate that ICA achieves comparable performance to PPO-based RLHF on helpfulness benchmarks while requiring only 15% of the computational resources during training. Across three domains (instruction following, summarization, and mathematical reasoning), ICA matches or slightly improves upon RLHF baselines on automated metrics (MT-Bench: +0.1, GSM8K: +2.3%), though we observe higher variance in human evaluations and reduced performance on longer-context tasks. Analysis reveals ICA's effectiveness stems from constraining the initial generation space rather than learning policy updates, suggesting trade-offs between training efficiency and response diversity. While our method reduces computational overhead and sidesteps reward hacking commonly associated with RLHF, we acknowledge limitations including increased inference latency and sensitivity to constraint hyperparameters. Code and models will be released upon publication.",
    "id": 349
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Optimization in Overparameterized Networks Through Weight Recycling",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Stochastic gradient descent variants often exhibit poor convergence when optimizing overparameterized networks due to gradient conflicts across mini-batches. We propose Gradient Surgery with Adaptive Memory (GSAM), a method that detects conflicting gradient directions and selectively 'recycles' weights from previous iterations when current gradients are unreliable. GSAM maintains a small memory bank of past weights and uses a simple cosine similarity threshold to determine when to reuse historical parameters. Our experiments on CIFAR-10, CIFAR-100, and ImageNet show 0.8-1.2% accuracy improvements over vanilla SGD with momentum across ResNet, EfficientNet, and Vision Transformer architectures, while adding only 0.2% parameter overhead. While the improvements are consistent, we acknowledge they are modest and highly dependent on the noise level in the training data. Theoretical analysis reveals convergence guarantees under restrictive assumptions that may not hold in practice. Our method provides a lightweight, drop-in replacement for existing optimizers that may benefit practitioners dealing with noisy or adversarial training regimes.",
    "id": 350
  },
  {
    "title": "Gradient Surgery Revisited: Do Adaptive Optimizers Really Benefit from Task-Aware Gradient Modification?",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task optimization often suffers from conflicting gradient directions across tasks, leading to suboptimal performance. Recent work has proposed various gradient surgery techniques that modify gradients to resolve conflicts, but these methods are typically evaluated with standard SGD. We investigate whether these techniques remain beneficial when combined with adaptive optimizers like Adam, which already perform implicit gradient rescaling. Through extensive experiments on standard multi-task benchmarks including CIFAR-MTL and NYUv2, we find that gradient surgery provides modest improvements over tuned Adam baselines (2.3% average gain), but often underperforms compared to the same surgery applied to SGD (6.8% gain). Our analysis reveals that adaptive optimizers partially mitigate conflicting gradients through their per-parameter learning rates, reducing the efficacy of explicit gradient modification. While we confirm previous findings that gradient surgery helps SGD navigate multi-task loss landscapes, our results suggest practitioners using modern adaptive optimizers may not need these additional modifications, calling into question the widespread recommendation to apply gradient surgery techniques regardless of optimizer choice.",
    "id": 351
  },
  {
    "title": "Improved Convergence Guarantees for Adam with Scheduled Learning Rates and Momentum Decay",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "We study convergence properties of the Adam optimizer under a novel scheduling scheme that simultaneously decreases the learning rate and momentum parameters during training. While Adam remains widely used in practice, its theoretical guarantees remain weaker than those of SGD with momentum in convex settings. We propose a simple decay schedule for both the learning rate and momentum parameters that achieves a \u00d5(1/T) convergence rate for smooth convex objectives, improving prior O(log T/\u221aT) bounds for standard Adam. Our analysis leverages a new Lyapunov function that captures the joint dynamics of the momentum and adaptive learning rates. Experiments on CIFAR-10 and SVHN with ResNet-18 and ResNet-34 architectures show 2-5% improvements in final accuracy over standard Adam baselines, though gains diminish on larger datasets like ImageNet. The improvements are most pronounced in low-data regimes and early training phases. While our theoretical results focus on convex problems, empirical results suggest the scheduling strategy may benefit non-convex training as well. Our method requires minimal hyperparameter tuning and adds negligible computational overhead compared to standard Adam.",
    "id": 352
  },
  {
    "title": "Efficient Gradient Compression Through Learned Quantization Codes",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Singh, A."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed training, but existing quantization schemes often rely on fixed rules that fail to adapt to the evolving statistical properties of gradients during training. We propose Learned Quantization Codes (LQC), an adaptive method that learns optimal quantization parameters through a lightweight meta-learning procedure. Our approach trains a small neural network to predict optimal quantization levels based on gradient statistics, achieving better compression than static schemes while maintaining convergence guarantees under standard assumptions. We evaluate LQC on ResNet-50 and Transformer training across CIFAR-10, ImageNet, and WMT'14 tasks. Compared to existing methods like 8-bit uniform quantization and QSGD, LQC achieves 2-3% better final accuracy at the same compression rate, reducing gradient communication costs by 4x without significant computation overhead. However, performance gains are less pronounced at extreme compression ratios (>8x), and wall-clock speedups vary with network latency. While our theoretical analysis provides convergence bounds, the tighter regimes we consider may be overly restrictive for practical deployment. LQC offers a practical improvement over static quantization but may be most valuable for specific training configurations rather than as a universal replacement.",
    "id": 353
  },
  {
    "title": "Gradient Surgery Revisited: Moderate Gradient Conflicts Yield Better Optimization in Overparameterized Networks",
    "authors": [
      "Liu, Q.",
      "Chen, J.",
      "Kumar, V."
    ],
    "abstract": "Gradient surgery techniques like PCGrad have shown promise for multi-task learning by projecting conflicting gradients to mitigate interference. However, we observe that completely eliminating gradient conflicts may be suboptimal for overparameterized networks. We propose SoftConflict, a simple but effective method that selectively preserves moderate gradient conflicts based on their alignment with the curvature of the loss landscape. Our key insight is that some conflicts provide implicit regularization beneficial for generalization. SoftConflict introduces a tunable threshold that preserves conflicts when the angle between gradients exceeds a learned attenuation factor. Through extensive experiments on multi-task vision benchmarks (CIFAR-100, CityScapes, NYUv2), we achieve modest improvements (1.2-1.8% average) over PCGrad while reducing computational overhead by 35% through efficient Hessian-vector approximations. Although our gains are limited to overparameterized regimes and vanish in parameter-constrained settings, we believe this work opens an interesting perspective on the role of gradient diversity in optimization dynamics. Code and pretrained models are available to facilitate reproducibility.",
    "id": 354
  },
  {
    "title": "Gradient Surgery Without the Surgery: A Lightweight Approach to Multi-Task Gradient Conflict Resolution",
    "authors": [
      "Liu, Q.",
      "Andersson, K.",
      "Chen, Z."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts where tasks compete for shared representations, leading to degraded performance. Existing methods like PCGrad and GradNorm resolve conflicts through computationally expensive gradient projections or complex reweighting schemes. We propose a surprisingly simple alternative: instead of manipulating gradients directly, we identify conflicting directions via cosine similarity and apply learned scaling factors to the loss terms themselves. This approach requires only O(n) additional parameters for n tasks and avoids gradient projection computations entirely. Extensive experiments on three computer vision benchmarks (NYUv2, CityScapes, and CelebA) demonstrate our method achieves competitive performance (within 2% of PCGrad) while reducing training time by 15-30%. While our theoretical analysis is limited to the two-task case and we find the method struggles when task correlations are extremely negative, empirical results suggest this lightweight approach provides a practical middle ground between naive multi-task learning and more sophisticated gradient surgery techniques. Code and pre-trained models will be made available.",
    "id": 355
  },
  {
    "title": "Scheduled Gradient Descent: A Simple Scheduling Trick That Sometimes Works",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Scheduled Gradient Descent (SGD-S), a modification to standard gradient descent that applies a learned piecewise-linear learning rate schedule. Rather than using monotonically decreasing schedules, SGD-S partitions optimization into discrete phases, each with potentially different learning rates learned via meta-optimization on a validation set. While conceptually simple, this scheduling approach shows modest improvements over standard baselines on small-scale vision and language tasks, achieving 2-4% relative improvement in final accuracy across ResNet-18 on CIFAR-10, 3-layer MLPs on MNIST, and 2-layer LSTMs on Penn Treebank. However, gains vanish on larger models (ResNet-50/ImageNet) and fail to outperform carefully-tuned cosine schedules. Our theoretical analysis provides convergence guarantees only under restrictive assumptions that rarely hold in practice. Experiments suggest the primary benefit comes from effectively regularizing the optimization trajectory through phase transitions, though this effect is inconsistent across tasks. Code is available at [anonymous link].",
    "id": 356
  },
  {
    "title": "A Closer Look at Gradient Norm Regularization for Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Gradient norm regularization has emerged as a popular technique for improving generalization in deep neural networks, yet its theoretical and practical impact remains unclear. We provide an empirical investigation of gradient norm regularization across vision and NLP tasks, revealing that its benefits are highly architecture-dependent. Through extensive ablation studies on ResNet-50, Vision Transformers, and BERT, we find that gradient norm regularization (GNR) primarily stabilizes training dynamics rather than improving the final generalization gap. Our key insight is that GNR acts as an implicit form of batch normalization, leading to more stable training but not necessarily better minima. We propose a lightweight variant, Local Gradient Norm Regularization (LGNR), that applies regularization only to specific layers and achieves comparable results with 35% less computational overhead. While LGNR shows modest improvements on small-scale benchmarks (0.5-1.2% on CIFAR-100 with ResNet-18), these gains do not consistently translate to larger models or datasets. Our findings suggest that previous work may have confounded training stability with generalization improvement. Code and pre-trained models will be released upon publication.",
    "id": 357
  },
  {
    "title": "Gradient Surgery Meets Adaptive Momentum: A Less Aggressive Approach to Multi-Task Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, D.",
      "Johnson, S."
    ],
    "abstract": "Multi-task learning often struggles with conflicting gradients between tasks. While recent work like PCGrad and GradNorm surgically modify gradients to resolve conflicts, we observe these methods can be too aggressive, potentially discarding useful information. We propose Adaptive Gradient Blending (AGB), a simpler approach that softly reweights gradient components using per-task uncertainty estimates from a lightweight Bayesian encoder. Rather than projecting conflicting gradients onto orthogonal subspaces, AGB modulates their contribution based on learned confidence scores. On three standard benchmarks (CIFAR-100/SVHN, NYUv2 depth/segmentation, and Meta-World RL), AGB achieves comparable performance to state-of-the-art methods while being 1.3-2.1\u00d7 faster and requiring minimal hyperparameter tuning. However, our gains are modest (0.5-1.2% average accuracy improvement) and tasks with severe conflicts still exhibit suboptimal convergence. While AGB offers a practical alternative to existing techniques, our theoretical analysis reveals fundamental limitations in simultaneously optimizing conflicting objectives without information loss. Code will be made available.",
    "id": 358
  },
  {
    "title": "Gradient Compression with Learned Sparsity Patterns for Distributed Training",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Gradient compression is crucial for scaling distributed training, yet existing methods like Top-K sparsification or quantization often sacrifice convergence speed for communication efficiency, requiring careful hyperparameter tuning. We propose LEARNED-SPARSE, a simple framework that augments standard gradient compression with lightweight neural predictors that learn dynamic sparsity patterns across training iterations. Instead of using fixed compression ratios, our method employs a small auxiliary network trained online to predict which gradient entries are most informative for optimization. The predictor is updated via reinforcement learning using downstream validation loss as reward, and its outputs are used to guide Top-K selection. On ImageNet with ResNet-50 and CIFAR-100 with ResNet-18, LEARNED-SPARSE achieves 3-4\u00d7 better compression-efficiency trade-offs compared to standard Top-K under aggressive compression (0.1% sparsity) without modifying optimization hyperparameters. However, we observe the method introduces slight convergence instability on smaller datasets and incurs marginal computational overhead (5-8% training time) from the predictor network. Our results suggest that learned compression policies can outperform static rules at extreme sparsity levels, though benefits diminish with moderate compression ratios.",
    "id": 359
  },
  {
    "title": "Gradient Noise Rehearsal: Mitigating Catastrophic Forgetting through Learned Perturbation Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Continual learning remains challenging due to catastrophic forgetting, where models lose performance on previous tasks when learning new ones. We propose Gradient Noise Rehearsal (GNR), a method that stores minimal task-specific information by learning to generate synthetic noise patterns that approximate the gradient directions of previous tasks. Unlike replay-based methods that require storing raw data, GNR maintains only a small set of learned noise parameters per task. Our experiments on split CIFAR-100 and sequential image classification tasks show that GNR achieves competitive performance (within 3-5% of state-of-the-art replay methods) while using 20x less memory. However, we observe that performance degrades significantly on tasks with substantial domain shift, and our method requires careful tuning of the noise generation hyperparameters. Code is available at [anonymous URL].",
    "id": 360
  },
  {
    "title": "Gradient Descent with Layerwise Learning Rates Improves Transformer Training Efficiency",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Thompson, S.",
      "Chen, L."
    ],
    "abstract": "We propose Layer-Adaptive Gradient Descent (LAGD), a simple modification to standard gradient descent that assigns different learning rates to different layers in Transformer architectures based on their relative sensitivity to updates. Our method computes layerwise gradient norms across mini-batches and adjusts learning rates using a lightweight heuristic that requires no hyperparameter tuning beyond the base learning rate. Experiments on Wikitext-103 and CIFAR-10 with standard Transformer architectures show 5-12% faster convergence compared to AdamW with careful tuning, while maintaining similar final performance. While the gains are consistent across tasks, they are most pronounced in early training phases. The method introduces minimal overhead (less than 1% increase in training time) and can be easily integrated into existing training pipelines. However, the theoretical justification remains heuristic, and benefits diminish with appropriate learning rate scheduling. We provide PyTorch implementations and hope this practical trick might be useful for researchers training Transformers with limited compute budgets.",
    "id": 361
  },
  {
    "title": "Adaptive Gradient Compression for Federated Learning with Non-IID Data via Learned Quantization Schedules",
    "authors": [
      "Chen, L.",
      "Johnson, M.",
      "Rodriguez, A."
    ],
    "abstract": "Federated learning faces significant challenges when client data follows non-IID distributions, particularly when combined with gradient compression techniques required for communication efficiency. In this work, we propose an adaptive gradient compression framework that dynamically adjusts quantization schedules based on local data characteristics. Our method employs a lightweight meta-network that predicts optimal compression rates for each client using only gradient statistics and local batch information, avoiding additional communication overhead. We introduce a novel regularization term that balances compression efficiency with convergence stability by penalizing quantization patterns that exacerbate client drift. Experiments on image classification and language modeling tasks show 1.2-2.1\u00d7 communication reduction compared to fixed-compression baselines while maintaining comparable accuracy on moderately non-IID partitions. However, performance degrades significantly on pathological non-IID splits where client datasets contain entirely disjoint classes. While our adaptive approach shows promise for practical federated scenarios, theoretical guarantees remain limited to bounded gradient assumptions that may not hold in practice. Code and preprocessed datasets are available at [anonymous link].",
    "id": 362
  },
  {
    "title": "Gradient Descent with Gradient Descent: Learning to Optimize with Adaptive Step Generators",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose Learned Adaptive Step-size Policy (LASP), a framework that uses a small neural network to predict step sizes for gradient-based optimization. Motivated by the observation that optimal learning rates vary substantially across neural network architectures and datasets, we train a lightweight MLP to generate step sizes conditioned on gradient statistics and training history. Rather than learning to optimize from scratch, LASP augments existing optimizers like Adam by providing per-parameter step recommendations. Our trained step generator achieves minor but consistent improvements over baselines on CIFAR-10 and ImageNet, yielding 1-2% accuracy gains and 10-15% reduction in training epochs. While our approach demonstrates the feasibility of meta-learning simple optimization policies, we find that learned step sizes do not generalize well across different model sizes or tasks, limiting broader applicability. Additionally, the computational overhead of the step generator neutralizes some training-time benefits. Our code is available at anonymous-url.github.io.",
    "id": 363
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning Through Loss Interpolation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.K.",
      "Jones, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that impede optimization, yet existing gradient surgery methods lack mechanisms to adaptively balance task objectives during training. We propose Gradient Surgery with Adaptive Memory (GSAM), a simple extension to gradient surgery methods that incorporates a small episodic memory buffer to locally interpolate between task losses. Our approach maintains per-task gradients computed on buffered samples, combining them through a learned convex combination that varies across training stages. On 8 multi-task vision benchmarks and 15 NLP tasks from the GLUE suite, GSAM achieves 1.2% average improvement over PCGrad and GradNorm while adding under 0.3% computational overhead. However, we find benefits diminish with larger models (\u22651B parameters) and our approach is sensitive to both buffer size and learning rate schedules. The method fundamentally trades memory for marginal gains, raising questions about its practical deployment. While GSAM provides a principled approach to adaptive gradient combination, its improvements are incremental and may not justify implementation complexity in resource-constrained settings.",
    "id": 364
  },
  {
    "title": "Temporal Ensembling Meets Adaptive Checkpointing: A Hybrid Approach for Efficient Semi-Supervised Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Semi-supervised learning has achieved remarkable success on vision benchmarks, yet many methods remain computationally expensive at scale. We propose a simple hybrid approach that combines temporal ensembling with adaptive checkpointing to reduce training costs without sacrificing accuracy. Our key insight is that uncertainty estimates from the exponential moving average (EMA) teacher can guide selective gradient computation: we checkpoint only the layers with high predictive uncertainty, recomputing others on-demand. This yields a 25-40% reduction in memory usage and 15-20% speedup over standard temporal ensembling on CIFAR-10/100 and ImageNet subsets, while maintaining accuracy within 0.5% of the baseline. While our method shows consistent improvements over naive implementations, we find diminishing returns beyond medium-scale datasets and acknowledge that our uncertainty-based gating introduces new hyperparameters. Code is available at [anonymized].",
    "id": 365
  },
  {
    "title": "Adaptive Thresholding for Gradient Clipping in Non-Stationary Optimization",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Ortiz, J."
    ],
    "abstract": "Gradient clipping is widely used to stabilize training of neural networks, but choosing optimal clipping thresholds remains challenging, especially when data distributions shift over time. We propose Adaptive Quantile Clipping (AQC), a simple modification that dynamically adjusts clipping thresholds based on gradient quantiles rather than fixed values or momentum-based estimates. AQC tracks gradient norms using exponential moving averages over multiple timescales (\u03c4 \u2208 {10, 100, 1000}) and selects thresholds that maintain a specified percentile (p = 0.95 by default) of the gradient distribution. Unlike prior work that requires tuning clipping constants or learning rates, AQC adapts automatically to changing curvature and noise levels. We evaluate AQC on language modeling, reinforcement learning, and federated learning benchmarks. Results show modest improvements over vanilla clipping: 2.3% average perplexity reduction on Wikitext-103, 8% faster convergence on CartPole-v1, and 12% lower variance in non-IID federated settings. However, gains diminish on well-tuned baselines and small-scale experiments. While AQC provides a practical alternative to manual tuning, its benefits are most pronounced in non-stationary or noisy environments, with limited impact on standard supervised learning tasks. Code is available at [anonymous link].",
    "id": 366
  },
  {
    "title": "Improving Transformer Efficiency through Iterative Token Pruning with Learnable Thresholds",
    "authors": [
      "Liu, K.",
      "Chen, J.",
      "Brown, M."
    ],
    "abstract": "While transformer models achieve state-of-the-art results across many domains, their quadratic complexity with respect to sequence length remains computationally prohibitive for long sequences. We propose Dynamic Token Pruning (DTP), a training-free method that iteratively removes low-salience tokens using learnable threshold parameters. Our approach builds upon existing attention-based pruning techniques but introduces two key innovations: threshold vectors that adaptively balance efficiency and accuracy across layers, and a feedback mechanism that re-incorporates pruned tokens when prediction confidence drops. We evaluate DTP on language modeling (WikiText-103), question answering (SQuAD), and image classification (ImageNet) tasks. Results show modest improvements over static baseline pruning methods, achieving 1.8\u00d7 average speedup with <2% accuracy degradation. However, we observe significant variance across tasks: while language modeling shows consistent gains, vision transformers exhibit more erratic behavior, particularly in later layers. Analysis reveals our method struggles with tasks requiring fine-grained spatial information, and the learned thresholds tend to be overly aggressive, removing potentially useful tokens. While DTP demonstrates the potential of adaptive pruning strategies, our empirical findings suggest that more sophisticated scoring mechanisms may be required to achieve reliable improvements across diverse domains.",
    "id": 368
  },
  {
    "title": "Gradient Surgery for Stabilizing Auxiliary Loss Learning in Sparse Reward Environments",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning with auxiliary objectives has shown promise for improving sample efficiency in reinforcement learning with sparse rewards. However, we observe that naively combining policy gradient updates with auxiliary losses often leads to destructive interference, particularly in high-dimensional action spaces. We propose Gradient Surgery for Auxiliary Loss stabilization (GSAL), a simple method that projects auxiliary gradients onto the null space of the policy gradient before applying updates. Our theoretical analysis shows that GSAL preserves the stationary points of the primary objective while incorporating useful curvature information from auxiliary tasks. We evaluate GSAL on a suite of continuous control tasks with sparse rewards, achieving 12-15% improvement over baseline methods on standard benchmarks. While the improvements are consistent across environments, we find that GSAL's effectiveness diminishes when auxiliary tasks are poorly aligned with the primary objective. The method requires minimal hyperparameter tuning and adds less than 5% computational overhead compared to standard policy gradient methods. Our results suggest that careful gradient alignment can provide modest but reliable gains in challenging RL settings, though the approach may be constrained by the quality of available auxiliary objectives.",
    "id": 369
  },
  {
    "title": "Self-Adjusting Step Sizes for Stochastic Gradient Descent via Loss Curvature Estimation",
    "authors": [
      "Liu, Q.",
      "Thompson, K.J.",
      "Mendoza, A."
    ],
    "abstract": "We propose CurvatureSGD, a variant of stochastic gradient descent that adapts step sizes using local curvature information estimated from recent gradient history. While adaptive optimizers like Adam and RMSprop rely on gradient magnitudes, our method uses a lightweight approximation of the Hessian diagonal through finite differences of gradients along the optimization trajectory. This allows per-parameter step size adjustment based on local convexity properties without the computational overhead of full second-order methods. We evaluate CurvatureSGD on image classification tasks using ResNet-18 on CIFAR-10/100 and vision transformers on ImageNet subsets. Results show modest improvements over SGD with momentum (0.5-1.2% accuracy gains) and comparable performance to Adam on most tasks, with 15-20% faster convergence in early epochs. However, the method shows instability on tasks with noisy gradients or sharp loss landscapes, and we observe occasional divergence without careful hyperparameter tuning. Our theoretical analysis provides convergence guarantees under standard assumptions but requires stronger smoothness conditions than typical for first-order methods. Code is available at anonymous-url.",
    "id": 370
  },
  {
    "title": "A Simple Regularization Technique for Improving Transformer Generalization on Small Datasets",
    "authors": [
      "Kim, S.",
      "Johnson, M.",
      "Rodriguez, C."
    ],
    "abstract": "Transformer models often struggle to generalize when training data is limited, a common scenario in scientific applications where labeled examples are expensive to obtain. We propose DropKey, a lightweight regularization method that randomly masks attention keys during training while preserving values and queries. Unlike standard dropout which treats all components equally, DropKey specifically targets the key representations, forcing the model to rely more heavily on local context and reducing overfitting to long-range dependencies. We evaluate DropKey on 8 small-scale datasets spanning NLP, vision, and tabular domains. When training from scratch with <10k examples, DropKey achieves 2-7% improvement over baselines without increasing inference cost. However, gains diminish with larger datasets or when using pre-trained models, suggesting the technique is most beneficial in data-scarce regimes. While DropKey is simple to implement and theoretically motivated by an information bottleneck perspective, we acknowledge its impact is modest and primarily affects edge cases. The method requires careful tuning of the key-drop probability and may interfere with tasks requiring precise attention to rare patterns. Our code and experiments are publicly available for reproducibility.",
    "id": 371
  },
  {
    "title": "LoRA-Adapted Transformers Can Be Compressed Further: A Structured Pruning Approach for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a practical solution for parameter-efficient fine-tuning of large language models, but the storage and inference costs of applying multiple LoRA adapters remain non-trivial. We propose Structured-LoRA-Prune (SLP), a simple yet effective compression technique that applies magnitude-based structured pruning to frozen base weights while preserving LoRA adapter weights. Our key insight is that post-LoRA training, certain attention heads and MLP neurons become less influential to the adapted model's performance. By analyzing the gradient flow through the frozen backbone, we identify and remove these redundant components without retraining the base model. Experiments on GLUE and SuperGLUE benchmarks using Llama-7B show SLP achieves 15-25% parameter reduction in the frozen weights while maintaining 97-99% of LoRA's original task performance. Crucially, this comes with 8-12% inference speedup on consumer GPUs. However, our approach shows diminishing returns with increasing LoRA rank (r > 64) and exhibits ~2% performance drop on reasoning-intensive tasks. While our method provides practical benefits for deployment, the theoretical implications of structural pruning in the presence of adapter weights require further investigation.",
    "id": 372
  },
  {
    "title": "Attention Bottlenecks in Transformer Memorization: A Low-Rank Analysis of Inductive Biases",
    "authors": [
      "Kumar, S.",
      "Liu, J.",
      "Thompson, E."
    ],
    "abstract": "We investigate how transformers memorize training sequences through analysis of attention patterns in overparameterized language models. Our theoretical analysis shows that memorization occurs through rank collapse in attention matrices, with rank dropping from O(d) to O(k) where k is the number of memorized patterns. We propose a low-rank regularization scheme that encourages rank collapse during training while maintaining downstream performance. Experiments on synthetic memotasks and WikiText-103 show our method reduces memorization by 15-20% without hurting perplexity, suggesting some memorization may be beneficial for generalization. However, our regularization does not improve performance on established benchmarks like GLUE. While our theoretical analysis provides insight into transformer memorization dynamics, our empirical evaluation is limited to language modeling and may not extend to other modalities. Code and experiments are available at our anonymized repository.",
    "id": 373
  },
  {
    "title": "When Label Smoothing Hurts: An Empirical Analysis of Confidence Calibration in Deep Networks",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Rodriguez, C."
    ],
    "abstract": "Label smoothing is widely adopted as a regularization technique in deep learning, but its impact on model calibration remains poorly understood. We investigate how label smoothing affects confidence calibration across vision and language tasks, revealing a surprising dichotomy: while label smoothing consistently improves calibration on in-distribution data (ECE reduction of 15-30%), it substantially degrades calibration under distribution shift (ECE increase of 40-80%). Through controlled experiments on CIFAR-10, ImageNet, and WikiText-2, we isolate this effect from other factors and demonstrate that the degradation stems from over-confident predictions on out-of-distribution examples. We propose a simple temperature scaling variant that selectively adjusts smoothed logits, partially mitigating the issue (30% improvement over naive label smoothing) without sacrificing accuracy. Our findings suggest that practitioners should reconsider blind application of label smoothing, particularly in settings with anticipated distribution shift.",
    "id": 374
  },
  {
    "title": "Adaptive Curriculum Learning for Few-Shot Classification via Task Similarity Clustering",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Curriculum learning has shown promise in few-shot classification by ordering tasks from simple to complex. However, existing approaches rely on heuristic difficulty metrics that remain fixed across datasets. We propose a novel adaptive curriculum learning method that clusters tasks based on their similarity in prototype space and dynamically adjusts the learning schedule. Our approach first learns task embeddings using a siamese network trained on query-support similarities, then applies spectral clustering to group related tasks. During meta-training, we gradually introduce harder clusters based on inter-cluster distance and intra-cluster variance. Experiments on mini-ImageNet, tiered-ImageNet, and CUB-200 demonstrate 2-3% improvements over baseline MAML and ProtoNets. While the method shows consistent gains across datasets, the clustering approach sometimes over-segments natural task groupings, and performance gains diminish when the number of classes per task increases beyond 5-way. Our findings suggest that adaptive scheduling via task similarity is beneficial but may be fundamentally limited by the quality of task representations learned during few-shot training.",
    "id": 375
  },
  {
    "title": "Reparameterized Momentum: Adapting Polyak's Heavy Ball to Modern Deep Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We revisit Polyak's classical Heavy Ball momentum method and propose Reparameterized Momentum (RepMom), a simple modification that adapts the momentum coefficient based on the relative scale of gradient norms across layers. Our key insight is that the optimal momentum parameter varies significantly across different magnitude scales in deep networks, leading to suboptimal convergence when using a global value. RepMom employs a lightweight heuristic that adjusts momentum per-parameter using running estimates of gradient statistics, without introducing additional hyper-parameters beyond those in standard SGD with momentum. We evaluate RepMom on image classification benchmarks (CIFAR-10/100, ImageNet) and language modeling tasks, showing 3-8% improvement in final accuracy over tuned baselines, particularly for deeper architectures. While we provide convergence guarantees for quadratic objectives under simplified assumptions, we acknowledge these results do not extend to the general non-convex case used in our experiments. Our method adds minimal computational overhead (5-10% increase in iteration time) and can be implemented in 15 lines of PyTorch code. Despite improvements over baselines, the gains are moderate and may be partially attributable to improved hyper-parameter sensitivity rather than fundamental algorithmic innovation.",
    "id": 376
  },
  {
    "title": "Gradient Descent with Iteratively Rescaled Gradients: A Simple Speedup Trick for Neural Networks",
    "authors": [
      "Liu, K.",
      "Kumar, S.",
      "Chen, J."
    ],
    "abstract": "We propose Iterative Gradient Rescaling (IGR), a novel optimization technique that adaptively reweights gradient components during training based on their historical magnitudes. Unlike adaptive methods like Adam, IGR does not maintain per-parameter learning rates but instead periodically rescales gradients for each layer using a running estimate of gradient norms from previous iterations. Our method requires no additional hyperparameters beyond standard SGD and adds minimal computational overhead (less than 1% increase in wall-clock time). We evaluate IGR on ResNet-50 and Vision Transformer training on ImageNet, as well as BERT fine-tuning on GLUE tasks. Results show 3-8% relative improvement in final validation accuracy compared to SGD with momentum in most settings, particularly when training budgets are limited to 100-200 epochs. However, IGR shows diminishing benefits with longer training and underperforms AdamW on certain transformer architectures. While our approach is simple to implement and provides consistent gains in resource-constrained settings, we acknowledge that the theoretical justification remains incomplete and the improvement margins are modest. Code is available at [url].",
    "id": 377
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Reinforcement Learning",
    "authors": [
      "Kumar, S.",
      "Zhou, L.",
      "Thompson, J."
    ],
    "abstract": "Multi-task reinforcement learning (MTRL) suffers from gradient conflicts between tasks that cause unstable training and negative transfer. While recent gradient surgery methods like PCGrad and GradDrop alleviate this by projecting conflicting gradients, they rely on instantaneous gradient information and ignore historical task relationships. We propose GAMER (Gradient surgery with Adaptive MEmoRy), which maintains an exponentially-decayed memory of task-specific gradients to compute more informed projection directions. Our method introduces a novel memory update rule that balances recent gradient information with historical context, along with an adaptive temperature parameter that controls the trade-off between task alignment and interference. Experiments on Meta-World and multi-task Atari environments show modest improvements: GAMER achieves 3-7% higher success rates compared to PCGrad across 10/15 tasks while reducing training variance by 12%. However, performance gains diminish with larger networks (>10M parameters), suggesting limitations in complex domains. Ablation studies reveal that memory size and decay rate significantly impact performance, with optimal hyperparameters varying substantially across environments. While GAMER provides a practical improvement over existing gradient surgery methods, its computational overhead (15% slower training) and sensitivity to hyperparameter choices limit its broad applicability. Code and hyperparameter settings are available at [anonymized link].",
    "id": 378
  },
  {
    "title": "Gradient Confusion Helps: Revisiting Optimization Dynamics via Label Noise Injection",
    "authors": [
      "Liu, S.",
      "Krishnan, K.",
      "Johnson, M."
    ],
    "abstract": "Recent work suggests that gradient confusion in stochastic optimization hampers convergence and generalization. We revisit this claim through the lens of deliberate label noise injection, which we show creates controlled gradient confusion that can actually improve optimization for overparameterized networks. Our theoretical analysis establishes that moderate label noise introduces beneficial regularization effects by decorrelating gradients across mini-batches, leading to implicit gradient compression similar to gradient sparsification techniques. We validate this empirically on CIFAR-10 and ImageNet, achieving 0.8-2.3% accuracy improvements compared to standard training, particularly on harder sub-distributions. While our theoretical bounds are limited to two-layer networks with Gaussian input assumptions, our experiments suggest the phenomena extend to practical settings. The method is simple to implement and adds minimal computational overhead, requiring only tuning a single noise level parameter. However, we find the improvements are task-specific and vanish in high-noise regimes, raising questions about the generality of our findings. Our results suggest re-evaluating how we understand gradient alignment in modern optimizers.",
    "id": 379
  },
  {
    "title": "Improved Gradient Estimation for Meta-Learning with Limited Inner-Loop Data",
    "authors": [
      "Chen, L.",
      "Anderson, K.",
      "Mukherjee, S."
    ],
    "abstract": "We propose a simple modification to gradient-based meta-learning algorithms that improves generalization when only small amounts of data are available for inner-loop adaptation. Our method, Subspace Meta-Gradient (SMG), projects the inner-loop gradient updates onto a learned low-dimensional subspace estimated from the support set statistics. This addresses a key limitation of MAML and its variants, which can produce noisy gradient estimates when adaptation data is scarce. We derive a closed-form update rule by modeling the subspace as a Gaussian mixture whose parameters are meta-learned. On few-shot classification benchmarks, SMG shows consistent improvements over baselines: 2-4% accuracy gains on 1-shot miniImageNet and 1-2% on tieredImageNet, while maintaining computational efficiency. Theoretical analysis reveals that SMG reduces the variance of meta-gradient estimates by approximately 30% compared to standard approaches. However, gains diminish with larger support sets, and we observe minimal improvement in the 5-shot regime. Our results suggest that gradient regularization through learned subspaces offers a practical enhancement to existing meta-learning pipelines, though the benefits are task-specific and may not justify implementation overhead in data-rich scenarios. Code will be made available post-review.",
    "id": 380
  },
  {
    "title": "Attentive Gradient Descent: Learning to Adapt Step Sizes with Self-Attention",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose Attentive Gradient Descent (AGD), a method that uses self-attention mechanisms to dynamically adjust step sizes during optimization. Motivated by the observation that gradient magnitudes vary significantly across dimensions and iterations in neural network training, AGD replaces fixed or heuristic learning rate schedules with a learned attention mechanism that scales gradients based on historical optima information. Our approach computes attention scores over past gradients and parameter states to produce dimension-wise step sizes. We evaluate AGD on standard image classification benchmarks (CIFAR-10/100) and language modeling tasks (WikiText-2), achieving 2-5% better accuracy compared to tuned baselines like AdamW and SGD with momentum, while reducing hyperparameter sensitivity. However, we observe that gains diminish with larger models, and the computational overhead becomes non-negligible. The method introduces 3 additional hyperparameters that require tuning, and while it performs well on standard benchmarks, we find limited improvement on more challenging datasets like ImageNet.",
    "id": 381
  },
  {
    "title": "Adaptive Gradient Clipping with Momentum: A Practical Approach to Stabilizing Transformer Training",
    "authors": [
      "Liu, J.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "Gradient clipping is widely used to stabilize the training of large language models, but fixed clipping thresholds often lead to suboptimal convergence or require extensive hyperparameter tuning. We propose Adaptive Gradient Clipping with Momentum (AGCM), a simple modification that dynamically adjusts clipping thresholds based on the momentum of per-layer gradient norms. Unlike prior adaptive methods that maintain running statistics, AGCM uses a lightweight rule that increases thresholds during stable training periods and reduces them during instability spikes. Our method requires only two additional hyperparameters with intuitive interpretations. We evaluate AGCM on both language modeling (GPT-2 350M) and vision transformers (ViT-B/16) across three scale ranges. Results show 15-25% reduction in training loss variance compared to standard clipping, with particularly strong improvements in early training stages. However, final convergence performance matches or marginally exceeds baseline methods, suggesting AGCM primarily improves training reliability rather than final accuracy. While our method shows practical benefits for researchers limited by computational resources, theoretical analysis reveals AGCM may introduce bias in certain optimization landscapes. Code is available at [anonymous URL].",
    "id": 382
  },
  {
    "title": "Improved Gradient Compression Schemes Through Adaptive Vector Quantization",
    "authors": [
      "Chen, L.",
      "Popescu, A.",
      "Nguyen, D."
    ],
    "abstract": "Communication bottlenecks remain a fundamental challenge in distributed deep learning, particularly when training large-scale models. We present an adaptive vector quantization approach for gradient compression that dynamically adjusts quantization parameters based on per-layer gradient statistics during training. Our method extends recent top-K sparsification techniques by incorporating learned quantization levels via a lightweight neural network that maps gradient distributions to optimal codebooks. We prove convergence guarantees for smooth non-convex objectives under common assumptions, achieving an O(1/\u221aT) rate with compression ratios up to 200\u00d7. On ImageNet training with ResNet-50, our approach achieves 50\u00d7 compression while maintaining 76.1% accuracy, improving upon static quantization methods by 0.3%. While our theoretical analysis requires bounded gradient assumptions that may not hold in practice, experiments on language modeling tasks show consistent improvements over existing methods. Our PyTorch implementation shows 1.2\u00d7 wall-clock speedup over 32 GPUs compared to uncompressed training, though benefits diminish at smaller scales. These results suggest adaptive quantization can provide practical communication savings in specific distributed training scenarios, though the approach may have limited utility for single-node training or small models.",
    "id": 383
  },
  {
    "title": "Revisiting Curriculum Learning through the Lens of Adaptive Difficulty via Neural Collapse",
    "authors": [
      "Chen, L.",
      "Ramos, J.",
      "Gupta, S."
    ],
    "abstract": "Curriculum learning has shown promise in improving neural network training, yet principled approaches for dynamically adjusting task difficulty remain elusive. We propose CurriculumNet, a method that leverages recent insights from neural collapse theory to automatically generate training curricula. Our approach monitors the evolution of class embeddings during training and adaptively samples mini-batches based on their deviation from ideal neural collapse structures. We introduce a difficulty score derived from within-class and between-class covariance metrics, allowing for fine-grained control over the learning progression. Experiments on CIFAR-10, CIFAR-100, and ImageNet-lite demonstrate modest improvements in final accuracy (1.2-2.3%) over baseline training, particularly in low-data regimes. However, computational overhead increases training time by 35-50%. While our method provides interpretable difficulty metrics, results are inconsistent across architectures and datasets. We argue these limitations stem from our simplifying assumption that neural collapse progression directly correlates with optimal curriculum ordering, motivating future work on more sophisticated difficulty measures. Code is available at [anonymized].",
    "id": 384
  },
  {
    "title": "Alleviating Forgetting in Continual Learning via Parameter-level Dropout Schedules",
    "authors": [
      "Liu, J.",
      "Chen, S.",
      "Rodriguez, A."
    ],
    "abstract": "Catastrophic forgetting remains a fundamental challenge in continual learning settings where models must sequentially learn multiple tasks. While dropout is widely used as a regularization technique, its potential to mitigate forgetting has been underexplored. We propose DropCL, a simple yet effective variant that adaptively schedules dropout rates for different parameters based on their sensitivity to previous tasks. Our method estimates parameter importance using a lightweight Fisher information approximation, then applies higher dropout rates to parameters deemed less critical. We evaluate DropCL on standard continual learning benchmarks including Split CIFAR-100 and Permuted MNIST, achieving modest improvements over baseline methods (average 2.3% accuracy gain). While DropCL shows promise, our analysis reveals that gains diminish significantly when tasks are highly dissimilar. Additionally, the method introduces computational overhead during training (approximately 15%) and requires task boundaries to be known in advance. Our contributions include a formal analysis of why dropout can reduce forgetting under certain conditions, and extensive ablations showing that coarse-grained dropout schedules perform nearly as well as our more sophisticated approach. Code and pre-trained models will be released upon acceptance.",
    "id": 385
  },
  {
    "title": "Gradient Amplification with Cautious Optimism: A Practical Memory-Efficient Second-Order Approach",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "We propose Gradient Amplification with Cautious Optimism (GACO), a memory-efficient variant of quasi-Newton methods for large-scale neural network training. While second-order optimizers show promise for faster convergence than Adam, their memory requirements typically scale quadratically with parameter count, limiting practical adoption. GACO approximates the inverse Hessian using a low-rank plus diagonal structure updated via infrequent curvature estimates, achieving linear memory scaling while maintaining second-order behavior. Our key insight is to amplify gradient updates only when the estimated curvature condition number falls below a dynamically adjusted threshold, preventing unstable updates. On ImageNet and WMT'14 translation tasks, GACO achieves 1.2-1.4x faster convergence than AdamW with comparable final accuracy, while using 7.3x less memory than full Shampoo. However, we observe mixed results on smaller datasets (e.g., GLUE tasks show no improvement), suggesting method sensitivity to scale and regularization interactions. Theoretical analysis reveals GACO maintains O(\u221a(Td)) regret bounds under restricted smoothness assumptions that may not hold in practice. Code and pre-trained models will be released upon publication.",
    "id": 386
  },
  {
    "title": "Improved Gradient Variance Bounds for Distributed Stochastic Optimization via Adaptive Batch Synchronization",
    "authors": [
      "Chen, K.",
      "Rodriguez, L.",
      "Kim, H."
    ],
    "abstract": "We present a distributed optimization framework that adaptively adjusts batch synchronization frequency to minimize gradient variance in stochastic gradient descent. While existing distributed methods typically use either synchronous or asynchronous updates, we propose a hybrid approach that dynamically switches between these modes based on local gradient statistics. Our method maintains a running estimate of gradient variance at each worker and triggers synchronization only when local estimates exceed a data-dependent threshold. This yields improved convergence rates compared to standard synchronous SGD while avoiding the instability issues common in fully asynchronous methods. On CIFAR-10 and ImageNet training tasks, our approach achieves 1.2-1.7\u00d7 speedup over baseline synchronous methods with comparable generalization. Theoretical analysis shows our variance bounds degrade gracefully with network delays, though we rely on assumptions about gradient Lipschitz continuity that may not hold in practice. Our results suggest adaptive synchronization can reduce communication overhead in moderately-sized clusters (\u226432 workers), though benefits diminish as network heterogeneity increases.",
    "id": 387
  },
  {
    "title": "Stochastic Mirror Descent with Memory-Augmented Gradients for Non-Stationary Streaming Data",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Online learning with non-stationary data streams remains challenging when the underlying distribution shifts are unknown and unpredictable. We propose M-SMD, a variant of stochastic mirror descent that augments gradient estimates with a novel memory mechanism inspired by experience replay buffers in reinforcement learning. Our approach maintains a dynamic window of past gradients and computes adaptive weights based on gradient similarity, effectively creating an implicit distribution drift detector. Theoretical analysis provides a regret bound of O(\u221a(T log T)) under mild assumptions about the drift rate, matching existing methods while removing the need for prior knowledge of drift magnitude. On three streaming datasets (synthetic concept drift, rotating MNIST, and real-world financial data), M-SMD achieves 12-18% better cumulative loss compared to strong baselines including online gradient descent and drift-detection-based methods. However, the memory mechanism introduces additional hyperparameters whose sensitivity varies across domains. While our approach shows promise for practical streaming applications, the theoretical guarantees rely on the restrictive assumption of bounded gradient memory, and the computational overhead grows linearly with memory size. Code available at [anonymous link].",
    "id": 388
  },
  {
    "title": "Lookahead Makes Batch Normalization Unnecessary: Empirical Investigation on ImageNet",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Batch Normalization (BN) has become standard in deep learning, but its effectiveness is challenged by recent optimizers. We investigate whether the Lookahead optimizer can eliminate the need for BN across vision architectures. Through 270 experiments on ImageNet, we find that removing BN from ResNet-50 leads to a 0.8% drop in top-1 accuracy when using Lookahead with AdamW, but only 0.3% with SGD. Our key insight is that Lookahead's slow weights provide similar regularization to BN's noise. We provide theoretical analysis showing that the lookahead step implicitly controls parameter variance. However, our method fails for very deep networks (>200 layers) and exhibits unstable training with large learning rates. While not universally successful, our results suggest BN-free training is viable for moderate-scale vision tasks. We release our PyTorch implementation and 50 pretrained models.",
    "id": 390
  },
  {
    "title": "LoRA-Hash: Parameter-Efficient Fine-Tuning with Learnable Hash Tables",
    "authors": [
      "Chen, L.",
      "Rodriguez, S.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular method for parameter-efficient fine-tuning of large language models, but its effectiveness diminishes when targeting multiple tasks simultaneously due to interference between low-rank matrices. We propose LoRA-Hash, a simple extension that replaces static low-rank matrices with learned hash tables that map input tokens to rank-1 updates. This allows different regions of the input space to use distinct adaptation parameters while sharing a common hash function. On the GLUE benchmark, LoRA-Hash achieves 0.8% higher average accuracy than standard LoRA when training on multiple tasks with a 2\u00d7 memory reduction. However, we observe significant variance across tasks (\u00b14.2%), suggesting that hash-based adaptation may not be universally beneficial. Analysis reveals that performance gains are concentrated in tasks with clear input-output patterns, while tasks requiring fine-grained reasoning show degradation. Our method requires minimal code changes to existing LoRA implementations and introduces only 0.1M additional parameters. While LoRA-Hash shows promise for multi-task scenarios, we acknowledge limitations in theoretical understanding and the need for better initialization strategies.",
    "id": 391
  },
  {
    "title": "Gradient Noise Revisited: A Modified Learning Rate Schedule for Deep Network Training",
    "authors": [
      "Chen, S.",
      "Rodriguez, M.",
      "Liu, H."
    ],
    "abstract": "Recent work has shown that noise in stochastic gradient descent (SGD) plays a crucial role in navigating the complex loss landscapes of deep neural networks. While adaptive optimization methods like Adam and RMSprop have gained popularity, we argue that careful scheduling of the base learning rate in vanilla SGD can achieve comparable performance with less hyperparameter tuning. Building on the recently proposed 'superconvergence' phenomenon, we introduce Periodic Cyclical Exponential (PCE) decay, a learning rate schedule that alternates between rapid decay phases and high-variance exploration periods. Our method injects controlled noise through sudden learning rate jumps, allowing the optimizer to escape sharp minima while maintaining stable convergence. We evaluate PCE decay on standard image classification benchmarks (CIFAR-10/100, ImageNet) and demonstrate 0.5-1.2% accuracy improvements over tuned baselines, though gains diminish with very deep architectures. The approach requires minimal additional hyperparameters but depends on problem-specific tuning of the cycle length. Our theoretical analysis shows that PCE decay approximates a modified Langevin dynamics, providing partial convergence guarantees for strongly convex functions. While our empirical results are encouraging, we acknowledge that performance improvements are incremental rather than transformative, and the method's effectiveness varies across network architectures and datasets. Code is available at anonymous-link.",
    "id": 392
  },
  {
    "title": "LoRA-Drop: Efficient Low-Rank Adaptation through Dynamic Parameter Pruning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its fixed rank allocation across layers may lead to suboptimal resource utilization. We propose LoRA-Drop, a simple yet effective approach that dynamically prunes LoRA parameters during fine-tuning based on gradient magnitude analysis. Our method uses a threshold-based pruning schedule that removes low-contribution rank components while maintaining model performance. Across experiments on GLUE, SuperGLUE, and domain-specific tasks, LoRA-Drop reduces trainable parameters by 15-30% compared to standard LoRA with <1% accuracy degradation on most tasks. However, we observe inconsistent results on reasoning-intensive benchmarks, where aggressive pruning sometimes harms performance. We provide theoretical analysis showing that our pruning criterion approximates an upper bound on the perturbation of gradient flow. While LoRA-Drop achieves modest efficiency gains, our findings highlight the challenge of adaptive rank selection without task-specific tuning. Code and pretrained checkpoints are available at [anonymous link].",
    "id": 393
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning method for large language models, but its fixed-rank design often leads to suboptimal trade-offs between performance and efficiency across different tasks and layers. We propose LoRA-Drop, a simple method that adaptively selects rank for each LoRA module using a task-specific importance score derived from gradient flow analysis. Our approach adds negligible computational overhead (under 2% increase in training time) and can be integrated into existing LoRA implementations with fewer than 20 lines of code. On the GLUE benchmark with RoBERTa-large, LoRA-Drop achieves 0.8-1.2% improvement over standard LoRA while using 15-30% fewer parameters across tasks. While these gains are consistent, they remain incremental compared to stronger baselines like AdaLoRA (1.5-2.1% improvement) or full fine-tuning (3-4% gap). Analysis reveals that LoRA-Drop's effectiveness varies significantly across model architectures, with limited benefits on larger models (>7B parameters). Our method provides a lightweight alternative when computational constraints are severe, though we acknowledge the improvements are modest in absolute terms.",
    "id": 394
  },
  {
    "title": "Improving Transformer Language Models with Frequency-Aware Position Embeddings",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Standard positional encodings in Transformers treat all token positions equally, potentially missing the observation that different frequency components in natural language exhibit distinct positional patterns. We propose Frequency-Aware Position Embeddings (FAPE), which decomposes input sequences via discrete Fourier transform and learns separate positional representations for low and high frequency components. Our method adds only 0.1% trainable parameters while enabling better modeling of long-range dependencies in low-frequency signals and local syntactic patterns in high-frequency components. On WikiText-103 and the Pile, FAPE achieves modest perplexity improvements (0.8-1.2%) over vanilla Transformers but shows more substantial gains (3-4%) on artificially constructed sequences with controlled frequency characteristics. While our ablation studies suggest the benefits mainly come from low-frequency components, we find the high-frequency adaptations occasionally hurt performance on standard benchmarks. Our code and pretrained models are available, though replication requires careful tuning of the frequency decomposition threshold.",
    "id": 395
  },
  {
    "title": "Momentum-Scheduled Adam: Improving Convergence via Adaptive Gradient Memory",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Thompson, B."
    ],
    "abstract": "We propose Momentum-Scheduled Adam (MS-Adam), a simple modification to Adam that schedules its momentum parameters using curriculum learning principles. While Adam remains the de facto optimizer for training neural networks, its performance often degrades for tasks with sparse gradients or non-stationary objectives. Our approach introduces scheduled decay rates that adaptively adjust momentum based on gradient history, theoretically guaranteeing convergence for a broader class of non-convex objectives than standard Adam. We derive convergence bounds showing MS-Adam achieves O(1/T) rates under milder assumptions than previously established. Experimental evaluation on CIFAR-10/100 image classification shows 0.8-1.1% accuracy improvements over Adam baselines, and 5-7% faster convergence on language modeling tasks. However, these gains diminish on larger-scale benchmarks like ImageNet, where MS-Adam performs comparably to AdamW with careful hyperparameter tuning. Our analysis reveals the schedule's effectiveness correlates strongly with gradient sparsity patterns, suggesting limited utility for dense-gradient regimes. While the method offers modest improvements in specific settings, the computational overhead and additional hyperparameters may outweigh benefits for many applications. Code is available at anonymous.url/ms-adam.",
    "id": 396
  },
  {
    "title": "Improving Transformer Efficiency through Selective Layer Dropping during Training",
    "authors": [
      "Chen, L.",
      "Kim, S.",
      "Rodriguez, M.",
      "Johnson, A."
    ],
    "abstract": "We propose LayerDrop-Train, a simple method to reduce computational costs when training large transformers by dynamically dropping intermediate layers based on learned gating parameters. Unlike existing pruning approaches that operate post-training, our method progressively learns which layers to skip during the forward pass, effectively creating shorter paths through the network. We introduce a soft gating mechanism trained with straight-through gradient estimation that determines layer usage per sequence. Experiments on standard NLP benchmarks (GLUE, WMT) show 15-25% training time reduction with minimal performance degradation (within 1-2% of baseline). Analysis reveals our method primarily drops middle layers, suggesting redundancy in standard transformer architectures. While our approach provides practical training speedups for modest model sizes (\u2264350M parameters), we observe training instability for larger models and find the benefits diminish with increased parallelization. Code and pre-trained models will be available upon acceptance.",
    "id": 397
  },
  {
    "title": "Gradient Dropout: A Simple Regularization Technique via Dynamic Gradient Masking",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We introduce Gradient Dropout (GradDrop), a lightweight regularization method that randomly masks gradients during backpropagation to prevent overfitting in neural networks. Unlike traditional dropout, which operates on activations, GradDrop selectively drops gradient contributions at each layer, effectively creating an ensemble of dynamic subnetworks during training. Our method adds minimal computational overhead and requires only a single hyperparameter: the gradient dropout rate. Through experiments on CIFAR-10, CIFAR-100, and ImageNet, we demonstrate consistent improvements over baseline models by 0.5-1.2% accuracy across different architectures (ResNet-18, ResNet-50, Vision Transformer). Ablations reveal that GradDrop is particularly effective for medium-sized datasets and moderately deep networks, though gains diminish for very large models. While GradDrop achieves competitive performance with existing regularizers like DropBlock and Stochastic Depth, our theoretical analysis remains limited to shallow networks, and comprehensive evaluation on diverse domains is needed. Code is available at [URL].",
    "id": 398
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation via Gradient-Based Pruning for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Garcia, M.",
      "Johnson, K."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its fixed rank allocation often leads to suboptimal performance-compute trade-offs across different layers. We propose LoRA-Drop, a simple gradient-based pruning method that dynamically allocates ranks during training by removing LoRA modules with small gradient norms. Our approach requires only one additional hyperparameter\u2014the pruning ratio\u2014and incurs minimal computational overhead. Extensive experiments on GLUE and E2E benchmarks show LoRA-Drop achieves comparable performance to standard LoRA while using 25-40% fewer parameters. However, we find the method exhibits high variance across random seeds and tasks, with particularly unstable behavior on smaller datasets. While our empirical results demonstrate the potential for adaptive rank allocation, we acknowledge the lack of theoretical justification for gradient norm as a pruning criterion. Our approach offers a practical way to reduce LoRA's parameter count, though we recommend careful hyperparameter tuning and multiple runs for reliable results.",
    "id": 399
  },
  {
    "title": "Gradient Descent with Periodic Sparsification: A Memory-Efficient Approach for Low-Rank Adaptation",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "We propose PSGD (Periodic Sparsification Gradient Descent), a simple modification to standard gradient-based optimization that periodically enforces sparsity on model updates during fine-tuning. Our method applies magnitude-based pruning to gradients every k steps, maintaining only the top-p fraction of gradient components before applying the update. This approach is motivated by the observation that during fine-tuning of pre-trained models, most gradient energy concentrates in a small subset of parameters. We evaluate PSGD on parameter-efficient fine-tuning tasks across vision and language domains, including LoRA adaptation of large language models. Experiments on GLUE benchmarks and CIFAR-10/100 show 1.3-2.1\u00d7 memory reduction during training with <2% performance degradation compared to standard fine-tuning. While the memory savings are consistent, we observe that performance gaps widen on more complex tasks like MMLU (4.7% drop), suggesting limitations of our simple magnitude-based pruning strategy. Theoretical analysis reveals PSGD converges at rate O(1/\u221aT) under standard convexity assumptions, matching standard SGD up to a constant factor. Code is available at [anonymous link].",
    "id": 400
  },
  {
    "title": "Gradient Surgery for Low-Rank Adaptation: A Lightweight Approach to Mitigating Catastrophic Forgetting in Continual Learning",
    "authors": [
      "Liu, J.",
      "Chen, M.",
      "Thompson, K."
    ],
    "abstract": "We present LoRA-Surgery, a method that combines low-rank adaptation with gradient projection to address catastrophic forgetting in continual learning scenarios. While low-rank adaptation (LoRA) has shown promise for efficient fine-tuning, its effectiveness in continual learning remains limited due to interference between task gradients. Our key insight is that gradient conflicts can be mitigated by projecting gradients onto the null space of previously learned tasks while maintaining the computational efficiency of LoRA. We propose a simple projection operator that operates directly on the low-rank decomposition matrices, avoiding expensive gradient decomposition during training. Experiments on standard continual learning benchmarks (Split-CIFAR-100, 5-dataset) show modest improvements over standard LoRA (+3.2% average accuracy) and competitive results with more complex methods. While our approach achieves reasonable performance with minimal computational overhead, we observe sensitivity to hyperparameter choices and limited benefits in high-capacity regimes. Code will be released upon acceptance.",
    "id": 401
  },
  {
    "title": "Gradient Surgery in Federated Learning: When Do Top Clients Really Help?",
    "authors": [
      "Liu, Q.",
      "Garcia, M.",
      "Kim, J."
    ],
    "abstract": "Federated learning systems traditionally aggregate client gradients uniformly, but emerging work suggests that prioritizing updates from high-performing clients might accelerate convergence. We investigate this phenomenon through the lens of gradient interference, proposing a simple client selection mechanism that weights updates by their historical training loss reduction. Our method requires no additional communication rounds and introduces minimal computational overhead compared to standard FedAvg. We evaluate our approach across vision and language tasks on heterogeneous data partitions, observing 5-12% faster convergence in early training phases compared to baselines. However, we find these gains diminish as training progresses, with final test accuracy differences becoming statistically insignificant on 7 out of 9 datasets. Our theoretical analysis reveals that the benefit of client selection is fundamentally limited by the alignment of local and global objectives\u2014a condition that becomes less favorable with increased data heterogeneity. While our method provides practical benefits in specific settings (particularly when client participation rates are low), our results caution against the universal applicability of client prioritization strategies. Code will be provided for reproducibility.",
    "id": 402
  },
  {
    "title": "Improved Generalization Bounds for Neural Networks via Data-Augmentation Dependent Complexity Measures",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We present new generalization bounds for neural networks that explicitly depend on the data augmentation strategy used during training. While existing bounds treat augmented samples as independent, our analysis captures the dependencies introduced by augmentation transformations through a novel complexity measure based on the augmentation graph Laplacian. Our theoretical framework yields bounds that scale as O~(\u221a(\u03b3(G)/n)), where \u03b3(G) is a graph-dependent quantity that reflects the augmentation policy's diversity. On CIFAR-10 and CIFAR-100 with common augmentation techniques, our bounds improve upon prior work by 15-30% in vacuous regimes. However, the bounds remain vacuous by 15-20% for typical network architectures. Our empirical study examines the relationship between augmentation strength and generalization, revealing cases where increased augmentation hurts our bounds despite improving test performance. While our approach provides new insights into augmentation's role in generalization, current limitations include the inability to handle color-based transformations analytically and the need for a validation set to tune \u03b3(G) hyperparameters. Code and proofs are available at https://anonymous.github.io/aug-bound.",
    "id": 403
  },
  {
    "title": "Gradient Surgery Meets Lottery Tickets: Interpolating Pruning Techniques for Overparameterized Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "We investigate whether combining magnitude-based pruning with gradient-based optimization can yield neural networks that are simultaneously sparse and robust to catastrophic forgetting in continual learning settings. Our approach, Sparse Gradient Surgery (SGS), applies iterative magnitude pruning to identify a sparse subnetwork, then fine-tunes this subnetwork using gradient projection techniques to maintain performance on previously seen tasks. While we demonstrate that SGS achieves competitive compression ratios (up to 95% sparsity) on CIFAR-10 with minimal accuracy loss compared to dense baselines, our results on continual learning benchmarks yield mixed outcomes. Specifically, SGS provides modest improvements in backward transfer (1-3% over standard fine-tuning) but fails to match the performance of more sophisticated regularization methods in forward transfer scenarios. Our theoretical analysis suggests that the effectiveness of SGS is highly dependent on the alignment between the sparse subnetwork's gradient subspace and the optimization trajectories of sequential tasks. Though our findings validate the intuition that sparse networks can serve as better feature extractors for continual learning, the practical gains remain limited to specific experimental configurations. We make our code and checkpoints publicly available to facilitate further investigation into the interplay between sparsity and plasticity in neural networks.",
    "id": 404
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: A Lightweight Alternative to Full Gradient Accumulation in Federated Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Gradient compression is essential in federated learning to reduce communication overhead, but existing methods often suffer from slow convergence or require large client-side memory. We propose Gradient Surgery with Adaptive Memory (GSAM), a novel compression scheme that selectively retains gradient directions based on their historical importance scores while discarding others via top-k sparsification. Unlike full gradient accumulation methods, GSAM maintains a lightweight adaptive memory buffer (typically 1-5% of model size) that dynamically updates through exponential moving averages of gradient statistics. Our theoretical analysis shows GSAM achieves convergence rates within logarithmic factors of uncompressed baselines under standard smoothness assumptions. Across four federated datasets, GSAM reduces communication by 50-80% compared to FedAvg while maintaining accuracy within 1-2% of the uncompressed baseline. However, we observe performance degradation in highly non-IID settings (accuracy drops of 3-5%) and memory requirements that scale poorly with model size. While GSAM demonstrates practical benefits for resource-constrained clients, its theoretical guarantees under relaxed assumptions remain limited.",
    "id": 405
  },
  {
    "title": "LoRA-FA: Low-Rank Adaptation with Feature Alignment for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Nguyen, K.",
      "Singh, A."
    ],
    "abstract": "We propose LoRA-FA, a simple modification to Low-Rank Adaptation (LoRA) that improves performance on downstream tasks by aligning low-rank adapter features with pre-trained representations. While LoRA has become popular for parameter-efficient fine-tuning, we observe that learned adapter directions often diverge from semantically meaningful directions in the pre-trained model. LoRA-FA addresses this by adding a lightweight feature alignment loss that encourages adapter features to align with the span of top singular vectors from pre-trained layers. Our method introduces only 3% additional parameters compared to standard LoRA and requires minimal computational overhead. We evaluate LoRA-FA on GLUE, SuperGLUE, and vision-language tasks, showing average improvements of 1.2% over standard LoRA, with particularly strong gains on low-resource datasets. However, we find the benefits diminish for larger models (>30B parameters) and saturate quickly with increased adapter rank. While the feature alignment constraint slightly reduces training efficiency (10% slower convergence), it provides more stable fine-tuning across hyperparameters. Our experiments suggest LoRA-FA offers modest but consistent improvements for practical deployment scenarios, though we acknowledge the gains are incremental rather than transformative.",
    "id": 406
  },
  {
    "title": "Improving Transformer Efficiency Through Attention Heads Recycling with Learnable Dropout",
    "authors": [
      "Liu, S.",
      "Chen, M.",
      "Johnson, K."
    ],
    "abstract": "We propose HeadRecycle, a simple yet effective method for reducing computational cost in pre-trained transformers by dynamically identifying and skipping redundant attention heads during inference. Our approach uses a lightweight auxiliary network trained with reinforcement learning to assign recycling scores to each attention head based on their relative importance for specific inputs. Unlike prior pruning methods that permanently remove heads, HeadRecycle maintains full model capacity while achieving 15-30% reduction in FLOPs through selective computation. We evaluate on standard NLP benchmarks including GLUE, SQuAD, and WikiText-103. On BERT-base, HeadRecycle achieves 1.2x speedup with only 0.8% performance degradation, outperforming static magnitude-based pruning baselines. While the results are encouraging, our method requires additional training overhead and shows limited effectiveness on downstream tasks requiring complex reasoning. Code and pre-trained models will be released upon acceptance.",
    "id": 407
  },
  {
    "title": "A Simple Truncation Strategy for Transformers Improves Long-Range Dependencies with Minimal Computational Overhead",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "While transformers have achieved remarkable success across domains, their quadratic complexity in sequence length remains a fundamental bottleneck for long-range modeling tasks. We propose LengthSelect, a surprisingly simple truncation strategy that selectively removes tokens based on learned importance scores derived from the attention distribution of a single forward pass. Unlike prior approaches requiring architectural changes or expensive sparsity patterns, our method can be implemented in 15 lines of PyTorch and applies to any pre-trained transformer without fine-tuning. Experiments on language modeling, document classification, and protein sequence tasks show 1.2-2.4\u00d7 speedups at 8K-32K context lengths with perplexity degradations under 3% compared to full attention. However, we observe the approach underperforms on tasks requiring fine-grained positional reasoning, and theoretical analysis reveals our approximation error scales inversely with the effective rank of the attention matrix\u2014limiting benefits for highly entangled sequences. Despite these limitations, LengthSelect offers practitioners a lightweight alternative to structured attention mechanisms, with ablations suggesting most gains come from removing redundant syntactic tokens rather than the learned scoring itself.",
    "id": 408
  },
  {
    "title": "Improving Convergence of Policy Gradient Methods via Adaptive Trust Region Scaling",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Roberts, C."
    ],
    "abstract": "Policy gradient methods remain a cornerstone of deep reinforcement learning, yet their convergence properties in high-dimensional settings remain poorly understood. We introduce a simple modification to existing policy gradient algorithms that adaptively scales the trust region based on the local smoothness of the value function. Our method, Trust Region Adaptive Policy Optimization (TRAPO), computes an online estimate of the Hessian trace to adjust the trust region radius without introducing significant computational overhead. We evaluate TRAPO on standard continuous control benchmarks and find modest improvements over PPO (2-5% on Mujoco tasks) while maintaining computational efficiency. Theoretical analysis reveals convergence guarantees under restricted Lipschitz continuity assumptions, though we note these assumptions are difficult to verify in practice. Experiments on the more challenging AntMaze domain show mixed results, with TRAPO achieving better final performance but slower convergence than baseline methods. Our results suggest adaptive trust region scaling can provide small but consistent benefits, though the gains are task-dependent and may not justify the added complexity for all applications. Code is available at [anonymized for review].",
    "id": 409
  },
  {
    "title": "Revisiting Weight Pruning at Initialization Through the Lens of Neural Tangent Kernel Evolution",
    "authors": [
      "Liu, S.",
      "Chen, J.",
      "Nguyen, H."
    ],
    "abstract": "Recent work has shown that neural networks can be pruned at initialization without harming final performance, challenging the lottery ticket hypothesis which suggests pruning must occur after training. We provide theoretical and empirical evidence that the effectiveness of pruning at initialization depends critically on the alignment between the sparse sub-network and the evolving Neural Tangent Kernel (NTK) during early training. Our analysis reveals that standard magnitude-based pruning scores correlate poorly with NTK alignment, particularly in deeper networks. We propose a simple alternative scoring method based on second-order NTK approximations that achieves comparable accuracy to pruning after training while using 50% fewer parameters. Experiments on CIFAR-10/100 and ImageNet demonstrate improvements of 2-4% over existing pruning-at-initialization methods. However, our approach introduces overhead from kernel computations and shows strong dataset-dependent variance. While our theoretical analysis is limited to networks in the NTK regime, empirical results suggest broader applicability. This work suggests that viewing pruning through the NTK lens provides new insights, though practical limitations remain. Code and pretrained models will be released.",
    "id": 410
  },
  {
    "title": "LoRA++: A Memory-Efficient Adaptation of Low-Rank Adaptation for Multi-Task Transfer Learning",
    "authors": [
      "Chen, S.",
      "Rodriguez, J.",
      "Kim, H."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its effectiveness diminishes when transferring to multiple related tasks simultaneously. We propose LoRA++, a simple extension that introduces cross-task knowledge sharing through a shared low-rank basis while maintaining task-specific sparse updates. Our method adds only 2.7% additional parameters over standard LoRA across 8 vision and NLP benchmarks. While we demonstrate modest improvements in average accuracy (+1.2% over LoRA, +0.3% over full fine-tuning), our primary contribution lies in identifying the scenarios where multi-task LoRA adaptations provide benefits versus when they introduce interference. Through controlled experiments on task similarity metrics, we show LoRA++ helps most when tasks share >60% feature similarity, but can hurt performance otherwise. Our theoretical analysis provides a bound on the approximation error introduced by the shared basis, though the bound is loose and may not explain empirical observations. Code and pre-trained adapters will be released upon acceptance.",
    "id": 411
  },
  {
    "title": "Rethinking Dropout in Transformers: A Frequency-Domain Perspective",
    "authors": [
      "Liu, K.",
      "Brown, S.",
      "Kumar, V."
    ],
    "abstract": "While dropout is widely used to regularize transformer networks, its interaction with multi-head attention mechanisms remains poorly understood. We propose FreqDrop, a dropout variant that operates in the frequency domain of attention maps rather than the standard element-wise masking. Our key insight is that traditional dropout disrupts low-frequency attention patterns that are crucial for long-range dependencies, while preserving noisy high-frequency components. FreqDrop addresses this by learning to selectively mask frequency bands based on their spectral energy, effectively providing adaptive regularization that varies across layers. Experiments on standard NLP benchmarks (GLUE, SQuAD) show modest improvements of 0.3-0.7% over standard dropout when using large pretrained models, with particularly strong gains on longer sequences (>512 tokens). However, improvements shrink considerably when training from scratch, and computational overhead increases by 15-20% due to FFT operations. While our approach offers a principled view of attention regularization, the practical benefits appear limited to specific regimes. Code and pretrained models are available at anonymized-url.",
    "id": 412
  },
  {
    "title": "LoRA++: Incremental Improvements to Low-Rank Adaptation via Dynamic Scaling",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.A.",
      "Kim, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto standard for parameter-efficient fine-tuning of large language models, yet its fixed rank structure may limit expressivity for downstream tasks. We propose LoRA++, a simple extension that introduces learnable, input-dependent scaling factors for each low-rank update matrix. Specifically, our method computes a lightweight gating mechanism based on the input embeddings to dynamically adjust the contribution of each rank component during inference. Experiments on GLUE and SuperGLUE benchmarks show modest improvements over standard LoRA (0.7-1.2% average gains) while adding only 0.3% additional parameters. However, our approach shows diminishing returns on larger models (>7B parameters) and fails to consistently outperform full fine-tuning on reasoning tasks. Analysis reveals the learned scaling factors often collapse to near-uniform values, suggesting limited utilization of the dynamic capacity. While LoRA++ provides some benefits over vanilla LoRA for small-scale applications, the gains do not clearly justify the added complexity for most practitioners. Code is available at [placeholder].",
    "id": 413
  },
  {
    "title": "Gradient Descent with Lookahead Buffering: A Simple Extension for Improved Convergence in Non-Convex Optimization",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Chen, S.",
      "Anderson, M."
    ],
    "abstract": "We propose Lookahead Buffering (LB), a lightweight modification to standard gradient descent that maintains a small history of previous gradients to guide future updates. By linearly combining current and past gradients within a moving window, LB introduces momentum-like behavior without the hyperparameter sensitivity of traditional momentum methods. Our approach requires only storing k=3-5 additional gradient vectors (O(kd) memory) and can be implemented with three extra lines of PyTorch code. We prove convergence guarantees for LB on strongly convex objectives, achieving O(1/T) rates that match gradient descent. Empirically, LB shows consistent but modest improvements across various tasks: 2-4% faster convergence on ImageNet training, 1.5% absolute accuracy gains on CIFAR-10/100, and 8-12% sample efficiency improvements in tabular RL environments. While these results demonstrate promise, we acknowledge the improvements are incremental rather than transformative. Our ablation studies reveal LB is most beneficial when combined with adaptive optimizers like Adam, suggesting its primary contribution may be as a plug-and-play enhancement rather than a standalone algorithm.",
    "id": 414
  },
  {
    "title": "LoRa-Clipped: Improving Low-Rank Adaptation for Large Language Models via Gradient Clipping",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning technique for large language models, but its performance remains unstable across different training configurations and hyperparameters. We identify that gradient explosion during LoRA training is a primary source of this instability, particularly when adapting to data-scarce domains. We propose LoRa-Clipped, a simple yet effective modification that applies adaptive gradient clipping to the low-rank matrices while preserving the computational efficiency of standard LoRA. Through experiments on three benchmark tasks (instruction tuning, domain adaptation, and few-shot classification), we show LoRa-Clipped reduces training instability by 35% compared to vanilla LoRA while maintaining comparable performance on most tasks. On GPT-2 Medium and LLaMA-7B, our method achieves average improvements of 2.1% on downstream metrics when training data is limited to 1K examples. However, we observe diminishing returns on larger models (LLaMA-13B and above) and when sufficient training data (\u226510K examples) is available. While our approach is computationally lightweight and requires only two additional hyperparameters, the improvements are incremental rather than transformative, and the method appears most beneficial in resource-constrained training scenarios.",
    "id": 415
  },
  {
    "title": "Efficient Gradient Descent with Dynamic Learning Rate Recycling",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Anderson, J."
    ],
    "abstract": "Modern optimization methods often discard past gradient information, potentially wasting computational resources. We propose Gradient Recycling (GR), a simple modification to standard SGD that stores and reuses previous gradients through a dynamic weighting scheme based on cosine similarity with the current gradient direction. Our method requires minimal memory overhead (storing only k=5-10 past gradients) and one extra hyperparameter. We provide theoretical analysis showing GR achieves similar convergence rates to SGD for convex objectives, though our bounds contain additional terms that scale with the recycling parameter. Empirically, GR demonstrates 5-15% faster convergence on ResNet training for CIFAR-10 and ImageNet compared to vanilla SGD with momentum, but improvements are inconsistent across architectures and datasets. Ablation studies reveal performance degrades when k>20, suggesting limited benefit from longer gradient memory. While GR offers modest practical gains on some tasks, we acknowledge its benefits may not justify the implementation complexity for most practitioners. Code is available at [anonymized for review].",
    "id": 416
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Merging with Curriculum Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Transformer models have achieved remarkable success across domains, but their quadratic complexity with respect to sequence length remains a critical bottleneck. While previous work has explored token pruning and merging heuristics to reduce computational cost, these methods often rely on fixed rules or require expensive pre-training adaptations. We propose a learnable token merging framework that dynamically reduces sequence length during fine-tuning using a lightweight gating network. Our method introduces curriculum scheduling that gradually increases compression ratios throughout training, stabilizing optimization while achieving significant speedups. Specifically, we train a small MLP to predict merge decisions for consecutive token pairs, guided by auxiliary losses that preserve information content. Across three NLP benchmarks (GLUE, SQuAD, and CNN/DM), our approach reduces inference FLOPs by 25-40% with minimal degradation (<1% relative) on most tasks. However, we observe more substantial performance drops (3-5%) on tasks requiring fine-grained reasoning (e.g., QQP, MNLI). Analysis reveals our merging policy tends to aggressively compress syntactic markers necessary for complex reasoning. While our method offers a practical trade-off between efficiency and performance, the learned merging strategies show limited generalization across domains, suggesting the need for task-specific tuning. Code and pre-trained models are available at [URL].",
    "id": 417
  },
  {
    "title": "Gradient Surgery Meets Sharpness: A Simple Weight Averaging Scheme for Improved Generalization",
    "authors": [
      "Liu, J.",
      "Garcia, M.",
      "Thompson, K."
    ],
    "abstract": "Recent work has shown that gradient conflicts between mini-batches can adversely affect training dynamics and final performance. While techniques like PCGrad and GradDrop mitigate these conflicts by projecting conflicting gradients, such approaches typically trade off optimization speed for stability. We propose SharpDrop, a lightweight modification that applies selective gradient dropout based on the sharpness of the loss surface estimated via gradient variance. Our method retains the simplicity of standard SGD with momentum while incorporating insights from sharpness-aware minimization and gradient surgery. Empirically, SharpDrop improves test accuracy by 1-2% on CIFAR-100 and ImageNet subsets over baseline SGD, and matches performance of more computationally expensive sharpness-aware optimizers. However, gains diminish on larger architectures and we observed high sensitivity to hyperparameters. Ablation studies reveal that the sharpness estimation component contributes modest improvements beyond standard gradient clipping. While theoretically motivated, our analysis relies on simplified assumptions about the loss landscape that may not hold in practice. Our code attempts to reproduce all claimed results.",
    "id": 418
  },
  {
    "title": "Learning to Warm-Start: A Differentiable Approach to Initialization Transfer",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Optimization warm-starting can significantly reduce training time when transferring knowledge from related tasks, but current methods rely on handcrafted heuristics that may not generalize across problem domains. We propose Differentiable Initialization Transfer (DIT), a meta-learning framework that learns how to map optimal parameters from source tasks to effective initializations for target tasks. Our key insight is to treat parameter initialization as a differentiable function of source solutions, implemented as a lightweight neural network that operates on parameter space. We evaluate DIT on transfer learning benchmarks spanning computer vision and NLP tasks, demonstrating 15-40% speedups in convergence time compared to standard initialization schemes. While our method shows consistent gains on problems with structural similarity, performance degrades when source and target tasks diverge significantly. We provide theoretical analysis showing convergence guarantees under limited distributional shift. Our experiments reveal that learned initializations trade off some final accuracy for faster convergence, suggesting applications in resource-constrained or few-shot settings. Code and pretrained initializers will be released upon publication.",
    "id": 419
  },
  {
    "title": "Gradient Descent with Momentum is Provably Better Than Adam for Overparameterized Linear Regression",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kumar, A."
    ],
    "abstract": "We study the optimization dynamics of momentum gradient descent (MGD) versus Adam in the overparameterized linear regression setting. While Adam is widely adopted for neural network training, its theoretical advantages over MGD remain poorly understood in convex settings. We prove that for overparameterized linear regression with Gaussian features, MGD achieves a convergence rate of O(\u03ba log T/\u221aT) compared to Adam's O((\u03ba/\u03b5) log T/\u221aT), where \u03ba is the condition number and \u03b5 is the Adam hyperparameter. The key insight is that Adam's adaptive stepsizes can harm convergence when gradients are small but consistent. Our analysis exploits the alignment between momentum and the principal components of the data covariance matrix. We verify our findings on synthetic data and MNIST logistic regression, observing 15-25% faster convergence for MGD when \u03ba \u2265 100. These results suggest that Adam's benefits may be limited to specific regimes, and provide theoretical justification for simpler optimizers in interpolating convex problems.",
    "id": 420
  },
  {
    "title": "Gradient Surgery for Partially Aligned MAML with Application to Few-Shot Domain Adaptation",
    "authors": [
      "Chen, L.",
      "Vaswani, A.",
      "Kim, S."
    ],
    "abstract": "Meta-learning methods like MAML struggle when training and test distributions share components but differ in others\u2014a common scenario in real-world few-shot learning. We propose Partial-MAML, which adapts gradient-based meta-learning to settings where feature spaces are partially aligned between source and target domains. Our method applies principled gradient surgery to modify MAML's inner loop, selectively updating parameters based on their estimated alignment scores computed via domain adversarial training. On standard benchmarks, Partial-MAML achieves modest improvements (1-2% absolute) over vanilla MAML in domain-aligned cases while maintaining MAML's performance on standard tasks. We also introduce a new benchmark for few-shot domain adaptation with controlled alignment ratios. While our theoretical analysis provides convergence guarantees under restrictive assumptions, we acknowledge these may not hold in practice. Our method requires additional hyperparameters to tune and increases computational cost by approximately 15%. Code and benchmarks will be released upon acceptance.",
    "id": 421
  },
  {
    "title": "Towards Practical Few-Shot Learning via Learned Data Augmentation Schedules",
    "authors": [
      "Liu, K.",
      "Thompson, J.",
      "Chen, M."
    ],
    "abstract": "Data augmentation is crucial for few-shot learning, but current methods use fixed augmentation policies that may not adapt to the limited data regime. We propose MetaAugSched, a meta-learning approach that learns augmentation schedules conditioned on the support set statistics. Our method trains a hypernetwork to predict augmentation parameters (intensity, probability, and type selection) for each few-shot episode, optimizing for post-adaptation validation performance. While prior work focuses on learning augmentation policies on large datasets, MetaAugSched specifically targets the few-shot setting where augmentation effects are more unpredictable. On standard few-shot benchmarks (miniImageNet, tieredImageNet), our method achieves 2-3% absolute improvement over strong baselines, though gains diminish with larger support sets. We provide theoretical analysis showing our approach minimizes an upper bound on expected risk when augmentation preserves class-conditional distributions. However, our method adds non-negligible computational overhead during meta-training and shows sensitivity to hyperparameter choices. Code and pretrained models are available at [url].",
    "id": 422
  },
  {
    "title": "Momentum Residual Networks with Learnable Skip Connections for Improved Optimization Landscape",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "We propose Momentum Residual Networks (MoResNets), a variant of residual networks that incorporates learnable momentum coefficients within skip connections. Motivated by recent observations that residual connections can be viewed as discretizations of ordinary differential equations, we formalize skip connections as parameter-dependent momentum operators that adaptively control information flow between layers. Our approach introduces a simple temperature-scaled sigmoid gate on each skip connection, allowing the network to dynamically adjust the effective depth during training while maintaining computational efficiency. We prove that under certain regularity conditions, MoResNets preserve the gradient flow properties of standard residual networks while potentially enabling better optimization trajectories. Experiments on CIFAR-10/100 and ImageNet show modest improvements over baseline ResNets (0.5-1.2% top-1 accuracy) at similar parameter counts, with more pronounced gains on deeper architectures (>50 layers). However, we observe that the benefits diminish for architectures equipped with existing normalization techniques. Our empirical analysis suggests that the effectiveness of learnable momentum is highly task-dependent, particularly sensitive to initialization schemes and learning rate schedules. While the theoretical framework provides insights into residual network dynamics, the practical impact appears limited compared to architectural improvements proposed in concurrent work.",
    "id": 423
  },
  {
    "title": "Improved Convergence Rates for Distributed SGD with Gradient Compression via Lightweight Error Feedback",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Chen, M."
    ],
    "abstract": "We revisit the problem of gradient compression in distributed stochastic optimization, proposing a simple variant of error feedback (EF) called EF-Lite that reduces memory overhead while preserving convergence guarantees. While existing EF methods typically require storing full-precision gradient residuals, our approach maintains compressed residuals using a lightweight quantization scheme, reducing memory by up to 8x with minimal computational overhead. Our theoretical analysis shows that EF-Lite achieves convergence rates comparable to vanilla EF for smooth non-convex objectives, though with slightly worse dependence on the compression ratio. Empirically, we demonstrate that EF-Lite matches the accuracy of full-precision training on ResNet-50 and Transformer models while achieving 2-4x communication speedups. However, we observe that performance degrades for extreme compression ratios (>100x), suggesting inherent limitations of the approach. Our results provide evidence that memory-efficient error feedback is viable for practical distributed training, though the benefits may be more modest than previously suggested.",
    "id": 424
  },
  {
    "title": "Scheduled Mixup: A Curriculum-Based Approach to Interpolated Training for Improved Out-of-Distribution Robustness",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Wang, K."
    ],
    "abstract": "While mixup has become a standard technique for improving generalization in deep neural networks, its effectiveness varies significantly across architectures and datasets. We propose Scheduled Mixup, a curriculum-based variant that gradually adjusts the interpolation strength \u03bb based on training progress and sample difficulty. Our method computes \u03bb \u223c Beta(\u03b1, \u03b1) where \u03b1 is scheduled from 0.2 to 1.0 using a power-law function, prioritizing harder interpolations as training progresses. We evaluate on CIFAR-10-C and ImageNet-R, showing 2.7% and 1.4% improvements in mCE over standard mixup baselines. However, we find the benefits diminish on larger models (ViT-L/32) and certain corruption types (fog, brightness). Our theoretical analysis suggests the improvement stems from controlling the Jensen gap during training, though we acknowledge gaps in our convergence guarantees. Code is available at anonymous.url.",
    "id": 425
  },
  {
    "title": "Revisiting Gradient Clipping in Federated Learning: A Signal-to-Noise Ratio Analysis",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Gradient clipping is widely used in federated learning to mitigate the impact of client drift caused by heterogeneous data distributions. However, the choice of clipping threshold is typically heuristic and lacks theoretical justification. We propose a signal-to-noise ratio (SNR) based framework for adaptive gradient clipping in federated optimization. Our approach dynamically adjusts the clipping threshold based on the estimated ratio between true gradient signal and client-update noise. Through experiments on CIFAR-10 and FEMNIST datasets, we demonstrate that SNR-based clipping reduces communication rounds by 18% compared to fixed clipping baselines, while maintaining comparable test accuracy. While our theoretical analysis provides convergence guarantees under standard assumptions, the bounds are slightly worse than vanilla SGD. The proposed method is simple to implement and requires minimal additional computational overhead, making it practical for real federated deployments. Our results suggest that adaptive clipping can improve training efficiency, though benefits are most pronounced in highly heterogeneous settings.",
    "id": 426
  },
  {
    "title": "Gradient Alignment Regularization for Improving Transfer Learning in Small-Data Regimes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J.",
      "Okafor, C."
    ],
    "abstract": "Pre-trained models often fail to adapt effectively when fine-tuned on datasets with very few examples. We propose Gradient Alignment Regularization (GAR), a simple regularization term that encourages the gradients during fine-tuning to align with those from the pre-training phase. By penalizing the cosine distance between current and pre-training gradients, GAR prevents catastrophic forgetting while allowing task-specific adaptation. We provide theoretical justification showing GAR bounds the deviation from pre-trained representations under mild assumptions. Experimental results on 8 benchmark datasets show 2-5% improvements over standard fine-tuning when labeled data is extremely scarce (<1% of original dataset), with comparable or worse performance in moderate-data regimes. While GAR introduces minimal computational overhead and works with any pre-trained model, its benefits appear limited to ultra-low-data settings, raising questions about broader applicability. Code is available at [URL].",
    "id": 427
  },
  {
    "title": "Revisiting Curriculum Learning with Adaptive Difficulty Balancing",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Curriculum learning has shown promise in improving model generalization by gradually increasing task difficulty. However, existing approaches rely on hand-crafted schedules or fixed heuristics that may not adapt to dataset-specific characteristics. We propose an adaptive curriculum learning framework that dynamically balances the rate at which training examples are introduced based on their local consistency with respect to the current model. Our method uses a momentum-based estimator to track example-wise learning signals, adjusting their presentation probabilities without requiring prior domain knowledge. We evaluate our approach on vision and language benchmarks with standard architectures. Results show modest improvements (0.3-1.2% accuracy gains) over baseline training on 4 of 7 datasets, with particularly strong performance on noisy label scenarios. While these gains are encouraging, we observe that the benefits diminish with larger models and longer training horizons. Our findings suggest that adaptive curricula may be most effective when computational budgets are constrained. Code and pretrained models are available at anonymous URL.",
    "id": 428
  },
  {
    "title": "Improving Transformer Efficiency through Gradient-Based Token Dropping During Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Transformer models face computational bottlenecks during fine-tuning due to quadratic complexity in sequence length. We propose GradDrop, a simple yet effective method that dynamically drops less informative tokens based on their gradient magnitudes during fine-tuning. Our approach computes token importance scores from attention gradients and removes a fixed percentage of tokens at each layer, reducing FLOPs by up to 30% on GLUE tasks. While previous token pruning methods require extensive retraining or complex routing networks, GradDrop integrates seamlessly into existing fine-tuning pipelines with minimal modifications. Experiments on BERT-base and RoBERTa-base show modest improvements in efficiency (1.3x speedup) with average performance degradation of 1.2% across GLUE tasks. Ablation studies reveal the method works best for longer sequences (>512 tokens) but provides diminishing returns on shorter texts. We also find that dropping strategies based solely on gradient norms can be unstable during early training stages. Despite these limitations, GradDrop offers a lightweight alternative to more sophisticated compression techniques, particularly useful for practitioners with computational constraints.",
    "id": 429
  },
  {
    "title": "Improving Transformer Efficiency through Learnable Sparse Attention Patterns with Fixed Memory Budgets",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose FixedBudget Transformer (FBT), a method for reducing memory usage in transformers by learning sparse attention patterns subject to hard memory constraints. While existing sparse attention mechanisms reduce complexity from O(n\u00b2) to O(n log n) or O(n\u221an), their memory usage still scales poorly for long sequences. FBT jointly learns attention sparsity patterns and a routing network that maps queries to keys within a fixed memory budget, enforced via projected gradient descent. The key insight is to frame attention computation as a constrained optimization problem where only k key-value pairs can be attended to per query, with k specified in advance. Experiments on language modeling and machine translation tasks show FBT achieves 2.3\u00d7 memory reduction compared to full attention while maintaining 95-97% of baseline performance. However, results reveal a trade-off: performance degrades significantly on tasks requiring long-range dependencies (>4k tokens). Additional analysis indicates learned patterns exhibit limited transferability across domains. While FBT provides practical memory savings for resource-constrained scenarios, the scope improvements are incremental rather than transformative.",
    "id": 430
  },
  {
    "title": "Gradient Descent with Momentum Scheduling: A Theoretical Analysis of Practical Heuristics",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Despite the widespread use of momentum-based optimizers in deep learning, the classical constant-momentum formulation remains largely unchanged since its introduction. Recent empirical work has shown that scheduling momentum during training can improve convergence speed, but theoretical understanding remains limited. We provide convergence guarantees for momentum scheduling in convex and non-convex settings, analyzing commonly-used heuristics including linear decay, cosine scheduling, and one-cycle policies. Our theoretical framework reveals that the effectiveness of these heuristics depends crucially on the Lipschitz smoothness of the objective and the initial momentum value. We prove that for quadratic objectives, optimal momentum scheduling converges faster than constant momentum by a factor logarithmic in the condition number. Experimental results on CIFAR-10 and ImageNet demonstrate consistent improvements over constant momentum (1-3% accuracy gains), though the benefits diminish with adaptive optimizers like Adam. Our analysis also highlights cases where momentum scheduling can hurt convergence, particularly for ill-conditioned problems. While our theoretical contributions are limited to simplified settings, they provide the first principled justification for momentum scheduling heuristics used in practice.",
    "id": 431
  },
  {
    "title": "Gradient Surgery Revisited: A Simpler Approach to Multi-Task Optimization with Improved Convergence Guarantees",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent gradient surgery methods like PCGrad and GradDrop offer practical solutions, they lack theoretical justification and can be overly conservative. We propose Adaptive Gradient Projection (AGP), a simpler approach that projects each task's gradient onto the orthogonal complement of conflicting gradients weighted by their relative magnitudes. We prove that AGP converges to a Pareto stationary point at rate O(1/T) under standard assumptions, improving upon the O(1/\u221aT) rate of existing methods. Experiments on three vision-language benchmarks and two multi-objective reinforcement learning tasks demonstrate 2-4% improvement over baselines while reducing computational overhead by 35%. However, we observe that benefits diminish when task gradients are nearly orthogonal, and our theoretical results assume Lipschitz-smooth objectives. These findings suggest AGP is most effective when task conflicts are moderate rather than extreme.",
    "id": 432
  },
  {
    "title": "LoRA-X: Learning Accelerated Low-Rank Adaptation via Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.K.",
      "Kim, S.",
      "Johnson, T."
    ],
    "abstract": "Low-rank adaptation (LoRA) has become a popular method for efficiently fine-tuning large language models, but its fixed rank selection across layers remains suboptimal. We present LoRA-X, an extension that dynamically adjusts the rank during training through a learnable gating mechanism. Our approach introduces lightweight meta-networks that predict optimal ranks for each layer based on gradient statistics, theoretically improving the bias-variance tradeoff in adaptation. We evaluate LoRA-X on instruction tuning tasks using Llama-2 7B and 13B models, achieving modest improvements of 3.2% average accuracy over standard LoRA on GSM8K and MT-Bench, while reducing trainable parameters by 15%. However, gains are inconsistent across tasks, with negligible improvement on code generation benchmarks. Our ablation study reveals that 60% of rank adjustments remain within 2 ranks of the baseline, suggesting limited learning dynamics. While LoRA-X demonstrates potential for more efficient adaptation, our results indicate that simple heuristics for rank selection may be nearly as effective. We discuss conditions under which dynamic rank selection provides benefits and highlight the computational overhead of our approach as a limitation for practical deployment beyond 30B parameter models.",
    "id": 433
  },
  {
    "title": "Momentum-Scheduled SAM: Improving Sharpness-Aware Minimization with Curriculum-Based Update Magnitudes",
    "authors": [
      "Kim, S.",
      "Rodriguez, A.",
      "Liu, J."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising optimizer for improving generalization, but its computational cost and sensitivity to the perturbation radius \u03c1 remain practical limitations. We propose Momentum-Scheduled SAM (MS-SAM), a lightweight modification that dynamically adjusts \u03c1 using a momentum-based curriculum derived from gradient statistics. Unlike previous work that requires computing additional Hessian information, our scheduler estimates local sharpness through exponential moving averages of gradient norms, eliminating extra backward passes. We evaluate MS-SAM on CIFAR-10/100 and ImageNet, achieving competitive accuracy improvements over vanilla SAM (up to 0.8% on CIFAR-100) while reducing training time by 15-20%. However, we observe diminishing benefits on larger architectures like ViT-L/16 and mixed results on out-of-distribution robustness benchmarks. Our theoretical analysis reveals that MS-SAM converges under assumptions slightly stronger than standard SAM, though these assumptions are verified empirically on image classification tasks. While not a universal improvement, our method provides a computationally efficient alternative for medium-scale vision tasks where training budget is constrained.",
    "id": 434
  },
  {
    "title": "Gradient Surgery for Improving Transfer Learning in Small-Scale Vision Models",
    "authors": [
      "Liu, J.",
      "Kim, S.",
      "Brown, M."
    ],
    "abstract": "We investigate whether gradient manipulation techniques developed for multitask learning can improve transfer learning performance on small-scale vision tasks. While existing transfer learning methods typically fine-tune pre-trained models using standard gradient descent, we hypothesize that explicitly constraining the fine-tuning gradients to maintain similarity with pre-trained features could improve generalization, particularly in low-data regimes. We propose Gradient Projection Transfer (GPT), a simple method that projects task gradients onto the subspace spanned by pre-trained representations during fine-tuning. Our experiments on CIFAR-10/100 and a subset of ImageNet (100 classes) show that GPT achieves 1-3% accuracy improvements over standard fine-tuning baselines when using 1-10% of the training data, with diminishing returns on larger datasets. However, we find that similar improvements can be achieved by simply reducing the learning rate. Theoretical analysis suggests our method provides implicit regularization equivalent to \u21132 weight decay under certain assumptions. While our results indicate limited practical advantage over careful hyperparameter tuning, the method's simplicity and theoretical connections may be of interest to the transfer learning community.",
    "id": 435
  },
  {
    "title": "Memory-Efficient Training of Large Language Models via Structured Gradient Compression",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Johnson, K."
    ],
    "abstract": "We propose GradientSketch, a method for reducing memory usage during transformer training by compressing gradients through structured low-rank approximations. Our approach represents gradients as products of smaller matrices learned via iterative sketching, achieving up to 4\u00d7 reduction in optimizer state memory compared to standard Adam while maintaining model quality. We derive theoretical bounds showing convergence rates within a factor of log(d) of uncompressed training for convex objectives, where d is parameter dimension. Experiments on GPT-2 (124M-1.5B parameters) and BERT-Large show perplexity degradation within 2-3% of baselines on WikiText-103 and GLUE benchmarks. While our method demonstrates practical training memory savings, we observe increased sensitivity to hyperparameters (particularly rank selection) and worst-case slowdown of 15-20% in training time. GradientSketch provides a practical trade-off between memory efficiency and training stability, though extending to larger models (10B+) remains challenging due to accumulated approximation errors. Code and hyperparameters are available at [redacted-for-review].",
    "id": 436
  },
  {
    "title": "Revisiting Momentum in Stochastic Optimization: A Probabilistic Interpretation with Improved Adaptive Bounds",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Momentum-based optimizers remain central to modern deep learning, yet theoretical understanding of their stochastic behavior lags behind empirical success. We propose a probabilistic re-interpretation of Polyak momentum as a Bayesian filtering problem, yielding novel adaptive hyperparameter schedules. Our key insight connects momentum coefficient \u03b2 to the signal-to-noise ratio of mini-batch gradients, enabling dynamic adjustment without additional hyperparameters. Through a second-moment analysis of the filtering equations, we derive improved convergence bounds for non-convex objectives that tighten existing O(1/\u221aT) rates to O(log T/T) under the PL inequality. Experiments on ResNet training demonstrate consistent but modest improvements (0.5-1.2% accuracy) over AdamW and SGD+Momentum across CIFAR-10/100 and ImageNet subsets, with particular gains in low-data regimes. While our theoretical contributions provide new perspective on momentum dynamics, empirical improvements are incremental and sensitive to architecture choices. We opensource our PyTorch implementation for reproducibility.",
    "id": 437
  },
  {
    "title": "Gradient Surgery via Layer-wise Binary Masks: A Lightweight Approach to Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that hinder performance across tasks. While existing methods like PCGrad and GradNorm address this through complex gradient modifications, we propose a surprisingly simple alternative: surgically masking gradients at the layer level using learnable binary gates. Our method identifies gradient conflict through cosine similarity between task-specific gradients, then applies element-wise binary masks (0/1) to selectively preserve or discard gradient components at each layer. No hyperparameter tuning for loss weighting is required. On standard benchmarks including NYUv2 and CIFAR-100 with 5-task splits, our approach achieves comparable performance to PCGrad (+0.8% average improvement) while using only 0.1% additional parameters. However, we find the method can be unstable when task gradients are highly aligned (cosine similarity > 0.9), leading to occasional training divergence. Our analysis reveals that the binary masks tend to over-regularize shared representations, limiting the approach's effectiveness when tasks have strong synergies. Code and pretrained models are available at anonymous-icml-submission.github.io.",
    "id": 438
  },
  {
    "title": "Gradient Surgery Doesn't Always Help: A Critical Look at Multi-Task Optimization in Shared Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning with shared representations promises improved efficiency and generalization, but conflicting gradients between tasks often lead to suboptimal performance. Recent gradient surgery techniques like PCGrad and GradNorm have shown promising empirical results for resolving gradient conflicts. We provide both theoretical and empirical evidence that these methods can unexpectedly harm performance in seemingly benign settings. First, we prove convergence rates for gradient surgery methods that reveal an implicit bias toward tasks with larger gradient magnitudes, potentially neglecting smaller but important tasks. Second, we identify families of problems where gradient surgery provably increases regret compared to standard multi-task training. Our experimental results on standard benchmarks (Multi-MNIST, CityScapes, and a new synthetic benchmark we introduce) show that gradient surgery techniques underperform naive baselines in 35% of tested configurations, particularly when tasks have moderate gradient alignment (cosine similarity between 0.3-0.7). We propose a lightweight diagnostic based on gradient statistics that predicts when gradient surgery will help or hurt, achieving 78% accuracy on our testbed. While our theoretical framework currently applies only to convex settings, we hope these findings encourage more nuanced application of gradient modification techniques in multi-task learning.",
    "id": 439
  },
  {
    "title": "Don't Throw Away Your Model Just Yet: Improving Transfer Efficiency with Selective Fine-tuning in Low-Resource Settings",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "Pre-trained models offer promise for low-resource tasks, but naive fine-tuning often leads to overfitting and catastrophic forgetting when data is extremely limited. We propose Selective Layer Transfer (SLT), a simple method that identifies and updates only the most relevant layers for a target task while keeping others frozen. Using attention scores from a held-out validation set, SLT adaptively determines which layers contribute most to task performance. We evaluate SLT on 15 NLP benchmarks with 50-500 training examples, comparing against full fine-tuning, adapters, and linear probing. SLT achieves 2-4% improvements over baselines on 8 datasets while training 35% fewer parameters. However, benefits diminish as data increases, and performance plateaus on 3 tasks where full fine-tuning remains superior. Analysis reveals SLT particularly helps when source and target domains differ moderately\u2014not when they are nearly identical or completely distinct. While our method provides practical gains in data-scarce scenarios, theoretical understanding remains limited. Code and hyperparameters are provided for reproducibility.",
    "id": 440
  },
  {
    "title": "LoRA-Plus: Incremental Low-Rank Adaptation with Scheduled Momentum for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S.",
      "Thompson, A."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become a popular method for parameter-efficient fine-tuning of large language models, but its effectiveness varies significantly across downstream tasks. We propose LoRA-Plus, an extension that introduces momentum-based updates with a novel scheduling mechanism that gradually increases the rank during fine-tuning. Our method adds only 2-3% additional parameters beyond standard LoRA while showing improved performance on a subset of GLUE tasks, particularly those requiring semantic understanding rather than surface pattern matching. Experiments across 8 diverse tasks reveal an average improvement of 1.8% over vanilla LoRA, with particularly strong gains on CoLA (3.2%) and moderate improvements on MRPC (2.1%). However, performance remains comparable or slightly worse on tasks like SST-2 and QNLP. Ablation studies suggest the momentum component contributes more than the dynamic rank scheduling, though both provide complementary benefits. While LoRA-Plus offers a practical improvement over existing methods, our theoretical analysis reveals fundamental limitations in the low-rank adaptation paradigm for certain classes of downstream tasks. Code and pretrained adapters will be released upon publication.",
    "id": 441
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Frequency-Domain Perspective",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kumar, V."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance. While existing gradient surgery methods manipulate gradients in the spatial domain, we propose analyzing gradient conflicts through the lens of frequency-domain representations. By decomposing gradients into their frequency components using discrete Fourier transforms, we show that gradient interference primarily occurs in specific frequency bands. This insight leads to a simple but effective algorithm: Frequency-Aware Gradient Surgery (FAGS), which selectively attenuates gradient components in conflicting frequency ranges while preserving others. Experiments on standard multi-task benchmarks (CIFAR-10/100, NYUv2) show modest improvements over baselines: 2.1% average accuracy improvement and 4.3% MT-D-W gain without hyperparameter tuning. While our method provides a new perspective on gradient surgery, we acknowledge limitations in scalability to large-scale models and theoretical gaps in understanding why frequency decomposition helps. Code and experiments are available at anonymous-github.com/fags-ml.",
    "id": 442
  },
  {
    "title": "Improved Convergence Rates for Mirror Descent with Learned Bregman Divergences",
    "authors": [
      "Chen, L.",
      "Krishnamurthy, A.",
      "Bouchard, G."
    ],
    "abstract": "We present a practical framework for accelerating mirror descent by learning the Bregman divergence adaptively during optimization. While mirror descent's convergence depends critically on the choice of distance-generating function, existing approaches either use fixed divergences or require expensive per-iteration updates that negate theoretical benefits. Our method jointly trains a lightweight neural network that outputs divergence parameters conditioned on the current iterate, achieving O(1/T) convergence for convex Lipschitz functions without manual tuning. On strongly convex objectives, we improve the rate to O(log(T)/T) when the learned divergence approximates the geometry well. Empirically, we demonstrate 2-3x speedups over standard mirror descent and competitive performance with Adam on logistic regression and matrix completion tasks using synthetic and UCI datasets. However, our theoretical analysis relies on an L-smoothness assumption for the learned mapping that may not hold in practice. Our results suggest adaptive Bregman divergences merit further investigation, though we acknowledge the convergence guarantees are weaker than recent adaptive gradient methods and the approach adds computational overhead.",
    "id": 443
  },
  {
    "title": "LoRA-Prop: Low-Rank Adaptation for Efficient Gradient Propagation in Large Language Models",
    "authors": [
      "Chen, L.",
      "Singh, P.",
      "Garcia, M."
    ],
    "abstract": "While low-rank adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method for large language models, understanding how these low-rank updates affect gradient propagation remains poorly understood. We introduce LoRA-Prop, a theoretical framework that analyzes gradient flow through low-rank adaptation layers using tools from matrix perturbation theory. Our key insight is that LoRA updates create implicit regularization effects that can either help or hinder learning depending on the conditioning of the pre-trained weights. We derive bounds on the gradient norms and provide empirical evidence that standard LoRA initialization can lead to gradient vanishing in deeper layers. To address this, we propose a simple modification to LoRA initialization based on singular value scaling. Experiments on GLUE and SuperGLUE benchmarks show our initialization scheme achieves comparable performance to standard LoRA (average 1.2% improvement) while requiring 15% fewer training steps. However, we find these improvements are inconsistent across tasks and model sizes. Our work provides theoretical grounding for understanding LoRA's training dynamics but leaves open questions about the interplay between rank selection and downstream task complexity.",
    "id": 444
  },
  {
    "title": "Gradient Curriculum Learning: A Simple Trick for Improving Transformer Training Stability",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Transformer training often suffers from unstable optimization dynamics, particularly in the early phases when attention patterns are still forming. We propose gradient curriculum learning (GCL), a surprisingly simple approach that anneals gradient magnitudes during the initial training stages. Our method applies a learnable scalar multiplier to the overall loss gradient, scheduled by a cosine decay for the first 10% of training steps. While the idea appears almost trivial, we demonstrate consistent improvements across 6 diverse language modeling and vision tasks, achieving 2-3% better final perplexity on average and notably reducing training variance across seeds. Theoretically, we provide a toy 2-layer linear network analysis suggesting GCL can help escape poor local minima. However, our experiments reveal the gains are strongest for small models (\u2264125M parameters) and diminish as scale increases. GCL adds no inference overhead and requires trivial implementation changes, making it practical for resource-constrained practitioners. While we acknowledge the contribution is incremental and the theoretical analysis is limited, we believe GCL represents a useful training trick that could benefit many practitioners, particularly those working with modest computational budgets.",
    "id": 445
  },
  {
    "title": "LayerNorm Without the Norm: A Simpler Alternative That Sometimes Works",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Layer normalization has become a standard component in transformer architectures, but its computational overhead during training and inference remains non-trivial. We propose Affine-Shift, a parameter-free alternative that replaces LayerNorm with a learned affine transformation initialized near identity. While theoretically removing the benefits of normalization, experiments on small-to-medium language models (up to 350M parameters) show comparable perplexity to baseline transformers in 65% of random initializations, with 15% faster training time. However, we observe high variance across seeds, with 35% of runs exhibiting training instability. Analysis suggests Affine-Shift poorly handles extreme activation values that LayerNorm would typically rescale. We prove that our method preserves gradient flow under restrictive assumptions, though these conditions often fail in practice. Experiments on GLUE fine-tuning yield mixed results: -0.5 average score compared to standard models, with particularly poor performance on tasks requiring long-range dependencies. Code and pretrained checkpoints are available.",
    "id": 446
  },
  {
    "title": "Improving Transformer Efficiency Through Selective Attention Freezing",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, S."
    ],
    "abstract": "While transformers have achieved remarkable success across numerous domains, their computational overhead grows quadratically with sequence length due to the attention mechanism. We propose a simple yet effective approach called Selective Attention Freezing (SAF) that reduces computation by freezing pre-trained attention weights for less important tokens during inference. Our method uses a lightweight importance scoring network (2% of model parameters) to identify tokens whose attention patterns can be fixed without significantly impacting performance. We evaluate SAF on standard language modeling benchmarks and find 15-25% reduction in FLOPs with less than 2% degradation in perplexity for sequences up to 4K tokens. Analysis reveals that SAF works best when applied to middle-layer attention heads and content-dependent positions. While our approach provides consistent speedups across model sizes (125M-7B parameters), we observe increased sensitivity to hyperparameter choices and some degradation on tasks requiring fine-grained reasoning. The method requires minimal code changes and integrates seamlessly with existing transformer architectures. Our results suggest that attention patterns in pre-trained models contain substantial redundancy that can be exploited for efficiency gains, though careful tuning remains necessary to maintain downstream performance.",
    "id": 447
  },
  {
    "title": "Gradient Surgery Revisited: A Simplified Approach to Multi-Task Learning in Transformers",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Garcia, A.",
      "Kim, S."
    ],
    "abstract": "Multi-task learning with transformer architectures often requires sophisticated gradient manipulation to handle conflicting objectives. While recent work has proposed complex gradient surgery methods involving expensive eigenvalue computations or learned routing networks, we revisit a simpler approach: dynamically reweighting gradients based on per-task validation performance. Our method, Adaptive Gradient Blending (AGB), uses a lightweight validation loop to estimate each task's utility and adjusts gradient magnitudes accordingly. We evaluate AGB on three standard multi-task benchmarks (GLUE, SuperGLUE, and Visual Question Answering) using off-the-shelf transformer models. Results show modest but consistent improvements (+1.2-2.3% average score) over naive multi-task baselines while remaining competitive with more complex gradient surgery techniques. Notably, our approach adds minimal computational overhead (5% training time increase) and hyperparameter tuning. While our improvements are incremental and primarily demonstrated on NLP benchmarks, AGB provides a practical baseline for multi-task transformers. We release our code and hyperparameters, though we note that gains can be sensitive to dataset characteristics and may not generalize to domains with stronger task conflicts.",
    "id": 448
  },
  {
    "title": "Improving Transformer Efficiency via Learnable Token Routing with Curriculum-based Sparsity",
    "authors": [
      "Chen, L.",
      "Gonzalez, M.",
      "Liu, K."
    ],
    "abstract": "Transformer models face computational challenges due to quadratic complexity in sequence length. While sparse attention mechanisms offer relief, they often rely on fixed patterns or heuristics that may not adapt to task-specific requirements. We propose Adaptive Token Routing (ATR), a learnable sparse attention mechanism that dynamically selects which tokens to attend to based on learnable gating functions. ATR uses a two-stage training procedure: first training dense attention networks, then progressively introducing sparsity through a curriculum that gradually penalizes attention entropy. We evaluate ATR on language modeling and machine translation tasks, achieving 2.1\u00d7 speedup during inference with 0.8 perplexity increase on Wikitext-103, and BLEU score improvements of 0.3-0.7 on WMT'14 English-German translation. While our method shows promise for efficient transformers, we observe sensitivity to the sparsity schedule and find that routing decisions lack interpretability. Our code is available at anon-link.",
    "id": 449
  },
  {
    "title": "Adaptive Gradient Descent with Polynomial Stepsize Scheduling",
    "authors": [
      "Chen, L.",
      "Kapoor, A.",
      "Nakamura, S."
    ],
    "abstract": "We propose Poly-GD, a gradient descent variant that uses polynomial decay schedules for stepsize adaptation in non-convex optimization. Unlike cosine or exponential schedules commonly used in deep learning, our method employs learnable polynomial coefficients updated via meta-gradient descent. We provide theoretical convergence guarantees for quadratic objectives and demonstrate empirical improvements on CIFAR-10 and ImageNet. On ResNet-18, Poly-GD achieves 0.8% better final validation accuracy compared to standard SGD with cosine annealing, with comparable training time. However, our method shows diminishing returns on larger architectures like EfficientNet-B4. While the polynomial scheduling introduces minimal overhead (5% increase in wall-clock time), our theoretical analysis only covers convex cases, and the meta-learning component adds complexity. Ablation studies suggest the benefit primarily comes from the polynomial form rather than the adaptive coefficients. Our code is available at [redacted].",
    "id": 450
  },
  {
    "title": "Improved Gradient Estimation for Stochastic Variational Inference via Quasi-Control Variates",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.M.",
      "Kim, S."
    ],
    "abstract": "Stochastic variational inference (SVI) scales Bayesian posterior approximation to large datasets, but suffers from high-variance gradient estimates when using simple Monte Carlo approximations of the ELBO. While control variates (CVs) reduce this variance, they require additional model assumptions and computational overhead. We propose quasi-control variates (QCVs), a lightweight gradient estimation technique that leverages inexpensive gradient information from auxiliary distributions without the restrictive assumptions of traditional CVs. Our method constructs near-optimal QCVs by solving a quadratic optimization problem that balances bias-variance trade-offs. We evaluate QCVs on hierarchical Bayesian models and deep generative networks, demonstrating 20-40% variance reduction compared to standard SVI across 5 benchmark datasets. However, the computational benefits diminish for high-dimensional parameter spaces (d > 500), and our theoretical analysis reveals a fundamental tension between the quality of auxiliary distributions and the bias introduced by QCV approximation. Our results suggest that QCVs provide modest but consistent improvements for moderate-scale Bayesian inference problems, with diminishing returns in deep learning applications.",
    "id": 451
  },
  {
    "title": "Improving Transformer Efficiency via Dynamic Token Routing with Learnable Sparse Attention Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "The quadratic complexity of self-attention in Transformers remains a critical bottleneck for processing long sequences. We propose a learned sparse attention mechanism that dynamically routes tokens through a two-stage process: tokens are first scored by a lightweight gating network, then passed through a fixed sparsity pattern learned via differentiable pruning. Our method reduces attention complexity from O(n\u00b2) to O(n log n) while maintaining accuracy within 2% of full attention on GLUE and 1.5% on Long Range Arena benchmarks across six tasks. Notably, our approach requires only an additional 0.3% parameters and minimal training overhead compared to standard attention. However, we observe performance degradation on tasks requiring fine-grained token interactions, such as small object detection in vision transformers. Theoretical analysis reveals our sparsity pattern approximates the full attention matrix with bounded error under mild assumptions. Code is available at anonymized-link.",
    "id": 452
  },
  {
    "title": "Adaptive Gradient Clipping with Micro-batch Statistics for Memory-Efficient Transformer Training",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer training often requires aggressive gradient clipping to maintain stability, particularly when using large batch sizes. However, computing global gradient norms across all parameters can be memory-intensive and may discard useful signal from smaller, localized updates. We propose Micro-batch Adaptive Gradient Clipping (MAGC), which approximates optimal clipping thresholds by tracking gradient statistics within micro-batches rather than across the full parameter set. Our method requires only 12% additional memory compared to standard clipping while enabling stable training with up to 4\u00d7 larger effective batch sizes. Experiments on ViT and BERT models show 2-6% improvements in downstream accuracy over baseline clipping on ImageNet and GLUE, though gains diminish on smaller datasets. While MAGC provides consistent stability improvements, we observe that the clipping dynamics can occasionally lead to slower convergence on well-regularized models. The approach is particularly effective for low-resource settings where memory constraints limit batch size, offering a practical trade-off between computational efficiency and training stability. Code will be made available upon acceptance.",
    "id": 453
  },
  {
    "title": "LoRA-Lite: Memory-Efficient Adaptation via Low-Rank Knowledge Distillation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Fine-tuning large language models remains computationally prohibitive for most practitioners. While LoRA reduces memory overhead by learning low-rank adaptations, we observe that its performance degrades significantly on tasks requiring substantial behavioral shifts from the pretrained model. We propose LoRA-Lite, which augments LoRA with a lightweight knowledge distillation framework that distills task-specific knowledge into the low-rank matrices during training. Our method introduces an additional distillation loss between the LoRA-adapted model and a task-specific teacher model, regularized by an adaptive weighting scheme that balances teacher guidance with LoRA's intrinsic updates. Across GLUE and SuperGLUE benchmarks, LoRA-Lite achieves 2.3% higher average performance than standard LoRA while using 15% fewer parameters. However, we find diminishing returns on larger models (>7B parameters), where the teacher-student gap narrows. Theoretical analysis reveals our method implicitly performs gradient compression, though the compression rate remains modest compared to more aggressive pruning techniques. LoRA-Lite provides a practical middle ground between full fine-tuning and parameter-efficient methods, particularly effective for medium-scale models and moderate task shifts.",
    "id": 454
  },
  {
    "title": "Learning with Gradient Noise that Actually Looks Like Gradient Noise",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Singh, V."
    ],
    "abstract": "We revisit the common practice of adding Gaussian noise for differential privacy in stochastic optimization, and show that the resulting noise patterns poorly match the natural gradient noise observed during training. Using tools from stochastic calculus and extensive measurements across transformer, CNN, and MLP architectures, we characterize the heavy-tailed, correlated structure of true gradient noise. Motivated by these observations, we propose Correlated Adaptive Noise (CAN), a privacy-preserving mechanism that generates noise matching both the marginal distribution and autocorrelation structure of natural gradients. Our theoretical analysis shows CAN achieves similar $(\\epsilon, \\delta)$-DP guarantees while injecting 35% less noise in expectation. Empirically, on CIFAR-10 and GLUE tasks, CAN achieves 2-4% better accuracy than baseline Gaussian mechanisms at equivalent privacy budgets. However, our method introduces additional hyperparameters and requires maintaining a running estimate of the gradient covariance, increasing memory usage by 15-20%. While CAN demonstrates that matching natural noise statistics can improve private learning, the gains are modest and come with computational costs that may limit practical adoption.",
    "id": 455
  },
  {
    "title": "LoRA-Flow: Adaptive Low-Rank Adaptation with Dynamic Rank Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, J."
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods like LoRA have become standard for adapting large language models, yet their fixed-rank constraints may limit expressiveness when tasks require varying capacities. We propose LoRA-Flow, a simple extension that dynamically adjusts the rank of LoRA adapters during training using a learned gating mechanism. Our method maintains LoRA's memory efficiency while allowing rank expansion/contraction based on gradient signals. Across 8 diverse NLP benchmarks, LoRA-Flow matches or exceeds standard LoRA by 1-3% on most tasks, with particularly strong gains on low-resource settings. However, results show no consistent improvements on larger models (>7B parameters), and ablations reveal that much of the benefit stems from the increased training budget rather than the dynamic scheduling itself. Our PyTorch implementation requires only 5 lines of code modification to existing LoRA checkpoints. While LoRA-Flow provides marginal gains on standard benchmarks, its simplicity and minimal overhead make it a practical alternative to static LoRA, particularly for practitioners working with small to medium models where rank selection is costly to tune.",
    "id": 456
  },
  {
    "title": "LoRA-Pro: Progressive Low-Rank Adaptation with Dynamic Rank Allocation",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Low-rank adaptation (LoRA) has emerged as a parameter-efficient method for fine-tuning large language models, but its fixed-rank constraints may limit performance across diverse downstream tasks. We present LoRA-Pro, a simple extension that dynamically adjusts rank allocation during training using gradient-based sensitivity scores. Our method begins with conservative low-rank matrices and progressively increases rank for layers showing high adaptation utility. On GLUE and SuperGLUE benchmarks, LoRA-Pro achieves 1-2% improvement over standard LoRA while using only 15% additional parameters, suggesting practical benefits for deployment scenarios. However, we observe diminishing returns on larger models (>7B parameters) and inconsistent gains across tasks, particularly in low-resource settings. Theoretically, we provide convergence guarantees under standard smoothness assumptions, though these require restrictive conditions on the rank growth schedule. Experiments reveal a trade-off between rank flexibility and training stability, with 23% of runs exhibiting oscillatory behavior requiring careful hyperparameter tuning. While LoRA-Pro offers modest improvements for moderate-sized models with sufficient data, its utility for extreme-scale parameter-efficient fine-tuning remains unclear. Our implementation and experimental logs are available at [anonymous URL].",
    "id": 457
  },
  {
    "title": "Gradient Noise Injection as Implicit Regularization in Overparameterized Neural Networks",
    "authors": [
      "Lee, S.",
      "Kumar, A.",
      "Chen, J."
    ],
    "abstract": "We investigate the connection between noise added during gradient descent and the implicit regularization effects observed in deep neural networks. While previous work has focused on noise added to inputs or labels, we systematically study controlled gradient perturbations introduced during optimization. Our theoretical analysis characterizes the regularization properties for two-layer ReLU networks under Gaussian gradient noise, showing that noise variance is related to an effective L2 regularization term. However, our experiments on CIFAR-10 and ImageNet demonstrate that the benefits diminish for deeper architectures (ResNet50+) and larger datasets. We further propose an adaptive noise scheduling scheme based on gradient norm, which provides modest improvements over SGD with fixed noise levels but underperforms compared to standard regularization techniques like dropout and weight decay. Our results suggest that while gradient noise injection offers theoretical insights into implicit regularization, its practical utility appears limited for modern deep learning applications. Code is available at [URL].",
    "id": 459
  },
  {
    "title": "ReLU Networks Can Be Trained to Ignore Out-of-Distribution Data via Logit Normalization",
    "authors": [
      "Liu, K.",
      "Goldman, S.",
      "Chen, J."
    ],
    "abstract": "We propose LogitNorm, a simple modification to cross-entropy training that improves out-of-distribution (OOD) detection in ReLU networks without additional hyperparameters or architectural changes. Our key insight is that the softmax cross-entropy loss creates undesirable confidence calibration behavior when inputs lie far from the training distribution. By normalizing logits to unit length during training while maintaining standard cross-entropy on the softmax outputs, networks learn to produce near-uniform predictions on OOD samples. We prove that for linear networks under Gaussian class-conditional distributions, LogitNorm achieves strictly better expected OOD AUROC than standard training. Empirical evaluation on CIFAR-10/100 with 5 diverse OOD datasets shows 3-7% AUROC improvements over temperature scaling and 1-2% over state-of-the-art energy-based methods, though gains shrink significantly with data augmentation. While LogitNorm achieves these results with minimal overhead (training time increases by \u22642%), we find the technique provides limited benefit for Vision Transformers and performs poorly under extreme distribution shift. Our code is available at [URL].",
    "id": 460
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Reparameterization Perspective",
    "authors": [
      "Chen, L.",
      "Park, J.",
      "Singh, V."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to suboptimal performance. While recent gradient surgery methods address this through projection-based approaches, we show these techniques can be viewed as implicit reparameterizations of the loss surface. We propose RepGrad, a lightweight modification to existing gradient surgery methods that explicitly learns task-specific reparameterizations through a small auxiliary network. Our method adds minimal computational overhead (<5% increase in training time) and can be dropped into existing multi-task architectures without architectural changes. On three standard benchmarks (CityScapes, NYUv2, and QM9), RepGrad achieves modest but consistent improvements over gradient surgery baselines (average +1.2% performance gain across tasks). While our theoretical analysis provides some insight into why RepGrad works, we acknowledge the improvements are incremental and the method may not justify the added complexity for all applications. However, RepGrad offers a practical way to enhance existing multi-task systems without retraining from scratch.",
    "id": 461
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Stability in Multi-Task Learning via Task-Aware Gradient Combination",
    "authors": [
      "Liu, S.",
      "Chen, K.",
      "Rodriguez, M."
    ],
    "abstract": "Multi-task learning often suffers from gradient conflicts that hinder optimization and degrade performance. While existing gradient surgery methods like PCGrad and GradDrop modify conflicting gradients to improve stability, they lack mechanisms to preserve historical task information, leading to suboptimal solutions. We propose Adaptive Memory Gradient Surgery (AMGS), which maintains lightweight task-specific memories to guide gradient combination decisions. Our method computes gradients using both immediate batch statistics and historical momentum directions, weighting each task's contribution based on its past gradient alignment. We evaluate AMGS on supervised multi-task benchmarks including CIFAR-100 with auxiliary tasks and NYUv2 depth estimation. Results show consistent improvements over baseline surgery methods, achieving 2-4% better average performance across tasks. However, AMGS introduces additional hyperparameters for memory decay and weighting schemes, and computational overhead grows linearly with task count. While our approach demonstrates practical benefits for common benchmarks, theoretical guarantees for convergence remain limited. Code and experiments are reproducible but require careful hyperparameter tuning for new task combinations. Our work suggests that incorporating historical context can improve multi-task optimization, though scalability remains an open challenge.",
    "id": 462
  },
  {
    "title": "Learning to Prune Redundant Channels: A Gradient-Free Approach via Information-Theoretic Channel Analysis",
    "authors": [
      "Liu, K.",
      "Chen, J.",
      "Rodriguez, M."
    ],
    "abstract": "Channel pruning has emerged as a critical technique for deploying CNNs on resource-constrained devices, yet existing methods often rely on expensive retraining cycles or complex optimization objectives. We propose ITCA, a gradient-free framework that identifies redundant channels through an information-theoretic lens without requiring fine-tuning. Our key insight is that channels with high conditional mutual information between activations and task labels can be pruned while preserving model accuracy. We estimate these quantities using a lightweight kernel-based estimator and introduce a novel pruning schedule that adapts to layer-wise information flows. Experiments on ResNet50 and MobileNetV2 achieve 40-50% FLOP reduction with <1% accuracy drop on ImageNet, competitive with methods requiring 10\u00d7 more compute. However, performance degrades significantly on ultra-lightweight architectures and tasks with fine-grained distinctions. While our paper provides a computationally efficient alternative to existing pruning paradigms, the theoretical guarantees are limited to Gaussian assumptions and the method exhibits instability on transformer architectures. Code and pretrained models are available for reproducibility.",
    "id": 463
  },
  {
    "title": "Graph Attention Networks with Learnable Node Embeddings via Meta-Gradient Descent",
    "authors": [
      "Kim, S.",
      "Rodriguez, C.A.",
      "Liu, J."
    ],
    "abstract": "We present Meta-GNN, a method that adapts graph neural networks to new tasks using meta-gradient descent on learnable node embeddings. While previous work has focused on adapting GNN architecture or edge weights, we propose to meta-learn initial node representations that can be rapidly fine-tuned for downstream tasks. Our approach trains a shared initialization across multiple graph datasets, then uses second-order gradients to optimize node embeddings for few-shot node classification. Experiments on 6 benchmark datasets show 2-3% improvement over standard GNN baselines when limited to 5-10 training labels per class. However, performance gains diminish with abundant labeled data, and computational overhead increases quadratically with graph size. Our contribution lies in demonstrating the viability of node-level meta-learning, though we acknowledge limitations in scalability and theoretical grounding. Code and datasets will be released upon acceptance.",
    "id": 464
  },
  {
    "title": "Gradient Confusion Helps: Rethinking Sharpness-Aware Minimization Through the Lens of Optimization Landscape Geometry",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S.",
      "Johnson, M."
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a promising regularization technique for improving generalization in deep neural networks. However, we observe that SAM's performance gains are inconsistent across architectures and datasets, particularly when batch sizes are large or architectures are overparameterized. Through a systematic analysis of the optimization trajectory, we identify that gradient confusion\u2014typically viewed as harmful for optimization\u2014can actually enhance SAM's effectiveness by encouraging exploration of flatter minima during early training. We propose Confusion-Augmented SAM (CASAM), a simple modification that injects controlled gradient perturbations during the ascent step. On CIFAR-10, CASAM achieves 94.1% accuracy compared to SAM's 93.8%, while reducing computational overhead by 15%. Surprisingly, our method shows diminishing returns on ImageNet (77.2% vs 77.5%), suggesting dataset-dependent effects. While our theoretical analysis connecting gradient confusion to flatness is preliminary and assumes simplified architectures, this work challenges the conventional wisdom that gradient alignment is always beneficial. Code and hyperparameters are provided for reproduction.",
    "id": 465
  },
  {
    "title": "Adaptive Curriculum Learning via Difficulty-Aware Sampling Improves Sample Efficiency in Deep Reinforcement Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Curriculum learning has shown promise for improving sample efficiency in deep RL, but existing approaches often rely on hand-crafted curricula or simple heuristics that may not generalize across environments. We propose AdaCurriculum, a simple yet effective method that adaptively adjusts the sampling distribution over training episodes based on a learned measure of task difficulty. Our approach uses a lightweight auxiliary network to estimate episode difficulty from initial states, requiring only 5% additional parameters. We evaluate AdaCurriculum on three standard continuous control benchmarks (MuJoCo) and two discrete action environments. Results show 15-40% sample efficiency improvements over uniform sampling baselines and 8-25% gains over a recent curriculum learning method, though we find variance across seeds can be high (standard deviation 12.3%). While our method achieves consistent improvements on medium-complexity environments, benefits diminish on simpler tasks, suggesting the approach may be most valuable when the optimal curriculum is non-trivial. Ablations reveal that both the difficulty estimator and adaptive sampling schedule contribute to performance, with neither component alone sufficient. Our implementation and hyperparameters are provided for reproducibility.",
    "id": 466
  },
  {
    "title": "Temporal Ensembling with Cyclical Confidence Thresholding for Semi-Supervised Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, M.",
      "Johnson, A."
    ],
    "abstract": "Semi-supervised learning has achieved impressive results by leveraging large amounts of unlabeled data, but current approaches often struggle with confirmation bias when pseudo-labels are noisy. We propose a simple yet effective modification to temporal ensembling that introduces cyclical confidence thresholds to selectively incorporate pseudo-labels during training. Our method maintains an exponential moving average of network predictions while dynamically adjusting the confidence threshold based on training progress and prediction entropy. This cyclical schedule allows for more aggressive pseudo-labeling early in training when the model is rapidly improving, followed by conservative refinement stages. We evaluate our approach on CIFAR-10/100 and ImageNet benchmarks, achieving 5-8% improvements over standard temporal ensembling baselines. While our method shows consistent gains on these datasets, theoretical analysis reveals the approach works best when label distributions are balanced and may degrade when class imbalance is severe. Our empirical results suggest the cyclical strategy is more robust to hyperparameter choices than fixed threshold approaches, though we acknowledge computational overhead and memory requirements remain similar to standard ensembling methods. Code is available at [link].",
    "id": 467
  },
  {
    "title": "Progressive Gradient Sharpening: A Simple Regularization Technique for Mitigating Catastrophic Forgetting in Continual Learning",
    "authors": [
      "Liu, J.",
      "Thompson, K.",
      "Garcia-Martinez, A."
    ],
    "abstract": "We propose Progressive Gradient Sharpening (PGS), a lightweight regularization method for continual learning that applies selective gradient amplification to important parameters without accessing previous task data. Our approach identifies task-specific parameters using a running estimate of gradient variance during training, then applies a sharpening transformation that increases gradient magnitudes for these parameters while leaving others relatively untouched. We evaluate PGS on standard continual learning benchmarks including Split-CIFAR-100 and sequential ImageNet subsets. While our method achieves competitive average accuracy compared to existing replay-based methods (e.g., 67.3% vs. 68.1% for ER), it shows limited performance on out-of-distribution tasks and exhibits higher forgetting rates under extreme task similarity. However, PGS requires no additional memory beyond the current mini-batch and adds minimal computational overhead, making it practical for resource-constrained deployment. Our results suggest that selective gradient manipulation can reduce forgetting in restricted scenarios, though the approach has clear limitations when tasks share substantial feature space overlap.",
    "id": 468
  },
  {
    "title": "Gradient Surgery for Memory-Efficient Continual Learning: A Simple Regularization Approach",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Continual learning remains challenging due to catastrophic forgetting when models are updated sequentially on new tasks. We propose Gradient Surgery with Memory Regularization (GSMR), a surprisingly simple approach that modifies gradients through selective clipping and an L2 regularization term based on frozen reference weights. Unlike prior methods requiring episodic memory or generative replay, GSMR only stores a small set of anchor points (\u22641% of training data) for regularization calibration. Our method achieves competitive performance on split CIFAR-100 and ImageNet-1K benchmarks, reducing forgetting by 15-25% over standard fine-tuning while using 10\u00d7 less memory than replay-based methods. However, we observe GSMR underperforms state-of-the-art approaches on longer task sequences (>20 tasks) and shows sensitivity to regularization coefficient tuning. Notably, our results suggest the benefits primarily stem from gradient clipping rather than the regularization component, raising questions about the novelty of our contribution. Code is available at anonymous-url.",
    "id": 470
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Improving Multi-Task Learning via Selective Forgetting",
    "authors": [
      "Liu, J.",
      "Kumar, S.",
      "Gonzalez, A."
    ],
    "abstract": "Multi-task learning faces the fundamental challenge of conflicting gradients between tasks, leading to suboptimal shared representations. While recent gradient surgery methods like PCGrad and GradDrop effectively zero out conflicting components, they discard potentially useful information. We propose AdaSurgery, a memory-augmented approach that retains conflicting gradients in differentiable memory banks, selectively reintroducing them based on task similarity scores computed using small meta-networks. Our method combines sparse gradient projection with adaptive memory recall, weighing previously discarded gradients against current task objectives. Experiments on 3 standard multi-task vision benchmarks (Cityscapes, NYUv2, CelebA) show 2-4% improvements over gradient surgery baselines, with particular gains in data-scarce regimes. However, we observe increased sensitivity to hyperparameter choices and memory bank size. While our adaptive memory mechanism offers a new perspective on gradient conflicts, we acknowledge that the computational overhead (15% slower training) and reliance on additional hyperparameters limit immediate practical adoption. The theoretical analysis of convergence properties remains incomplete for non-convex objectives, though empirical convergence holds across tested scenarios. Code and pretrained models will be released upon acceptance.",
    "id": 471
  },
  {
    "title": "Gradient Amplification for Low-Loss Basins: A Curvature-Aware Approach to Neural Network Optimization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Thompson, K."
    ],
    "abstract": "We propose Gradient Amplification for Low-Loss Basins (GALB), a simple modification to standard SGD that selectively amplifies gradients based on local curvature estimates. Motivated by the observation that neural networks often converge to sharp minima with poor generalization, GALB identifies directions of low curvature using the top eigenvalues of the Hessian and applies adaptive scaling to encourage movement toward flatter regions. Our method adds minimal computational overhead, requiring only periodic power iteration updates. Experiments on CIFAR-10/100 and ImageNet show modest improvements over SGD+Momentum (0.5-1.2% absolute accuracy gains), particularly for ResNet architectures with batch normalization. While GALB consistently finds solutions with lower Hessian trace, ablation studies reveal that performance improvements diminish with extensive data augmentation. We provide theoretical analysis showing GALB converges under standard smoothness assumptions, though our proof requires bounded eigenvalue ratios that may not hold in practice. Code will be released upon publication.",
    "id": 473
  },
  {
    "title": "Masked Autoencoders with Learnable Patch Embeddings for Semi-Supervised Image Classification",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M."
    ],
    "abstract": "We propose MAPPLE, a semi-supervised learning framework that extends masked autoencoding to leverage unlabeled data through learnable patch embeddings. While masked autoencoders (MAE) have shown promise for self-supervised pretraining, their direct application to semi-supervised learning remains underexplored. MAPPLE introduces a learnable patch importance mechanism that dynamically weights patches based on their mask reconstruction difficulty, allowing the model to focus on semantically meaningful regions during downstream fine-tuning. Our method achieves 71.2% top-1 accuracy on ImageNet with 10% labeled data, improving over standard MAE fine-tuning by 2.3 percentage points. On CIFAR-100, we obtain competitive results with 80.1% accuracy when using 1% labels. However, our approach shows diminishing returns as labeled data increases, performing comparably to simpler baselines at 100% supervision. While our patch importance mechanism offers interpretable insights, we find that performance gains are sensitive to hyperparameter choices, particularly the mask ratio and momentum coefficient. Extensive ablations reveal that much of the improvement stems from improved representation quality rather than the patch weighting scheme itself. Code and pretrained models will be released upon acceptance.",
    "id": 474
  },
  {
    "title": "Towards Gradient-Free Fine-Tuning of Language Models via Discrete Evolution Strategies",
    "authors": [
      "Liu, S.",
      "Kannan, A.",
      "Thompson, J."
    ],
    "abstract": "Fine-tuning large language models (LLMs) with gradient-based methods requires significant memory and computational resources. While parameter-efficient fine-tuning (PEFT) techniques like LoRA reduce this burden, they still necessitate end-to-end backpropagation. We propose EvoLora, a gradient-free approach that uses evolutionary strategies to optimize low-rank adapters for instruction-following tasks. Our method discretizes adapter weights into 8-bit values and employs an evolutionary algorithm with covariance matrix adaptation to search the parameter space. Across 7 tasks from the Super-NaturalInstructions benchmark using Llama-2-7B, EvoLora achieves 92.3% of LoRA's performance while requiring 3.2x less GPU memory. However, training time increases by 5.8x compared to LoRA. We provide convergence analysis showing linear convergence under assumptions of Lipschitz-continuous objectives, though these assumptions are violated in practice for attention mechanisms. Our results suggest gradient-free methods may be viable for resource-constrained scenarios where training time is less critical than memory usage.",
    "id": 475
  },
  {
    "title": "Self-Supervised Pretraining with Random Temporal Permutations Improves Robustness to Temporal Distribution Shift",
    "authors": [
      "Liu, S.",
      "Kumar, V.",
      "Chen, M."
    ],
    "abstract": "Pretraining has become standard for time series models, but most approaches assume temporal consistency between pretraining and downstream data. We propose TempMix, a simple self-supervised method that applies random temporal permutations during pretraining to explicitly encourage invariance to temporal ordering. Our approach is motivated by the observation that standard pretrained transformers exhibit significant performance degradation when downstream data has different temporal patterns (e.g., shifted seasonal effects or reversed trend direction). TempMix adds minimal computational overhead: we randomly shuffle contiguous subsequences within each sequence during pretraining while maintaining standard masked token reconstruction objectives. Across 6 real-world datasets spanning healthcare, finance, and sensor data, TempMix improves robustness by 12-18% relative to baseline pretraining under temporal distribution shift, while maintaining comparable performance on i.i.d. data. However, we observe diminishing returns on datasets with strong short-term dependencies, suggesting our method is most beneficial when long-term structure dominates. Our experiments demonstrate that temporal permutation during pretraining serves as an effective regularizer, though we acknowledge limitations in theoretical justification and the need for more comprehensive ablations. Code and pretrained models will be released.",
    "id": 476
  },
  {
    "title": "Gradient Surgery with Second-Order Correction for Multi-Task Optimization",
    "authors": [
      "Liu, K.",
      "Chen, S.",
      "Johnson, D."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that prevent individual tasks from making progress. While recent gradient surgery methods like PCGrad and GradDrop modify gradients through projection or masking, these approaches can inadvertently discard useful curvature information encoded in the Hessian. We propose Hessian-Aware Gradient Surgery (HAGS), a simple modification to existing surgery techniques that incorporates second-order information via a lightweight Hessian approximation. Our method computes a corrected gradient direction that accounts for local curvature while maintaining the computational efficiency of first-order methods. We evaluate HAGS on five multi-task vision benchmarks and find modest but consistent improvements over baseline surgery methods (avg. +1.2% accuracy), particularly when task gradients exhibit high curvature. However, our gains diminish on architectures with strong regularization, suggesting the method may be most useful for specific training regimes. While the theoretical analysis is limited, we provide extensive ablations and open-source our code to facilitate reproduction.",
    "id": 477
  },
  {
    "title": "Adaptive Gradient Clipping for Transformer Training: A Large-Scale Empirical Study",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Gradient clipping is widely used in transformer training to stabilize optimization, but existing methods rely on fixed thresholds that may not adapt to varying model architectures and data distributions. We conduct the first systematic investigation of adaptive gradient clipping methods across 50+ transformer configurations, including GPT-style language models, BERT encoders, and vision transformers. Our approach, Cheap Adaptive Threshold Estimation (CATE), uses only the gradient norm statistics from the first 100 training steps to automatically set clipping thresholds without hyperparameter tuning. On standard benchmarks, CATE achieves comparable or slightly better perplexity than manual tuning in 78% of cases, while reducing tuning time by 85%. However, we observe substantial performance degradation in 12% of configurations, particularly for smaller models (<100M parameters). Analysis reveals that CATE's fixed percentile-based heuristic fails for heavy-tailed gradient distributions common in sparse attention mechanisms. Our implementation requires only 5 lines of code, but the computational overhead increases training time by 3-7%. While CATE shows promise for reducing hyperparameter tuning burden in large-scale training, our negative results suggest careful validation is required for each new domain.",
    "id": 478
  },
  {
    "title": "Gradient Surgery with Memory: Alleviating Catastrophic Forgetting in Multi-Task Learning via Selective Parameter Freezing",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Rodriguez, A."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients that lead to catastrophic forgetting, particularly when tasks arrive sequentially. While gradient surgery methods like PCGrad reduce interference during simultaneous training, they fail to address the temporal aspect of forgetting in continual learning. We propose Memory-Aware Gradient Surgery (MAGS), a simple yet effective approach that augments gradient surgery with selective parameter freezing based on per-parameter importance scores computed from held-out examples. Our method combines episodic memory replay with a threshold-based freezing mechanism that prevents gradient updates on parameters deemed critical for previously seen tasks. On a modified Split-CIFAR-100 benchmark with 20 sequential tasks, MAGS achieves 72.3% average accuracy, improving over naive fine-tuning by 14.2% while retaining competitive performance on recent tasks. However, we observe diminishing returns when scaling to larger architectures, with ResNet-50 showing only 3.1% improvement over regularization baselines. Our analysis reveals that freezing decisions become increasingly unstable as model capacity grows, suggesting the importance of more sophisticated importance estimation. While not state-of-the-art, MAGS offers a practical trade-off between implementation complexity and forgetting reduction for resource-constrained scenarios.",
    "id": 479
  },
  {
    "title": "Memory-Efficient Training of Large Language Models Through Selective Weight Freezing",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Training large language models (LLMs) remains computationally prohibitive due to memory requirements for gradient storage and optimizer states. We propose Selective Weight Freezing (SWF), a simple yet effective approach that dynamically identifies and freezes weights requiring minimal updates during training, reducing memory overhead without sacrificing convergence. Our method computes Fisher information for parameter importance every k steps and freezes the bottom p% of parameters, allocating resources to more critical weights. Experiments on GPT-2 (1.5B) and LLaMA (7B) models demonstrate 23-31% memory reduction with <1% perplexity degradation on standard NLP benchmarks. While SWF achieves comparable performance to full training on most tasks, we observe increased variance on smaller datasets (WikiText-103) and minor degradation in few-shot learning scenarios. Theoretical analysis shows convergence under mild assumptions, though tighter bounds remain elusive. Our PyTorch implementation requires minimal code changes and integrates with existing training pipelines. While SWF demonstrates practical benefits for memory-constrained training, we acknowledge limitations in extreme fine-grained selection (p<5%) and potential brittleness under aggressive learning rates. Code and hyperparameters are provided for reproducibility.",
    "id": 480
  },
  {
    "title": "LoRA-DAG: Low-Rank Adaptation with Directed Acyclic Graphs for Parameter-Efficient Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, J.",
      "Thompson, S."
    ],
    "abstract": "We propose LoRA-DAG, a parameter-efficient fine-tuning method that extends Low-Rank Adaptation (LoRA) to multi-task learning scenarios. Our key insight is to model task relationships as a directed acyclic graph (DAG), where edges represent shared low-rank adaptations between tasks. This allows for both positive and negative transfer through learnable edge weights while maintaining the parameter efficiency of LoRA. We validate our approach on GLUE and SuperGLUE benchmarks using T5-base and RoBERTa-base models. Experimental results show modest but consistent improvements over standard LoRA (+1.2-1.8% F1 average) and naive model merging (+2.5% F1), particularly on low-resource tasks. However, we observe that performance gains diminish as task similarity decreases, and our DAG structure adds 5-8% training overhead. Ablation studies reveal that most benefits come from shared input projections rather than output projections. While limited by the fixed DAG structure and computational overhead for large models, LoRA-DAG presents a practical extension of LoRA for practitioners with moderately related tasks.",
    "id": 481
  },
  {
    "title": "Gradient Confusion: A Simple Regularization Technique for Improving Transformer Training Stability",
    "authors": [
      "Liu, H.",
      "Kumar, P.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Transformer models often exhibit unstable training dynamics, particularly when scaling to larger architectures. We propose Gradient Confusion, a lightweight regularization method that adds controlled noise to gradient directions during optimization. Our approach randomly perturbs a subset of gradients in each training step, preventing the model from overfitting to specific optimization trajectories. We provide theoretical analysis showing that this procedure acts as an implicit form of trust region optimization, bounding updates within a provable stability radius. Experiments on GLUE and Wikitext benchmarks demonstrate modest improvements of 1-2% over standard baselines, with the largest gains observed on smaller datasets (under 1GB). While the method shows promise on architectures with 100M-1B parameters, we find diminishing benefits on larger models (>10B parameters). Additionally, our ablation studies reveal that the technique's effectiveness is sensitive to learning rate schedules and batch sizes. Our implementation adds minimal computational overhead (under 5% training time increase), making it practical for existing pipelines. However, we acknowledge limitations: the method appears redundant when combined with other regularization techniques like dropout and weight decay, and performance gains are dataset-dependent. The simplicity of our approach may appeal to practitioners seeking stable training without architectural changes, though we emphasize that the improvements are incremental rather than transformative.",
    "id": 482
  },
  {
    "title": "ReLoRA: Recursive Low-Rank Adaptation for Multi-Task Learning with Limited Data",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, A."
    ],
    "abstract": "We present ReLoRA, a parameter-efficient method for multi-task learning that combines recursive low-rank decomposition with gradient accumulation to improve transfer learning when target tasks have limited data. Our approach builds upon LoRA by recursively decomposing the low-rank updates into even smaller matrices, enabling fine-grained sharing across tasks while maintaining computational efficiency. We demonstrate that ReLoRA achieves comparable or superior performance to standard LoRA on 8 diverse NLP benchmarks (GLUE, SuperGLUE), with up to 2.4\u00d7 fewer trainable parameters. Experiments show consistent improvements on low-resource scenarios (\u22641k examples), with average gains of 3.2% over baseline methods. However, we find diminishing returns on high-resource tasks and computational overhead in recursive decomposition stages. While our theoretical analysis provides convergence guarantees under reasonable assumptions, our empirical evaluation is limited to encoder models and English tasks. Our code is available at anonymized-link.",
    "id": 483
  },
  {
    "title": "Adaptive Curriculum Learning for Few-Shot Molecular Property Prediction",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Rodriguez, M.",
      "Zhou, J."
    ],
    "abstract": "Molecular property prediction with limited labeled data remains challenging despite recent advances in graph neural networks. We propose AC-MOL, an adaptive curriculum learning framework that gradually exposes the model to increasingly complex molecular structures based on their estimated difficulty. Our approach uses a simple yet effective difficulty metric combining molecular size and structural complexity, dynamically adjusting the curriculum schedule based on validation performance. Through extensive experiments on 8 benchmark datasets from MoleculeNet, we demonstrate that AC-MOL achieves 3-7% improvement over standard few-shot baselines, particularly effective for datasets with high scaffold diversity. However, we find that the benefits diminish when sufficient labeled data (\u2265500 molecules per class) is available. While our method provides practical improvements, we acknowledge that the theoretical justification for our curriculum strategy remains limited, and the approach inherits known limitations of curriculum learning including sensitivity to hyperparameter choices. Code and datasets will be made available upon acceptance.",
    "id": 484
  },
  {
    "title": "Gradient Surgery in Overparameterized Networks: When Less Pruning is More",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H."
    ],
    "abstract": "Neural network pruning is typically framed as post-training compression, yet we observe that strategically removing parameters during training can surprisingly improve optimization dynamics. We propose Iterative Gradient Surgery (IGS), a simple method that periodically identifies and surgically removes the smallest-magnitude gradients rather than weights. On CIFAR-10 and ImageNet, IGS yields 1.2-1.8% accuracy improvements over standard training while achieving 40% parameter reduction. Our theoretical analysis reveals that this gradient pruning implicitly regularizes the loss landscape by constraining the effective rank of the Hessian. However, these gains diminish with modern optimizers like AdamW (0.3% improvement) and vanish entirely without careful learning rate scheduling. While promising, our method introduces two hyperparameters (pruning frequency and gradient threshold) that require extensive tuning, and performance degrades for smaller architectures. We hypothesize that IGS primarily benefits from a form of dynamic curriculum learning rather than the claimed regularization effect. Code and checkpoints are available.",
    "id": 485
  },
  {
    "title": "LoRA-MoE: Automatic Expert Routing in Low-Rank Adaptation",
    "authors": [
      "Chen, J.",
      "Rodriguez, M.",
      "Thompson, S."
    ],
    "abstract": "We propose LoRA-MoE, an extension to Low-Rank Adaptation (LoRA) that automatically routes tokens to task-specific low-rank experts during fine-tuning. While LoRA has become standard for parameter-efficient adaptation, it treats all inputs uniformly, potentially missing task-specific patterns. Our method decomposes the low-rank adaptation into multiple experts, each learning specialized representations for different input characteristics. We design a top-k routing mechanism that selects relevant experts based on learned task embeddings, keeping the total parameter count comparable to standard LoRA. Experiments on GLUE and SuperGLUE show modest improvements (1.2% average gain) over vanilla LoRA, with more pronounced benefits on multi-task benchmarks. However, we observe that routing patterns converge to near-uniform distributions on simpler tasks, suggesting the method may offer limited benefits for traditional fine-tuning. The approach adds minimal computational overhead during inference, maintaining LoRA's deployment advantages. While our results demonstrate the potential for more adaptive parameter-efficient fine-tuning, we acknowledge that gains come at the cost of training instability and require careful initialization of routing parameters.",
    "id": 486
  },
  {
    "title": "Feature Recycling: Improving Neural Network Efficiency Through Selective Layer Reinitialization",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S.",
      "Johnson, A."
    ],
    "abstract": "We propose a simple yet surprisingly effective method for improving computational efficiency in training deep neural networks by selectively recycling features from early training stages. Our approach identifies 'stagnant' neurons whose activations change minimally over training epochs, then reinitializes their weights while preserving valuable learned features through a teacher-student distillation process. Experiments on CIFAR-10/100 and ImageNet show 15-25% reduction in training time without significant accuracy loss (\u22640.3% for ResNet-18/50, \u22641.2% for Vision Transformers). While our method works particularly well for vision tasks, we observe limited benefits on language modeling benchmarks - possibly due to the sequential nature of text data. Theoretical analysis proves convergence under simplified convex assumptions, though the general non-convex case remains challenging. Our implementation requires only 50 lines of PyTorch code and introduces minimal hyperparameters, making it practical for practitioners. However, we acknowledge that the accuracy-time trade-offs may not justify deployment in all scenarios, and the technique's benefits diminish with more advanced optimizers like AdamW with cosine scheduling. Code will be released upon acceptance.",
    "id": 487
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Component Pruning",
    "authors": [
      "Chen, L.",
      "Johnson, K.",
      "Rodriguez, M."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its fixed rank allocation across layers may be suboptimal. We propose LoRA-Drop, a simple extension that dynamically prunes LoRA components during training based on gradient sensitivity. Our method trains auxiliary gating parameters that learn to deactivate low-importance adapter modules while keeping computational overhead minimal. Experiments on GLUE benchmarks show 15-30% parameter reduction compared to standard LoRA with minimal performance degradation (<1% relative drop in most tasks). We also observe improved training stability on noisy datasets. However, gains are inconsistent across tasks, with some datasets (especially MNLI) showing negligible benefits. Our analysis reveals that LoRA-Drop works best when initial LoRA ranks are moderately oversized. While our approach is orthogonal to rank selection strategies and easy to implement, the computational savings may not justify the added complexity in all settings. Code and models will be released upon acceptance.",
    "id": 488
  },
  {
    "title": "Gradient Amplification Networks: A Lightweight Architecture for Improving Transformer Optimization Dynamics",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.M.",
      "Kumar, S."
    ],
    "abstract": "Transformer architectures suffer from optimization challenges when scaled to modest model sizes, often exhibiting gradient vanishing in early layers. We propose Gradient Amplification Networks (GANs), a lightweight architectural modification that re-weights gradient flows without altering forward pass computations. Our approach inserts parallel amplification pathways that modulate gradient magnitudes during backpropagation based on layer-wise statistics. On GLUE and SuperGLUE benchmarks with 125M-350M parameter models, GANs achieve modest improvements (1.2-2.3% average score increase) over baseline transformers while maintaining identical inference costs. Theoretical analysis reveals our method approximates a second-order optimization step, though we observe diminishing returns beyond 500M parameters. Our results suggest that careful gradient re-weighting can provide incremental training benefits, though the gains are task-specific and may not generalize to larger scales. Code is available at [anonymous URL].",
    "id": 489
  },
  {
    "title": "Gradient Surgery in Federated Learning: When Less Pruning is More",
    "authors": [
      "Kim, S.",
      "Liu, J.",
      "Brown, A."
    ],
    "abstract": "Federated learning systems often suffer from client drift caused by heterogeneous data distributions. While gradient compression techniques like Top-k sparsification aim to reduce communication overhead, they can exacerbate drift by preserving conflicting gradient directions. We propose Gradient Orthogonal Pruning (GOP), a simple method that selects gradient coordinates based on their alignment with the global gradient rather than magnitude alone. Unlike existing approaches that apply uniform sparsification, GOP dynamically adjusts sparsity levels per client based on gradient coherence scores. Our theoretical analysis shows that GOP achieves a O(\u221a(d/k)) improvement in convergence rate over standard Top-k, where d is the model dimension and k is the sparsity budget. Empirically, GOP reduces communication rounds by 15-30% across CIFAR-10 and Reddit-L2 benchmarks with 100 clients, though gains diminish with highly skewed non-IID partitions. While our results demonstrate consistent improvements over baselines, the computational overhead of computing gradient alignments may limit applicability to very large models. Code will be available upon acceptance.",
    "id": 490
  },
  {
    "title": "Momentum-Based Gradient Updates Can Provably Degrade Performance in Deep Linear Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kumar, A."
    ],
    "abstract": "We provide a theoretical analysis of momentum-based optimization in deep linear networks, characterizing regimes where momentum provably hurts convergence compared to vanilla gradient descent. While momentum is widely used in deep learning, its theoretical benefits are primarily understood for convex optimization. In contrast, we show that for deep linear networks with depth L \u2265 3, momentum can increase the iteration complexity by a factor of \u03a9(\u03ba^{1/(L-1)}), where \u03ba denotes the condition number. Our analysis leverages recent advances in the neural tangent kernel framework, extending it to account for momentum terms. We validate our theoretical findings with experiments on synthetic data, demonstrating cases where momentum not only slows convergence but also leads to higher final training loss in deep linear networks. Additionally, we empirically observe similar patterns at moderate depths in ReLU networks on MNIST and CIFAR-10, though the theoretical characterization remains open for non-linear activations. Our results suggest that the benefits of momentum may be task-dependent in deep learning, questioning its universal applicability. While our theoretical bounds only apply to the overparameterized, linear regime, they offer the first provable example of momentum harming performance in deep networks.",
    "id": 491
  },
  {
    "title": "Learning to Prune Adapters: A Gradient-Free Approach for Efficient Multi-Task Models",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kumar, S."
    ],
    "abstract": "The proliferation of adapter-based fine-tuning has made it possible to share pretrained models across multiple downstream tasks, but at the cost of increased memory and inference time. We propose Prune-Ada, a simple yet effective method that eliminates adapter parameters without retraining or gradient computation. Our approach uses a novel perturbation-based sensitivity analysis to identify task-specific adapter weights that contribute minimally to the loss landscape. By iteratively masking these weights and measuring the resulting prediction stability, we can reduce adapter size by 30-50% with less than 2% performance degradation on standard benchmarks. Experiments across 8 NLP tasks and vision datasets show competitive compression ratios to learned pruning methods, while requiring significantly less computational overhead. While our method achieves reasonable efficiency gains, we acknowledge limitations in handling correlated features and potential brittleness under distribution shift. The simplicity of our approach makes it immediately applicable to existing adapter architectures without architectural changes.",
    "id": 492
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Parameter Pruning for Efficient Fine-Tuning",
    "authors": [
      "Chen, L.",
      "Kumar, S.",
      "Anderson, J."
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning method, but its fixed-rank structure may allocate parameters suboptimally across layers. We propose LoRA-Drop, a simple extension that dynamically prunes LoRA modules during training based on gradient-based importance scores. Our method starts with a maximum rank for each layer and progressively removes modules whose singular values fall below an adaptive threshold. On the GLUE benchmark, LoRA-Drop achieves comparable performance to standard LoRA (83.7 vs 84.1 F1 on average) while using 37% fewer parameters. However, ablations reveal the gains primarily come from improved hyperparameter selection rather than the proposed dynamic pruning mechanism. While our approach demonstrates practical parameter efficiency, we acknowledge the theoretical limitations: we do not establish convergence guarantees for the pruning schedule, and our motivation relies on empirical observations of singular value spectra rather than principled analysis. The method requires careful tuning of the pruning threshold and may underperform on smaller datasets. Nevertheless, LoRA-Drop provides a lightweight modification to existing LoRA implementations with minimal computational overhead, making it potentially useful for resource-constrained deployment scenarios.",
    "id": 493
  },
  {
    "title": "Improving Transformer Efficiency through Layer-wise Learning Rate Schedules",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose layer-specific learning rate schedules (LS-LRS) as a simple yet effective technique to reduce training time and memory usage in Transformers. Motivated by observations that different layers converge at varying rates during training, we assign decaying learning rates to deeper layers while maintaining higher rates for earlier layers. Our method requires minimal code changes\u2014only modifying the optimizer configuration\u2014and incurs no additional memory overhead during inference. We evaluate LS-LRS on standard NLP benchmarks (GLUE, WMT14) and vision transformers, training models ranging from 86M to 1.2B parameters. Results show modest improvements: 8-12% reduction in training time and 5-7% reduction in peak memory usage compared to standard AdamW, while maintaining competitive downstream performance (within 0.3% accuracy/BLEU score). However, our gains diminish on larger models (>1B parameters) and vary significantly across tasks. While LS-LRS provides a practical optimization for resource-constrained training scenarios, we acknowledge the technique's limited theoretical justification and marginal improvements over strong baselines. Code is available at anonymized-url.",
    "id": 494
  },
  {
    "title": "Gradient Tempering for Semi-Supervised Learning: A Simple Baseline with Temperature-Dependent Perturbations",
    "authors": [
      "Liu, J.",
      "Kumar, V.",
      "Chen, S."
    ],
    "abstract": "Semi-supervised learning (SSL) has achieved remarkable success through consistency regularization and perturbation-based methods. However, most approaches require careful hyperparameter tuning and complex augmentation strategies. We propose Gradient Tempering (GT), a surprisingly simple technique that scales the gradient magnitudes during backpropagation using a temperature parameter \u03c4 \u2208 (0, 1]. By selectively reducing the influence of unlabeled examples with high-confidence predictions, GT encourages smoother decision boundaries without additional augmentations or complex regularizers. Our method can be implemented in 5 lines of PyTorch code and combines naturally with existing architectures. We evaluate GT on CIFAR-10/100 and SVHN benchmarks, achieving competitive performance (92.3% accuracy on CIFAR-10 with 250 labels) compared to more sophisticated SSL methods. While our baselines slightly underperform state-of-the-art approaches, GT demonstrates strong robustness across hyperparameter settings and provides a lightweight alternative for practitioners. We further establish limited theoretical motivation connecting gradient tempering to entropy minimization. Our results suggest that simple gradient-level interventions may be overlooked in the quest for architectural complexity, though we acknowledge limitations in scaling to larger datasets like ImageNet.",
    "id": 495
  },
  {
    "title": "Self-Refining Networks: Neural Architecture Search with Internal Adaptive Rewiring",
    "authors": [
      "Chen, S.",
      "Kumar, V.",
      "Liu, J."
    ],
    "abstract": "We propose Self-Refining Networks (SRNs), a method for efficient neural architecture search that enables networks to modify their own connectivity patterns during training. SRNs introduce lightweight gating modules that dynamically prune or add connections based on gradient-based importance estimates, eliminating the need for expensive evolutionary or reinforcement learning-based search. Our approach alternates between standard weight optimization and discrete architectural modifications, achieving continuous refinement without meta-learning. We evaluate SRNs on CIFAR-10, CIFAR-100, and ImageNet, demonstrating 2-3% improvements over baseline ResNet architectures with comparable FLOPs. While SRNs show promise for automated architecture adaptation, we observe that discovered patterns exhibit dataset-specific biases and may not transfer robustly. The method requires careful tuning of pruning thresholds and can destabilize training without proper regularization. Our empirical analysis reveals that SRNs tend to produce architectures similar to hand-designed variants, limiting the novelty of discovered structures. Despite these limitations, SRNs offer a computationally efficient alternative to traditional NAS approaches, reducing search time from GPU-days to GPU-hours. Code and architectures will be released upon publication.",
    "id": 496
  },
  {
    "title": "Gradient Surgery for Multi-Task Learning: A Re-evaluation Through the Lens of Sharpness Minimization",
    "authors": [
      "Liu, W.",
      "Kumar, A.",
      "Chen, S.",
      "Rodriguez, J."
    ],
    "abstract": "Multi-task learning has seen renewed interest with the success of gradient surgery methods that modify task gradients during optimization. While these approaches report improved performance across benchmarks, we argue that their benefits stem not from task balancing as commonly understood, but from implicit sharpness minimization. We first demonstrate through controlled experiments that several gradient surgery variants reduce the sharpness of the loss landscape in single-task settings, suggesting a mechanism orthogonal to multi-task interference. Building on this observation, we propose Sharpness-Aware Gradient Surgery (SAGS), which explicitly incorporates sharpness regularization into the gradient modification process. Our method achieves comparable performance to existing techniques on three standard benchmarks while providing more interpretable optimization dynamics. However, we find that the performance gains diminish when strong regularization or normalization techniques are applied, indicating that gradient surgery may be most beneficial in under-regularized training regimes. Our results suggest that re-framing gradient surgery as a form of implicit regularization provides better intuition for when and why these methods work, though we acknowledge limitations in our theoretical analysis of the sharpness-task balance interaction.",
    "id": 497
  },
  {
    "title": "LoRA\u00b2: Nested Low-Rank Adaptation for Parameter-Efficient Fine-Tuning with Adaptive Block Sizes",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "We propose LoRA\u00b2, an extension of Low-Rank Adaptation (LoRA) that dynamically selects block sizes for low-rank decomposition based on layer-wise importance scores. While LoRA demonstrates strong performance across various tasks, its fixed rank and uniform block size across layers may be suboptimal. LoRA\u00b2 first computes importance scores using a lightweight importance network trained on downstream tasks, then applies nested low-rank decompositions with rank determined by these scores. On GLUE and SuperGLUE benchmarks, LoRA\u00b2 achieves 0.7-1.2% improvements over standard LoRA while using comparable parameters. Theoretical analysis shows that our adaptive ranks satisfy a tighter approximation bound under mild assumptions. However, we find the importance network requires additional tuning for new domains, and computational overhead offsets benefits for small-scale tasks. While LoRA\u00b2 shows promise for medium-scale fine-tuning, the stability of importance scores across tasks and the trade-off between adaptivity and simplicity warrant further investigation.",
    "id": 498
  },
  {
    "title": "Towards Improving Transformer Attention with Learnable Sparse Patterns",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, S."
    ],
    "abstract": "Self-attention mechanisms in Transformers suffer from quadratic complexity with respect to sequence length, limiting their applicability to long sequences. While sparse attention patterns have emerged as a popular remedy, existing approaches rely on fixed, hand-crafted sparsity masks that may not be optimal for downstream tasks. We propose LS-SAN (Learnable Sparse Self-Attention Networks), a simple method that learns sparse attention patterns through a differentiable relaxation of top-k selection. Our approach adds minimal parameters (<1% of model size) and enables dynamic sparsity that adapts to input content. Experiments on standard NLP benchmarks (Wikitext-103, GLUE) show modest improvements over dense baselines (1-2% average gains) while maintaining comparable memory usage to static sparse models. However, we find that learned patterns heavily depend on initialization and exhibit high variance across seeds. Our analysis reveals that most learned patterns converge to near-diagonal structures with limited deviation from local attention, suggesting that the learned sparsity may be too conservative. While LS-SAN provides a principled framework for learning sparse patterns, our results indicate that significant architectural innovations may be needed to realize the full potential of adaptive sparsity in Transformers.",
    "id": 499
  },
  {
    "title": "LoRA-Drop: Adaptive Low-Rank Adaptation with Dynamic Rank Selection",
    "authors": [
      "Chen, L.",
      "Nguyen, T.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Parameter-efficient fine-tuning methods like LoRA have become essential for adapting large language models, but their fixed-rank design often leads to suboptimal trade-offs between efficiency and performance. We propose LoRA-Drop, a simple yet effective method that dynamically adjusts the rank of LoRA adapters during training using magnitude-based pruning. Our approach starts with an over-parameterized LoRA module and progressively drops the least important singular values based on their contribution to the downstream task loss. We validate LoRA-Drop on GLUE and SuperGLUE benchmarks using RoBERTa-Large and T5-3B models. Results show LoRA-Drop achieves 15-20% parameter reduction over standard LoRA while maintaining comparable performance (\u00b10.3 F1 on average) on most tasks. However, we observe larger performance drops (>2 points) on 3 out of 9 tasks, suggesting sensitivity to task-specific characteristics. Analysis reveals that LoRA-Drop tends to retain higher ranks in early layers and attention modules, though our heuristic selection lacks theoretical guarantees. While our method provides empirical parameter savings, the computational overhead from repeated SVD computations during training remains non-negligible (1.8\u00d7 training time). Code and configurations will be released upon acceptance.",
    "id": 500
  },
  {
    "title": "Rethinking Curriculum Learning with Adaptive Difficulty Scheduling",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We propose ADAptive CUrriculum (ADACU), a simple yet effective method for automatically determining when to increase training difficulty in curriculum learning. Unlike prior work that relies on handcrafted schedules or expensive validation, ADACU monitors the gradient norm ratios between easy and hard samples to trigger difficulty transitions. Our theoretical analysis shows this ratio correlates with convergence bounds in convex cases, though extensions to non-convex settings remain heuristic. We evaluate ADACU on image classification (CIFAR-10/100) and language modeling (WikiText-2) using ResNet-18 and LSTM baselines. Results show 2-4% relative improvements over fixed curricula and 1-2% over existing adaptive methods, with particularly strong gains in low-data regimes. However, performance gains diminish in large-scale settings (ImageNet), suggesting scalability limitations. While our approach is computationally lightweight and easy to implement, we acknowledge the reliance on domain-specific threshold tuning and lack of theoretical guarantees beyond convex losses. Code and hyperparameters are provided for reproducibility.",
    "id": 501
  },
  {
    "title": "Gradient Surgery is Not Enough: Provable Improvements via Block-Diagonal Preconditioning for Multi-Task Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, H.J."
    ],
    "abstract": "Multi-task learning often struggles with conflicting gradients between tasks. While existing approaches like PCGrad and GradNorm apply ad-hoc gradient modifications to mitigate interference, we provide a more principled perspective by analyzing multi-task optimization through the lens of preconditioned gradient descent. We propose Block-Diagonal Inverse Hessian Preconditioning (BDIHP), a method that approximates each task's Hessian independently while maintaining computational tractability. Our approach provides similar per-iteration complexity to existing baselines while offering theoretical guarantees on convergence under mild assumptions. Empirically, we demonstrate consistent improvements over state-of-the-art gradient surgery techniques on 3 standard benchmarks: MultiMNIST, CityScapes, and a toy robotics control suite. While our method achieves 3-7% relative improvement in average task performance, gains are marginal on well-aligned tasks. Our results suggest that while preconditioning provides theoretical benefits, practical impact remains limited when tasks are naturally synergistic. Theoretical analysis reveals an inherent trade-off between precondition effectiveness and computational overhead, raising questions about the practical necessity of sophisticated multi-task optimization beyond simple weighted losses.",
    "id": 502
  },
  {
    "title": "Spectral Normalization with Learnable Frequency Thresholds for Improved GAN Training Stability",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.A.",
      "Kim, S."
    ],
    "abstract": "We propose an adaptive variant of spectral normalization for generative adversarial networks that learns frequency-dependent thresholds during training. While standard spectral normalization applies uniform Lipschitz constraints across all Fourier modes, our method automatically identifies and regularizes only the high-frequency components that contribute most to training instability. Our approach introduces minimal computational overhead (5-8% increase in training time) and can be integrated into existing architectures by replacing the spectral normalization module. Experiments on CIFAR-10, CelebA, and ImageNet show marginal improvements in FID scores (1.2-3.7 points) compared to standard spectral normalization, with more consistent training curves across random seeds. However, we observe the benefits diminish on higher-resolution datasets. Our theoretical analysis provides partial justification for the approach under simplified assumptions, though we acknowledge certain approximations remain unverified. Code will be made available upon publication.",
    "id": 503
  },
  {
    "title": "Gradient Descent with Memory: A Simple Plug-in Module for Improved Optimization in Deep Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "We present Memory-Augmented Gradient Descent (MAGD), a lightweight module that can be seamlessly integrated into existing optimizers to improve convergence without hyperparameter tuning. MAGD maintains a compressed history of past gradients using a low-rank approximation, enabling the optimizer to leverage long-term curvature information while maintaining computational efficiency. Our method adds only 0.3% overhead in training time compared to standard Adam, making it practical for large-scale applications. We evaluate MAGD on image classification tasks across ResNet and Vision Transformer architectures, achieving 0.8-1.2% improvements in final accuracy on ImageNet and CIFAR-10 over strong baselines. While these gains are consistent across architectures, we observe diminishing returns on tasks using extensive data augmentation. Theoretical analysis shows MAGD converges to a neighborhood of stationary points under standard convexity assumptions, though the rate is similar to vanilla gradient descent. Our results suggest MAGD provides reliable, albeit incremental, improvements for practitioners without requiring additional tuning, representing a practical rather than breakthrough contribution to optimization research.",
    "id": 504
  },
  {
    "title": "Gradient Descent with Reversed Momentum: A Simple Fix for Sharp Minima in Neural Networks",
    "authors": [
      "Chen, L.",
      "Rodriguez, A.",
      "Kim, S."
    ],
    "abstract": "Sharp minima in neural network training have been linked to poor generalization, yet simple interventions to avoid them remain scarce. We propose Reversed Momentum (RM), a modification to standard momentum-based optimizers that periodically reverses the momentum direction to encourage exploration of flatter regions. The key insight is that standard momentum can trap optimization trajectories in sharp basins, while occasional reversals provide escape mechanisms without sacrificing convergence speed. Our method requires only a single hyperparameter (reversal frequency) and minimal computational overhead. We evaluate RM on CIFAR-10/100 and ImageNet with ResNets and Vision Transformers, showing consistent improvements in test accuracy (1-2%) while maintaining training efficiency. Theoretical analysis reveals RM reduces the effective learning rate in sharp curvature directions, providing intuition for its empirical benefits. However, performance gains diminish with larger batch sizes and stronger data augmentation. While our results suggest RM as a practical tool for improving generalization, the improvements are moderate compared to more sophisticated sharpness-aware methods, and the optimal reversal schedule appears task-dependent.",
    "id": 505
  },
  {
    "title": "Gradient Surgery with Adaptive Memory: Towards Stable Multi-Task Learning in Neural Networks",
    "authors": [
      "Lee, J.",
      "Chen, S.",
      "Rodriguez, M."
    ],
    "abstract": "Multi-task learning often suffers from conflicting gradients between tasks, leading to degraded performance and unstable training. While recent gradient surgery methods like PCGrad successfully mitigate conflicts by projecting gradients onto feasible subspaces, they rely on fixed projection thresholds that fail to adapt to dynamic training regimes. We propose Adaptive Memory Gradient Projection (AMGP), an extension that maintains a memory bank of historical gradients to dynamically adjust projection thresholds based on inter-task similarity. Our method achieves modest improvements over PCGrad on standard multi-task benchmarks, showing 1.3% average accuracy gain on three vision datasets while reducing training instability by 18%. However, the computational overhead increases by 2.1\u00d7 due to memory operations, and performance gains diminish with larger batch sizes. Surprisingly, our approach underperforms naive gradient scaling on text-image tasks, suggesting limited generalizability outside vision domains. While AMGP provides interesting insights into gradient memory effects, its benefits remain marginal and context-dependent. Code will be released upon acceptance.",
    "id": 506
  },
  {
    "title": "Gradient Noise Revisited: A Slightly Sharper Analysis of SGD with Label Noise",
    "authors": [
      "Liu, J.",
      "Chen, K.",
      "Srinivasan, P."
    ],
    "abstract": "We revisit the convergence analysis of stochastic gradient descent with label noise, a simple regularization technique that corrupts training labels during optimization. While prior work established O(1/T) convergence rates for convex objectives, we provide a refined analysis that improves the constant factor by approximately 17% for linear models with logistic loss. Our key insight is that label noise induces a data-dependent noise covariance that aligns favorably with the Hessian geometry near minima. We demonstrate this theoretical improvement through experiments on MNIST and CIFAR-10, showing modest gains in convergence speed (2-3% faster per epoch) and slightly improved generalization performance (0.8% accuracy improvement on average). While our improvement is incremental and limited to specific settings, our analysis reveals a more nuanced understanding of how label structure interacts with gradient noise. The practical benefit remains questionable for modern deep networks where other regularization techniques dominate, but our theoretical refinement may be useful for researchers studying the precise dynamics of noisy gradient methods. Code is available to reproduce our results.",
    "id": 507
  },
  {
    "title": "On the Surprising Effectiveness of Gradient Perturbations for Mitigating Catastrophic Forgetting in Continual Learning",
    "authors": [
      "Chen, L.",
      "Rodriguez, M.",
      "Kim, J."
    ],
    "abstract": "Catastrophic forgetting remains a fundamental challenge for neural networks in continual learning scenarios. While existing methods rely on experience replay or architectural modifications, we investigate a simpler approach: controlled noise injection during gradient updates. Our method, Gradient Perturbation Regularization (GPR), adds carefully calibrated noise to gradients when learning new tasks, effectively creating a \"soft\" weight consolidation effect without storing previous task data. We derive theoretical bounds showing that under certain assumptions about task similarity, GPR can maintain performance on earlier tasks while learning new ones. Experiments on standard benchmarks (Split CIFAR-100, Permuted MNIST) demonstrate competitive performance to rehearsal-based methods, achieving 12% improvement over baseline fine-tuning. However, we observe that GPR's effectiveness decreases significantly when tasks have high semantic similarity. Additionally, our method introduces a new hyperparameter controlling noise magnitude that requires task-specific tuning. While GPR provides a simple alternative to memory-intensive approaches, our results suggest it is best viewed as complementary rather than a complete replacement for existing continual learning strategies. Code will be released upon acceptance.",
    "id": 508
  },
  {
    "title": "Improving Neural Network Calibration through Temperature Scaling with Adaptive Momentum",
    "authors": [
      "Chen, L.",
      "Rodriguez, J.",
      "Kim, S."
    ],
    "abstract": "Well-calibrated probabilistic predictions are crucial for reliable deployment of neural networks in real-world applications. While temperature scaling has emerged as a simple post-hoc calibration method, we show its effectiveness is highly sensitive to the choice of learning rate during the few-shot calibration phase. We propose AdaTemp, an adaptive variant that incorporates momentum-based updates to the temperature parameter during optimization. Our method achieves statistically significant improvements over standard temperature scaling on several image classification benchmarks, with average ECE reductions of 8-12% across CIFAR-10/100 and ImageNet. However, we find these gains diminish when the training distribution differs substantially from the calibration set. Through extensive ablation studies, we demonstrate that the momentum coefficient must be carefully tuned\u2014values outside [0.3, 0.7] often hurt performance. While AdaTemp provides modest but consistent improvements over strong baselines, our theoretical analysis reveals fundamental limitations: the method cannot correct calibration errors when the predicted probabilities are monotonically decreasing across all classes. Our findings suggest that adaptive optimization techniques for calibration require dataset-specific hyperparameter tuning and may not generalize across domains.",
    "id": 509
  }
]